W1114 14:23:17.360000 4160835 site-packages/torch/distributed/run.py:793] 
W1114 14:23:17.360000 4160835 site-packages/torch/distributed/run.py:793] *****************************************
W1114 14:23:17.360000 4160835 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1114 14:23:17.360000 4160835 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to False
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=3072, bias=False)
        (wo): Linear(in_features=2048, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=14336, bias=False)
        (w2): Linear(in_features=7168, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 0.85 seconds
Compiling prefill with dynamic=False
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
Prefill latency: 26.664017318747938 sec
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 35.143593123182654 sec
Compilation time: 61.81 seconds
Compilation time: 61.79 seconds
Prefill latency: 0.8789776591584086 sec
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 3.0042086401954293 sec
Prefill latency: 0.21637049783021212 sec
Decode latency: 3.00486766314134 sec
Prefill latency: 0.21761862188577652 sec
Decode latency: 3.0055337343364954 sec
Prefill latency: 0.21781464712694287 sec
Decode latency: 3.0050132642500103 sec
Prefill latency: 0.21588355721905828 sec
Decode latency: 3.0047606490552425 sec
Time for inference 1: 3.22 sec total, 1270.93 tokens/sec
Decode latency: 3.00 sec
Prefill latency: 0.22 sec
Bandwidth achieved: 10206.26 GB/s
FLOPS achieved: 30.62 TF/s

Prefill latency: 0.21825790591537952 sec
Decode latency: 3.0044832336716354 sec
Time for inference 2: 3.22 sec total, 1270.25 tokens/sec
Decode latency: 3.00 sec
Prefill latency: 0.22 sec
Bandwidth achieved: 10200.74 GB/s
FLOPS achieved: 30.60 TF/s

Prefill latency: 0.21628365525975823 sec
Decode latency: 3.0037605087272823 sec
Time for inference 3: 3.22 sec total, 1271.18 tokens/sec
Decode latency: 3.00 sec
Prefill latency: 0.22 sec
Bandwidth achieved: 10208.26 GB/s
FLOPS achieved: 30.62 TF/s

Prefill latency: 0.21697781002148986 sec
Decode latency: 3.004116787109524 sec
Time for inference 4: 3.22 sec total, 1270.90 tokens/sec
Decode latency: 3.00 sec
Prefill latency: 0.22 sec
Bandwidth achieved: 10206.00 GB/s
FLOPS achieved: 30.62 TF/s

Prefill latency: 0.21634201565757394 sec
Decode latency: 3.0042902906425297 sec
Time for inference 5: 3.22 sec total, 1271.07 tokens/sec
Decode latency: 3.00 sec
Prefill latency: 0.22 sec
Bandwidth achieved: 10207.36 GB/s
FLOPS achieved: 30.62 TF/s

Prefill latency: 0.21677063591778278 sec
Decode latency: 3.002720146905631 sec
Time for inference 6: 3.22 sec total, 1271.49 tokens/sec
Decode latency: 3.00 sec
Prefill latency: 0.22 sec
Bandwidth achieved: 10210.71 GB/s
FLOPS achieved: 30.63 TF/s

Prefill latency: 0.2173822778277099 sec
Decode latency: 3.001853280235082 sec
Time for inference 7: 3.22 sec total, 1271.63 tokens/sec
Decode latency: 3.00 sec
Prefill latency: 0.22 sec
Bandwidth achieved: 10211.89 GB/s
FLOPS achieved: 30.64 TF/s

Prefill latency: 0.21804657066240907 sec
Decode latency: 3.002722898963839 sec
Time for inference 8: 3.22 sec total, 1271.04 tokens/sec
Decode latency: 3.00 sec
Prefill latency: 0.22 sec
Bandwidth achieved: 10207.15 GB/s
FLOPS achieved: 30.62 TF/s

Prefill latency: 0.21716976398602128 sec
Decode latency: 3.0034561823122203 sec
Time for inference 9: 3.22 sec total, 1271.02 tokens/sec
Decode latency: 3.00 sec
Prefill latency: 0.22 sec
Bandwidth achieved: 10206.96 GB/s
FLOPS achieved: 30.62 TF/s

Prefill latency: 0.21753810066729784 sec
Decode latency: 3.000774397049099 sec
Time for inference 10: 3.22 sec total, 1272.02 tokens/sec
Decode latency: 3.00 sec
Prefill latency: 0.22 sec
Bandwidth achieved: 10214.97 GB/s
FLOPS achieved: 30.64 TF/s

==========
Batch Size: 8
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 3.0033 sec
Average prefill latency: 0.2171 sec
Average tokens/sec: 1271.15
Memory used: 15.08 GB
Done. we are killing the process
[rank0]:[W1114 14:25:13.780540249 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
