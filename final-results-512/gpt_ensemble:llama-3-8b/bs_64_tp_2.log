W1114 14:53:42.530000 4188469 site-packages/torch/distributed/run.py:793] 
W1114 14:53:42.530000 4188469 site-packages/torch/distributed/run.py:793] *****************************************
W1114 14:53:42.530000 4188469 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1114 14:53:42.530000 4188469 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to False
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=3072, bias=False)
        (wo): Linear(in_features=2048, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=14336, bias=False)
        (w2): Linear(in_features=7168, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 0.86 seconds
Compiling prefill with dynamic=False
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
Prefill latency: 28.084251010790467 sec
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 39.760879119858146 sec
Compilation time: 67.85 seconds
Compilation time: 67.85 seconds
Prefill latency: 2.616987130139023 sec
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 5.897898531984538 sec
Prefill latency: 1.8351201582700014 sec
Decode latency: 5.895063824020326 sec
Prefill latency: 1.8385694799944758 sec
Decode latency: 5.895954662933946 sec
Prefill latency: 1.8328998717479408 sec
Decode latency: 5.891791956033558 sec
Prefill latency: 1.8306717537343502 sec
Decode latency: 5.891279221046716 sec
Time for inference 1: 7.72 sec total, 4242.41 tokens/sec
Decode latency: 5.89 sec
Prefill latency: 1.83 sec
Bandwidth achieved: 34068.82 GB/s
FLOPS achieved: 102.21 TF/s

Prefill latency: 1.833845121320337 sec
Decode latency: 5.891836534254253 sec
Time for inference 2: 7.73 sec total, 4240.36 tokens/sec
Decode latency: 5.89 sec
Prefill latency: 1.83 sec
Bandwidth achieved: 34052.36 GB/s
FLOPS achieved: 102.16 TF/s

Prefill latency: 1.8357520890422165 sec
Decode latency: 5.890821178909391 sec
Time for inference 3: 7.73 sec total, 4239.92 tokens/sec
Decode latency: 5.89 sec
Prefill latency: 1.84 sec
Bandwidth achieved: 34048.79 GB/s
FLOPS achieved: 102.15 TF/s

Prefill latency: 1.8329665111377835 sec
Decode latency: 5.890955199953169 sec
Time for inference 4: 7.73 sec total, 4241.31 tokens/sec
Decode latency: 5.89 sec
Prefill latency: 1.83 sec
Bandwidth achieved: 34059.97 GB/s
FLOPS achieved: 102.18 TF/s

Prefill latency: 1.8423583772964776 sec
Decode latency: 5.889332885853946 sec
Time for inference 5: 7.73 sec total, 4237.06 tokens/sec
Decode latency: 5.89 sec
Prefill latency: 1.84 sec
Bandwidth achieved: 34025.86 GB/s
FLOPS achieved: 102.08 TF/s

Prefill latency: 1.8343366798944771 sec
Decode latency: 5.8927893387153745 sec
Time for inference 6: 7.73 sec total, 4239.57 tokens/sec
Decode latency: 5.89 sec
Prefill latency: 1.83 sec
Bandwidth achieved: 34045.96 GB/s
FLOPS achieved: 102.14 TF/s

Prefill latency: 1.8354631830006838 sec
Decode latency: 5.890773025806993 sec
Time for inference 7: 7.73 sec total, 4240.09 tokens/sec
Decode latency: 5.89 sec
Prefill latency: 1.84 sec
Bandwidth achieved: 34050.19 GB/s
FLOPS achieved: 102.15 TF/s

Prefill latency: 1.8331934660673141 sec
Decode latency: 5.891165423672646 sec
Time for inference 8: 7.73 sec total, 4241.17 tokens/sec
Decode latency: 5.89 sec
Prefill latency: 1.83 sec
Bandwidth achieved: 34058.86 GB/s
FLOPS achieved: 102.18 TF/s

Prefill latency: 1.837573867291212 sec
Decode latency: 5.89113501412794 sec
Time for inference 9: 7.73 sec total, 4238.73 tokens/sec
Decode latency: 5.89 sec
Prefill latency: 1.84 sec
Bandwidth achieved: 34039.21 GB/s
FLOPS achieved: 102.12 TF/s

Prefill latency: 1.8362909401766956 sec
Decode latency: 5.893070003949106 sec
Time for inference 10: 7.73 sec total, 4238.34 tokens/sec
Decode latency: 5.89 sec
Prefill latency: 1.84 sec
Bandwidth achieved: 34036.10 GB/s
FLOPS achieved: 102.11 TF/s

==========
Batch Size: 64
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 5.8913 sec
Average prefill latency: 1.8352 sec
Average tokens/sec: 4239.90
Memory used: 54.12 GB
Done. we are killing the process
[rank0]:[W1114 14:56:47.244465736 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
