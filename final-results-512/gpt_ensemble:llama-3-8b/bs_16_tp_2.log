W1114 14:32:16.983000 4170094 site-packages/torch/distributed/run.py:793] 
W1114 14:32:16.983000 4170094 site-packages/torch/distributed/run.py:793] *****************************************
W1114 14:32:16.983000 4170094 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1114 14:32:16.983000 4170094 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to False
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=3072, bias=False)
        (wo): Linear(in_features=2048, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=14336, bias=False)
        (w2): Linear(in_features=7168, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 0.86 seconds
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Compiling prefill with dynamic=False
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
Prefill latency: 25.953008859883994 sec
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 36.95378437591717 sec
Compilation time: 62.91 seconds
Compilation time: 62.91 seconds
Prefill latency: 1.103028992190957 sec
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 3.40147019084543 sec
Prefill latency: 0.42673082323744893 sec
Decode latency: 3.400555141735822 sec
Prefill latency: 0.4269594978541136 sec
Decode latency: 3.4019160619936883 sec
Prefill latency: 0.4282246343791485 sec
Decode latency: 3.399557867553085 sec
Prefill latency: 0.42776653496548533 sec
Decode latency: 3.400771793909371 sec
Time for inference 1: 3.83 sec total, 2138.70 tokens/sec
Decode latency: 3.40 sec
Prefill latency: 0.43 sec
Bandwidth achieved: 17174.86 GB/s
FLOPS achieved: 51.52 TF/s

Prefill latency: 0.4286158732138574 sec
Decode latency: 3.3997261812910438 sec
Time for inference 2: 3.83 sec total, 2138.87 tokens/sec
Decode latency: 3.40 sec
Prefill latency: 0.43 sec
Bandwidth achieved: 17176.29 GB/s
FLOPS achieved: 51.53 TF/s

Prefill latency: 0.4286193121224642 sec
Decode latency: 3.400814182125032 sec
Time for inference 3: 3.83 sec total, 2138.22 tokens/sec
Decode latency: 3.40 sec
Prefill latency: 0.43 sec
Bandwidth achieved: 17171.01 GB/s
FLOPS achieved: 51.51 TF/s

Prefill latency: 0.4297129358164966 sec
Decode latency: 3.3996519381180406 sec
Time for inference 4: 3.83 sec total, 2138.30 tokens/sec
Decode latency: 3.40 sec
Prefill latency: 0.43 sec
Bandwidth achieved: 17171.66 GB/s
FLOPS achieved: 51.51 TF/s

Prefill latency: 0.42777777509763837 sec
Decode latency: 3.402087850961834 sec
Time for inference 5: 3.83 sec total, 2138.00 tokens/sec
Decode latency: 3.40 sec
Prefill latency: 0.43 sec
Bandwidth achieved: 17169.30 GB/s
FLOPS achieved: 51.51 TF/s

Prefill latency: 0.4294739398173988 sec
Decode latency: 3.4026053389534354 sec
Time for inference 6: 3.83 sec total, 2136.76 tokens/sec
Decode latency: 3.40 sec
Prefill latency: 0.43 sec
Bandwidth achieved: 17159.27 GB/s
FLOPS achieved: 51.48 TF/s

Prefill latency: 0.42923951614648104 sec
Decode latency: 3.4012141968123615 sec
Time for inference 7: 3.83 sec total, 2137.70 tokens/sec
Decode latency: 3.40 sec
Prefill latency: 0.43 sec
Bandwidth achieved: 17166.89 GB/s
FLOPS achieved: 51.50 TF/s

Prefill latency: 0.4284190530888736 sec
Decode latency: 3.401871579233557 sec
Time for inference 8: 3.83 sec total, 2137.78 tokens/sec
Decode latency: 3.40 sec
Prefill latency: 0.43 sec
Bandwidth achieved: 17167.50 GB/s
FLOPS achieved: 51.50 TF/s

Prefill latency: 0.4296388980001211 sec
Decode latency: 3.401404649950564 sec
Time for inference 9: 3.83 sec total, 2137.38 tokens/sec
Decode latency: 3.40 sec
Prefill latency: 0.43 sec
Bandwidth achieved: 17164.25 GB/s
FLOPS achieved: 51.49 TF/s

Prefill latency: 0.42915461771190166 sec
Decode latency: 3.4023150680586696 sec
Time for inference 10: 3.83 sec total, 2137.03 tokens/sec
Decode latency: 3.40 sec
Prefill latency: 0.43 sec
Bandwidth achieved: 17161.46 GB/s
FLOPS achieved: 51.48 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 3.4012 sec
Average prefill latency: 0.4288 sec
Average tokens/sec: 2137.87
Memory used: 20.73 GB
Done. we are killing the process
[rank0]:[W1114 14:34:22.701948382 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
