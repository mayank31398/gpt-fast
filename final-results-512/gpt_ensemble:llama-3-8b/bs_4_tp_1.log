flash_kv_decode is set to False
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
        (wo): Linear(in_features=4096, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=28672, bias=False)
        (w2): Linear(in_features=14336, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.00 seconds
Compiling prefill with dynamic=False
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
Prefill latency: 32.97512609604746 sec
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 30.07130507240072 sec
Compilation time: 63.05 seconds
Prefill latency: 0.8292763088829815 sec
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 3.9400731050409377 sec
Prefill latency: 0.19689097302034497 sec
Decode latency: 3.940210347995162 sec
Prefill latency: 0.1961709577590227 sec
Decode latency: 3.9398643309250474 sec
Prefill latency: 0.19684947188943624 sec
Decode latency: 3.9397987090051174 sec
Prefill latency: 0.1971389283426106 sec
Decode latency: 3.940154436044395 sec
Time for inference 1: 4.14 sec total, 494.75 tokens/sec
Decode latency: 3.94 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 7426.13 GB/s
FLOPS achieved: 22.28 TF/s

Prefill latency: 0.1970607158727944 sec
Decode latency: 3.9402909190393984 sec
Time for inference 2: 4.14 sec total, 494.75 tokens/sec
Decode latency: 3.94 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 7426.12 GB/s
FLOPS achieved: 22.28 TF/s

Prefill latency: 0.19713639793917537 sec
Decode latency: 3.93988126097247 sec
Time for inference 3: 4.14 sec total, 494.79 tokens/sec
Decode latency: 3.94 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 7426.66 GB/s
FLOPS achieved: 22.28 TF/s

Prefill latency: 0.19692763313651085 sec
Decode latency: 3.9397859191522 sec
Time for inference 4: 4.14 sec total, 494.82 tokens/sec
Decode latency: 3.94 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 7427.12 GB/s
FLOPS achieved: 22.28 TF/s

Prefill latency: 0.19709089724346995 sec
Decode latency: 3.935691423714161 sec
Time for inference 5: 4.13 sec total, 495.29 tokens/sec
Decode latency: 3.94 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 7434.24 GB/s
FLOPS achieved: 22.30 TF/s

Prefill latency: 0.19688194198533893 sec
Decode latency: 3.9352798438631 sec
Time for inference 6: 4.13 sec total, 495.37 tokens/sec
Decode latency: 3.94 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 7435.35 GB/s
FLOPS achieved: 22.31 TF/s

Prefill latency: 0.19654716504737735 sec
Decode latency: 3.936459460295737 sec
Time for inference 7: 4.14 sec total, 495.26 tokens/sec
Decode latency: 3.94 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 7433.80 GB/s
FLOPS achieved: 22.30 TF/s

Prefill latency: 0.19781047198921442 sec
Decode latency: 3.9357666638679802 sec
Time for inference 8: 4.14 sec total, 495.19 tokens/sec
Decode latency: 3.94 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 7432.76 GB/s
FLOPS achieved: 22.30 TF/s

Prefill latency: 0.19665620801970363 sec
Decode latency: 3.9355946015566587 sec
Time for inference 9: 4.13 sec total, 495.34 tokens/sec
Decode latency: 3.94 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 7434.91 GB/s
FLOPS achieved: 22.30 TF/s

Prefill latency: 0.19672408886253834 sec
Decode latency: 3.935766614973545 sec
Time for inference 10: 4.13 sec total, 495.32 tokens/sec
Decode latency: 3.94 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 7434.67 GB/s
FLOPS achieved: 22.30 TF/s

==========
Batch Size: 4
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 3.9375 sec
Average prefill latency: 0.1970 sec
Average tokens/sec: 495.09
Memory used: 20.30 GB
Done. we are killing the process
[rank0]:[W1114 14:14:43.716386119 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
