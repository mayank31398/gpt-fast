W1114 20:10:23.413000 2890139 site-packages/torch/distributed/run.py:793] 
W1114 20:10:23.413000 2890139 site-packages/torch/distributed/run.py:793] *****************************************
W1114 20:10:23.413000 2890139 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1114 20:10:23.413000 2890139 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to False
Using device=cuda
Loading model ...
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
GPTParallel(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x ParallelTransformerBlock(
      semi_compiled = False
      (attention): FuseAttentionMLP(
        (wqkv1): Linear(in_features=8192, out_features=16896, bias=False)
        (wo): Linear(in_features=2048, out_features=8192, bias=False)
        (w2): Linear(in_features=7168, out_features=8192, bias=False)
      )
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 1.49 seconds
Compiling prefill with dynamic=False
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Prefill latency: 97.53771656588651 sec
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 94.97212328901514 sec
Compilation time: 192.51 seconds
Compilation time: 192.96 seconds
Compilation time: 193.06 seconds
Compilation time: 193.01 seconds
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Prefill latency: 3.375853944104165 sec
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 11.358949673129246 sec
Prefill latency: 1.6786197510082275 sec
Decode latency: 11.365606874926016 sec
Prefill latency: 1.6704040009062737 sec
Decode latency: 11.363932117819786 sec
Prefill latency: 1.676746352110058 sec
Decode latency: 11.363286227919161 sec
Prefill latency: 1.6711253528483212 sec
Decode latency: 11.361348537961021 sec
Time for inference 1: 13.04 sec total, 628.46 tokens/sec
Decode latency: 11.36 sec
Prefill latency: 1.67 sec
Bandwidth achieved: 22830.66 GB/s
FLOPS achieved: 68.49 TF/s

Prefill latency: 1.6788486638106406 sec
Decode latency: 11.361526407999918 sec
Time for inference 2: 13.04 sec total, 628.01 tokens/sec
Decode latency: 11.36 sec
Prefill latency: 1.68 sec
Bandwidth achieved: 22814.36 GB/s
FLOPS achieved: 68.44 TF/s

Prefill latency: 1.6735541999805719 sec
Decode latency: 11.361966639989987 sec
Time for inference 3: 13.04 sec total, 628.32 tokens/sec
Decode latency: 11.36 sec
Prefill latency: 1.67 sec
Bandwidth achieved: 22825.77 GB/s
FLOPS achieved: 68.48 TF/s

Prefill latency: 1.6756388079375029 sec
Decode latency: 11.363701577065513 sec
Time for inference 4: 13.04 sec total, 628.08 tokens/sec
Decode latency: 11.36 sec
Prefill latency: 1.68 sec
Bandwidth achieved: 22817.15 GB/s
FLOPS achieved: 68.45 TF/s

Prefill latency: 1.6682291640900075 sec
Decode latency: 11.369484046008438 sec
Time for inference 5: 13.04 sec total, 628.21 tokens/sec
Decode latency: 11.37 sec
Prefill latency: 1.67 sec
Bandwidth achieved: 22821.82 GB/s
FLOPS achieved: 68.47 TF/s

Prefill latency: 1.6500098300166428 sec
Decode latency: 11.36577706807293 sec
Time for inference 6: 13.02 sec total, 629.23 tokens/sec
Decode latency: 11.37 sec
Prefill latency: 1.65 sec
Bandwidth achieved: 22858.80 GB/s
FLOPS achieved: 68.58 TF/s

Prefill latency: 1.6716130368877202 sec
Decode latency: 11.364420693134889 sec
Time for inference 7: 13.04 sec total, 628.30 tokens/sec
Decode latency: 11.36 sec
Prefill latency: 1.67 sec
Bandwidth achieved: 22824.87 GB/s
FLOPS achieved: 68.47 TF/s

Prefill latency: 1.6750184199772775 sec
Decode latency: 11.36824610712938 sec
Time for inference 8: 13.05 sec total, 627.93 tokens/sec
Decode latency: 11.37 sec
Prefill latency: 1.68 sec
Bandwidth achieved: 22811.40 GB/s
FLOPS achieved: 68.43 TF/s

Prefill latency: 1.6878460380248725 sec
Decode latency: 11.364328263094649 sec
Time for inference 9: 13.06 sec total, 627.50 tokens/sec
Decode latency: 11.36 sec
Prefill latency: 1.69 sec
Bandwidth achieved: 22795.90 GB/s
FLOPS achieved: 68.39 TF/s

Prefill latency: 1.6728428560309112 sec
Decode latency: 11.363933117128909 sec
Time for inference 10: 13.04 sec total, 628.26 tokens/sec
Decode latency: 11.36 sec
Prefill latency: 1.67 sec
Bandwidth achieved: 22823.37 GB/s
FLOPS achieved: 68.47 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 11.3645 sec
Average prefill latency: 1.6725 sec
Average tokens/sec: 628.23
Memory used: 51.66 GB
Done. we are killing the process
[rank0]:[W1114 20:16:52.633422400 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
