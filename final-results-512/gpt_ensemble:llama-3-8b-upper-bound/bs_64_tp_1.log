flash_kv_decode is set to False
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
        (wo): Linear(in_features=4096, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=28672, bias=False)
        (w2): Linear(in_features=14336, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.00 seconds
Compiling prefill with dynamic=False
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
Prefill latency: 33.71491409372538 sec
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 36.21460543014109 sec
Compilation time: 69.93 seconds
Prefill latency: 4.058499914128333 sec
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 9.823705736082047 sec
Prefill latency: 3.391603282187134 sec
Decode latency: 9.820699003059417 sec
Prefill latency: 3.38659753696993 sec
Decode latency: 9.819585018791258 sec
Prefill latency: 3.382319108117372 sec
Decode latency: 9.820652690716088 sec
Prefill latency: 3.3848432009108365 sec
Decode latency: 9.820755294058472 sec
Time for inference 1: 13.21 sec total, 2480.94 tokens/sec
Decode latency: 9.82 sec
Prefill latency: 3.38 sec
Bandwidth achieved: 37238.48 GB/s
FLOPS achieved: 111.72 TF/s

Prefill latency: 3.3751341179013252 sec
Decode latency: 9.823650205042213 sec
Time for inference 2: 13.20 sec total, 2482.22 tokens/sec
Decode latency: 9.82 sec
Prefill latency: 3.38 sec
Bandwidth achieved: 37257.68 GB/s
FLOPS achieved: 111.77 TF/s

Prefill latency: 3.376114540733397 sec
Decode latency: 9.819784785620868 sec
Time for inference 3: 13.20 sec total, 2482.77 tokens/sec
Decode latency: 9.82 sec
Prefill latency: 3.38 sec
Bandwidth achieved: 37265.95 GB/s
FLOPS achieved: 111.80 TF/s

Prefill latency: 3.387985568959266 sec
Decode latency: 9.820426648948342 sec
Time for inference 4: 13.21 sec total, 2480.40 tokens/sec
Decode latency: 9.82 sec
Prefill latency: 3.39 sec
Bandwidth achieved: 37230.37 GB/s
FLOPS achieved: 111.69 TF/s

Prefill latency: 3.398512869141996 sec
Decode latency: 9.82000040076673 sec
Time for inference 5: 13.22 sec total, 2478.53 tokens/sec
Decode latency: 9.82 sec
Prefill latency: 3.40 sec
Bandwidth achieved: 37202.30 GB/s
FLOPS achieved: 111.61 TF/s

Prefill latency: 3.401067950297147 sec
Decode latency: 9.820899887941778 sec
Time for inference 6: 13.22 sec total, 2477.86 tokens/sec
Decode latency: 9.82 sec
Prefill latency: 3.40 sec
Bandwidth achieved: 37192.30 GB/s
FLOPS achieved: 111.58 TF/s

Prefill latency: 3.396115496288985 sec
Decode latency: 9.822096828836948 sec
Time for inference 7: 13.22 sec total, 2478.58 tokens/sec
Decode latency: 9.82 sec
Prefill latency: 3.40 sec
Bandwidth achieved: 37203.16 GB/s
FLOPS achieved: 111.61 TF/s

Prefill latency: 3.3844336005859077 sec
Decode latency: 9.821016216650605 sec
Time for inference 8: 13.21 sec total, 2480.95 tokens/sec
Decode latency: 9.82 sec
Prefill latency: 3.38 sec
Bandwidth achieved: 37238.64 GB/s
FLOPS achieved: 111.72 TF/s

Prefill latency: 3.3902329709380865 sec
Decode latency: 9.820385383907706 sec
Time for inference 9: 13.21 sec total, 2480.02 tokens/sec
Decode latency: 9.82 sec
Prefill latency: 3.39 sec
Bandwidth achieved: 37224.67 GB/s
FLOPS achieved: 111.67 TF/s

Prefill latency: 3.374868029728532 sec
Decode latency: 9.820898754987866 sec
Time for inference 10: 13.20 sec total, 2482.67 tokens/sec
Decode latency: 9.82 sec
Prefill latency: 3.37 sec
Bandwidth achieved: 37264.47 GB/s
FLOPS achieved: 111.79 TF/s

==========
Batch Size: 64
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 9.8210 sec
Average prefill latency: 3.3869 sec
Average tokens/sec: 2480.49
Memory used: 79.89 GB
[rank0]:[W1114 12:48:07.777841611 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Done. we are killing the process
[rank0]:[W1114 12:48:09.801472206 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
