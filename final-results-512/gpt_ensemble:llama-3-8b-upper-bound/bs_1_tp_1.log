flash_kv_decode is set to False
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
        (wo): Linear(in_features=4096, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=28672, bias=False)
        (w2): Linear(in_features=14336, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.00 seconds
Compiling prefill with dynamic=False
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
Prefill latency: 29.676045960746706 sec
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 41.000860466156155 sec
Compilation time: 70.68 seconds
Prefill latency: 0.6835595588199794 sec
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 3.1870416370220482 sec
Prefill latency: 0.05105581786483526 sec
Decode latency: 3.1881863712333143 sec
Prefill latency: 0.0511916009709239 sec
Decode latency: 3.1876099882647395 sec
Prefill latency: 0.050713310949504375 sec
Decode latency: 3.1880149482749403 sec
Prefill latency: 0.0509504359215498 sec
Decode latency: 3.188956256955862 sec
Time for inference 1: 3.24 sec total, 157.91 tokens/sec
Decode latency: 3.19 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 2370.27 GB/s
FLOPS achieved: 7.11 TF/s

Prefill latency: 0.05071663111448288 sec
Decode latency: 3.1876849508844316 sec
Time for inference 2: 3.24 sec total, 157.99 tokens/sec
Decode latency: 3.19 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 2371.48 GB/s
FLOPS achieved: 7.11 TF/s

Prefill latency: 0.05115463025867939 sec
Decode latency: 3.188072968740016 sec
Time for inference 3: 3.24 sec total, 157.94 tokens/sec
Decode latency: 3.19 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 2370.60 GB/s
FLOPS achieved: 7.11 TF/s

Prefill latency: 0.05061643896624446 sec
Decode latency: 3.187670940067619 sec
Time for inference 4: 3.24 sec total, 158.00 tokens/sec
Decode latency: 3.19 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 2371.58 GB/s
FLOPS achieved: 7.11 TF/s

Prefill latency: 0.0507544819265604 sec
Decode latency: 3.188112508971244 sec
Time for inference 5: 3.24 sec total, 157.98 tokens/sec
Decode latency: 3.19 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 2371.21 GB/s
FLOPS achieved: 7.11 TF/s

Prefill latency: 0.05104784807190299 sec
Decode latency: 3.1885393112897873 sec
Time for inference 6: 3.24 sec total, 157.92 tokens/sec
Decode latency: 3.19 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 2370.37 GB/s
FLOPS achieved: 7.11 TF/s

Prefill latency: 0.050697862170636654 sec
Decode latency: 3.188537190668285 sec
Time for inference 7: 3.24 sec total, 157.96 tokens/sec
Decode latency: 3.19 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 2370.92 GB/s
FLOPS achieved: 7.11 TF/s

Prefill latency: 0.050746703054755926 sec
Decode latency: 3.188015529885888 sec
Time for inference 8: 3.24 sec total, 157.98 tokens/sec
Decode latency: 3.19 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 2371.25 GB/s
FLOPS achieved: 7.11 TF/s

Prefill latency: 0.05063679115846753 sec
Decode latency: 3.1881518228910863 sec
Time for inference 9: 3.24 sec total, 157.98 tokens/sec
Decode latency: 3.19 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 2371.25 GB/s
FLOPS achieved: 7.11 TF/s

Prefill latency: 0.05134199466556311 sec
Decode latency: 3.1878358661197126 sec
Time for inference 10: 3.24 sec total, 157.95 tokens/sec
Decode latency: 3.19 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 2370.77 GB/s
FLOPS achieved: 7.11 TF/s

==========
Batch Size: 1
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 3.1882 sec
Average prefill latency: 0.0509 sec
Average tokens/sec: 157.96
Memory used: 17.53 GB
[rank0]:[W1114 12:03:34.567392346 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Done. we are killing the process
[rank0]:[W1114 12:03:36.605945548 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
