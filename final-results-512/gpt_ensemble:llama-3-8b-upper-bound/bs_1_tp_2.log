W1114 12:03:39.935000 3937747 site-packages/torch/distributed/run.py:793] 
W1114 12:03:39.935000 3937747 site-packages/torch/distributed/run.py:793] *****************************************
W1114 12:03:39.935000 3937747 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1114 12:03:39.935000 3937747 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to False
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=3072, bias=False)
        (wo): Linear(in_features=2048, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=14336, bias=False)
        (w2): Linear(in_features=7168, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 0.86 seconds
Compiling prefill with dynamic=False
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
Prefill latency: 20.318116168957204 sec
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 31.203957768157125 sec
Compilation time: 51.52 seconds
Prefill latency: 0.6564086587168276 sec
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 1.9278047429397702 sec
Prefill latency: 0.027964164037257433 sec
Decode latency: 1.9276960003189743 sec
Prefill latency: 0.02799970516934991 sec
Decode latency: 1.9278943249955773 sec
Prefill latency: 0.027989765163511038 sec
Decode latency: 1.9284446062520146 sec
Prefill latency: 0.028097947128117085 sec
Decode latency: 1.9291144306771457 sec
Time for inference 1: 1.96 sec total, 261.34 tokens/sec
Decode latency: 1.93 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 2098.73 GB/s
FLOPS achieved: 6.30 TF/s

Prefill latency: 0.027979085221886635 sec
Decode latency: 1.9277104008942842 sec
Time for inference 2: 1.96 sec total, 261.55 tokens/sec
Decode latency: 1.93 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 2100.41 GB/s
FLOPS achieved: 6.30 TF/s

Prefill latency: 0.027995944023132324 sec
Decode latency: 1.9284509858116508 sec
Time for inference 3: 1.96 sec total, 261.45 tokens/sec
Decode latency: 1.93 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 2099.56 GB/s
FLOPS achieved: 6.30 TF/s

Prefill latency: 0.027988614980131388 sec
Decode latency: 1.927798192948103 sec
Time for inference 4: 1.96 sec total, 261.55 tokens/sec
Decode latency: 1.93 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 2100.35 GB/s
FLOPS achieved: 6.30 TF/s

Prefill latency: 0.02792381402105093 sec
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 1.928218790795654 sec
Time for inference 5: 1.96 sec total, 261.50 tokens/sec
Decode latency: 1.93 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 2100.00 GB/s
FLOPS achieved: 6.30 TF/s

Prefill latency: 0.027969553135335445 sec
Compilation time: 71.09 seconds
Decode latency: 1.9275802080519497 sec
Time for inference 6: 1.96 sec total, 261.57 tokens/sec
Decode latency: 1.93 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 2100.56 GB/s
FLOPS achieved: 6.30 TF/s

/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Prefill latency: 0.028463273774832487 sec
Decode latency: 1.9280695994384587 sec
Time for inference 7: 1.96 sec total, 261.45 tokens/sec
Decode latency: 1.93 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 2099.54 GB/s
FLOPS achieved: 6.30 TF/s

Prefill latency: 0.02797069400548935 sec
Decode latency: 1.928220931906253 sec
Time for inference 8: 1.96 sec total, 261.50 tokens/sec
Decode latency: 1.93 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 2099.96 GB/s
FLOPS achieved: 6.30 TF/s

Prefill latency: 0.027914983220398426 sec
Decode latency: 1.9281156989745796 sec
Time for inference 9: 1.96 sec total, 261.41 tokens/sec
Decode latency: 1.93 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 2099.23 GB/s
FLOPS achieved: 6.30 TF/s

Prefill latency: 0.028026346117258072 sec
Decode latency: 1.9281542897224426 sec
Time for inference 10: 1.96 sec total, 261.50 tokens/sec
Decode latency: 1.93 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 2100.01 GB/s
FLOPS achieved: 6.30 TF/s

==========
Batch Size: 1
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 1.9281 sec
Average prefill latency: 0.0280 sec
Average tokens/sec: 261.48
Memory used: 9.89 GB
[rank0]:[W1114 12:05:06.505076042 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank1]:[W1114 12:05:25.979916041 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Done. we are killing the process
[rank0]:[W1114 12:05:28.670665868 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
