flash_kv_decode is set to False
Using device=cuda
Loading model ...
GPTParallel(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x ParallelTransformerBlock(
      semi_compiled = False
      (attention): FuseAttentionMLP(
        (wqkv1): Linear(in_features=4096, out_features=34816, bias=False)
        (wo): Linear(in_features=4096, out_features=4096, bias=False)
        (w2): Linear(in_features=14336, out_features=4096, bias=False)
      )
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 0.97 seconds
Compiling prefill with dynamic=False
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
Prefill latency: 45.891228302847594 sec
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 35.41008074581623 sec
Compilation time: 81.30 seconds
Prefill latency: 0.6657190881669521 sec
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 3.1015459951013327 sec
Prefill latency: 0.04999543074518442 sec
Decode latency: 3.101854581851512 sec
Prefill latency: 0.050824027974158525 sec
Decode latency: 3.1025266759097576 sec
Prefill latency: 0.05016861483454704 sec
Decode latency: 3.1030578669160604 sec
Prefill latency: 0.050216926261782646 sec
Decode latency: 3.1025489163585007 sec
Time for inference 1: 3.15 sec total, 162.29 tokens/sec
Decode latency: 3.10 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 2435.91 GB/s
FLOPS achieved: 7.31 TF/s

Prefill latency: 0.05025292560458183 sec
Decode latency: 3.102564075961709 sec
Time for inference 2: 3.15 sec total, 162.29 tokens/sec
Decode latency: 3.10 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 2435.89 GB/s
FLOPS achieved: 7.31 TF/s

Prefill latency: 0.05019012512639165 sec
Decode latency: 3.1028770431876183 sec
Time for inference 3: 3.16 sec total, 162.27 tokens/sec
Decode latency: 3.10 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 2435.65 GB/s
FLOPS achieved: 7.31 TF/s

Prefill latency: 0.050304727628827095 sec
Decode latency: 3.10304695693776 sec
Time for inference 4: 3.16 sec total, 162.26 tokens/sec
Decode latency: 3.10 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 2435.45 GB/s
FLOPS achieved: 7.31 TF/s

Prefill latency: 0.05015647364780307 sec
Decode latency: 3.1025697966106236 sec
Time for inference 5: 3.15 sec total, 162.29 tokens/sec
Decode latency: 3.10 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 2435.90 GB/s
FLOPS achieved: 7.31 TF/s

Prefill latency: 0.050197564996778965 sec
Decode latency: 3.1028273422271013 sec
Time for inference 6: 3.16 sec total, 162.28 tokens/sec
Decode latency: 3.10 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 2435.73 GB/s
FLOPS achieved: 7.31 TF/s

Prefill latency: 0.050431979820132256 sec
Decode latency: 3.102594417054206 sec
Time for inference 7: 3.16 sec total, 162.27 tokens/sec
Decode latency: 3.10 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 2435.67 GB/s
FLOPS achieved: 7.31 TF/s

Prefill latency: 0.05025203712284565 sec
Decode latency: 3.1025732061825693 sec
Time for inference 8: 3.15 sec total, 162.29 tokens/sec
Decode latency: 3.10 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 2435.85 GB/s
FLOPS achieved: 7.31 TF/s

Prefill latency: 0.05020264536142349 sec
Decode latency: 3.102425443008542 sec
Time for inference 9: 3.15 sec total, 162.29 tokens/sec
Decode latency: 3.10 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 2435.97 GB/s
FLOPS achieved: 7.31 TF/s

Prefill latency: 0.05086075933650136 sec
Decode latency: 3.102149427868426 sec
Time for inference 10: 3.16 sec total, 162.28 tokens/sec
Decode latency: 3.10 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 2435.69 GB/s
FLOPS achieved: 7.31 TF/s

==========
Batch Size: 1
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 3.1026 sec
Average prefill latency: 0.0503 sec
Average tokens/sec: 162.28
Memory used: 17.57 GB
Done. we are killing the process
[rank0]:[W1114 12:58:14.386448179 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
