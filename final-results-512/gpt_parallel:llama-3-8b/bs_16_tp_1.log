flash_kv_decode is set to False
Using device=cuda
Loading model ...
GPTParallel(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x ParallelTransformerBlock(
      semi_compiled = False
      (attention): FuseAttentionMLP(
        (wqkv1): Linear(in_features=4096, out_features=34816, bias=False)
        (wo): Linear(in_features=4096, out_features=4096, bias=False)
        (w2): Linear(in_features=14336, out_features=4096, bias=False)
      )
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 0.98 seconds
Compiling prefill with dynamic=False
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
Prefill latency: 50.13388933381066 sec
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 36.68512846529484 sec
Compilation time: 86.82 seconds
Prefill latency: 1.3744465559720993 sec
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 5.226972023025155 sec
Prefill latency: 0.7799394810572267 sec
Decode latency: 5.230968786869198 sec
Prefill latency: 0.7794960308820009 sec
Decode latency: 5.228315870743245 sec
Prefill latency: 0.7780936728231609 sec
Decode latency: 5.229215010069311 sec
Prefill latency: 0.7806069240905344 sec
Decode latency: 5.231399606913328 sec
Time for inference 1: 6.02 sec total, 1361.84 tokens/sec
Decode latency: 5.23 sec
Prefill latency: 0.78 sec
Bandwidth achieved: 20440.62 GB/s
FLOPS achieved: 61.32 TF/s

Prefill latency: 0.7796651041135192 sec
Decode latency: 5.228628238197416 sec
Time for inference 2: 6.01 sec total, 1362.90 tokens/sec
Decode latency: 5.23 sec
Prefill latency: 0.78 sec
Bandwidth achieved: 20456.55 GB/s
FLOPS achieved: 61.37 TF/s

Prefill latency: 0.7807682678103447 sec
Decode latency: 5.230573767796159 sec
Time for inference 3: 6.01 sec total, 1362.18 tokens/sec
Decode latency: 5.23 sec
Prefill latency: 0.78 sec
Bandwidth achieved: 20445.81 GB/s
FLOPS achieved: 61.34 TF/s

Prefill latency: 0.781160606071353 sec
Decode latency: 5.230526318307966 sec
Time for inference 4: 6.02 sec total, 1361.89 tokens/sec
Decode latency: 5.23 sec
Prefill latency: 0.78 sec
Bandwidth achieved: 20441.41 GB/s
FLOPS achieved: 61.32 TF/s

Prefill latency: 0.778570391703397 sec
Decode latency: 5.2277922802604735 sec
Time for inference 5: 6.01 sec total, 1363.35 tokens/sec
Decode latency: 5.23 sec
Prefill latency: 0.78 sec
Bandwidth achieved: 20463.38 GB/s
FLOPS achieved: 61.39 TF/s

Prefill latency: 0.781368630938232 sec
Decode latency: 5.229912625160068 sec
Time for inference 6: 6.01 sec total, 1361.97 tokens/sec
Decode latency: 5.23 sec
Prefill latency: 0.78 sec
Bandwidth achieved: 20442.63 GB/s
FLOPS achieved: 61.33 TF/s

Prefill latency: 0.7815244630910456 sec
Decode latency: 5.229724060744047 sec
Time for inference 7: 6.01 sec total, 1362.25 tokens/sec
Decode latency: 5.23 sec
Prefill latency: 0.78 sec
Bandwidth achieved: 20446.87 GB/s
FLOPS achieved: 61.34 TF/s

Prefill latency: 0.7768465960398316 sec
Decode latency: 5.227932093665004 sec
Time for inference 8: 6.01 sec total, 1363.34 tokens/sec
Decode latency: 5.23 sec
Prefill latency: 0.78 sec
Bandwidth achieved: 20463.23 GB/s
FLOPS achieved: 61.39 TF/s

Prefill latency: 0.7805484728887677 sec
Decode latency: 5.230547161772847 sec
Time for inference 9: 6.01 sec total, 1362.27 tokens/sec
Decode latency: 5.23 sec
Prefill latency: 0.78 sec
Bandwidth achieved: 20447.14 GB/s
FLOPS achieved: 61.34 TF/s

Prefill latency: 0.7792707760818303 sec
Decode latency: 5.22925909422338 sec
Time for inference 10: 6.01 sec total, 1362.87 tokens/sec
Decode latency: 5.23 sec
Prefill latency: 0.78 sec
Bandwidth achieved: 20456.13 GB/s
FLOPS achieved: 61.37 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 5.2296 sec
Average prefill latency: 0.7800 sec
Average tokens/sec: 1362.49
Memory used: 33.24 GB
Done. we are killing the process
[rank0]:[W1114 13:28:41.308454797 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
