flash_kv_decode is set to False
Using device=cuda
Loading model ...
GPTLadder(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x LadderTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
        (wo): Linear(in_features=4096, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=28672, bias=False)
        (w2): Linear(in_features=14336, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 0.23 seconds
Compiling prefill with dynamic=False
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
Prefill latency: 43.917760014068335 sec
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 42.60485712997615 sec
Compilation time: 86.53 seconds
Prefill latency: 1.025567365810275 sec
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 4.382754111662507 sec
Prefill latency: 0.3902063798159361 sec
Decode latency: 4.382331832777709 sec
Prefill latency: 0.39124892093241215 sec
Decode latency: 4.38076816033572 sec
Prefill latency: 0.3907633717171848 sec
Decode latency: 4.375897248275578 sec
Prefill latency: 0.39173520216718316 sec
Decode latency: 4.37720958609134 sec
Time for inference 1: 4.77 sec total, 858.49 tokens/sec
Decode latency: 4.38 sec
Prefill latency: 0.39 sec
Bandwidth achieved: 12885.79 GB/s
FLOPS achieved: 38.66 TF/s

Prefill latency: 0.3908061729744077 sec
Decode latency: 4.377996262162924 sec
Time for inference 2: 4.77 sec total, 858.51 tokens/sec
Decode latency: 4.38 sec
Prefill latency: 0.39 sec
Bandwidth achieved: 12886.16 GB/s
FLOPS achieved: 38.66 TF/s

Prefill latency: 0.3916404200717807 sec
Decode latency: 4.3811710979789495 sec
Time for inference 3: 4.78 sec total, 857.72 tokens/sec
Decode latency: 4.38 sec
Prefill latency: 0.39 sec
Bandwidth achieved: 12874.21 GB/s
FLOPS achieved: 38.62 TF/s

Prefill latency: 0.3918863059952855 sec
Decode latency: 4.379446491599083 sec
Time for inference 4: 4.77 sec total, 858.05 tokens/sec
Decode latency: 4.38 sec
Prefill latency: 0.39 sec
Bandwidth achieved: 12879.13 GB/s
FLOPS achieved: 38.64 TF/s

Prefill latency: 0.3912127707153559 sec
Decode latency: 4.377703876234591 sec
Time for inference 5: 4.77 sec total, 858.49 tokens/sec
Decode latency: 4.38 sec
Prefill latency: 0.39 sec
Bandwidth achieved: 12885.84 GB/s
FLOPS achieved: 38.66 TF/s

Prefill latency: 0.3913147528655827 sec
Decode latency: 4.378636606037617 sec
Time for inference 6: 4.77 sec total, 857.98 tokens/sec
Decode latency: 4.38 sec
Prefill latency: 0.39 sec
Bandwidth achieved: 12878.11 GB/s
FLOPS achieved: 38.63 TF/s

Prefill latency: 0.39166874112561345 sec
Decode latency: 4.379999654367566 sec
Time for inference 7: 4.77 sec total, 857.95 tokens/sec
Decode latency: 4.38 sec
Prefill latency: 0.39 sec
Bandwidth achieved: 12877.73 GB/s
FLOPS achieved: 38.63 TF/s

Prefill latency: 0.3911356292665005 sec
Decode latency: 4.377189734950662 sec
Time for inference 8: 4.77 sec total, 858.59 tokens/sec
Decode latency: 4.38 sec
Prefill latency: 0.39 sec
Bandwidth achieved: 12887.35 GB/s
FLOPS achieved: 38.66 TF/s

Prefill latency: 0.3918755347840488 sec
Decode latency: 4.377857565879822 sec
Time for inference 9: 4.77 sec total, 858.35 tokens/sec
Decode latency: 4.38 sec
Prefill latency: 0.39 sec
Bandwidth achieved: 12883.76 GB/s
FLOPS achieved: 38.65 TF/s

Prefill latency: 0.39251272892579436 sec
Decode latency: 4.378004929050803 sec
Time for inference 10: 4.77 sec total, 857.88 tokens/sec
Decode latency: 4.38 sec
Prefill latency: 0.39 sec
Bandwidth achieved: 12876.61 GB/s
FLOPS achieved: 38.63 TF/s

==========
Batch Size: 8
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 4.3785 sec
Average prefill latency: 0.3916 sec
Average tokens/sec: 858.20
Memory used: 24.63 GB
Done. we are killing the process
[rank0]:[W1114 10:00:29.820169887 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
