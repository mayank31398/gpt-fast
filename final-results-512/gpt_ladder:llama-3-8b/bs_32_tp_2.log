W1114 10:22:19.726000 3739268 site-packages/torch/distributed/run.py:793] 
W1114 10:22:19.726000 3739268 site-packages/torch/distributed/run.py:793] *****************************************
W1114 10:22:19.726000 3739268 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1114 10:22:19.726000 3739268 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to False
Using device=cuda
Loading model ...
GPTLadder(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x LadderTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=3072, bias=False)
        (wo): Linear(in_features=2048, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=14336, bias=False)
        (w2): Linear(in_features=7168, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 0.06 seconds
Compiling prefill with dynamic=False
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
Prefill latency: 42.13034173287451 sec
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 51.47250877413899 sec
Compilation time: 93.61 seconds
Compilation time: 93.61 seconds
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Prefill latency: 1.6613510139286518 sec
/dccstor/mayankmish2/miniconda3/envs/ai/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 4.194451468065381 sec
Prefill latency: 0.9034638032317162 sec
Decode latency: 4.192972954828292 sec
Prefill latency: 0.9042830942198634 sec
Decode latency: 4.193922069855034 sec
Prefill latency: 0.9095942047424614 sec
Decode latency: 4.191144274082035 sec
Prefill latency: 0.9073394369333982 sec
Decode latency: 4.195689219981432 sec
Time for inference 1: 5.11 sec total, 3209.27 tokens/sec
Decode latency: 4.20 sec
Prefill latency: 0.91 sec
Bandwidth achieved: 25772.11 GB/s
FLOPS achieved: 77.32 TF/s

Prefill latency: 0.9043184737674892 sec
Decode latency: 4.1924927812069654 sec
Time for inference 2: 5.10 sec total, 3213.24 tokens/sec
Decode latency: 4.19 sec
Prefill latency: 0.90 sec
Bandwidth achieved: 25804.03 GB/s
FLOPS achieved: 77.41 TF/s

Prefill latency: 0.9052942451089621 sec
Decode latency: 4.1922785472124815 sec
Time for inference 3: 5.10 sec total, 3212.82 tokens/sec
Decode latency: 4.19 sec
Prefill latency: 0.91 sec
Bandwidth achieved: 25800.65 GB/s
FLOPS achieved: 77.40 TF/s

Prefill latency: 0.9076043129898608 sec
Decode latency: 4.191954270936549 sec
Time for inference 4: 5.10 sec total, 3211.50 tokens/sec
Decode latency: 4.19 sec
Prefill latency: 0.91 sec
Bandwidth achieved: 25790.01 GB/s
FLOPS achieved: 77.37 TF/s

Prefill latency: 0.9081815956160426 sec
Decode latency: 4.190983348060399 sec
Time for inference 5: 5.10 sec total, 3211.82 tokens/sec
Decode latency: 4.19 sec
Prefill latency: 0.91 sec
Bandwidth achieved: 25792.59 GB/s
FLOPS achieved: 77.38 TF/s

Prefill latency: 0.9068152271211147 sec
Decode latency: 4.190867635887116 sec
Time for inference 6: 5.10 sec total, 3212.70 tokens/sec
Decode latency: 4.19 sec
Prefill latency: 0.91 sec
Bandwidth achieved: 25799.66 GB/s
FLOPS achieved: 77.40 TF/s

Prefill latency: 0.9079657401889563 sec
Decode latency: 4.191338475327939 sec
Time for inference 7: 5.10 sec total, 3211.73 tokens/sec
Decode latency: 4.19 sec
Prefill latency: 0.91 sec
Bandwidth achieved: 25791.92 GB/s
FLOPS achieved: 77.38 TF/s

Prefill latency: 0.9085570629686117 sec
Decode latency: 4.194374748039991 sec
Time for inference 8: 5.11 sec total, 3209.35 tokens/sec
Decode latency: 4.19 sec
Prefill latency: 0.91 sec
Bandwidth achieved: 25772.73 GB/s
FLOPS achieved: 77.32 TF/s

Prefill latency: 0.9082643557339907 sec
Decode latency: 4.19404879398644 sec
Time for inference 9: 5.10 sec total, 3209.59 tokens/sec
Decode latency: 4.19 sec
Prefill latency: 0.91 sec
Bandwidth achieved: 25774.71 GB/s
FLOPS achieved: 77.32 TF/s

Prefill latency: 0.9094199808314443 sec
Decode latency: 4.19129992602393 sec
Time for inference 10: 5.10 sec total, 3210.85 tokens/sec
Decode latency: 4.19 sec
Prefill latency: 0.91 sec
Bandwidth achieved: 25784.85 GB/s
FLOPS achieved: 77.35 TF/s

==========
Batch Size: 32
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 4.1925 sec
Average prefill latency: 0.9074 sec
Average tokens/sec: 3211.29
Memory used: 33.31 GB
Done. we are killing the process
[rank0]:[W1114 10:25:13.545347614 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
