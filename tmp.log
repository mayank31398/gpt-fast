W0928 16:59:30.515000 22800408598336 torch/distributed/run.py:779] 
W0928 16:59:30.515000 22800408598336 torch/distributed/run.py:779] *****************************************
W0928 16:59:30.515000 22800408598336 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0928 16:59:30.515000 22800408598336 torch/distributed/run.py:779] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTParallel(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x ParallelTransformerBlock(
      semi_compiled = False
      (attention): FuseAttentionMLP(
        (wqkv1): Linear(in_features=4096, out_features=8704, bias=False)
        (wo): Linear(in_features=1024, out_features=4096, bias=False)
        (w2): Linear(in_features=3584, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=4096, bias=False)
      )
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 0.88 seconds
Prefill latency: 0.04853289108723402 sec
Decode latency: 0.4029837856069207 sec
Compilation time: 0.46 seconds
Compilation time: 0.42 seconds
Compilation time: 0.45 seconds
Compilation time: 0.42 seconds
Prefill latency: 0.015370294451713562 sec
Decode latency: 0.4024208318442106 sec
Prefill latency: 0.01534043438732624 sec
Decode latency: 0.4025927381590009 sec
Prefill latency: 0.015277664177119732 sec
Decode latency: 0.40245071705430746 sec
Prefill latency: 0.015369103290140629 sec
Decode latency: 0.40263633988797665 sec
Prefill latency: 0.015370551496744156 sec
Decode latency: 0.4026617603376508 sec
Time for inference 1: 0.42 sec total, 1223.33 tokens/sec
Decode latency: 0.40 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 9002.68 GB/s
FLOPS achieved: 27.01 TF/s

Prefill latency: 0.015376593917608261 sec
Decode latency: 0.4026887873187661 sec
Time for inference 2: 0.42 sec total, 1223.20 tokens/sec
Decode latency: 0.40 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 9001.72 GB/s
FLOPS achieved: 27.01 TF/s

==========
Batch Size: 8
Prompt Length: 128
Generated tokens: 64
Average decode latency: 0.4027 sec
Average prefill latency: 0.0154 sec
Average tokens/sec: 1223.26
Memory used: 9.34 GB
Done. we are killing the process
