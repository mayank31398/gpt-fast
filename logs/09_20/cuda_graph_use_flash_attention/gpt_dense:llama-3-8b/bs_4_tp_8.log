W0920 07:13:04.900000 22533094086464 torch/distributed/run.py:779] 
W0920 07:13:04.900000 22533094086464 torch/distributed/run.py:779] *****************************************
W0920 07:13:04.900000 22533094086464 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0920 07:13:04.900000 22533094086464 torch/distributed/run.py:779] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTDense(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x DenseTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=768, bias=False)
        (wo): Linear(in_features=512, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=3584, bias=False)
        (w2): Linear(in_features=1792, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.02 seconds
Prefill latency: 0.043444354087114334 sec
Decode latency: 1.9225098714232445 sec
Compilation time: 2.01 seconds
Compilation time: 2.01 seconds
Compilation time: 2.01 seconds
Compilation time: 2.01 seconds
Compilation time: 2.00 seconds
Compilation time: 2.00 seconds
Compilation time: 2.00 seconds
Compilation time: 1.97 seconds
Prefill latency: 0.04294168762862682 sec
Decode latency: 1.9228025004267693 sec
Prefill latency: 0.042905423790216446 sec
Decode latency: 1.9228498917073011 sec
Prefill latency: 0.042832570150494576 sec
Decode latency: 1.922666285187006 sec
Prefill latency: 0.04285156540572643 sec
Decode latency: 1.9229756649583578 sec
Prefill latency: 0.04287230223417282 sec
Decode latency: 1.9228072687983513 sec
Time for inference 1: 1.97 sec total, 520.79 tokens/sec
Decode latency: 1.92 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 1456.15 GB/s
FLOPS achieved: 7.28 TF/s

Prefill latency: 0.0428889375180006 sec
Decode latency: 1.9228763468563557 sec
Time for inference 2: 1.97 sec total, 520.77 tokens/sec
Decode latency: 1.92 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 1456.09 GB/s
FLOPS achieved: 7.28 TF/s

Prefill latency: 0.0428522452712059 sec
Decode latency: 1.9226214159280062 sec
Time for inference 3: 1.97 sec total, 520.85 tokens/sec
Decode latency: 1.92 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 1456.31 GB/s
FLOPS achieved: 7.28 TF/s

Prefill latency: 0.04290927015244961 sec
Decode latency: 1.9230919871479273 sec
Time for inference 4: 1.97 sec total, 520.71 tokens/sec
Decode latency: 1.92 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 1455.94 GB/s
FLOPS achieved: 7.28 TF/s

Prefill latency: 0.04290380701422691 sec
Decode latency: 1.9223042652010918 sec
Time for inference 5: 1.97 sec total, 520.92 tokens/sec
Decode latency: 1.92 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 1456.51 GB/s
FLOPS achieved: 7.28 TF/s

Prefill latency: 0.043243901804089546 sec
Decode latency: 1.9228519201278687 sec
Time for inference 6: 1.97 sec total, 520.68 tokens/sec
Decode latency: 1.92 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 1455.84 GB/s
FLOPS achieved: 7.28 TF/s

Prefill latency: 0.04294663667678833 sec
Decode latency: 1.9229482430964708 sec
Time for inference 7: 1.97 sec total, 520.71 tokens/sec
Decode latency: 1.92 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 1455.93 GB/s
FLOPS achieved: 7.28 TF/s

Prefill latency: 0.04291536845266819 sec
Decode latency: 1.9229268245398998 sec
Time for inference 8: 1.97 sec total, 520.75 tokens/sec
Decode latency: 1.92 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 1456.05 GB/s
FLOPS achieved: 7.28 TF/s

Prefill latency: 0.04287486523389816 sec
Decode latency: 1.9227054808288813 sec
Time for inference 9: 1.97 sec total, 520.83 tokens/sec
Decode latency: 1.92 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 1456.25 GB/s
FLOPS achieved: 7.28 TF/s

Prefill latency: 0.04290269315242767 sec
Decode latency: 1.9229168761521578 sec
Time for inference 10: 1.97 sec total, 520.77 tokens/sec
Decode latency: 1.92 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 1456.08 GB/s
FLOPS achieved: 7.28 TF/s

==========
Batch Size: 4
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 1.9228 sec
Average prefill latency: 0.0429 sec
Average tokens/sec: 520.78
Memory used: 7.46 GB
W0920 07:17:13.576000 22533094086464 torch/distributed/elastic/agent/server/api.py:688] Received Signals.SIGTERM death signal, shutting down workers
W0920 07:17:13.578000 22533094086464 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1790601 closing signal SIGTERM
W0920 07:17:13.612000 22533094086464 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1790602 closing signal SIGTERM
W0920 07:17:13.612000 22533094086464 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1790603 closing signal SIGTERM
W0920 07:17:13.612000 22533094086464 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1790604 closing signal SIGTERM
W0920 07:17:13.612000 22533094086464 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1790605 closing signal SIGTERM
W0920 07:17:13.612000 22533094086464 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1790606 closing signal SIGTERM
W0920 07:17:13.612000 22533094086464 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1790607 closing signal SIGTERM
W0920 07:17:13.612000 22533094086464 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1790608 closing signal SIGTERM
Traceback (most recent call last):
  File "/home/charlie/anaconda3/envs/gpt-fast/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 680, in run
    result = self._invoke_run(role)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 835, in _invoke_run
    time.sleep(monitor_interval)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 79, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 1790529 got signal: 15
