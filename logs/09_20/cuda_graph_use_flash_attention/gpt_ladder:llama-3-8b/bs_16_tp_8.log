W0920 05:16:49.571000 23275415521088 torch/distributed/run.py:779] 
W0920 05:16:49.571000 23275415521088 torch/distributed/run.py:779] *****************************************
W0920 05:16:49.571000 23275415521088 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0920 05:16:49.571000 23275415521088 torch/distributed/run.py:779] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTLadder(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x LadderTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=768, bias=False)
        (wo): Linear(in_features=512, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=3584, bias=False)
        (w2): Linear(in_features=1792, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 0.10 seconds
Prefill latency: 0.1337733119726181 sec
Decode latency: 2.3536226712167263 sec
Compilation time: 2.49 seconds
Compilation time: 2.50 seconds
Compilation time: 2.49 seconds
Compilation time: 2.48 secondsCompilation time: 2.50 seconds

Compilation time: 2.50 secondsCompilation time: 2.51 seconds

Compilation time: 2.49 seconds
Prefill latency: 0.12771009095013142 sec
Decode latency: 2.3507312312722206 sec
Prefill latency: 0.12769431248307228 sec
Decode latency: 2.3514882549643517 sec
Prefill latency: 0.12818915769457817 sec
Decode latency: 2.3488725274801254 sec
Prefill latency: 0.12752398662269115 sec
Decode latency: 2.3499658703804016 sec
Prefill latency: 0.1277085691690445 sec
Decode latency: 2.3502406124025583 sec
Time for inference 1: 2.48 sec total, 1652.58 tokens/sec
Decode latency: 2.35 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 4620.67 GB/s
FLOPS achieved: 23.10 TF/s

Prefill latency: 0.1275725867599249 sec
Decode latency: 2.3504278361797333 sec
Time for inference 2: 2.48 sec total, 1652.53 tokens/sec
Decode latency: 2.35 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 4620.55 GB/s
FLOPS achieved: 23.10 TF/s

Prefill latency: 0.1279471479356289 sec
Decode latency: 2.35280629619956 sec
Time for inference 3: 2.48 sec total, 1650.71 tokens/sec
Decode latency: 2.35 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 4615.45 GB/s
FLOPS achieved: 23.08 TF/s

Prefill latency: 0.1276459526270628 sec
Decode latency: 2.3519924115389585 sec
Time for inference 4: 2.48 sec total, 1651.51 tokens/sec
Decode latency: 2.35 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 4617.67 GB/s
FLOPS achieved: 23.09 TF/s

Prefill latency: 0.12773190438747406 sec
Decode latency: 2.352866878733039 sec
Time for inference 5: 2.48 sec total, 1650.86 tokens/sec
Decode latency: 2.35 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 4615.86 GB/s
FLOPS achieved: 23.08 TF/s

Prefill latency: 0.128050047904253 sec
Decode latency: 2.3527913335710764 sec
Time for inference 6: 2.48 sec total, 1650.70 tokens/sec
Decode latency: 2.35 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 4615.42 GB/s
FLOPS achieved: 23.08 TF/s

Prefill latency: 0.1276978999376297 sec
Decode latency: 2.353521617129445 sec
Time for inference 7: 2.48 sec total, 1650.44 tokens/sec
Decode latency: 2.35 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 4614.69 GB/s
FLOPS achieved: 23.07 TF/s

Prefill latency: 0.12811491265892982 sec
Decode latency: 2.354495918378234 sec
Time for inference 8: 2.48 sec total, 1649.52 tokens/sec
Decode latency: 2.35 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 4612.11 GB/s
FLOPS achieved: 23.06 TF/s

Prefill latency: 0.12785528041422367 sec
Decode latency: 2.35417571477592 sec
Time for inference 9: 2.48 sec total, 1649.95 tokens/sec
Decode latency: 2.35 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 4613.31 GB/s
FLOPS achieved: 23.07 TF/s

Prefill latency: 0.12839300557971 sec
Decode latency: 2.3529072888195515 sec
Time for inference 10: 2.48 sec total, 1650.37 tokens/sec
Decode latency: 2.35 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 4614.50 GB/s
FLOPS achieved: 23.07 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 2.3526 sec
Average prefill latency: 0.1279 sec
Average tokens/sec: 1650.92
Memory used: 18.99 GB
[rank3]:[E920 05:28:01.690618704 ProcessGroupNCCL.cpp:1076] [PG 0 (default_pg) Rank 3] Future for ProcessGroup abort timed out after 600000 ms
[rank1]:[E920 05:28:01.690619934 ProcessGroupNCCL.cpp:1076] [PG 0 (default_pg) Rank 1] Future for ProcessGroup abort timed out after 600000 ms
[rank6]:[E920 05:28:01.690635342 ProcessGroupNCCL.cpp:1076] [PG 0 (default_pg) Rank 6] Future for ProcessGroup abort timed out after 600000 ms
[rank7]:[E920 05:28:01.690641173 ProcessGroupNCCL.cpp:1076] [PG 0 (default_pg) Rank 7] Future for ProcessGroup abort timed out after 600000 ms
[rank4]:[E920 05:28:01.690640743 ProcessGroupNCCL.cpp:1076] [PG 0 (default_pg) Rank 4] Future for ProcessGroup abort timed out after 600000 ms
[rank0]:[E920 05:28:01.690640672 ProcessGroupNCCL.cpp:1076] [PG 0 (default_pg) Rank 0] Future for ProcessGroup abort timed out after 600000 ms
[rank5]:[E920 05:28:01.690659069 ProcessGroupNCCL.cpp:1076] [PG 0 (default_pg) Rank 5] Future for ProcessGroup abort timed out after 600000 ms
[rank2]:[E920 05:28:01.690685908 ProcessGroupNCCL.cpp:1076] [PG 0 (default_pg) Rank 2] Future for ProcessGroup abort timed out after 600000 ms
[rank4]:[E920 05:37:03.902680528 ProcessGroupNCCL.cpp:1375] [PG 0 (default_pg) Rank 4] First PG on this rank that detected no heartbeat of its watchdog.
[rank4]:[E920 05:37:03.902763257 ProcessGroupNCCL.cpp:1413] [PG 0 (default_pg) Rank 4] Heartbeat monitor timed out! Process will be terminated after dumping debug info. workMetaList_.size()=0
[rank3]:[E920 05:37:03.929315673 ProcessGroupNCCL.cpp:1375] [PG 0 (default_pg) Rank 3] First PG on this rank that detected no heartbeat of its watchdog.
[rank3]:[E920 05:37:03.929372020 ProcessGroupNCCL.cpp:1413] [PG 0 (default_pg) Rank 3] Heartbeat monitor timed out! Process will be terminated after dumping debug info. workMetaList_.size()=0
[rank2]:[E920 05:37:03.963374300 ProcessGroupNCCL.cpp:1375] [PG 0 (default_pg) Rank 2] First PG on this rank that detected no heartbeat of its watchdog.
[rank2]:[E920 05:37:03.963423262 ProcessGroupNCCL.cpp:1413] [PG 0 (default_pg) Rank 2] Heartbeat monitor timed out! Process will be terminated after dumping debug info. workMetaList_.size()=0
[rank6]:[E920 05:37:03.968088512 ProcessGroupNCCL.cpp:1375] [PG 0 (default_pg) Rank 6] First PG on this rank that detected no heartbeat of its watchdog.
[rank6]:[E920 05:37:03.968140715 ProcessGroupNCCL.cpp:1413] [PG 0 (default_pg) Rank 6] Heartbeat monitor timed out! Process will be terminated after dumping debug info. workMetaList_.size()=0
[rank1]:[E920 05:37:03.975135231 ProcessGroupNCCL.cpp:1375] [PG 0 (default_pg) Rank 1] First PG on this rank that detected no heartbeat of its watchdog.
[rank1]:[E920 05:37:03.975192072 ProcessGroupNCCL.cpp:1413] [PG 0 (default_pg) Rank 1] Heartbeat monitor timed out! Process will be terminated after dumping debug info. workMetaList_.size()=0
[rank5]:[E920 05:37:03.988763128 ProcessGroupNCCL.cpp:1375] [PG 0 (default_pg) Rank 5] First PG on this rank that detected no heartbeat of its watchdog.
[rank5]:[E920 05:37:03.988816464 ProcessGroupNCCL.cpp:1413] [PG 0 (default_pg) Rank 5] Heartbeat monitor timed out! Process will be terminated after dumping debug info. workMetaList_.size()=0
[rank0]:[E920 05:37:03.003328064 ProcessGroupNCCL.cpp:1375] [PG 0 (default_pg) Rank 0] First PG on this rank that detected no heartbeat of its watchdog.
[rank0]:[E920 05:37:03.003384975 ProcessGroupNCCL.cpp:1413] [PG 0 (default_pg) Rank 0] Heartbeat monitor timed out! Process will be terminated after dumping debug info. workMetaList_.size()=0
[rank7]:[E920 05:37:03.013467784 ProcessGroupNCCL.cpp:1375] [PG 0 (default_pg) Rank 7] First PG on this rank that detected no heartbeat of its watchdog.
[rank7]:[E920 05:37:03.013520035 ProcessGroupNCCL.cpp:1413] [PG 0 (default_pg) Rank 7] Heartbeat monitor timed out! Process will be terminated after dumping debug info. workMetaList_.size()=0
[rank4]:[F920 05:47:03.903176473 ProcessGroupNCCL.cpp:1224] [PG 0 (default_pg) Rank 4] [PG 0 (default_pg) Rank 4] ProcessGroupNCCL's watchdog got stuck for 600 seconds without making progress in monitoring enqueued collectives. This typically indicates a NCCL/CUDA API hang blocking the watchdog, and could be triggered by another thread holding the GIL inside a CUDA api, or other deadlock-prone behaviors.If you suspect the watchdog is not actually stuck and a longer timeout would help, you can either increase the timeout (TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC) to a larger value or disable the heartbeat monitor (TORCH_NCCL_ENABLE_MONITORING=0).If either of aforementioned helps, feel free to file an issue to PyTorch about the short timeout or false positive abort; otherwise, please attempt to debug the hang. workMetaList_.size() = 0
[rank3]:[F920 05:47:03.929756905 ProcessGroupNCCL.cpp:1224] [PG 0 (default_pg) Rank 3] [PG 0 (default_pg) Rank 3] ProcessGroupNCCL's watchdog got stuck for 600 seconds without making progress in monitoring enqueued collectives. This typically indicates a NCCL/CUDA API hang blocking the watchdog, and could be triggered by another thread holding the GIL inside a CUDA api, or other deadlock-prone behaviors.If you suspect the watchdog is not actually stuck and a longer timeout would help, you can either increase the timeout (TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC) to a larger value or disable the heartbeat monitor (TORCH_NCCL_ENABLE_MONITORING=0).If either of aforementioned helps, feel free to file an issue to PyTorch about the short timeout or false positive abort; otherwise, please attempt to debug the hang. workMetaList_.size() = 0
[rank2]:[F920 05:47:03.963783444 ProcessGroupNCCL.cpp:1224] [PG 0 (default_pg) Rank 2] [PG 0 (default_pg) Rank 2] ProcessGroupNCCL's watchdog got stuck for 600 seconds without making progress in monitoring enqueued collectives. This typically indicates a NCCL/CUDA API hang blocking the watchdog, and could be triggered by another thread holding the GIL inside a CUDA api, or other deadlock-prone behaviors.If you suspect the watchdog is not actually stuck and a longer timeout would help, you can either increase the timeout (TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC) to a larger value or disable the heartbeat monitor (TORCH_NCCL_ENABLE_MONITORING=0).If either of aforementioned helps, feel free to file an issue to PyTorch about the short timeout or false positive abort; otherwise, please attempt to debug the hang. workMetaList_.size() = 0
[rank6]:[F920 05:47:03.968502880 ProcessGroupNCCL.cpp:1224] [PG 0 (default_pg) Rank 6] [PG 0 (default_pg) Rank 6] ProcessGroupNCCL's watchdog got stuck for 600 seconds without making progress in monitoring enqueued collectives. This typically indicates a NCCL/CUDA API hang blocking the watchdog, and could be triggered by another thread holding the GIL inside a CUDA api, or other deadlock-prone behaviors.If you suspect the watchdog is not actually stuck and a longer timeout would help, you can either increase the timeout (TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC) to a larger value or disable the heartbeat monitor (TORCH_NCCL_ENABLE_MONITORING=0).If either of aforementioned helps, feel free to file an issue to PyTorch about the short timeout or false positive abort; otherwise, please attempt to debug the hang. workMetaList_.size() = 0
[rank1]:[F920 05:47:03.975554607 ProcessGroupNCCL.cpp:1224] [PG 0 (default_pg) Rank 1] [PG 0 (default_pg) Rank 1] ProcessGroupNCCL's watchdog got stuck for 600 seconds without making progress in monitoring enqueued collectives. This typically indicates a NCCL/CUDA API hang blocking the watchdog, and could be triggered by another thread holding the GIL inside a CUDA api, or other deadlock-prone behaviors.If you suspect the watchdog is not actually stuck and a longer timeout would help, you can either increase the timeout (TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC) to a larger value or disable the heartbeat monitor (TORCH_NCCL_ENABLE_MONITORING=0).If either of aforementioned helps, feel free to file an issue to PyTorch about the short timeout or false positive abort; otherwise, please attempt to debug the hang. workMetaList_.size() = 0
[rank5]:[F920 05:47:03.989087009 ProcessGroupNCCL.cpp:1224] [PG 0 (default_pg) Rank 5] [PG 0 (default_pg) Rank 5] ProcessGroupNCCL's watchdog got stuck for 600 seconds without making progress in monitoring enqueued collectives. This typically indicates a NCCL/CUDA API hang blocking the watchdog, and could be triggered by another thread holding the GIL inside a CUDA api, or other deadlock-prone behaviors.If you suspect the watchdog is not actually stuck and a longer timeout would help, you can either increase the timeout (TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC) to a larger value or disable the heartbeat monitor (TORCH_NCCL_ENABLE_MONITORING=0).If either of aforementioned helps, feel free to file an issue to PyTorch about the short timeout or false positive abort; otherwise, please attempt to debug the hang. workMetaList_.size() = 0
[rank0]:[F920 05:47:03.003743817 ProcessGroupNCCL.cpp:1224] [PG 0 (default_pg) Rank 0] [PG 0 (default_pg) Rank 0] ProcessGroupNCCL's watchdog got stuck for 600 seconds without making progress in monitoring enqueued collectives. This typically indicates a NCCL/CUDA API hang blocking the watchdog, and could be triggered by another thread holding the GIL inside a CUDA api, or other deadlock-prone behaviors.If you suspect the watchdog is not actually stuck and a longer timeout would help, you can either increase the timeout (TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC) to a larger value or disable the heartbeat monitor (TORCH_NCCL_ENABLE_MONITORING=0).If either of aforementioned helps, feel free to file an issue to PyTorch about the short timeout or false positive abort; otherwise, please attempt to debug the hang. workMetaList_.size() = 0
[rank7]:[F920 05:47:03.013912176 ProcessGroupNCCL.cpp:1224] [PG 0 (default_pg) Rank 7] [PG 0 (default_pg) Rank 7] ProcessGroupNCCL's watchdog got stuck for 600 seconds without making progress in monitoring enqueued collectives. This typically indicates a NCCL/CUDA API hang blocking the watchdog, and could be triggered by another thread holding the GIL inside a CUDA api, or other deadlock-prone behaviors.If you suspect the watchdog is not actually stuck and a longer timeout would help, you can either increase the timeout (TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC) to a larger value or disable the heartbeat monitor (TORCH_NCCL_ENABLE_MONITORING=0).If either of aforementioned helps, feel free to file an issue to PyTorch about the short timeout or false positive abort; otherwise, please attempt to debug the hang. workMetaList_.size() = 0
W0920 05:47:04.245000 23275415521088 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1768641 closing signal SIGTERM
W0920 05:47:04.245000 23275415521088 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1768642 closing signal SIGTERM
W0920 05:47:04.245000 23275415521088 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1768643 closing signal SIGTERM
W0920 05:47:04.245000 23275415521088 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1768645 closing signal SIGTERM
W0920 05:47:04.245000 23275415521088 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1768646 closing signal SIGTERM
W0920 05:47:04.245000 23275415521088 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1768647 closing signal SIGTERM
W0920 05:47:04.245000 23275415521088 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1768648 closing signal SIGTERM
E0920 05:47:06.777000 23275415521088 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: -6) local_rank: 3 (pid: 1768644) of binary: /home/charlie/anaconda3/envs/gpt-fast/bin/python
Traceback (most recent call last):
  File "/home/charlie/anaconda3/envs/gpt-fast/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
========================================================
benchmark.py FAILED
--------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
--------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-09-20_05:47:04
  host      : mk-xii-09.cloud.together.ai
  rank      : 3 (local_rank: 3)
  exitcode  : -6 (pid: 1768644)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 1768644
========================================================
