flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTDense(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x DenseTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
        (wo): Linear(in_features=4096, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=28672, bias=False)
        (w2): Linear(in_features=14336, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.13 seconds
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Prefill latency: 2.148503076285124 sec
Decode latency: 7.25817465223372 sec
Compilation time: 9.41 seconds
Prefill latency: 0.5080038644373417 sec
Decode latency: 6.981386808678508 sec
Prefill latency: 0.5080558564513922 sec
Decode latency: 7.064980659633875 sec
Prefill latency: 0.5077776703983545 sec
Decode latency: 6.90461135096848 sec
Prefill latency: 0.5082228258252144 sec
Decode latency: 6.969823569059372 sec
Prefill latency: 0.5070735868066549 sec
Decode latency: 6.911311695352197 sec
Time for inference 1: 7.42 sec total, 552.08 tokens/sec
Decode latency: 6.91 sec
Prefill latency: 0.51 sec
Bandwidth achieved: 8286.57 GB/s
FLOPS achieved: 41.43 TF/s

Prefill latency: 0.5082103610038757 sec
Decode latency: 6.953964289277792 sec
Time for inference 2: 7.46 sec total, 548.82 tokens/sec
Decode latency: 6.95 sec
Prefill latency: 0.51 sec
Bandwidth achieved: 8237.73 GB/s
FLOPS achieved: 41.19 TF/s

Prefill latency: 0.5067196749150753 sec
Decode latency: 6.9586811773478985 sec
Time for inference 3: 7.47 sec total, 548.60 tokens/sec
Decode latency: 6.96 sec
Prefill latency: 0.51 sec
Bandwidth achieved: 8234.35 GB/s
FLOPS achieved: 41.17 TF/s

Prefill latency: 0.5064130742102861 sec
Decode latency: 6.942356521263719 sec
Time for inference 4: 7.45 sec total, 549.81 tokens/sec
Decode latency: 6.94 sec
Prefill latency: 0.51 sec
Bandwidth achieved: 8252.50 GB/s
FLOPS achieved: 41.26 TF/s

Prefill latency: 0.5058105420321226 sec
Decode latency: 7.016661560162902 sec
Time for inference 5: 7.52 sec total, 544.43 tokens/sec
Decode latency: 7.02 sec
Prefill latency: 0.51 sec
Bandwidth achieved: 8171.84 GB/s
FLOPS achieved: 40.86 TF/s

Prefill latency: 0.50774210318923 sec
Decode latency: 6.948946325108409 sec
Time for inference 6: 7.46 sec total, 549.23 tokens/sec
Decode latency: 6.95 sec
Prefill latency: 0.51 sec
Bandwidth achieved: 8243.90 GB/s
FLOPS achieved: 41.22 TF/s

Prefill latency: 0.5071892291307449 sec
Decode latency: 6.9163086879998446 sec
Time for inference 7: 7.42 sec total, 551.69 tokens/sec
Decode latency: 6.92 sec
Prefill latency: 0.51 sec
Bandwidth achieved: 8280.76 GB/s
FLOPS achieved: 41.40 TF/s

Prefill latency: 0.5074821431189775 sec
Decode latency: 6.990557694807649 sec
Time for inference 8: 7.50 sec total, 546.21 tokens/sec
Decode latency: 6.99 sec
Prefill latency: 0.51 sec
Bandwidth achieved: 8198.48 GB/s
FLOPS achieved: 40.99 TF/s

Prefill latency: 0.5066744051873684 sec
Decode latency: 6.9666603952646255 sec
Time for inference 9: 7.47 sec total, 548.01 tokens/sec
Decode latency: 6.97 sec
Prefill latency: 0.51 sec
Bandwidth achieved: 8225.57 GB/s
FLOPS achieved: 41.13 TF/s

Prefill latency: 0.508637523278594 sec
Decode latency: 6.925090188160539 sec
Time for inference 10: 7.43 sec total, 550.92 tokens/sec
Decode latency: 6.93 sec
Prefill latency: 0.51 sec
Bandwidth achieved: 8269.29 GB/s
FLOPS achieved: 41.35 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 6.9531 sec
Average prefill latency: 0.5072 sec
Average tokens/sec: 548.98
Memory used: 26.43 GB
