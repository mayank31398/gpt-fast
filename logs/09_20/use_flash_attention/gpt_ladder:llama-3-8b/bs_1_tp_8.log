W0920 01:12:57.637000 23198484531008 torch/distributed/run.py:779] 
W0920 01:12:57.637000 23198484531008 torch/distributed/run.py:779] *****************************************
W0920 01:12:57.637000 23198484531008 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0920 01:12:57.637000 23198484531008 torch/distributed/run.py:779] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTLadder(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x LadderTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=768, bias=False)
        (wo): Linear(in_features=512, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=3584, bias=False)
        (w2): Linear(in_features=1792, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 0.04 seconds
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Prefill latency: 1.0576691683381796 sec
Decode latency: 5.779471762478352 sec
Compilation time: 6.81 secondsCompilation time: 6.79 secondsCompilation time: 6.76 seconds
Compilation time: 6.83 seconds


Compilation time: 6.79 seconds
Compilation time: 6.84 seconds
Compilation time: 6.84 seconds
Compilation time: 6.76 seconds
Prefill latency: 0.034924061968922615 sec
Decode latency: 5.759811596944928 sec
Prefill latency: 0.03478180058300495 sec
Decode latency: 5.742479328066111 sec
Prefill latency: 0.03490538150072098 sec
Decode latency: 5.754332954064012 sec
Prefill latency: 0.034735314548015594 sec
Decode latency: 5.747370099648833 sec
Prefill latency: 0.034905048087239265 sec
Decode latency: 5.790608631446958 sec
Time for inference 1: 5.83 sec total, 43.94 tokens/sec
Decode latency: 5.79 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 122.85 GB/s
FLOPS achieved: 0.61 TF/s

Prefill latency: 0.03485986217856407 sec
Decode latency: 5.749778475612402 sec
Time for inference 2: 5.79 sec total, 44.25 tokens/sec
Decode latency: 5.75 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 123.72 GB/s
FLOPS achieved: 0.62 TF/s

Prefill latency: 0.03464445099234581 sec
Decode latency: 5.711362324655056 sec
Time for inference 3: 5.75 sec total, 44.55 tokens/sec
Decode latency: 5.71 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 124.55 GB/s
FLOPS achieved: 0.62 TF/s

Prefill latency: 0.03466105833649635 sec
Decode latency: 5.752273982390761 sec
Time for inference 4: 5.79 sec total, 44.23 tokens/sec
Decode latency: 5.75 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 123.67 GB/s
FLOPS achieved: 0.62 TF/s

Prefill latency: 0.034776363521814346 sec
Decode latency: 5.757963854819536 sec
Time for inference 5: 5.79 sec total, 44.19 tokens/sec
Decode latency: 5.76 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 123.55 GB/s
FLOPS achieved: 0.62 TF/s

Prefill latency: 0.034859951585531235 sec
Decode latency: 5.72425458393991 sec
Time for inference 6: 5.76 sec total, 44.44 tokens/sec
Decode latency: 5.72 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 124.27 GB/s
FLOPS achieved: 0.62 TF/s

Prefill latency: 0.03473643213510513 sec
Decode latency: 5.7301911525428295 sec
Time for inference 7: 5.77 sec total, 44.40 tokens/sec
Decode latency: 5.73 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 124.14 GB/s
FLOPS achieved: 0.62 TF/s

Prefill latency: 0.03495169244706631 sec
Decode latency: 5.821346638724208 sec
Time for inference 8: 5.86 sec total, 43.71 tokens/sec
Decode latency: 5.82 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 122.21 GB/s
FLOPS achieved: 0.61 TF/s

Prefill latency: 0.03479045629501343 sec
Decode latency: 5.719237640500069 sec
Time for inference 9: 5.75 sec total, 44.48 tokens/sec
Decode latency: 5.72 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 124.38 GB/s
FLOPS achieved: 0.62 TF/s

Prefill latency: 0.03468133322894573 sec
Decode latency: 5.691286344081163 sec
Time for inference 10: 5.73 sec total, 44.70 tokens/sec
Decode latency: 5.69 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 124.99 GB/s
FLOPS achieved: 0.62 TF/s

==========
Batch Size: 1
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 5.7448 sec
Average prefill latency: 0.0348 sec
Average tokens/sec: 44.29
Memory used: 4.34 GB
