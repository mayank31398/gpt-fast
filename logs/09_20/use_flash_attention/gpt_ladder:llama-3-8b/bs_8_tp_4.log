W0920 01:24:34.945000 22527399720768 torch/distributed/run.py:779] 
W0920 01:24:34.945000 22527399720768 torch/distributed/run.py:779] *****************************************
W0920 01:24:34.945000 22527399720768 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0920 01:24:34.945000 22527399720768 torch/distributed/run.py:779] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTLadder(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x LadderTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=1536, bias=False)
        (wo): Linear(in_features=1024, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 0.05 seconds
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Prefill latency: 4.49084741063416 sec
Decode latency: 6.512503890320659 sec
Compilation time: 10.97 secondsCompilation time: 10.96 seconds

Compilation time: 11.01 seconds
Compilation time: 11.00 seconds
Prefill latency: 0.09941577538847923 sec
Decode latency: 6.4260895401239395 sec
Prefill latency: 0.09941994026303291 sec
Decode latency: 6.422034282237291 sec
Prefill latency: 0.09914586879312992 sec
Decode latency: 6.392998389899731 sec
Prefill latency: 0.09933380037546158 sec
Decode latency: 6.378398660570383 sec
Prefill latency: 0.0993537250906229 sec
Decode latency: 6.403703171759844 sec
Time for inference 1: 6.50 sec total, 314.88 tokens/sec
Decode latency: 6.40 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 1429.83 GB/s
FLOPS achieved: 7.15 TF/s

Prefill latency: 0.0995585285127163 sec
Decode latency: 6.373008629307151 sec
Time for inference 2: 6.47 sec total, 316.37 tokens/sec
Decode latency: 6.37 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 1436.58 GB/s
FLOPS achieved: 7.18 TF/s

Prefill latency: 0.09961327910423279 sec
Decode latency: 6.379779880866408 sec
Time for inference 3: 6.48 sec total, 316.03 tokens/sec
Decode latency: 6.38 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 1435.07 GB/s
FLOPS achieved: 7.18 TF/s

Prefill latency: 0.09943148866295815 sec
Decode latency: 6.386024272069335 sec
Time for inference 4: 6.49 sec total, 315.74 tokens/sec
Decode latency: 6.39 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 1433.73 GB/s
FLOPS achieved: 7.17 TF/s

Prefill latency: 0.09951010346412659 sec
Decode latency: 6.423097390681505 sec
Time for inference 5: 6.52 sec total, 313.94 tokens/sec
Decode latency: 6.42 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 1425.54 GB/s
FLOPS achieved: 7.13 TF/s

Prefill latency: 0.09964132867753506 sec
Decode latency: 6.41670561209321 sec
Time for inference 6: 6.52 sec total, 314.24 tokens/sec
Decode latency: 6.42 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 1426.93 GB/s
FLOPS achieved: 7.13 TF/s

Prefill latency: 0.09920794703066349 sec
Decode latency: 6.420454282313585 sec
Time for inference 7: 6.52 sec total, 314.08 tokens/sec
Decode latency: 6.42 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 1426.19 GB/s
FLOPS achieved: 7.13 TF/s

Prefill latency: 0.09952033124864101 sec
Decode latency: 6.3889672718942165 sec
Time for inference 8: 6.49 sec total, 315.59 tokens/sec
Decode latency: 6.39 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 1433.05 GB/s
FLOPS achieved: 7.17 TF/s

Prefill latency: 0.09928149916231632 sec
Decode latency: 6.400571407750249 sec
Time for inference 9: 6.50 sec total, 315.04 tokens/sec
Decode latency: 6.40 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 1430.54 GB/s
FLOPS achieved: 7.15 TF/s

Prefill latency: 0.09964854829013348 sec
Decode latency: 6.3853924330323935 sec
Time for inference 10: 6.49 sec total, 315.75 tokens/sec
Decode latency: 6.39 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 1433.80 GB/s
FLOPS achieved: 7.17 TF/s

==========
Batch Size: 8
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 6.3978 sec
Average prefill latency: 0.0995 sec
Average tokens/sec: 315.17
Memory used: 8.94 GB
