W0920 01:32:49.071000 22621351749440 torch/distributed/run.py:779] 
W0920 01:32:49.071000 22621351749440 torch/distributed/run.py:779] *****************************************
W0920 01:32:49.071000 22621351749440 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0920 01:32:49.071000 22621351749440 torch/distributed/run.py:779] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTLadder(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x LadderTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=768, bias=False)
        (wo): Linear(in_features=512, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=3584, bias=False)
        (w2): Linear(in_features=1792, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 0.07 seconds
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Prefill latency: 9.185968186706305 sec
Decode latency: 6.546951808035374 sec
Compilation time: 15.73 secondsCompilation time: 15.77 seconds

Compilation time: 15.72 secondsCompilation time: 15.74 seconds
Compilation time: 15.72 seconds

Compilation time: 15.74 seconds
Compilation time: 15.73 seconds
Compilation time: 15.79 seconds
Prefill latency: 0.12332480773329735 sec
Decode latency: 6.359362047165632 sec
Prefill latency: 0.12359524331986904 sec
Decode latency: 6.310175251215696 sec
Prefill latency: 0.12337720021605492 sec
Decode latency: 6.342456087470055 sec
Prefill latency: 0.1233099214732647 sec
Decode latency: 6.378689179196954 sec
Prefill latency: 0.12339273653924465 sec
Decode latency: 6.291502520442009 sec
Time for inference 1: 6.42 sec total, 638.43 tokens/sec
Decode latency: 6.29 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 1785.06 GB/s
FLOPS achieved: 8.93 TF/s

Prefill latency: 0.12327233329415321 sec
Decode latency: 6.301803769543767 sec
Time for inference 2: 6.43 sec total, 637.41 tokens/sec
Decode latency: 6.30 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 1782.21 GB/s
FLOPS achieved: 8.91 TF/s

Prefill latency: 0.12393663637340069 sec
Decode latency: 6.306625107303262 sec
Time for inference 3: 6.43 sec total, 636.87 tokens/sec
Decode latency: 6.31 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 1780.70 GB/s
FLOPS achieved: 8.90 TF/s

Prefill latency: 0.12358207441866398 sec
Decode latency: 6.3117643520236015 sec
Time for inference 4: 6.44 sec total, 636.40 tokens/sec
Decode latency: 6.31 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 1779.39 GB/s
FLOPS achieved: 8.90 TF/s

Prefill latency: 0.12331748567521572 sec
Decode latency: 6.332053484395146 sec
Time for inference 5: 6.46 sec total, 634.42 tokens/sec
Decode latency: 6.33 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 1773.85 GB/s
FLOPS achieved: 8.87 TF/s

Prefill latency: 0.12332038767635822 sec
Decode latency: 6.3108630776405334 sec
Time for inference 6: 6.44 sec total, 636.51 tokens/sec
Decode latency: 6.31 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 1779.70 GB/s
FLOPS achieved: 8.90 TF/s

Prefill latency: 0.12315155379474163 sec
Decode latency: 6.300268666818738 sec
Time for inference 7: 6.42 sec total, 637.58 tokens/sec
Decode latency: 6.30 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 1782.69 GB/s
FLOPS achieved: 8.91 TF/s

Prefill latency: 0.12345254048705101 sec
Decode latency: 6.348929552361369 sec
Time for inference 8: 6.47 sec total, 632.75 tokens/sec
Decode latency: 6.35 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 1769.19 GB/s
FLOPS achieved: 8.85 TF/s

Prefill latency: 0.12313032522797585 sec
Decode latency: 6.228683706372976 sec
Time for inference 9: 6.35 sec total, 644.76 tokens/sec
Decode latency: 6.23 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 1802.78 GB/s
FLOPS achieved: 9.01 TF/s

Prefill latency: 0.12340296059846878 sec
Decode latency: 6.298333449289203 sec
Time for inference 10: 6.42 sec total, 637.74 tokens/sec
Decode latency: 6.30 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 1783.15 GB/s
FLOPS achieved: 8.92 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 6.3031 sec
Average prefill latency: 0.1234 sec
Average tokens/sec: 637.29
Memory used: 9.78 GB
