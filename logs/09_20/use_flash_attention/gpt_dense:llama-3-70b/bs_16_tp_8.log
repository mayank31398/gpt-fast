W0920 02:34:40.083000 22546617263936 torch/distributed/run.py:779] 
W0920 02:34:40.083000 22546617263936 torch/distributed/run.py:779] *****************************************
W0920 02:34:40.083000 22546617263936 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0920 02:34:40.083000 22546617263936 torch/distributed/run.py:779] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTDense(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x DenseTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=1280, bias=False)
        (wo): Linear(in_features=1024, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 1.16 seconds
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Prefill latency: 12.54062607139349 sec
Decode latency: 13.344609396532178 sec
Compilation time: 25.84 secondsCompilation time: 25.83 secondsCompilation time: 25.94 secondsCompilation time: 25.95 seconds



Compilation time: 25.93 seconds
Compilation time: 25.99 seconds
Compilation time: 25.82 seconds
Compilation time: 25.89 seconds
Prefill latency: 0.7891060076653957 sec
Decode latency: 13.364611184224486 sec
Prefill latency: 0.7878373768180609 sec
Decode latency: 13.102754415944219 sec
Prefill latency: 0.7895237468183041 sec
Decode latency: 13.093852141872048 sec
Prefill latency: 0.7884455285966396 sec
Decode latency: 13.101000728085637 sec
Prefill latency: 0.7908941507339478 sec
Decode latency: 13.115326112136245 sec
Time for inference 1: 13.91 sec total, 294.53 tokens/sec
Decode latency: 13.12 sec
Prefill latency: 0.79 sec
Bandwidth achieved: 5659.86 GB/s
FLOPS achieved: 28.30 TF/s

Prefill latency: 0.7888506948947906 sec
Decode latency: 13.10032762773335 sec
Time for inference 2: 13.89 sec total, 294.89 tokens/sec
Decode latency: 13.10 sec
Prefill latency: 0.79 sec
Bandwidth achieved: 5666.78 GB/s
FLOPS achieved: 28.33 TF/s

Prefill latency: 0.7900263052433729 sec
Decode latency: 13.11665291339159 sec
Time for inference 3: 13.91 sec total, 294.52 tokens/sec
Decode latency: 13.12 sec
Prefill latency: 0.79 sec
Bandwidth achieved: 5659.67 GB/s
FLOPS achieved: 28.30 TF/s

Prefill latency: 0.7901590298861265 sec
Decode latency: 13.08534662798047 sec
Time for inference 4: 13.88 sec total, 295.18 tokens/sec
Decode latency: 13.09 sec
Prefill latency: 0.79 sec
Bandwidth achieved: 5672.40 GB/s
FLOPS achieved: 28.36 TF/s

Prefill latency: 0.7904605213552713 sec
Decode latency: 13.093698345124722 sec
Time for inference 5: 13.88 sec total, 295.00 tokens/sec
Decode latency: 13.09 sec
Prefill latency: 0.79 sec
Bandwidth achieved: 5668.85 GB/s
FLOPS achieved: 28.34 TF/s

Prefill latency: 0.7906803376972675 sec
Decode latency: 13.083371678367257 sec
Time for inference 6: 13.87 sec total, 295.21 tokens/sec
Decode latency: 13.08 sec
Prefill latency: 0.79 sec
Bandwidth achieved: 5672.98 GB/s
FLOPS achieved: 28.36 TF/s

Prefill latency: 0.7917273454368114 sec
Decode latency: 13.11007965914905 sec
Time for inference 7: 13.90 sec total, 294.62 tokens/sec
Decode latency: 13.11 sec
Prefill latency: 0.79 sec
Bandwidth achieved: 5661.66 GB/s
FLOPS achieved: 28.31 TF/s

Prefill latency: 0.7914398573338985 sec
Decode latency: 13.096162436529994 sec
Time for inference 8: 13.89 sec total, 294.92 tokens/sec
Decode latency: 13.10 sec
Prefill latency: 0.79 sec
Bandwidth achieved: 5667.46 GB/s
FLOPS achieved: 28.34 TF/s

Prefill latency: 0.7903695162385702 sec
Decode latency: 13.104794099926949 sec
Time for inference 9: 13.90 sec total, 294.76 tokens/sec
Decode latency: 13.10 sec
Prefill latency: 0.79 sec
Bandwidth achieved: 5664.40 GB/s
FLOPS achieved: 28.32 TF/s

Prefill latency: 0.7900171261280775 sec
Decode latency: 13.10708081163466 sec
Time for inference 10: 13.90 sec total, 294.72 tokens/sec
Decode latency: 13.11 sec
Prefill latency: 0.79 sec
Bandwidth achieved: 5663.62 GB/s
FLOPS achieved: 28.32 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 13.1013 sec
Average prefill latency: 0.7905 sec
Average tokens/sec: 294.84
Memory used: 28.57 GB
