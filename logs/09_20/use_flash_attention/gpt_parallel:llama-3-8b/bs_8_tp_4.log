W0920 03:01:16.368000 23220852750144 torch/distributed/run.py:779] 
W0920 03:01:16.368000 23220852750144 torch/distributed/run.py:779] *****************************************
W0920 03:01:16.368000 23220852750144 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0920 03:01:16.368000 23220852750144 torch/distributed/run.py:779] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTParallel(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x ParallelTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=1536, bias=False)
        (wo): Linear(in_features=1024, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.01 seconds
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Prefill latency: 6.4645267482846975 sec
Decode latency: 6.1787932477891445 sec
Compilation time: 12.59 secondsCompilation time: 12.58 secondsCompilation time: 12.63 seconds


Compilation time: 12.64 seconds
Prefill latency: 0.0946877934038639 sec
Decode latency: 6.1196993086487055 sec
Prefill latency: 0.09472211077809334 sec
Decode latency: 6.133155236020684 sec
Prefill latency: 0.09475170075893402 sec
Decode latency: 6.115768728777766 sec
Prefill latency: 0.0946518275886774 sec
Decode latency: 6.131374420598149 sec
Prefill latency: 0.0947533119469881 sec
Decode latency: 6.0981186311692 sec
Time for inference 1: 6.19 sec total, 330.66 tokens/sec
Decode latency: 6.10 sec
Prefill latency: 0.09 sec
Bandwidth achieved: 1501.47 GB/s
FLOPS achieved: 7.51 TF/s

Prefill latency: 0.09476453997194767 sec
Decode latency: 6.187034595757723 sec
Time for inference 2: 6.28 sec total, 325.97 tokens/sec
Decode latency: 6.19 sec
Prefill latency: 0.09 sec
Bandwidth achieved: 1480.20 GB/s
FLOPS achieved: 7.40 TF/s

Prefill latency: 0.0947383027523756 sec
Decode latency: 6.10399079695344 sec
Time for inference 3: 6.20 sec total, 330.33 tokens/sec
Decode latency: 6.10 sec
Prefill latency: 0.09 sec
Bandwidth achieved: 1500.00 GB/s
FLOPS achieved: 7.50 TF/s

Prefill latency: 0.09476585499942303 sec
Decode latency: 6.127979319542646 sec
Time for inference 4: 6.22 sec total, 329.07 tokens/sec
Decode latency: 6.13 sec
Prefill latency: 0.09 sec
Bandwidth achieved: 1494.25 GB/s
FLOPS achieved: 7.47 TF/s

Prefill latency: 0.09468739852309227 sec
Decode latency: 6.1139020044356585 sec
Time for inference 5: 6.21 sec total, 329.82 tokens/sec
Decode latency: 6.11 sec
Prefill latency: 0.09 sec
Bandwidth achieved: 1497.66 GB/s
FLOPS achieved: 7.49 TF/s

Prefill latency: 0.09491835162043571 sec
Decode latency: 6.121028643101454 sec
Time for inference 6: 6.22 sec total, 329.43 tokens/sec
Decode latency: 6.12 sec
Prefill latency: 0.09 sec
Bandwidth achieved: 1495.88 GB/s
FLOPS achieved: 7.48 TF/s

Prefill latency: 0.09483184292912483 sec
Decode latency: 6.119286146014929 sec
Time for inference 7: 6.21 sec total, 329.53 tokens/sec
Decode latency: 6.12 sec
Prefill latency: 0.09 sec
Bandwidth achieved: 1496.33 GB/s
FLOPS achieved: 7.48 TF/s

Prefill latency: 0.0948387086391449 sec
Decode latency: 6.232453916221857 sec
Time for inference 8: 6.33 sec total, 323.63 tokens/sec
Decode latency: 6.23 sec
Prefill latency: 0.09 sec
Bandwidth achieved: 1469.55 GB/s
FLOPS achieved: 7.35 TF/s

Prefill latency: 0.0948896873742342 sec
Decode latency: 6.1250562239438295 sec
Time for inference 9: 6.22 sec total, 329.21 tokens/sec
Decode latency: 6.13 sec
Prefill latency: 0.09 sec
Bandwidth achieved: 1494.91 GB/s
FLOPS achieved: 7.47 TF/s

Prefill latency: 0.09489629976451397 sec
Decode latency: 6.1368683986365795 sec
Time for inference 10: 6.23 sec total, 328.59 tokens/sec
Decode latency: 6.14 sec
Prefill latency: 0.09 sec
Bandwidth achieved: 1492.09 GB/s
FLOPS achieved: 7.46 TF/s

==========
Batch Size: 8
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 6.1366 sec
Average prefill latency: 0.0948 sec
Average tokens/sec: 328.62
Memory used: 8.74 GB
