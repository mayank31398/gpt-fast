W0920 02:55:26.093000 22588708063040 torch/distributed/run.py:779] 
W0920 02:55:26.093000 22588708063040 torch/distributed/run.py:779] *****************************************
W0920 02:55:26.093000 22588708063040 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0920 02:55:26.093000 22588708063040 torch/distributed/run.py:779] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTParallel(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x ParallelTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=1536, bias=False)
        (wo): Linear(in_features=1024, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.01 seconds
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Prefill latency: 6.414641696959734 sec
Decode latency: 6.02387080155313 sec
Compilation time: 12.37 seconds
Compilation time: 12.40 seconds
Compilation time: 12.44 seconds
Compilation time: 12.35 seconds
Prefill latency: 0.05462194047868252 sec
Decode latency: 5.938727220520377 sec
Prefill latency: 0.054624808952212334 sec
Decode latency: 5.933085288852453 sec
Prefill latency: 0.05453016795217991 sec
Decode latency: 5.9099225122481585 sec
Prefill latency: 0.05449254810810089 sec
Decode latency: 5.956047277897596 sec
Prefill latency: 0.05468177795410156 sec
Decode latency: 6.274723500013351 sec
Time for inference 1: 6.33 sec total, 161.76 tokens/sec
Decode latency: 6.27 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 734.53 GB/s
FLOPS achieved: 3.67 TF/s

Prefill latency: 0.05449116788804531 sec
Decode latency: 5.934250820428133 sec
Time for inference 2: 5.99 sec total, 170.96 tokens/sec
Decode latency: 5.93 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 776.30 GB/s
FLOPS achieved: 3.88 TF/s

Prefill latency: 0.05445222184062004 sec
Decode latency: 5.949934782460332 sec
Time for inference 3: 6.01 sec total, 170.52 tokens/sec
Decode latency: 5.95 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 774.29 GB/s
FLOPS achieved: 3.87 TF/s

Prefill latency: 0.054456207901239395 sec
Decode latency: 5.932830452919006 sec
Time for inference 4: 5.99 sec total, 171.00 tokens/sec
Decode latency: 5.93 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 776.50 GB/s
FLOPS achieved: 3.88 TF/s

Prefill latency: 0.054694853723049164 sec
Decode latency: 5.948362307623029 sec
Time for inference 5: 6.00 sec total, 170.55 tokens/sec
Decode latency: 5.95 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 774.46 GB/s
FLOPS achieved: 3.87 TF/s

Prefill latency: 0.05463515594601631 sec
Decode latency: 5.942312799394131 sec
Time for inference 6: 6.00 sec total, 170.72 tokens/sec
Decode latency: 5.94 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 775.24 GB/s
FLOPS achieved: 3.88 TF/s

Prefill latency: 0.0662310142070055 sec
Decode latency: 6.2185952216386795 sec
Time for inference 7: 6.29 sec total, 162.90 tokens/sec
Decode latency: 6.22 sec
Prefill latency: 0.07 sec
Bandwidth achieved: 739.72 GB/s
FLOPS achieved: 3.70 TF/s

Prefill latency: 0.054510023444890976 sec
Decode latency: 5.9021203480660915 sec
Time for inference 8: 5.96 sec total, 171.88 tokens/sec
Decode latency: 5.90 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 780.49 GB/s
FLOPS achieved: 3.90 TF/s

Prefill latency: 0.054765257984399796 sec
Decode latency: 5.936900865286589 sec
Time for inference 9: 5.99 sec total, 170.88 tokens/sec
Decode latency: 5.94 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 775.92 GB/s
FLOPS achieved: 3.88 TF/s

Prefill latency: 0.0546353105455637 sec
Decode latency: 5.944653702899814 sec
Time for inference 10: 6.00 sec total, 170.66 tokens/sec
Decode latency: 5.94 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 774.94 GB/s
FLOPS achieved: 3.87 TF/s

==========
Batch Size: 4
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 5.9985 sec
Average prefill latency: 0.0558 sec
Average tokens/sec: 169.18
Memory used: 7.47 GB
