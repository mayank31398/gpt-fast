W0920 03:38:41.400000 22446508750656 torch/distributed/run.py:779] 
W0920 03:38:41.400000 22446508750656 torch/distributed/run.py:779] *****************************************
W0920 03:38:41.400000 22446508750656 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0920 03:38:41.400000 22446508750656 torch/distributed/run.py:779] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=1536, bias=False)
        (wo): Linear(in_features=1024, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 0.95 seconds
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Prefill latency: 6.263990206643939 sec
Decode latency: 6.190213743597269 sec
Compilation time: 12.49 secondsCompilation time: 12.41 seconds

Compilation time: 12.46 seconds
Compilation time: 12.47 seconds
Prefill latency: 0.15942376293241978 sec
Decode latency: 5.64748683385551 sec
Prefill latency: 0.15934719890356064 sec
Decode latency: 5.6718234196305275 sec
Prefill latency: 0.15968059562146664 sec
Decode latency: 5.656190296635032 sec
Prefill latency: 0.15965081192553043 sec
Decode latency: 5.664961824193597 sec
Prefill latency: 0.15967516228556633 sec
Decode latency: 5.655885271728039 sec
Time for inference 1: 5.82 sec total, 704.21 tokens/sec
Decode latency: 5.66 sec
Prefill latency: 0.16 sec
Bandwidth achieved: 3197.70 GB/s
FLOPS achieved: 15.99 TF/s

Prefill latency: 0.15943712182343006 sec
Decode latency: 5.858655726537108 sec
Time for inference 2: 6.02 sec total, 680.51 tokens/sec
Decode latency: 5.86 sec
Prefill latency: 0.16 sec
Bandwidth achieved: 3090.12 GB/s
FLOPS achieved: 15.45 TF/s

Prefill latency: 0.159962959587574 sec
Decode latency: 5.639258157461882 sec
Time for inference 3: 5.80 sec total, 706.19 tokens/sec
Decode latency: 5.64 sec
Prefill latency: 0.16 sec
Bandwidth achieved: 3206.73 GB/s
FLOPS achieved: 16.03 TF/s

Prefill latency: 0.15977497398853302 sec
Decode latency: 5.6791486367583275 sec
Time for inference 4: 5.84 sec total, 701.39 tokens/sec
Decode latency: 5.68 sec
Prefill latency: 0.16 sec
Bandwidth achieved: 3184.93 GB/s
FLOPS achieved: 15.92 TF/s

Prefill latency: 0.15960928425192833 sec
Decode latency: 5.682949207723141 sec
Time for inference 5: 5.84 sec total, 700.95 tokens/sec
Decode latency: 5.68 sec
Prefill latency: 0.16 sec
Bandwidth achieved: 3182.92 GB/s
FLOPS achieved: 15.91 TF/s

Prefill latency: 0.16005950793623924 sec
Decode latency: 5.674376687034965 sec
Time for inference 6: 5.84 sec total, 701.93 tokens/sec
Decode latency: 5.67 sec
Prefill latency: 0.16 sec
Bandwidth achieved: 3187.36 GB/s
FLOPS achieved: 15.94 TF/s

Prefill latency: 0.1604808233678341 sec
Decode latency: 5.676407592371106 sec
Time for inference 7: 5.84 sec total, 701.64 tokens/sec
Decode latency: 5.68 sec
Prefill latency: 0.16 sec
Bandwidth achieved: 3186.04 GB/s
FLOPS achieved: 15.93 TF/s

Prefill latency: 0.1610942054539919 sec
Decode latency: 5.863713534548879 sec
Time for inference 8: 6.03 sec total, 679.76 tokens/sec
Decode latency: 5.86 sec
Prefill latency: 0.16 sec
Bandwidth achieved: 3086.69 GB/s
FLOPS achieved: 15.43 TF/s

Prefill latency: 0.16005146317183971 sec
Decode latency: 5.654138082638383 sec
Time for inference 9: 5.82 sec total, 704.38 tokens/sec
Decode latency: 5.65 sec
Prefill latency: 0.16 sec
Bandwidth achieved: 3198.48 GB/s
FLOPS achieved: 15.99 TF/s

Prefill latency: 0.16015278548002243 sec
Decode latency: 5.724336376413703 sec
Time for inference 10: 5.89 sec total, 695.95 tokens/sec
Decode latency: 5.72 sec
Prefill latency: 0.16 sec
Bandwidth achieved: 3160.21 GB/s
FLOPS achieved: 15.80 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 5.7109 sec
Average prefill latency: 0.1600 sec
Average tokens/sec: 697.69
Memory used: 11.85 GB
