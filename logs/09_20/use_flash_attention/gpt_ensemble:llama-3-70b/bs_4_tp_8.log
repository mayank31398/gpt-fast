W0920 03:51:44.486000 23400108619584 torch/distributed/run.py:779] 
W0920 03:51:44.486000 23400108619584 torch/distributed/run.py:779] *****************************************
W0920 03:51:44.486000 23400108619584 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0920 03:51:44.486000 23400108619584 torch/distributed/run.py:779] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=1280, bias=False)
        (wo): Linear(in_features=1024, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Time to load model: 1.22 seconds
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Prefill latency: 11.811888791620731 sec
Decode latency: 14.04422901570797 sec
Compilation time: 25.79 seconds
Compilation time: 25.79 seconds
Compilation time: 25.79 seconds
Compilation time: 25.97 seconds
Compilation time: 25.86 seconds
Compilation time: 25.79 seconds
Compilation time: 25.81 seconds
Compilation time: 25.81 seconds
Prefill latency: 0.19769102707505226 sec
Decode latency: 13.275922996923327 sec
Prefill latency: 0.19806873053312302 sec
Decode latency: 13.207187538966537 sec
Prefill latency: 0.19709963351488113 sec
Decode latency: 13.094758216291666 sec
Prefill latency: 0.19698419235646725 sec
Decode latency: 13.117541812360287 sec
Prefill latency: 0.1971674170345068 sec
Decode latency: 13.006356285884976 sec
Time for inference 1: 13.20 sec total, 77.55 tokens/sec
Decode latency: 13.01 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 1490.25 GB/s
FLOPS achieved: 7.45 TF/s

Prefill latency: 0.197445560246706 sec
Decode latency: 13.01658539287746 sec
Time for inference 2: 13.21 sec total, 77.49 tokens/sec
Decode latency: 13.02 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 1489.07 GB/s
FLOPS achieved: 7.45 TF/s

Prefill latency: 0.1974838562309742 sec
Decode latency: 13.051760962232947 sec
Time for inference 3: 13.25 sec total, 77.28 tokens/sec
Decode latency: 13.05 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 1485.12 GB/s
FLOPS achieved: 7.43 TF/s

Prefill latency: 0.1972940508276224 sec
Decode latency: 13.076770367100835 sec
Time for inference 4: 13.27 sec total, 77.14 tokens/sec
Decode latency: 13.08 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 1482.33 GB/s
FLOPS achieved: 7.41 TF/s

Prefill latency: 0.1975143924355507 sec
Decode latency: 13.037450093775988 sec
Time for inference 5: 13.24 sec total, 77.37 tokens/sec
Decode latency: 13.04 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 1486.71 GB/s
FLOPS achieved: 7.43 TF/s

Prefill latency: 0.19728181324899197 sec
Decode latency: 13.050218839198351 sec
Time for inference 6: 13.25 sec total, 77.29 tokens/sec
Decode latency: 13.05 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 1485.31 GB/s
FLOPS achieved: 7.43 TF/s

Prefill latency: 0.19739685207605362 sec
Decode latency: 13.041584301739931 sec
Time for inference 7: 13.24 sec total, 77.34 tokens/sec
Decode latency: 13.04 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 1486.26 GB/s
FLOPS achieved: 7.43 TF/s

Prefill latency: 0.19718656316399574 sec
Decode latency: 13.025554288178682 sec
Time for inference 8: 13.22 sec total, 77.44 tokens/sec
Decode latency: 13.03 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 1488.09 GB/s
FLOPS achieved: 7.44 TF/s

Prefill latency: 0.1973028015345335 sec
Decode latency: 13.024783413857222 sec
Time for inference 9: 13.22 sec total, 77.44 tokens/sec
Decode latency: 13.02 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 1488.17 GB/s
FLOPS achieved: 7.44 TF/s

Prefill latency: 0.19730384647846222 sec
Decode latency: 13.048481620848179 sec
Time for inference 10: 13.25 sec total, 77.30 tokens/sec
Decode latency: 13.05 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 1485.47 GB/s
FLOPS achieved: 7.43 TF/s

==========
Batch Size: 4
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 13.0380 sec
Average prefill latency: 0.1973 sec
Average tokens/sec: 77.36
Memory used: 23.43 GB
