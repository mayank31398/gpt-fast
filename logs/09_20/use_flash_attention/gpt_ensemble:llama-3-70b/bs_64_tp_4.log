W0920 04:15:27.655000 22903581095744 torch/distributed/run.py:779] 
W0920 04:15:27.655000 22903581095744 torch/distributed/run.py:779] *****************************************
W0920 04:15:27.655000 22903581095744 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0920 04:15:27.655000 22903581095744 torch/distributed/run.py:779] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=2560, bias=False)
        (wo): Linear(in_features=2048, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=14336, bias=False)
        (w2): Linear(in_features=7168, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Time to load model: 1.01 seconds
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Prefill latency: 11.471468932926655 sec
Decode latency: 13.28508597239852 sec
Compilation time: 24.96 secondsCompilation time: 24.93 seconds

Compilation time: 24.90 seconds
Compilation time: 24.76 seconds
Prefill latency: 5.07075610011816 sec
Decode latency: 13.245090261101723 sec
Prefill latency: 5.085219129920006 sec
Decode latency: 13.154003661125898 sec
Prefill latency: 5.091471264138818 sec
Decode latency: 13.234849467873573 sec
Prefill latency: 5.087875101715326 sec
Decode latency: 13.194742934778333 sec
Prefill latency: 5.068742224946618 sec
Decode latency: 13.144991287961602 sec
Time for inference 1: 18.21 sec total, 899.50 tokens/sec
Decode latency: 13.14 sec
Prefill latency: 5.07 sec
Bandwidth achieved: 32678.32 GB/s
FLOPS achieved: 163.39 TF/s

Prefill latency: 5.0752776470035315 sec
Decode latency: 13.182747585698962 sec
Time for inference 2: 18.26 sec total, 897.32 tokens/sec
Decode latency: 13.18 sec
Prefill latency: 5.08 sec
Bandwidth achieved: 32599.02 GB/s
FLOPS achieved: 163.00 TF/s

Prefill latency: 5.072837160900235 sec
Decode latency: 13.204497387632728 sec
Time for inference 3: 18.28 sec total, 896.37 tokens/sec
Decode latency: 13.20 sec
Prefill latency: 5.07 sec
Bandwidth achieved: 32564.51 GB/s
FLOPS achieved: 162.82 TF/s

Prefill latency: 5.072569830343127 sec
Decode latency: 13.190976092591882 sec
Time for inference 4: 18.26 sec total, 897.05 tokens/sec
Decode latency: 13.19 sec
Prefill latency: 5.07 sec
Bandwidth achieved: 32589.22 GB/s
FLOPS achieved: 162.95 TF/s

Prefill latency: 5.068070348352194 sec
Decode latency: 13.216840226203203 sec
Time for inference 5: 18.29 sec total, 895.99 tokens/sec
Decode latency: 13.22 sec
Prefill latency: 5.07 sec
Bandwidth achieved: 32551.05 GB/s
FLOPS achieved: 162.76 TF/s

Prefill latency: 5.081937290728092 sec
Decode latency: 13.211395608261228 sec
Time for inference 6: 18.29 sec total, 895.59 tokens/sec
Decode latency: 13.21 sec
Prefill latency: 5.08 sec
Bandwidth achieved: 32536.18 GB/s
FLOPS achieved: 162.68 TF/s

Prefill latency: 5.070424554869533 sec
Decode latency: 13.139344226568937 sec
Time for inference 7: 18.21 sec total, 899.69 tokens/sec
Decode latency: 13.14 sec
Prefill latency: 5.07 sec
Bandwidth achieved: 32685.38 GB/s
FLOPS achieved: 163.43 TF/s

Prefill latency: 5.075912879779935 sec
Decode latency: 13.207454152405262 sec
Time for inference 8: 18.28 sec total, 896.07 tokens/sec
Decode latency: 13.21 sec
Prefill latency: 5.08 sec
Bandwidth achieved: 32553.86 GB/s
FLOPS achieved: 162.77 TF/s

Prefill latency: 5.070235650986433 sec
Decode latency: 13.202236033976078 sec
Time for inference 9: 18.27 sec total, 896.61 tokens/sec
Decode latency: 13.20 sec
Prefill latency: 5.07 sec
Bandwidth achieved: 32573.34 GB/s
FLOPS achieved: 162.87 TF/s

Prefill latency: 5.09981082379818 sec
Decode latency: 13.213098362088203 sec
Time for inference 10: 18.31 sec total, 894.63 tokens/sec
Decode latency: 13.21 sec
Prefill latency: 5.10 sec
Bandwidth achieved: 32501.41 GB/s
FLOPS achieved: 162.51 TF/s

==========
Batch Size: 64
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 13.1914 sec
Average prefill latency: 5.0756 sec
Average tokens/sec: 896.88
Memory used: 74.43 GB
