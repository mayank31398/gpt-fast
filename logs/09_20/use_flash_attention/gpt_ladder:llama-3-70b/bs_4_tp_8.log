W0920 01:28:08.601000 23435185162048 torch/distributed/run.py:779] 
W0920 01:28:08.601000 23435185162048 torch/distributed/run.py:779] *****************************************
W0920 01:28:08.601000 23435185162048 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0920 01:28:08.601000 23435185162048 torch/distributed/run.py:779] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTLadder(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x LadderTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=1280, bias=False)
        (wo): Linear(in_features=1024, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 0.32 seconds
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Prefill latency: 9.747090028598905 sec
Decode latency: 14.170137345790863 sec
Compilation time: 23.91 seconds
Compilation time: 23.99 secondsCompilation time: 23.96 secondsCompilation time: 23.89 seconds
Compilation time: 23.95 seconds


Compilation time: 23.95 seconds
Compilation time: 23.92 seconds
Compilation time: 23.94 seconds
Prefill latency: 0.20164035260677338 sec
Decode latency: 13.23124790750444 sec
Prefill latency: 0.2010373491793871 sec
Decode latency: 13.393783077597618 sec
Prefill latency: 0.20092053338885307 sec
Decode latency: 13.268317075446248 sec
Prefill latency: 0.20141996257007122 sec
Decode latency: 13.107631796970963 sec
Prefill latency: 0.20057751052081585 sec
Decode latency: 13.540816452354193 sec
Time for inference 1: 13.74 sec total, 74.51 tokens/sec
Decode latency: 13.54 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 1431.93 GB/s
FLOPS achieved: 7.16 TF/s

Prefill latency: 0.20168721303343773 sec
Decode latency: 13.520766649395227 sec
Time for inference 2: 13.72 sec total, 74.62 tokens/sec
Decode latency: 13.52 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 1433.91 GB/s
FLOPS achieved: 7.17 TF/s

Prefill latency: 0.20077067241072655 sec
Decode latency: 13.356323882937431 sec
Time for inference 3: 13.56 sec total, 75.53 tokens/sec
Decode latency: 13.36 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 1451.40 GB/s
FLOPS achieved: 7.26 TF/s

Prefill latency: 0.20163368619978428 sec
Decode latency: 13.515401972457767 sec
Time for inference 4: 13.72 sec total, 74.65 tokens/sec
Decode latency: 13.52 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 1434.47 GB/s
FLOPS achieved: 7.17 TF/s

Prefill latency: 0.202256478369236 sec
Decode latency: 13.425915941596031 sec
Time for inference 5: 13.63 sec total, 75.13 tokens/sec
Decode latency: 13.43 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 1443.83 GB/s
FLOPS achieved: 7.22 TF/s

Prefill latency: 0.20159964449703693 sec
Decode latency: 13.432773366570473 sec
Time for inference 6: 13.64 sec total, 75.10 tokens/sec
Decode latency: 13.43 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 1443.16 GB/s
FLOPS achieved: 7.22 TF/s

Prefill latency: 0.20376070402562618 sec
Decode latency: 13.635128688067198 sec
Time for inference 7: 13.84 sec total, 73.99 tokens/sec
Decode latency: 13.64 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 1421.83 GB/s
FLOPS achieved: 7.11 TF/s

Prefill latency: 0.20131028071045876 sec
Decode latency: 13.077838266268373 sec
Time for inference 8: 13.28 sec total, 77.11 tokens/sec
Decode latency: 13.08 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 1481.78 GB/s
FLOPS achieved: 7.41 TF/s

Prefill latency: 0.20066630654037 sec
Decode latency: 13.020781027153134 sec
Time for inference 9: 13.22 sec total, 77.44 tokens/sec
Decode latency: 13.02 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 1488.24 GB/s
FLOPS achieved: 7.44 TF/s

Prefill latency: 0.20159455202519894 sec
Decode latency: 13.033925130963326 sec
Time for inference 10: 13.24 sec total, 77.36 tokens/sec
Decode latency: 13.03 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 1486.65 GB/s
FLOPS achieved: 7.43 TF/s

==========
Batch Size: 4
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 13.3560 sec
Average prefill latency: 0.2016 sec
Average tokens/sec: 75.54
Memory used: 23.57 GB
