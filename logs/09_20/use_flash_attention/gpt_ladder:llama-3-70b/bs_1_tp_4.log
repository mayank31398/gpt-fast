W0920 01:13:36.332000 23270847420224 torch/distributed/run.py:779] 
W0920 01:13:36.332000 23270847420224 torch/distributed/run.py:779] *****************************************
W0920 01:13:36.332000 23270847420224 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0920 01:13:36.332000 23270847420224 torch/distributed/run.py:779] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTLadder(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x LadderTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=2560, bias=False)
        (wo): Linear(in_features=2048, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=14336, bias=False)
        (w2): Linear(in_features=7168, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 0.22 seconds
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Prefill latency: 4.709501896053553 sec
Decode latency: 12.915359888225794 sec
Compilation time: 17.61 seconds
Compilation time: 17.61 seconds
Compilation time: 17.61 seconds
Compilation time: 17.63 seconds
Prefill latency: 0.10056404396891594 sec
Decode latency: 12.262593356892467 sec
Prefill latency: 0.10233869589865208 sec
Decode latency: 12.158238699659705 sec
Prefill latency: 0.10120722092688084 sec
Decode latency: 12.175011666491628 sec
Prefill latency: 0.10069966316223145 sec
Decode latency: 12.121484749019146 sec
Prefill latency: 0.10033121518790722 sec
Decode latency: 12.119382353499532 sec
Time for inference 1: 12.22 sec total, 20.95 tokens/sec
Decode latency: 12.12 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 761.04 GB/s
FLOPS achieved: 3.81 TF/s

Prefill latency: 0.10025426559150219 sec
Decode latency: 12.115846211090684 sec
Time for inference 2: 12.22 sec total, 20.95 tokens/sec
Decode latency: 12.12 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 761.26 GB/s
FLOPS achieved: 3.81 TF/s

Prefill latency: 0.10159867256879807 sec
Decode latency: 12.145694229751825 sec
Time for inference 3: 12.25 sec total, 20.90 tokens/sec
Decode latency: 12.15 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 759.33 GB/s
FLOPS achieved: 3.80 TF/s

Prefill latency: 0.10126065835356712 sec
Decode latency: 12.165669156238437 sec
Time for inference 4: 12.27 sec total, 20.87 tokens/sec
Decode latency: 12.17 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 758.11 GB/s
FLOPS achieved: 3.79 TF/s

Prefill latency: 0.10016638971865177 sec
Decode latency: 12.18141434341669 sec
Time for inference 5: 12.28 sec total, 20.84 tokens/sec
Decode latency: 12.18 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 757.21 GB/s
FLOPS achieved: 3.79 TF/s

Prefill latency: 0.10080955550074577 sec
Decode latency: 12.150581834837794 sec
Time for inference 6: 12.25 sec total, 20.89 tokens/sec
Decode latency: 12.15 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 759.08 GB/s
FLOPS achieved: 3.80 TF/s

Prefill latency: 0.10178238153457642 sec
Decode latency: 12.190237391740084 sec
Time for inference 7: 12.29 sec total, 20.83 tokens/sec
Decode latency: 12.19 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 756.57 GB/s
FLOPS achieved: 3.78 TF/s

Prefill latency: 0.10090125910937786 sec
Decode latency: 12.142671873793006 sec
Time for inference 8: 12.24 sec total, 20.91 tokens/sec
Decode latency: 12.14 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 759.55 GB/s
FLOPS achieved: 3.80 TF/s

Prefill latency: 0.10086851939558983 sec
Decode latency: 12.148358711972833 sec
Time for inference 9: 12.25 sec total, 20.90 tokens/sec
Decode latency: 12.15 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 759.21 GB/s
FLOPS achieved: 3.80 TF/s

Prefill latency: 0.10189974866807461 sec
Decode latency: 12.136941304430366 sec
Time for inference 10: 12.24 sec total, 20.92 tokens/sec
Decode latency: 12.14 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 759.86 GB/s
FLOPS achieved: 3.80 TF/s

==========
Batch Size: 1
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 12.1497 sec
Average prefill latency: 0.1010 sec
Average tokens/sec: 20.90
Memory used: 39.13 GB
