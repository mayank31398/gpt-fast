flash_kv_decode is set to False
Using device=cuda
Loading model ...
GPTDense(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x DenseTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
        (wo): Linear(in_features=4096, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=28672, bias=False)
        (w2): Linear(in_features=14336, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.05 seconds
Compiling prefill with dynamic=False
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
[rank0]:W0920 07:50:34.760000 23292063270720 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] xindex is not in var_ranges, defaulting to unknown range.
Prefill latency: 65.11887235799804 sec
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
[rank0]:W0920 07:51:40.370000 23292063270720 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] xindex is not in var_ranges, defaulting to unknown range.
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 64.30997777701123 sec
Compilation time: 129.43 seconds
Prefill latency: 1.3186964060005266 sec
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 2.5009699179790914 sec
Prefill latency: 0.5156464309839066 sec
Decode latency: 2.5009215509926435 sec
Prefill latency: 0.5178198859794065 sec
Decode latency: 2.5012610210105777 sec
Prefill latency: 0.5181598869967274 sec
Decode latency: 2.5014107630122453 sec
Prefill latency: 0.5166436640138272 sec
Decode latency: 2.501042805000907 sec
Time for inference 1: 3.02 sec total, 1356.73 tokens/sec
Decode latency: 2.50 sec
Prefill latency: 0.52 sec
Bandwidth achieved: 20364.26 GB/s
FLOPS achieved: 101.82 TF/s

Prefill latency: 0.5141419479914475 sec
Decode latency: 2.5024340490053874 sec
Time for inference 2: 3.02 sec total, 1357.33 tokens/sec
Decode latency: 2.50 sec
Prefill latency: 0.51 sec
Bandwidth achieved: 20373.27 GB/s
FLOPS achieved: 101.87 TF/s

Prefill latency: 0.5157645379949827 sec
Decode latency: 2.5025156470073853 sec
Time for inference 3: 3.02 sec total, 1356.51 tokens/sec
Decode latency: 2.50 sec
Prefill latency: 0.52 sec
Bandwidth achieved: 20361.05 GB/s
FLOPS achieved: 101.81 TF/s

Prefill latency: 0.5140062109858263 sec
Decode latency: 2.501336924993666 sec
Time for inference 4: 3.02 sec total, 1357.85 tokens/sec
Decode latency: 2.50 sec
Prefill latency: 0.51 sec
Bandwidth achieved: 20381.16 GB/s
FLOPS achieved: 101.91 TF/s

Prefill latency: 0.5172288479807321 sec
Decode latency: 2.501870370004326 sec
Time for inference 5: 3.02 sec total, 1356.17 tokens/sec
Decode latency: 2.50 sec
Prefill latency: 0.52 sec
Bandwidth achieved: 20355.90 GB/s
FLOPS achieved: 101.78 TF/s

Prefill latency: 0.5186073890072294 sec
Decode latency: 2.5022360049770214 sec
Time for inference 6: 3.02 sec total, 1355.41 tokens/sec
Decode latency: 2.50 sec
Prefill latency: 0.52 sec
Bandwidth achieved: 20344.52 GB/s
FLOPS achieved: 101.72 TF/s

Prefill latency: 0.5161288319795858 sec
Decode latency: 2.5021365830034483 sec
Time for inference 7: 3.02 sec total, 1356.58 tokens/sec
Decode latency: 2.50 sec
Prefill latency: 0.52 sec
Bandwidth achieved: 20362.02 GB/s
FLOPS achieved: 101.81 TF/s

Prefill latency: 0.5161790040147025 sec
Decode latency: 2.5026765009970404 sec
Time for inference 8: 3.02 sec total, 1356.31 tokens/sec
Decode latency: 2.50 sec
Prefill latency: 0.52 sec
Bandwidth achieved: 20357.99 GB/s
FLOPS achieved: 101.79 TF/s

Prefill latency: 0.5175524869991932 sec
Decode latency: 2.501671203004662 sec
Time for inference 9: 3.02 sec total, 1355.96 tokens/sec
Decode latency: 2.50 sec
Prefill latency: 0.52 sec
Bandwidth achieved: 20352.73 GB/s
FLOPS achieved: 101.76 TF/s

Prefill latency: 0.5173860570066608 sec
Decode latency: 2.5006545500073116 sec
Time for inference 10: 3.02 sec total, 1356.41 tokens/sec
Decode latency: 2.50 sec
Prefill latency: 0.52 sec
Bandwidth achieved: 20359.44 GB/s
FLOPS achieved: 101.80 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 2.5019 sec
Average prefill latency: 0.5164 sec
Average tokens/sec: 1356.52
Memory used: 30.65 GB
