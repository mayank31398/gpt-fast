flash_kv_decode is set to False
Using device=cuda
Loading model ...
GPTDense(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x DenseTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
        (wo): Linear(in_features=4096, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=28672, bias=False)
        (w2): Linear(in_features=14336, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.08 seconds
Compiling prefill with dynamic=False
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
[rank0]:W0920 07:03:09.419000 22741222115136 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] xindex is not in var_ranges, defaulting to unknown range.
Prefill latency: 43.7410903449927 sec
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
[rank0]:W0920 07:03:55.594000 22741222115136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] xindex is not in var_ranges, defaulting to unknown range.
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 61.59724218698102 sec
Compilation time: 105.34 seconds
Prefill latency: 0.7422692890104372 sec
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 1.5674151639977936 sec
Prefill latency: 0.03273971998714842 sec
Decode latency: 1.568106494989479 sec
Prefill latency: 0.032518244988750666 sec
Decode latency: 1.5682043790002353 sec
Prefill latency: 0.03204642600030638 sec
Decode latency: 1.5678227539756335 sec
Prefill latency: 0.03205017899745144 sec
Decode latency: 1.5683302039979026 sec
Time for inference 1: 1.60 sec total, 159.86 tokens/sec
Decode latency: 1.57 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 2399.46 GB/s
FLOPS achieved: 12.00 TF/s

Prefill latency: 0.03200731799006462 sec
Decode latency: 1.5676235179998912 sec
Time for inference 2: 1.60 sec total, 159.94 tokens/sec
Decode latency: 1.57 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 2400.61 GB/s
FLOPS achieved: 12.00 TF/s

Prefill latency: 0.03207388101145625 sec
Decode latency: 1.5654031120066065 sec
Time for inference 3: 1.60 sec total, 160.14 tokens/sec
Decode latency: 1.57 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 2403.70 GB/s
FLOPS achieved: 12.02 TF/s

Prefill latency: 0.03215991298202425 sec
Decode latency: 1.5668866240011994 sec
Time for inference 4: 1.60 sec total, 159.98 tokens/sec
Decode latency: 1.57 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 2401.35 GB/s
FLOPS achieved: 12.01 TF/s

Prefill latency: 0.032045189989730716 sec
Decode latency: 1.56685951599502 sec
Time for inference 5: 1.60 sec total, 160.01 tokens/sec
Decode latency: 1.57 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 2401.66 GB/s
FLOPS achieved: 12.01 TF/s

Prefill latency: 0.03203206698526628 sec
Decode latency: 1.5669813319982495 sec
Time for inference 6: 1.60 sec total, 159.99 tokens/sec
Decode latency: 1.57 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 2401.47 GB/s
FLOPS achieved: 12.01 TF/s

Prefill latency: 0.03208478700253181 sec
Decode latency: 1.566651645989623 sec
Time for inference 7: 1.60 sec total, 160.02 tokens/sec
Decode latency: 1.57 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 2401.83 GB/s
FLOPS achieved: 12.01 TF/s

Prefill latency: 0.032066998013760895 sec
Decode latency: 1.5671010620135348 sec
Time for inference 8: 1.60 sec total, 159.97 tokens/sec
Decode latency: 1.57 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 2401.15 GB/s
FLOPS achieved: 12.01 TF/s

Prefill latency: 0.0320647030021064 sec
Decode latency: 1.5668577319884207 sec
Time for inference 9: 1.60 sec total, 159.98 tokens/sec
Decode latency: 1.57 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 2401.20 GB/s
FLOPS achieved: 12.01 TF/s

Prefill latency: 0.032291160023305565 sec
Decode latency: 1.56695225200383 sec
Time for inference 10: 1.60 sec total, 159.96 tokens/sec
Decode latency: 1.57 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 2400.99 GB/s
FLOPS achieved: 12.00 TF/s

==========
Batch Size: 1
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 1.5670 sec
Average prefill latency: 0.0321 sec
Average tokens/sec: 159.98
Memory used: 17.38 GB
