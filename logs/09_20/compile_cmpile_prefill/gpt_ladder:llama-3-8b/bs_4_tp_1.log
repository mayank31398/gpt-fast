flash_kv_decode is set to False
Using device=cuda
Loading model ...
GPTLadder(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x LadderTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
        (wo): Linear(in_features=4096, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=28672, bias=False)
        (w2): Linear(in_features=14336, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 0.14 seconds
Compiling prefill with dynamic=False
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Prefill latency: 15.47273832699284 sec
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 14.694458167999983 sec
Compilation time: 30.17 seconds
Prefill latency: 0.330438987002708 sec
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 1.8935154060018249 sec
Prefill latency: 0.1221290899993619 sec
Decode latency: 1.893579552997835 sec
Prefill latency: 0.12289657699875534 sec
Decode latency: 1.893479827005649 sec
Prefill latency: 0.12327939399983734 sec
Decode latency: 1.8939391969906865 sec
Prefill latency: 0.12345211800129618 sec
Decode latency: 1.894209445992601 sec
Time for inference 1: 2.02 sec total, 507.26 tokens/sec
Decode latency: 1.89 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 7613.83 GB/s
FLOPS achieved: 38.07 TF/s

Prefill latency: 0.12327965500298887 sec
Decode latency: 1.8937847950001014 sec
Time for inference 2: 2.02 sec total, 507.40 tokens/sec
Decode latency: 1.89 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 7616.04 GB/s
FLOPS achieved: 38.08 TF/s

Prefill latency: 0.12164345099881757 sec
Decode latency: 1.8933347089914605 sec
Time for inference 3: 2.02 sec total, 507.92 tokens/sec
Decode latency: 1.89 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 7623.80 GB/s
FLOPS achieved: 38.12 TF/s

Prefill latency: 0.12356812399229966 sec
Decode latency: 1.8934703189879656 sec
Time for inference 4: 2.02 sec total, 507.42 tokens/sec
Decode latency: 1.89 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 7616.29 GB/s
FLOPS achieved: 38.08 TF/s

Prefill latency: 0.12324184700264595 sec
Decode latency: 1.893065452008159 sec
Time for inference 5: 2.02 sec total, 507.60 tokens/sec
Decode latency: 1.89 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 7618.94 GB/s
FLOPS achieved: 38.09 TF/s

Prefill latency: 0.12296684300235938 sec
Decode latency: 1.8935372150008334 sec
Time for inference 6: 2.02 sec total, 507.53 tokens/sec
Decode latency: 1.89 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 7617.95 GB/s
FLOPS achieved: 38.09 TF/s

Prefill latency: 0.12398016000224743 sec
Decode latency: 1.893699292006204 sec
Time for inference 7: 2.02 sec total, 507.26 tokens/sec
Decode latency: 1.89 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 7613.89 GB/s
FLOPS achieved: 38.07 TF/s

Prefill latency: 0.12334029100020416 sec
Decode latency: 1.8940015199914342 sec
Time for inference 8: 2.02 sec total, 507.31 tokens/sec
Decode latency: 1.89 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 7614.69 GB/s
FLOPS achieved: 38.07 TF/s

Prefill latency: 0.12262080299842637 sec
Decode latency: 1.8935974440100836 sec
Time for inference 9: 2.02 sec total, 507.61 tokens/sec
Decode latency: 1.89 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 7619.14 GB/s
FLOPS achieved: 38.10 TF/s

Prefill latency: 0.12305054800526705 sec
Decode latency: 1.8940930449898588 sec
Time for inference 10: 2.02 sec total, 507.39 tokens/sec
Decode latency: 1.89 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 7615.88 GB/s
FLOPS achieved: 38.08 TF/s

==========
Batch Size: 4
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 1.8937 sec
Average prefill latency: 0.1231 sec
Average tokens/sec: 507.47
Memory used: 19.58 GB
