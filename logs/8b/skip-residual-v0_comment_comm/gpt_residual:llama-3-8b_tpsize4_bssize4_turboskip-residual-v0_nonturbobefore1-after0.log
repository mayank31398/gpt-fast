[2024-09-13 08:50:36,801] torch.distributed.run: [WARNING] 
[2024-09-13 08:50:36,801] torch.distributed.run: [WARNING] *****************************************
[2024-09-13 08:50:36,801] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-09-13 08:50:36,801] torch.distributed.run: [WARNING] *****************************************
our tp world size is 4
our tp world size is 4
our tp world size is 4
our tp world size is 4
Using device=cuda
our tp world size is our tp world size is our tp world size is Loading model ...
444
our tp world size is 

our tp world size is 4
our tp world size is our tp world size is4
our tp world size is 4
 4our tp world size is4
our tp world size is 
 4our tp world size is 4
our tp world size is 
4
our tp world size is 4our tp world size is our tp world size is 4

4
4our tp world size is our tp world size is our tp world size is 
44
4
our tp world size is 
our tp world size is our tp world size is 4
our tp world size is 4
4
our tp world size is 4our tp world size is our tp world size is 4

44
our tp world size is our tp world size is 
our tp world size is 4
4
our tp world size is 4
our tp world size is our tp world size is 4our tp world size is 4
4

4
our tp world size is our tp world size is our tp world size is our tp world size is 4
4
4
4
our tp world size is our tp world size is our tp world size is our tp world size is 4
4
4
4
our tp world size is our tp world size is our tp world size isour tp world size is4
4 4 4our tp world size is 

our tp world size is
4
our tp world size is  4our tp world size is our tp world size is 4

4
4
our tp world size is our tp world size is our tp world size is our tp world size is 4
4
4
4
our tp world size is our tp world size is our tp world size is our tp world size is 4
4
4
4
our tp world size is our tp world size is our tp world size is our tp world size is 4
4
4
4
our tp world size is our tp world size is our tp world size is our tp world size is 4
4
4
4
our tp world size isour tp world size is our tp world size is our tp world size is  44
4
4
our tp world size isour tp world size is our tp world size is 
 444
our tp world size is 
our tp world size is
our tp world size is 4
 4our tp world size is 4
our tp world size is 
4our tp world size is 4
our tp world size is 
4our tp world size is 4our tp world size is 
4

4
our tp world size is our tp world size is our tp world size is our tp world size is 4
4
4
4
our tp world size is our tp world size is our tp world size is our tp world size is4
4
4
 4our tp world size is our tp world size is our tp world size is 
4
4
4
our tp world size is our tp world size is our tp world size is our tp world size is 4
4
4
4our tp world size is our tp world size is our tp world size is 
44
4
our tp world size is
our tp world size isour tp world size is 4our tp world size is   4
our tp world size is44

 4
our tp world size is our tp world size is
our tp world size is 4
 4
our tp world size is 4
we finish operating the TP!
our tp world size is 4
our tp world size is 4
our tp world size is 4
our tp world size is 4
our tp world size is 4
our tp world size is 4our tp world size is 4

4
our tp world size isour tp world size is our tp world size is 44 4
our tp world size is

 our tp world size is4Applying tensor parallel to model ...
 
our tp world size is4
 4we finish operating the TP!
our tp world size is
 our tp world size is 4
4
our tp world size is 4
we finish operating the TP!
we finish operating the TP!
GPTResidual(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x TurboTransformerBlock(
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=1536, bias=False)
        (wo): Linear(in_features=1024, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
models all reduce stream is None
Time to load model: 0.61 seconds
the shape of input is torch.Size([4, 1024])
tokens we generated: 1024
Time for prefill: 58.81 sec
Time for inference -4: 124.70 sec total, 8.21 tokens/sec
Bandwidth achieved: 37.29 GB/s
FLOPS achieved: 0.30 TF/s

the shape of input is torch.Size([4, 1024])
tokens we generated: 1024
Time for prefill: 0.45 sec
Time for inference -3: 4.94 sec total, 207.46 tokens/sec
Bandwidth achieved: 942.05 GB/s
FLOPS achieved: 7.54 TF/s

the shape of input is torch.Size([4, 1024])
tokens we generated: 1024
Time for prefill: 0.04 sec
Time for inference -2: 3.99 sec total, 256.40 tokens/sec
Bandwidth achieved: 1164.28 GB/s
FLOPS achieved: 9.31 TF/s

the shape of input is torch.Size([4, 1024])
tokens we generated: 1024
Time for prefill: 0.04 sec
Time for inference -1: 4.00 sec total, 255.82 tokens/sec
Bandwidth achieved: 1161.66 GB/s
FLOPS achieved: 9.29 TF/s

the shape of input is torch.Size([4, 1024])
Compilation time: 3.99 seconds
the shape of input is torch.Size([4, 1024])
tokens we generated: 1024
Time for prefill: 0.05 sec
Time for inference 1: 4.00 sec total, 256.23 tokens/sec
Bandwidth achieved: 1163.53 GB/s
FLOPS achieved: 9.31 TF/s

the shape of input is torch.Size([4, 1024])
tokens we generated: 1024
Time for prefill: 0.04 sec
Time for inference 2: 4.00 sec total, 256.19 tokens/sec
Bandwidth achieved: 1163.33 GB/s
FLOPS achieved: 9.31 TF/s

the shape of input is torch.Size([4, 1024])
tokens we generated: 1024
Time for prefill: 0.04 sec
Time for inference 3: 4.00 sec total, 256.30 tokens/sec
Bandwidth achieved: 1163.83 GB/s
FLOPS achieved: 9.31 TF/s

the shape of input is torch.Size([4, 1024])
tokens we generated: 1024
Time for prefill: 0.05 sec
Time for inference 4: 4.00 sec total, 255.82 tokens/sec
Bandwidth achieved: 1161.65 GB/s
FLOPS achieved: 9.29 TF/s

the shape of input is torch.Size([4, 1024])
tokens we generated: 1024
Time for prefill: 0.05 sec
Time for inference 5: 4.00 sec total, 256.29 tokens/sec
Bandwidth achieved: 1163.77 GB/s
FLOPS achieved: 9.31 TF/s

the shape of input is torch.Size([4, 1024])
tokens we generated: 1024
Time for prefill: 0.05 sec
Time for inference 6: 4.00 sec total, 255.83 tokens/sec
Bandwidth achieved: 1161.68 GB/s
FLOPS achieved: 9.29 TF/s

the shape of input is torch.Size([4, 1024])
tokens we generated: 1024
Time for prefill: 0.05 sec
Time for inference 7: 4.00 sec total, 256.21 tokens/sec
Bandwidth achieved: 1163.42 GB/s
FLOPS achieved: 9.31 TF/s

the shape of input is torch.Size([4, 1024])
tokens we generated: 1024
Time for prefill: 0.05 sec
Time for inference 8: 4.00 sec total, 255.92 tokens/sec
Bandwidth achieved: 1162.11 GB/s
FLOPS achieved: 9.30 TF/s

the shape of input is torch.Size([4, 1024])
tokens we generated: 1024
Time for prefill: 0.05 sec
Time for inference 9: 4.01 sec total, 255.26 tokens/sec
Bandwidth achieved: 1159.10 GB/s
FLOPS achieved: 9.27 TF/s

the shape of input is torch.Size([4, 1024])
tokens we generated: 1024
Time for prefill: 0.04 sec
Time for inference 10: 4.00 sec total, 256.05 tokens/sec
Bandwidth achieved: 1162.69 GB/s
FLOPS achieved: 9.30 TF/s

the shape of input is torch.Size([4, 1024])
tokens we generated: 1024
Time for prefill: 0.04 sec
Time for inference 11: 4.00 sec total, 256.22 tokens/sec
Bandwidth achieved: 1163.48 GB/s
FLOPS achieved: 9.31 TF/s

the shape of input is torch.Size([4, 1024])
tokens we generated: 1024
Time for prefill: 0.04 sec
Time for inference 12: 4.01 sec total, 255.17 tokens/sec
Bandwidth achieved: 1158.68 GB/s
FLOPS achieved: 9.27 TF/s

the shape of input is torch.Size([4, 1024])
tokens we generated: 1024
Time for prefill: 0.04 sec
Time for inference 13: 4.00 sec total, 256.01 tokens/sec
Bandwidth achieved: 1162.52 GB/s
FLOPS achieved: 9.30 TF/s

the shape of input is torch.Size([4, 1024])
tokens we generated: 1024
Time for prefill: 0.04 sec
Time for inference 14: 3.99 sec total, 256.44 tokens/sec
Bandwidth achieved: 1164.48 GB/s
FLOPS achieved: 9.32 TF/s

the shape of input is torch.Size([4, 1024])
tokens we generated: 1024
Time for prefill: 0.04 sec
Time for inference 15: 3.99 sec total, 256.55 tokens/sec
Bandwidth achieved: 1164.95 GB/s
FLOPS achieved: 9.32 TF/s

STAGE:2024-09-13 08:54:02 4167948:4167948 ActivityProfilerController.cpp:314] Completed Stage: Warm Up
STAGE:2024-09-13 08:54:02 4167948:4167948 ActivityProfilerController.cpp:320] Completed Stage: Collection
STAGE:2024-09-13 08:54:02 4167948:4167948 ActivityProfilerController.cpp:324] Completed Stage: Post Processing
the shape of input is torch.Size([4, 1024])
tokens we generated: 1024
Time for prefill: 0.04 sec
Time for inference 16: 4.10 sec total, 249.77 tokens/sec
Bandwidth achieved: 1134.16 GB/s
FLOPS achieved: 9.07 TF/s

==========
Batch Size: 4
Prompt Length: 1024
Generated tokens: 1024
Average prefill latency: 0.07 sec
Average tokens/sec: 253.16
Memory used: 9.10 GB
