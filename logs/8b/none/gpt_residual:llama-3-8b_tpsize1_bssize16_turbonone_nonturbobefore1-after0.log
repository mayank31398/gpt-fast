Using device=cuda
Loading model ...
our tp world size is 1
our tp world size is 1
our tp world size is 1
our tp world size is 1
our tp world size is 1
our tp world size is 1
our tp world size is 1
our tp world size is 1
our tp world size is 1
our tp world size is 1
our tp world size is 1
our tp world size is 1
our tp world size is 1
our tp world size is 1
our tp world size is 1
our tp world size is 1
our tp world size is 1
our tp world size is 1
our tp world size is 1
our tp world size is 1
our tp world size is 1
our tp world size is 1
our tp world size is 1
our tp world size is 1
our tp world size is 1
our tp world size is 1
our tp world size is 1
our tp world size is 1
our tp world size is 1
our tp world size is 1
our tp world size is 1
our tp world size is 1
GPTResidual(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x TurboTransformerBlock(
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
        (wo): Linear(in_features=4096, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=28672, bias=False)
        (w2): Linear(in_features=14336, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
we comment comm is False
models all reduce stream is None
Time to load model: 1.32 seconds
the shape of input is torch.Size([16, 1024])
tokens we generated: 1024
Time for prefill: 109.18 sec
Time for inference -4: 233.48 sec total, 4.39 tokens/sec
Bandwidth achieved: 65.83 GB/s
FLOPS achieved: 2.11 TF/s

the shape of input is torch.Size([16, 1024])
tokens we generated: 1024
Time for prefill: 1.27 sec
Time for inference -3: 16.44 sec total, 62.28 tokens/sec
Bandwidth achieved: 934.82 GB/s
FLOPS achieved: 29.91 TF/s

the shape of input is torch.Size([16, 1024])
tokens we generated: 1024
Time for prefill: 0.56 sec
Time for inference -2: 12.90 sec total, 79.38 tokens/sec
Bandwidth achieved: 1191.42 GB/s
FLOPS achieved: 38.13 TF/s

the shape of input is torch.Size([16, 1024])
tokens we generated: 1024
Time for prefill: 0.56 sec
Time for inference -1: 12.89 sec total, 79.42 tokens/sec
Bandwidth achieved: 1192.03 GB/s
FLOPS achieved: 38.15 TF/s

the shape of input is torch.Size([16, 1024])
Compilation time: 12.89 seconds
the shape of input is torch.Size([16, 1024])
tokens we generated: 1024
Time for prefill: 0.56 sec
Time for inference 1: 12.89 sec total, 79.42 tokens/sec
Bandwidth achieved: 1192.10 GB/s
FLOPS achieved: 38.15 TF/s

the shape of input is torch.Size([16, 1024])
tokens we generated: 1024
Time for prefill: 0.56 sec
Time for inference 2: 12.89 sec total, 79.44 tokens/sec
Bandwidth achieved: 1192.41 GB/s
FLOPS achieved: 38.16 TF/s

the shape of input is torch.Size([16, 1024])
tokens we generated: 1024
Time for prefill: 0.56 sec
Time for inference 3: 12.89 sec total, 79.43 tokens/sec
Bandwidth achieved: 1192.27 GB/s
FLOPS achieved: 38.15 TF/s

the shape of input is torch.Size([16, 1024])
tokens we generated: 1024
Time for prefill: 0.57 sec
Time for inference 4: 12.90 sec total, 79.40 tokens/sec
Bandwidth achieved: 1191.74 GB/s
FLOPS achieved: 38.14 TF/s

the shape of input is torch.Size([16, 1024])
tokens we generated: 1024
Time for prefill: 0.57 sec
Time for inference 5: 12.85 sec total, 79.66 tokens/sec
Bandwidth achieved: 1195.70 GB/s
FLOPS achieved: 38.26 TF/s

the shape of input is torch.Size([16, 1024])
tokens we generated: 1024
Time for prefill: 0.56 sec
Time for inference 6: 12.84 sec total, 79.73 tokens/sec
Bandwidth achieved: 1196.66 GB/s
FLOPS achieved: 38.29 TF/s

the shape of input is torch.Size([16, 1024])
tokens we generated: 1024
Time for prefill: 0.56 sec
Time for inference 7: 12.85 sec total, 79.70 tokens/sec
Bandwidth achieved: 1196.27 GB/s
FLOPS achieved: 38.28 TF/s

the shape of input is torch.Size([16, 1024])
tokens we generated: 1024
Time for prefill: 0.56 sec
Time for inference 8: 12.89 sec total, 79.44 tokens/sec
Bandwidth achieved: 1192.35 GB/s
FLOPS achieved: 38.16 TF/s

the shape of input is torch.Size([16, 1024])
tokens we generated: 1024
Time for prefill: 0.56 sec
Time for inference 9: 12.89 sec total, 79.44 tokens/sec
Bandwidth achieved: 1192.39 GB/s
FLOPS achieved: 38.16 TF/s

the shape of input is torch.Size([16, 1024])
tokens we generated: 1024
Time for prefill: 0.56 sec
Time for inference 10: 12.85 sec total, 79.68 tokens/sec
Bandwidth achieved: 1195.99 GB/s
FLOPS achieved: 38.27 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 1024
Average prefill latency: 0.62 sec
Average tokens/sec: 78.19
Memory used: 41.75 GB
