[2024-09-13 11:41:08,980] torch.distributed.run: [WARNING] 
[2024-09-13 11:41:08,980] torch.distributed.run: [WARNING] *****************************************
[2024-09-13 11:41:08,980] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-09-13 11:41:08,980] torch.distributed.run: [WARNING] *****************************************
our tp world size is 8
our tp world size is 8
our tp world size is 8
our tp world size is 8
our tp world size is 8
our tp world size is 8
our tp world size is 8
our tp world size is 8
our tp world size is 8
Using device=cuda
our tp world size is our tp world size is our tp world size is our tp world size is our tp world size isour tp world size isLoading model ...
our tp world size is 8
8
8
8
  8our tp world size is 8our tp world size is our tp world size is our tp world size is our tp world size is 8

our tp world size is8

8
88
8
our tp world size is  8our tp world size is our tp world size is our tp world size is 
our tp world size is our tp world size is 8

our tp world size is8
8
8
our tp world size is 8
8our tp world size is  8our tp world size is our tp world size is our tp world size is8
our tp world size is 
8

8
8
 8our tp world size is 8
our tp world size is our tp world size isour tp world size is our tp world size is our tp world size is 
8
our tp world size is 8
 88
8
8
our tp world size is our tp world size is 8
our tp world size is 
our tp world size isour tp world size is our tp world size is 8
8our tp world size is 8
our tp world size is  88
our tp world size is 
8
our tp world size is 88

our tp world size is 8
our tp world size isour tp world size is 8

our tp world size is our tp world size is 8
our tp world size is  8
our tp world size is our tp world size is 88
our tp world size is 8
8our tp world size is 8
8

our tp world size isour tp world size is 8our tp world size is 
8
our tp world size is our tp world size is 88
8
our tp world size is our tp world size is 8
 8

our tp world size is our tp world size is 8
8
our tp world size is 
our tp world size isour tp world size isour tp world size is 8
8
our tp world size is our tp world size is 8
 8 8our tp world size is our tp world size is8
8
our tp world size is 
8

8
 8our tp world size isour tp world size is8
our tp world size is our tp world size isour tp world size is our tp world size is
 8 8our tp world size is 8 88
 8our tp world size is 

8


our tp world size is
8
our tp world size is our tp world size is our tp world size is our tp world size is our tp world size is  our tp world size is our tp world size is 8
8
8
8
8
8
8
8
our tp world size isour tp world size is our tp world size is our tp world size isour tp world size is our tp world size is our tp world size is our tp world size is 88
8
 88
8
8 
our tp world size is our tp world size is 
our tp world size is our tp world size is 
8our tp world size is88
our tp world size is8
8
our tp world size is 
our tp world size is 8
our tp world size is  8our tp world size is our tp world size is 8
 
our tp world size is8

88
our tp world size is 8
our tp world size is 8our tp world size is our tp world size is
our tp world size is 8
our tp world size is  
8
 8our tp world size is 8
our tp world size is8
8
our tp world size is our tp world size is 
8our tp world size is our tp world size is our tp world size is 8
8
our tp world size is 
 888
8
our tp world size is our tp world size is 8our tp world size is 

our tp world size is our tp world size is 8
8
8
our tp world size is our tp world size is8
8
our tp world size is 
our tp world size isour tp world size is 8
 8our tp world size isour tp world size is8
our tp world size is 88
our tp world size is
 8 8our tp world size is  
our tp world size is  8our tp world size is 

our tp world size is8
8
our tp world size is8
our tp world size is8
our tp world size is  our tp world size is our tp world size is  8
 8our tp world size is 8
8
8
8

our tp world size is 
8our tp world size is our tp world size is our tp world size is our tp world size isour tp world size is 8
our tp world size is 
88
8
 88our tp world size is8
our tp world size is 
our tp world size is our tp world size is 

 8our tp world size is8our tp world size is8
8
our tp world size is our tp world size is 
 8
our tp world size is our tp world size is our tp world size is88
our tp world size is 
 88
 8
our tp world size is 8
our tp world size is 8

our tp world size is 
our tp world size is 8our tp world size is 8our tp world size is our tp world size is 8our tp world size is8

8

8
8

 8our tp world size is our tp world size is our tp world size is our tp world size is our tp world size is our tp world size is our tp world size is 
88
8
8
8
8
8
our tp world size is 
our tp world size is our tp world size is our tp world size isour tp world size is applying tp to block 0
our tp world size is 8
our tp world size is8
8 88applying tp to block 0
8our tp world size is  our tp world size is 


our tp world size isapplying tp to block 0
8
8
8
our tp world size isour tp world size is  8
our tp world size isour tp world size isour tp world size is our tp world size is  88
applying tp to block 0
 8 88
8

our tp world size is
our tp world size isapplying tp to block 0

our tp world size is
our tp world size is our tp world size is  our tp world size is  8applying tp to block 0 8our tp world size is8
88
8our tp world size is


 8our tp world size is 

 our tp world size is applying tp to block 0
our tp world size is
8our tp world size is our tp world size is 8
8
applying tp to block 0 8our tp world size is 
88
our tp world size is our tp world size is

8our tp world size is 
our tp world size is 8
 applying tp to block 0
our tp world size is 
8our tp world size is8
our tp world size is 8applying tp to block 0
8our tp world size is 
 8our tp world size is8
applying tp to block 0

8our tp world size is
 8
our tp world size isapplying tp to block 0
our tp world size is 
our tp world size is 8our tp world size is 
our tp world size is  8applying tp to block 08
 
8
our tp world size is 8


our tp world size is 8
our tp world size is our tp world size is 8
our tp world size is our tp world size is applying tp to block 08
our tp world size is 8
8
our tp world size is 8
8

our tp world size is8
our tp world size is our tp world size is 8
our tp world size is our tp world size is applying tp to block 0
 88
applying tp to block 0
8
our tp world size is 8
8
applying tp to block 0

applying tp to block 0
applying tp to block 0
our tp world size is 8
our tp world size is our tp world size is applying tp to block 0
our tp world size is applying tp to block 0
applying tp to block 08our tp world size is88
applying tp to block 0
8applying tp to block 0

 
our tp world size is applying tp to block 0


applying tp to block 0applying tp to block 0
our tp world size is8
applying tp to block 08
applying tp to block 0
our tp world size is 
applying tp to block 0applying tp to block 0
 8applying tp to block 0

Applying tensor parallel to model ...
applying tp to block 08

applying tp to block 0

applying tp to block 0
applying tp to block 0applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
our tp world size isapplying tp to block 0


applying tp to block 0
applying tp to block 0applying tp to block 0
applying tp to block 0
 8applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0

applying tp to block 0
applying tp to block 0

applying tp to block 0applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0applying tp to block 0

applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0

applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0applying tp to block 0applying tp to block 0applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0



applying tp to block 0applying tp to block 0
applying tp to block 0applying tp to block 0
applying tp to block 0applying tp to block 0
applying tp to block 0
applying tp to block 0

applying tp to block 0

applying tp to block 0

applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0applying tp to block 0
applying tp to block 0applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0

applying tp to block 0

applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0

applying tp to block 0
applying tp to block 0
applying tp to block 0applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
we finish operating the TP!
applying tp to block 0

applying tp to block 0
applying tp to block 0applying tp to block 0
applying tp to block 0applying tp to block 0
applying tp to block 0
applying tp to block 0applying tp to block 0
applying tp to block 0

applying tp to block 0
applying tp to block 0


applying tp to block 0
applying tp to block 0applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0applying tp to block 0
applying tp to block 0
applying tp to block 0applying tp to block 0
applying tp to block 0


applying tp to block 0applying tp to block 0

applying tp to block 0applying tp to block 0
applying tp to block 0applying tp to block 0

applying tp to block 0applying tp to block 0

applying tp to block 0applying tp to block 0

applying tp to block 0
applying tp to block 0
applying tp to block 0applying tp to block 0

applying tp to block 0applying tp to block 0applying tp to block 0

applying tp to block 0
applying tp to block 0


applying tp to block 0applying tp to block 0applying tp to block 0
applying tp to block 0

applying tp to block 0applying tp to block 0applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0




applying tp to block 0
applying tp to block 0
applying tp to block 0applying tp to block 0
applying tp to block 0applying tp to block 0applying tp to block 0
applying tp to block 0applying tp to block 0

applying tp to block 0


applying tp to block 0
applying tp to block 0applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0

applying tp to block 0
applying tp to block 0
applying tp to block 0applying tp to block 0
applying tp to block 0applying tp to block 0

applying tp to block 0applying tp to block 0
applying tp to block 0
applying tp to block 0

applying tp to block 0
applying tp to block 0
applying tp to block 0applying tp to block 0applying tp to block 0
applying tp to block 0

applying tp to block 0



applying tp to block 0
applying tp to block 0applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0applying tp to block 0
applying tp to block 0
applying tp to block 0applying tp to block 0
applying tp to block 0

applying tp to block 0
applying tp to block 0

applying tp to block 0applying tp to block 0
applying tp to block 0

applying tp to block 0
applying tp to block 0applying tp to block 0

applying tp to block 0
applying tp to block 0
applying tp to block 0applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
we finish operating the TP!


applying tp to block 0applying tp to block 0applying tp to block 0applying tp to block 0
applying tp to block 0applying tp to block 0

applying tp to block 0

applying tp to block 0

applying tp to block 0

applying tp to block 0applying tp to block 0
applying tp to block 0we finish operating the TP!
applying tp to block 0we finish operating the TP!
we finish operating the TP!



applying tp to block 0
applying tp to block 0
we finish operating the TP!applying tp to block 0
we finish operating the TP!

applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
we finish operating the TP!
GPTResidual(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x TurboTransformerBlock(
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=768, bias=False)
        (wo): Linear(in_features=512, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=3584, bias=False)
        (w2): Linear(in_features=1792, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
we comment comm is False
models all reduce stream is None
Time to load model: 1.92 seconds
the shape of input is torch.Size([32, 1024])
[rank3]:[W CUDAGraph.cpp:145] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
tokens we generated: 1024
Time for prefill: 76.03 sec
Time for inference -4: 171.45 sec total, 5.97 tokens/sec
Bandwidth achieved: 16.70 GB/s
FLOPS achieved: 1.07 TF/s

the shape of input is torch.Size([32, 1024])
tokens we generated: 1024
Time for prefill: 1.22 sec
Time for inference -3: 9.48 sec total, 108.03 tokens/sec
Bandwidth achieved: 302.07 GB/s
FLOPS achieved: 19.33 TF/s

the shape of input is torch.Size([32, 1024])
tokens we generated: 1024
Time for prefill: 0.24 sec
Time for inference -2: 7.44 sec total, 137.68 tokens/sec
Bandwidth achieved: 384.97 GB/s
FLOPS achieved: 24.64 TF/s

the shape of input is torch.Size([32, 1024])
tokens we generated: 1024
Time for prefill: 0.24 sec
Time for inference -1: 7.41 sec total, 138.17 tokens/sec
Bandwidth achieved: 386.32 GB/s
FLOPS achieved: 24.72 TF/s

the shape of input is torch.Size([32, 1024])
Compilation time: 7.41 seconds
the shape of input is torch.Size([32, 1024])
tokens we generated: 1024
Time for prefill: 0.24 sec
Time for inference 1: 7.43 sec total, 137.78 tokens/sec
Bandwidth achieved: 385.23 GB/s
FLOPS achieved: 24.65 TF/s

the shape of input is torch.Size([32, 1024])
tokens we generated: 1024
Time for prefill: 0.24 sec
Time for inference 2: 7.44 sec total, 137.69 tokens/sec
Bandwidth achieved: 384.98 GB/s
FLOPS achieved: 24.64 TF/s

the shape of input is torch.Size([32, 1024])
tokens we generated: 1024
Time for prefill: 0.24 sec
Time for inference 3: 7.45 sec total, 137.52 tokens/sec
Bandwidth achieved: 384.50 GB/s
FLOPS achieved: 24.61 TF/s

the shape of input is torch.Size([32, 1024])
tokens we generated: 1024
Time for prefill: 0.24 sec
Time for inference 4: 7.45 sec total, 137.44 tokens/sec
Bandwidth achieved: 384.30 GB/s
FLOPS achieved: 24.60 TF/s

the shape of input is torch.Size([32, 1024])
tokens we generated: 1024
Time for prefill: 0.25 sec
Time for inference 5: 7.40 sec total, 138.47 tokens/sec
Bandwidth achieved: 387.17 GB/s
FLOPS achieved: 24.78 TF/s

the shape of input is torch.Size([32, 1024])
tokens we generated: 1024
Time for prefill: 0.24 sec
Time for inference 6: 7.40 sec total, 138.38 tokens/sec
Bandwidth achieved: 386.92 GB/s
FLOPS achieved: 24.76 TF/s

the shape of input is torch.Size([32, 1024])
tokens we generated: 1024
Time for prefill: 0.24 sec
Time for inference 7: 7.40 sec total, 138.34 tokens/sec
Bandwidth achieved: 386.79 GB/s
FLOPS achieved: 24.75 TF/s

the shape of input is torch.Size([32, 1024])
tokens we generated: 1024
Time for prefill: 0.24 sec
Time for inference 8: 7.40 sec total, 138.33 tokens/sec
Bandwidth achieved: 386.77 GB/s
FLOPS achieved: 24.75 TF/s

the shape of input is torch.Size([32, 1024])
tokens we generated: 1024
Time for prefill: 0.24 sec
Time for inference 9: 7.43 sec total, 137.89 tokens/sec
Bandwidth achieved: 385.53 GB/s
FLOPS achieved: 24.67 TF/s

the shape of input is torch.Size([32, 1024])
tokens we generated: 1024
Time for prefill: 0.24 sec
Time for inference 10: 7.40 sec total, 138.29 tokens/sec
Bandwidth achieved: 386.66 GB/s
FLOPS achieved: 24.75 TF/s

==========
Batch Size: 32
Prompt Length: 1024
Generated tokens: 1024
Average prefill latency: 0.32 sec
Average tokens/sec: 135.69
Memory used: 28.24 GB
