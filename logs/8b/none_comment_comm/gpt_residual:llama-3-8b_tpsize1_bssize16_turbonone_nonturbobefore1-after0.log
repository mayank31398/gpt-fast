Using device=cuda
Loading model ...
our tp world size is 1
our tp world size is 1
our tp world size is 1
our tp world size is 1
our tp world size is 1
our tp world size is 1
our tp world size is 1
our tp world size is 1
our tp world size is 1
our tp world size is 1
our tp world size is 1
our tp world size is 1
our tp world size is 1
our tp world size is 1
our tp world size is 1
our tp world size is 1
our tp world size is 1
our tp world size is 1
our tp world size is 1
our tp world size is 1
our tp world size is 1
our tp world size is 1
our tp world size is 1
our tp world size is 1
our tp world size is 1
our tp world size is 1
our tp world size is 1
our tp world size is 1
our tp world size is 1
our tp world size is 1
our tp world size is 1
our tp world size is 1
GPTResidual(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x TurboTransformerBlock(
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
        (wo): Linear(in_features=4096, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=28672, bias=False)
        (w2): Linear(in_features=14336, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
we comment comm is True
models all reduce stream is None
Time to load model: 0.36 seconds
the shape of input is torch.Size([16, 1024])
tokens we generated: 1024
Time for prefill: 110.75 sec
Time for inference -4: 226.15 sec total, 4.53 tokens/sec
Bandwidth achieved: 67.96 GB/s
FLOPS achieved: 2.17 TF/s

the shape of input is torch.Size([16, 1024])
tokens we generated: 1024
Time for prefill: 0.90 sec
Time for inference -3: 13.45 sec total, 76.11 tokens/sec
Bandwidth achieved: 1142.46 GB/s
FLOPS achieved: 36.56 TF/s

the shape of input is torch.Size([16, 1024])
tokens we generated: 1024
Time for prefill: 0.56 sec
Time for inference -2: 12.84 sec total, 79.76 tokens/sec
Bandwidth achieved: 1197.17 GB/s
FLOPS achieved: 38.31 TF/s

the shape of input is torch.Size([16, 1024])
tokens we generated: 1024
Time for prefill: 0.56 sec
Time for inference -1: 12.84 sec total, 79.74 tokens/sec
Bandwidth achieved: 1196.84 GB/s
FLOPS achieved: 38.30 TF/s

the shape of input is torch.Size([16, 1024])
Compilation time: 12.85 seconds
the shape of input is torch.Size([16, 1024])
tokens we generated: 1024
Time for prefill: 0.56 sec
Time for inference 1: 12.84 sec total, 79.74 tokens/sec
Bandwidth achieved: 1196.85 GB/s
FLOPS achieved: 38.30 TF/s

the shape of input is torch.Size([16, 1024])
tokens we generated: 1024
Time for prefill: 0.56 sec
Time for inference 2: 12.84 sec total, 79.76 tokens/sec
Bandwidth achieved: 1197.17 GB/s
FLOPS achieved: 38.31 TF/s

the shape of input is torch.Size([16, 1024])
tokens we generated: 1024
Time for prefill: 0.56 sec
Time for inference 3: 12.84 sec total, 79.75 tokens/sec
Bandwidth achieved: 1197.05 GB/s
FLOPS achieved: 38.31 TF/s

the shape of input is torch.Size([16, 1024])
tokens we generated: 1024
Time for prefill: 0.56 sec
Time for inference 4: 12.84 sec total, 79.73 tokens/sec
Bandwidth achieved: 1196.70 GB/s
FLOPS achieved: 38.29 TF/s

the shape of input is torch.Size([16, 1024])
tokens we generated: 1024
Time for prefill: 0.57 sec
Time for inference 5: 12.84 sec total, 79.73 tokens/sec
Bandwidth achieved: 1196.74 GB/s
FLOPS achieved: 38.30 TF/s

the shape of input is torch.Size([16, 1024])
tokens we generated: 1024
Time for prefill: 0.56 sec
Time for inference 6: 12.86 sec total, 79.63 tokens/sec
Bandwidth achieved: 1195.22 GB/s
FLOPS achieved: 38.25 TF/s

the shape of input is torch.Size([16, 1024])
tokens we generated: 1024
Time for prefill: 0.56 sec
Time for inference 7: 12.85 sec total, 79.71 tokens/sec
Bandwidth achieved: 1196.41 GB/s
FLOPS achieved: 38.29 TF/s

the shape of input is torch.Size([16, 1024])
tokens we generated: 1024
Time for prefill: 0.57 sec
Time for inference 8: 12.85 sec total, 79.66 tokens/sec
Bandwidth achieved: 1195.72 GB/s
FLOPS achieved: 38.26 TF/s

the shape of input is torch.Size([16, 1024])
tokens we generated: 1024
Time for prefill: 0.57 sec
Time for inference 9: 12.85 sec total, 79.69 tokens/sec
Bandwidth achieved: 1196.10 GB/s
FLOPS achieved: 38.28 TF/s

the shape of input is torch.Size([16, 1024])
tokens we generated: 1024
Time for prefill: 0.56 sec
Time for inference 10: 12.84 sec total, 79.72 tokens/sec
Bandwidth achieved: 1196.59 GB/s
FLOPS achieved: 38.29 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 1024
Average prefill latency: 0.59 sec
Average tokens/sec: 79.44
Memory used: 41.75 GB
