W1119 16:28:34.791000 2999041 site-packages/torch/distributed/run.py:793] 
W1119 16:28:34.791000 2999041 site-packages/torch/distributed/run.py:793] *****************************************
W1119 16:28:34.791000 2999041 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1119 16:28:34.791000 2999041 site-packages/torch/distributed/run.py:793] *****************************************
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
flash_kv_decode is set to True
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
our world size=4
Using device=cuda
Loading model ...
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
GPTLadder(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x LadderTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=2560, bias=False)
        (wo): Linear(in_features=2048, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=14336, bias=False)
        (w2): Linear(in_features=7168, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 0.34 seconds
CUDA_GRAPH are activate
[rank0]:[W1119 16:28:44.312906516 CUDAGraph.cpp:133] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank3]:[W1119 16:28:44.322641013 CUDAGraph.cpp:133] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
Prefill latency: 0.3938918758649379 sec
Decode latency: 0.6782924390863627 sec
Compilation time: 1.05 secondsCompilation time: 1.01 seconds

Compilation time: 1.01 seconds
Compilation time: 1.07 seconds
Prefill latency: 0.32623222609981894 sec
Decode latency: 0.6782264620997012 sec
Prefill latency: 0.3258920689113438 sec
Decode latency: 0.6781422151252627 sec
Prefill latency: 0.32376072905026376 sec
Decode latency: 0.6782342239748687 sec
Prefill latency: 0.3247854229994118 sec
Decode latency: 0.6785304080694914 sec
Prefill latency: 0.32551302295178175 sec
Decode latency: 0.6783291839528829 sec
Time for inference 1: 1.00 sec total, 127.42 tokens/sec
Decode latency: 0.68 sec
Prefill latency: 0.33 sec
Bandwidth achieved: 4629.28 GB/s
FLOPS achieved: 152.77 TF/s

Prefill latency: 0.3276192650664598 sec
Decode latency: 0.6783382790163159 sec
Time for inference 2: 1.01 sec total, 127.17 tokens/sec
Decode latency: 0.68 sec
Prefill latency: 0.33 sec
Bandwidth achieved: 4619.85 GB/s
FLOPS achieved: 152.45 TF/s

Prefill latency: 0.3305092719383538 sec
Decode latency: 0.6789555901195854 sec
Time for inference 3: 1.01 sec total, 126.73 tokens/sec
Decode latency: 0.68 sec
Prefill latency: 0.33 sec
Bandwidth achieved: 4603.88 GB/s
FLOPS achieved: 151.93 TF/s

Prefill latency: 0.32580675883218646 sec
Decode latency: 0.6786799200344831 sec
Time for inference 4: 1.01 sec total, 127.36 tokens/sec
Decode latency: 0.68 sec
Prefill latency: 0.33 sec
Bandwidth achieved: 4626.96 GB/s
FLOPS achieved: 152.69 TF/s

Prefill latency: 0.3246330190449953 sec
Decode latency: 0.6792621989734471 sec
Time for inference 5: 1.00 sec total, 127.43 tokens/sec
Decode latency: 0.68 sec
Prefill latency: 0.32 sec
Bandwidth achieved: 4629.43 GB/s
FLOPS achieved: 152.77 TF/s

Prefill latency: 0.3254596299957484 sec
Decode latency: 0.6787018349859864 sec
Time for inference 6: 1.00 sec total, 127.40 tokens/sec
Decode latency: 0.68 sec
Prefill latency: 0.33 sec
Bandwidth achieved: 4628.40 GB/s
FLOPS achieved: 152.74 TF/s

Prefill latency: 0.32406945317052305 sec
Decode latency: 0.6787595350760967 sec
Time for inference 7: 1.00 sec total, 127.57 tokens/sec
Decode latency: 0.68 sec
Prefill latency: 0.32 sec
Bandwidth achieved: 4634.53 GB/s
FLOPS achieved: 152.94 TF/s

Prefill latency: 0.330613647820428 sec
Decode latency: 0.6786159728653729 sec
Time for inference 8: 1.01 sec total, 126.76 tokens/sec
Decode latency: 0.68 sec
Prefill latency: 0.33 sec
Bandwidth achieved: 4604.98 GB/s
FLOPS achieved: 151.96 TF/s

Prefill latency: 0.3252860461361706 sec
Decode latency: 0.6782395218033344 sec
Time for inference 9: 1.00 sec total, 127.49 tokens/sec
Decode latency: 0.68 sec
Prefill latency: 0.33 sec
Bandwidth achieved: 4631.69 GB/s
FLOPS achieved: 152.85 TF/s

Prefill latency: 0.32500703912228346 sec
Decode latency: 0.6775588269811124 sec
Time for inference 10: 1.00 sec total, 127.60 tokens/sec
Decode latency: 0.68 sec
Prefill latency: 0.33 sec
Bandwidth achieved: 4635.56 GB/s
FLOPS achieved: 152.97 TF/s

==========
Batch Size: 4
Prompt Length: 1024
Generated tokens: 32
Average decode latency: 0.6785 sec
Average prefill latency: 0.3265 sec
Average tokens/sec: 127.29
Memory used: 55.25 GB
Done. we are killing the process
[rank0]:[W1119 16:29:00.850622337 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
