W1119 16:27:00.316000 2997996 site-packages/torch/distributed/run.py:793] 
W1119 16:27:00.316000 2997996 site-packages/torch/distributed/run.py:793] *****************************************
W1119 16:27:00.316000 2997996 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1119 16:27:00.316000 2997996 site-packages/torch/distributed/run.py:793] *****************************************
rank: 1, global_rank: 1, world_size: 2, global_world_size: 2
rank: 0, global_rank: 0, world_size: 2, global_world_size: 2
rank: 0, global_rank: 0, world_size: 2, global_world_size: 2
rank: 0, global_rank: 0, world_size: 2, global_world_size: 2
rank: 0, global_rank: 0, world_size: 2, global_world_size: 2
rank: 0, global_rank: 0, world_size: 2, global_world_size: 2
flash_kv_decode is set to True
rank: 0, global_rank: 0, world_size: 2, global_world_size: 2
our world size=2
Using device=cuda
Loading model ...
rank: 1, global_rank: 1, world_size: 2, global_world_size: 2
rank: 1, global_rank: 1, world_size: 2, global_world_size: 2
rank: 1, global_rank: 1, world_size: 2, global_world_size: 2
rank: 1, global_rank: 1, world_size: 2, global_world_size: 2
rank: 1, global_rank: 1, world_size: 2, global_world_size: 2
GPTLadder(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x LadderTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=5120, bias=False)
        (wo): Linear(in_features=4096, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=28672, bias=False)
        (w2): Linear(in_features=14336, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 0.38 seconds
CUDA_GRAPH are activate
[rank0]:[W1119 16:27:07.098136717 CUDAGraph.cpp:133] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
Prefill latency: 0.16213110787793994 sec
Decode latency: 1.0035609651822597 sec
Compilation time: 1.17 seconds
Compilation time: 1.17 seconds
Prefill latency: 0.16152527392841876 sec
Decode latency: 1.0045608570799232 sec
Prefill latency: 0.16113668587058783 sec
Decode latency: 1.0047606711741537 sec
Prefill latency: 0.16202297084964812 sec
Decode latency: 1.0046195869799703 sec
Prefill latency: 0.16175021999515593 sec
Decode latency: 1.0020035249181092 sec
Prefill latency: 0.16139331902377307 sec
Decode latency: 1.0047160650137812 sec
Time for inference 1: 1.17 sec total, 27.43 tokens/sec
Decode latency: 1.00 sec
Prefill latency: 0.16 sec
Bandwidth achieved: 1935.25 GB/s
FLOPS achieved: 63.86 TF/s

Prefill latency: 0.1618243339471519 sec
Decode latency: 1.004465535050258 sec
Time for inference 2: 1.17 sec total, 27.43 tokens/sec
Decode latency: 1.00 sec
Prefill latency: 0.16 sec
Bandwidth achieved: 1934.98 GB/s
FLOPS achieved: 63.85 TF/s

Prefill latency: 0.16317268321290612 sec
Decode latency: 1.0034227559808642 sec
Time for inference 3: 1.17 sec total, 27.42 tokens/sec
Decode latency: 1.00 sec
Prefill latency: 0.16 sec
Bandwidth achieved: 1934.41 GB/s
FLOPS achieved: 63.84 TF/s

Prefill latency: 0.1614752309396863 sec
Decode latency: 1.0045043230056763 sec
Time for inference 4: 1.17 sec total, 27.43 tokens/sec
Decode latency: 1.00 sec
Prefill latency: 0.16 sec
Bandwidth achieved: 1935.49 GB/s
FLOPS achieved: 63.87 TF/s

Prefill latency: 0.1640683540608734 sec
Decode latency: 1.0043830692302436 sec
Time for inference 5: 1.17 sec total, 27.37 tokens/sec
Decode latency: 1.00 sec
Prefill latency: 0.16 sec
Bandwidth achieved: 1931.40 GB/s
FLOPS achieved: 63.74 TF/s

Prefill latency: 0.16179523803293705 sec
Decode latency: 1.0047092561144382 sec
Time for inference 6: 1.17 sec total, 27.42 tokens/sec
Decode latency: 1.00 sec
Prefill latency: 0.16 sec
Bandwidth achieved: 1934.62 GB/s
FLOPS achieved: 63.84 TF/s

Prefill latency: 0.16062071407213807 sec
Decode latency: 1.0037837328854948 sec
Time for inference 7: 1.16 sec total, 27.47 tokens/sec
Decode latency: 1.00 sec
Prefill latency: 0.16 sec
Bandwidth achieved: 1938.11 GB/s
FLOPS achieved: 63.96 TF/s

Prefill latency: 0.16233860095962882 sec
Decode latency: 1.003765907138586 sec
Time for inference 8: 1.17 sec total, 27.43 tokens/sec
Decode latency: 1.00 sec
Prefill latency: 0.16 sec
Bandwidth achieved: 1935.28 GB/s
FLOPS achieved: 63.86 TF/s

Prefill latency: 0.16308903088793159 sec
Decode latency: 1.0035918140783906 sec
Time for inference 9: 1.17 sec total, 27.42 tokens/sec
Decode latency: 1.00 sec
Prefill latency: 0.16 sec
Bandwidth achieved: 1934.30 GB/s
FLOPS achieved: 63.83 TF/s

Prefill latency: 0.16091808513738215 sec
Decode latency: 1.0033253298606724 sec
Time for inference 10: 1.16 sec total, 27.47 tokens/sec
Decode latency: 1.00 sec
Prefill latency: 0.16 sec
Bandwidth achieved: 1938.33 GB/s
FLOPS achieved: 63.96 TF/s

==========
Batch Size: 1
Prompt Length: 1024
Generated tokens: 32
Average decode latency: 1.0041 sec
Average prefill latency: 0.1621 sec
Average tokens/sec: 27.43
Memory used: 78.01 GB
Done. we are killing the process
[rank0]:[W1119 16:27:25.963804633 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
