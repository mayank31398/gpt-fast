W1119 16:30:07.338000 3000126 site-packages/torch/distributed/run.py:793] 
W1119 16:30:07.338000 3000126 site-packages/torch/distributed/run.py:793] *****************************************
W1119 16:30:07.338000 3000126 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1119 16:30:07.338000 3000126 site-packages/torch/distributed/run.py:793] *****************************************
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
flash_kv_decode is set to True
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
our world size=8
Using device=cuda
Loading model ...
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
GPTLadder(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x LadderTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=1280, bias=False)
        (wo): Linear(in_features=1024, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 0.72 seconds
CUDA_GRAPH are activate
Prefill latency: 0.7901150991674513 sec
Decode latency: 0.535475960932672 sec
Compilation time: 1.27 seconds
Compilation time: 1.27 seconds
Compilation time: 1.27 secondsCompilation time: 1.32 seconds

Compilation time: 1.26 seconds
Compilation time: 1.24 seconds
Compilation time: 1.30 seconds
Compilation time: 1.33 seconds
Prefill latency: 0.7056750629562885 sec
Decode latency: 0.5345622729510069 sec
Prefill latency: 0.7087914841249585 sec
Decode latency: 0.5320958809461445 sec
Prefill latency: 0.7077165439259261 sec
Decode latency: 0.5347827218938619 sec
Prefill latency: 0.7072210649494082 sec
Decode latency: 0.5328610690776259 sec
Prefill latency: 0.7082695940043777 sec
Decode latency: 0.5342367209959775 sec
Time for inference 1: 1.24 sec total, 411.87 tokens/sec
Decode latency: 0.53 sec
Prefill latency: 0.71 sec
Bandwidth achieved: 7914.84 GB/s
FLOPS achieved: 261.19 TF/s

Prefill latency: 0.7083437060937285 sec
Decode latency: 0.5326757519505918 sec
Time for inference 2: 1.24 sec total, 412.35 tokens/sec
Decode latency: 0.53 sec
Prefill latency: 0.71 sec
Bandwidth achieved: 7924.05 GB/s
FLOPS achieved: 261.49 TF/s

Prefill latency: 0.7083474269602448 sec
Decode latency: 0.5320422530639917 sec
Time for inference 3: 1.24 sec total, 412.57 tokens/sec
Decode latency: 0.53 sec
Prefill latency: 0.71 sec
Bandwidth achieved: 7928.22 GB/s
FLOPS achieved: 261.63 TF/s

Prefill latency: 0.709084850968793 sec
Decode latency: 0.5318757579661906 sec
Time for inference 4: 1.24 sec total, 412.39 tokens/sec
Decode latency: 0.53 sec
Prefill latency: 0.71 sec
Bandwidth achieved: 7924.77 GB/s
FLOPS achieved: 261.52 TF/s

Prefill latency: 0.7073621889576316 sec
Decode latency: 0.532378694973886 sec
Time for inference 5: 1.24 sec total, 412.63 tokens/sec
Decode latency: 0.53 sec
Prefill latency: 0.71 sec
Bandwidth achieved: 7929.39 GB/s
FLOPS achieved: 261.67 TF/s

Prefill latency: 0.7100402591750026 sec
Decode latency: 0.5326762821059674 sec
Time for inference 6: 1.24 sec total, 411.73 tokens/sec
Decode latency: 0.53 sec
Prefill latency: 0.71 sec
Bandwidth achieved: 7912.14 GB/s
FLOPS achieved: 261.10 TF/s

Prefill latency: 0.7098211790435016 sec
Decode latency: 0.5333188769873232 sec
Time for inference 7: 1.24 sec total, 411.65 tokens/sec
Decode latency: 0.53 sec
Prefill latency: 0.71 sec
Bandwidth achieved: 7910.58 GB/s
FLOPS achieved: 261.05 TF/s

Prefill latency: 0.7081154501065612 sec
Decode latency: 0.5361921859439462 sec
Time for inference 8: 1.24 sec total, 411.27 tokens/sec
Decode latency: 0.54 sec
Prefill latency: 0.71 sec
Bandwidth achieved: 7903.31 GB/s
FLOPS achieved: 260.81 TF/s

Prefill latency: 0.708664155099541 sec
Decode latency: 0.5337336738593876 sec
Time for inference 9: 1.24 sec total, 411.92 tokens/sec
Decode latency: 0.53 sec
Prefill latency: 0.71 sec
Bandwidth achieved: 7915.78 GB/s
FLOPS achieved: 261.22 TF/s

Prefill latency: 0.7097028840798885 sec
Decode latency: 0.5322900258470327 sec
Time for inference 10: 1.24 sec total, 412.05 tokens/sec
Decode latency: 0.53 sec
Prefill latency: 0.71 sec
Bandwidth achieved: 7918.22 GB/s
FLOPS achieved: 261.30 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 32
Average decode latency: 0.5331 sec
Average prefill latency: 0.7088 sec
Average tokens/sec: 412.04
Memory used: 71.09 GB
Done. we are killing the process
[rank0]:[W1119 16:30:43.951375016 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
