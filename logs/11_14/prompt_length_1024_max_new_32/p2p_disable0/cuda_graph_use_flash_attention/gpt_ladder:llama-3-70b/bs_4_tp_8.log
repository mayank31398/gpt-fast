W1119 16:29:03.452000 2999272 site-packages/torch/distributed/run.py:793] 
W1119 16:29:03.452000 2999272 site-packages/torch/distributed/run.py:793] *****************************************
W1119 16:29:03.452000 2999272 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1119 16:29:03.452000 2999272 site-packages/torch/distributed/run.py:793] *****************************************
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
flash_kv_decode is set to True
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
our world size=8
Using device=cuda
Loading model ...
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
GPTLadder(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x LadderTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=1280, bias=False)
        (wo): Linear(in_features=1024, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 0.62 seconds
CUDA_GRAPH are activate
Prefill latency: 0.27801766199991107 sec
Decode latency: 0.48861598293296993 sec
Compilation time: 0.75 seconds
Compilation time: 0.68 seconds
Compilation time: 0.71 secondsCompilation time: 0.75 secondsCompilation time: 0.72 seconds


Compilation time: 0.73 seconds
Compilation time: 0.68 seconds
Compilation time: 0.77 seconds
Prefill latency: 0.1875778289977461 sec
Decode latency: 0.48932867613621056 sec
Prefill latency: 0.18739874404855072 sec
Decode latency: 0.48851308482699096 sec
Prefill latency: 0.1877820109948516 sec
Decode latency: 0.4890860868617892 sec
Prefill latency: 0.18713480490259826 sec
Decode latency: 0.48830377496778965 sec
Prefill latency: 0.187671666033566 sec
Decode latency: 0.4884698048699647 sec
Time for inference 1: 0.68 sec total, 189.13 tokens/sec
Decode latency: 0.49 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 3634.52 GB/s
FLOPS achieved: 119.94 TF/s

Prefill latency: 0.18834875710308552 sec
Decode latency: 0.48925406602211297 sec
Time for inference 2: 0.68 sec total, 188.75 tokens/sec
Decode latency: 0.49 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 3627.12 GB/s
FLOPS achieved: 119.70 TF/s

Prefill latency: 0.18795810802839696 sec
Decode latency: 0.48932301602326334 sec
Time for inference 3: 0.68 sec total, 188.80 tokens/sec
Decode latency: 0.49 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 3628.19 GB/s
FLOPS achieved: 119.73 TF/s

Prefill latency: 0.1877946089953184 sec
Decode latency: 0.4889643311034888 sec
Time for inference 4: 0.68 sec total, 188.95 tokens/sec
Decode latency: 0.49 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 3631.03 GB/s
FLOPS achieved: 119.82 TF/s

Prefill latency: 0.18721134215593338 sec
Decode latency: 0.4883685258682817 sec
Time for inference 5: 0.68 sec total, 189.29 tokens/sec
Decode latency: 0.49 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 3637.48 GB/s
FLOPS achieved: 120.04 TF/s

Prefill latency: 0.18757209414616227 sec
Decode latency: 0.4884353370871395 sec
Time for inference 6: 0.68 sec total, 189.18 tokens/sec
Decode latency: 0.49 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 3635.43 GB/s
FLOPS achieved: 119.97 TF/s

Prefill latency: 0.18782282411120832 sec
Decode latency: 0.48907173704355955 sec
Time for inference 7: 0.68 sec total, 188.95 tokens/sec
Decode latency: 0.49 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 3631.06 GB/s
FLOPS achieved: 119.82 TF/s

Prefill latency: 0.18802027706988156 sec
Decode latency: 0.48995171999558806 sec
Time for inference 8: 0.68 sec total, 188.66 tokens/sec
Decode latency: 0.49 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 3625.37 GB/s
FLOPS achieved: 119.64 TF/s

Prefill latency: 0.18839092296548188 sec
Decode latency: 0.48895531613379717 sec
Time for inference 9: 0.68 sec total, 188.82 tokens/sec
Decode latency: 0.49 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 3628.49 GB/s
FLOPS achieved: 119.74 TF/s

Prefill latency: 0.18820041604340076 sec
Decode latency: 0.4888317510485649 sec
Time for inference 10: 0.68 sec total, 188.92 tokens/sec
Decode latency: 0.49 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 3630.39 GB/s
FLOPS achieved: 119.80 TF/s

==========
Batch Size: 4
Prompt Length: 1024
Generated tokens: 32
Average decode latency: 0.4890 sec
Average prefill latency: 0.1879 sec
Average tokens/sec: 188.95
Memory used: 33.84 GB
Done. we are killing the process
[rank0]:[W1119 16:29:29.713865832 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
