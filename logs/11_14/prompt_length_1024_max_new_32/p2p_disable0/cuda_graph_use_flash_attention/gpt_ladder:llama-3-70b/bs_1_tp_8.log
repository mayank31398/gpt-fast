W1119 16:27:52.075000 2998397 site-packages/torch/distributed/run.py:793] 
W1119 16:27:52.075000 2998397 site-packages/torch/distributed/run.py:793] *****************************************
W1119 16:27:52.075000 2998397 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1119 16:27:52.075000 2998397 site-packages/torch/distributed/run.py:793] *****************************************
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
flash_kv_decode is set to True
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
our world size=8
Using device=cuda
Loading model ...
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
GPTLadder(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x LadderTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=1280, bias=False)
        (wo): Linear(in_features=1024, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 0.59 seconds
CUDA_GRAPH are activate
Prefill latency: 0.06371046998538077 sec
Decode latency: 0.46394351590424776 sec
Compilation time: 0.56 seconds
Compilation time: 0.59 seconds
Compilation time: 0.56 seconds
Compilation time: 0.53 seconds
Compilation time: 0.58 seconds
Compilation time: 0.57 seconds
Compilation time: 0.53 seconds
Compilation time: 0.53 seconds
Prefill latency: 0.06212199083529413 sec
Decode latency: 0.4638284391257912 sec
Prefill latency: 0.062153434148058295 sec
Decode latency: 0.46389529295265675 sec
Prefill latency: 0.06212678994052112 sec
Decode latency: 0.4636537099722773 sec
Prefill latency: 0.062157050939276814 sec
Decode latency: 0.4638076489791274 sec
Prefill latency: 0.06222502002492547 sec
Decode latency: 0.46373530686832964 sec
Time for inference 1: 0.53 sec total, 60.76 tokens/sec
Decode latency: 0.46 sec
Prefill latency: 0.06 sec
Bandwidth achieved: 1167.56 GB/s
FLOPS achieved: 38.53 TF/s

Prefill latency: 0.06221025809645653 sec
Decode latency: 0.46379550779238343 sec
Time for inference 2: 0.53 sec total, 60.76 tokens/sec
Decode latency: 0.46 sec
Prefill latency: 0.06 sec
Bandwidth achieved: 1167.70 GB/s
FLOPS achieved: 38.53 TF/s

Prefill latency: 0.062095808098092675 sec
Decode latency: 0.4639585060067475 sec
Time for inference 3: 0.53 sec total, 60.76 tokens/sec
Decode latency: 0.46 sec
Prefill latency: 0.06 sec
Bandwidth achieved: 1167.63 GB/s
FLOPS achieved: 38.53 TF/s

Prefill latency: 0.062136526918038726 sec
Decode latency: 0.4638771009631455 sec
Time for inference 4: 0.53 sec total, 60.77 tokens/sec
Decode latency: 0.46 sec
Prefill latency: 0.06 sec
Bandwidth achieved: 1167.82 GB/s
FLOPS achieved: 38.54 TF/s

Prefill latency: 0.06219112384133041 sec
Decode latency: 0.46370934415608644 sec
Time for inference 5: 0.53 sec total, 60.78 tokens/sec
Decode latency: 0.46 sec
Prefill latency: 0.06 sec
Bandwidth achieved: 1168.06 GB/s
FLOPS achieved: 38.55 TF/s

Prefill latency: 0.06216723006218672 sec
Decode latency: 0.4637874630279839 sec
Time for inference 6: 0.53 sec total, 60.78 tokens/sec
Decode latency: 0.46 sec
Prefill latency: 0.06 sec
Bandwidth achieved: 1168.04 GB/s
FLOPS achieved: 38.55 TF/s

Prefill latency: 0.062088476959615946 sec
Decode latency: 0.46372087905183434 sec
Time for inference 7: 0.53 sec total, 60.78 tokens/sec
Decode latency: 0.46 sec
Prefill latency: 0.06 sec
Bandwidth achieved: 1168.03 GB/s
FLOPS achieved: 38.55 TF/s

Prefill latency: 0.06226337794214487 sec
Decode latency: 0.4636792941018939 sec
Time for inference 8: 0.53 sec total, 60.78 tokens/sec
Decode latency: 0.46 sec
Prefill latency: 0.06 sec
Bandwidth achieved: 1168.00 GB/s
FLOPS achieved: 38.54 TF/s

Prefill latency: 0.06224104296416044 sec
Decode latency: 0.4638794579077512 sec
Time for inference 9: 0.53 sec total, 60.76 tokens/sec
Decode latency: 0.46 sec
Prefill latency: 0.06 sec
Bandwidth achieved: 1167.62 GB/s
FLOPS achieved: 38.53 TF/s

Prefill latency: 0.062128666089847684 sec
Decode latency: 0.4636930769775063 sec
Time for inference 10: 0.53 sec total, 60.79 tokens/sec
Decode latency: 0.46 sec
Prefill latency: 0.06 sec
Bandwidth achieved: 1168.16 GB/s
FLOPS achieved: 38.55 TF/s

==========
Batch Size: 1
Prompt Length: 1024
Generated tokens: 32
Average decode latency: 0.4638 sec
Average prefill latency: 0.0622 sec
Average tokens/sec: 60.77
Memory used: 24.87 GB
Done. we are killing the process
[rank0]:[W1119 16:28:15.767251971 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
