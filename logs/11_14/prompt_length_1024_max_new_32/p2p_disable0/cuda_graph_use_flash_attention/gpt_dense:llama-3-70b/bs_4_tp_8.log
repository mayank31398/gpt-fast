W1119 16:23:36.366000 2993230 site-packages/torch/distributed/run.py:793] 
W1119 16:23:36.366000 2993230 site-packages/torch/distributed/run.py:793] *****************************************
W1119 16:23:36.366000 2993230 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1119 16:23:36.366000 2993230 site-packages/torch/distributed/run.py:793] *****************************************
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
flash_kv_decode is set to True
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
our world size=8
Using device=cuda
Loading model ...
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8rank: 3, global_rank: 3, world_size: 8, global_world_size: 8

rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
GPTDense(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x DenseTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=1280, bias=False)
        (wo): Linear(in_features=1024, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 1.47 seconds
CUDA_GRAPH are activate
Prefill latency: 0.2756963598076254 sec
Decode latency: 0.6018564170226455 sec
Compilation time: 0.92 secondsCompilation time: 0.83 seconds
Compilation time: 0.85 seconds
Compilation time: 0.82 seconds

Compilation time: 0.92 seconds
Compilation time: 0.89 seconds
Compilation time: 0.88 seconds
Compilation time: 0.83 seconds
Prefill latency: 0.21526859188452363 sec
Decode latency: 0.599395832978189 sec
Prefill latency: 0.21586887701414526 sec
Decode latency: 0.6001586250495166 sec
Prefill latency: 0.215727613074705 sec
Decode latency: 0.6001342998351902 sec
Prefill latency: 0.2161885779350996 sec
Decode latency: 0.6010330780409276 sec
Prefill latency: 0.21595806698314846 sec
Decode latency: 0.601746184984222 sec
Time for inference 1: 0.82 sec total, 156.43 tokens/sec
Decode latency: 0.60 sec
Prefill latency: 0.22 sec
Bandwidth achieved: 3006.15 GB/s
FLOPS achieved: 99.20 TF/s

Prefill latency: 0.21596640581265092 sec
Decode latency: 0.6015214829239994 sec
Time for inference 2: 0.82 sec total, 156.48 tokens/sec
Decode latency: 0.60 sec
Prefill latency: 0.22 sec
Bandwidth achieved: 3006.99 GB/s
FLOPS achieved: 99.23 TF/s

Prefill latency: 0.21548128710128367 sec
Decode latency: 0.6011619518976659 sec
Time for inference 3: 0.82 sec total, 156.60 tokens/sec
Decode latency: 0.60 sec
Prefill latency: 0.22 sec
Bandwidth achieved: 3009.42 GB/s
FLOPS achieved: 99.31 TF/s

Prefill latency: 0.2158644360024482 sec
Decode latency: 0.6006068650167435 sec
Time for inference 4: 0.82 sec total, 156.67 tokens/sec
Decode latency: 0.60 sec
Prefill latency: 0.22 sec
Bandwidth achieved: 3010.74 GB/s
FLOPS achieved: 99.35 TF/s

Prefill latency: 0.2156974789686501 sec
Decode latency: 0.5999512108974159 sec
Time for inference 5: 0.82 sec total, 156.82 tokens/sec
Decode latency: 0.60 sec
Prefill latency: 0.22 sec
Bandwidth achieved: 3013.49 GB/s
FLOPS achieved: 99.45 TF/s

Prefill latency: 0.21590271196328104 sec
Decode latency: 0.6001927307806909 sec
Time for inference 6: 0.82 sec total, 156.73 tokens/sec
Decode latency: 0.60 sec
Prefill latency: 0.22 sec
Bandwidth achieved: 3011.87 GB/s
FLOPS achieved: 99.39 TF/s

Prefill latency: 0.2165112670045346 sec
Decode latency: 0.6007368019782007 sec
Time for inference 7: 0.82 sec total, 156.53 tokens/sec
Decode latency: 0.60 sec
Prefill latency: 0.22 sec
Bandwidth achieved: 3007.90 GB/s
FLOPS achieved: 99.26 TF/s

Prefill latency: 0.21643286291509867 sec
Decode latency: 0.6009226408787072 sec
Time for inference 8: 0.82 sec total, 156.48 tokens/sec
Decode latency: 0.60 sec
Prefill latency: 0.22 sec
Bandwidth achieved: 3007.08 GB/s
FLOPS achieved: 99.23 TF/s

Prefill latency: 0.2161687919870019 sec
Decode latency: 0.6011761538684368 sec
Time for inference 9: 0.82 sec total, 156.50 tokens/sec
Decode latency: 0.60 sec
Prefill latency: 0.22 sec
Bandwidth achieved: 3007.37 GB/s
FLOPS achieved: 99.24 TF/s

Prefill latency: 0.2160011960659176 sec
Decode latency: 0.6004018760286272 sec
Time for inference 10: 0.82 sec total, 156.68 tokens/sec
Decode latency: 0.60 sec
Prefill latency: 0.22 sec
Bandwidth achieved: 3010.96 GB/s
FLOPS achieved: 99.36 TF/s

==========
Batch Size: 4
Prompt Length: 1024
Generated tokens: 32
Average decode latency: 0.6008 sec
Average prefill latency: 0.2160 sec
Average tokens/sec: 156.59
Memory used: 33.84 GB
Done. we are killing the process
[rank0]:[W1119 16:24:06.029043317 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
