W1119 16:22:16.171000 2991288 site-packages/torch/distributed/run.py:793] 
W1119 16:22:16.171000 2991288 site-packages/torch/distributed/run.py:793] *****************************************
W1119 16:22:16.171000 2991288 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1119 16:22:16.171000 2991288 site-packages/torch/distributed/run.py:793] *****************************************
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
flash_kv_decode is set to True
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
our world size=8
Using device=cuda
Loading model ...
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8rank: 5, global_rank: 5, world_size: 8, global_world_size: 8

rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
GPTDense(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x DenseTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=1280, bias=False)
        (wo): Linear(in_features=1024, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 1.35 seconds
CUDA_GRAPH are activate
Prefill latency: 0.08237199997529387 sec
Decode latency: 0.5724899121560156 sec
Compilation time: 0.66 seconds
Compilation time: 0.65 secondsCompilation time: 0.65 seconds

Compilation time: 0.69 seconds
Compilation time: 0.65 secondsCompilation time: 0.69 seconds

Compilation time: 0.64 seconds
Compilation time: 0.65 seconds
Prefill latency: 0.06898234714753926 sec
Decode latency: 0.572272741002962 sec
Prefill latency: 0.0694503968115896 sec
Decode latency: 0.5720748149324208 sec
Prefill latency: 0.06855274108238518 sec
Decode latency: 0.5717400810681283 sec
Prefill latency: 0.06965579302050173 sec
Decode latency: 0.5720392630901188 sec
Prefill latency: 0.06866978202015162 sec
Decode latency: 0.5725093099754304 sec
Time for inference 1: 0.64 sec total, 49.84 tokens/sec
Decode latency: 0.57 sec
Prefill latency: 0.07 sec
Bandwidth achieved: 957.70 GB/s
FLOPS achieved: 31.60 TF/s

Prefill latency: 0.0693069661501795 sec
Decode latency: 0.5719449040479958 sec
Time for inference 2: 0.64 sec total, 49.85 tokens/sec
Decode latency: 0.57 sec
Prefill latency: 0.07 sec
Bandwidth achieved: 957.91 GB/s
FLOPS achieved: 31.61 TF/s

Prefill latency: 0.06859280890785158 sec
Decode latency: 0.5718277769628912 sec
Time for inference 3: 0.64 sec total, 49.92 tokens/sec
Decode latency: 0.57 sec
Prefill latency: 0.07 sec
Bandwidth achieved: 959.27 GB/s
FLOPS achieved: 31.66 TF/s

Prefill latency: 0.06913692504167557 sec
Decode latency: 0.571677410043776 sec
Time for inference 4: 0.64 sec total, 49.89 tokens/sec
Decode latency: 0.57 sec
Prefill latency: 0.07 sec
Bandwidth achieved: 958.72 GB/s
FLOPS achieved: 31.64 TF/s

Prefill latency: 0.06904575182124972 sec
Decode latency: 0.5722589229699224 sec
Time for inference 5: 0.64 sec total, 49.85 tokens/sec
Decode latency: 0.57 sec
Prefill latency: 0.07 sec
Bandwidth achieved: 958.04 GB/s
FLOPS achieved: 31.62 TF/s

Prefill latency: 0.06873022182844579 sec
Decode latency: 0.5720723799895495 sec
Time for inference 6: 0.64 sec total, 49.90 tokens/sec
Decode latency: 0.57 sec
Prefill latency: 0.07 sec
Bandwidth achieved: 958.85 GB/s
FLOPS achieved: 31.64 TF/s

Prefill latency: 0.06919747777283192 sec
Decode latency: 0.5715336531866342 sec
Time for inference 7: 0.64 sec total, 49.89 tokens/sec
Decode latency: 0.57 sec
Prefill latency: 0.07 sec
Bandwidth achieved: 958.80 GB/s
FLOPS achieved: 31.64 TF/s

Prefill latency: 0.06955028208903968 sec
Decode latency: 0.5717416561674327 sec
Time for inference 8: 0.64 sec total, 49.85 tokens/sec
Decode latency: 0.57 sec
Prefill latency: 0.07 sec
Bandwidth achieved: 957.91 GB/s
FLOPS achieved: 31.61 TF/s

Prefill latency: 0.06867795111611485 sec
Decode latency: 0.571624618023634 sec
Time for inference 9: 0.64 sec total, 49.94 tokens/sec
Decode latency: 0.57 sec
Prefill latency: 0.07 sec
Bandwidth achieved: 959.62 GB/s
FLOPS achieved: 31.67 TF/s

Prefill latency: 0.0693160449154675 sec
Decode latency: 0.5724115690682083 sec
Time for inference 10: 0.64 sec total, 49.82 tokens/sec
Decode latency: 0.57 sec
Prefill latency: 0.07 sec
Bandwidth achieved: 957.36 GB/s
FLOPS achieved: 31.59 TF/s

==========
Batch Size: 1
Prompt Length: 1024
Generated tokens: 32
Average decode latency: 0.5720 sec
Average prefill latency: 0.0690 sec
Average tokens/sec: 49.87
Memory used: 24.87 GB
Done. we are killing the process
[rank0]:[W1119 16:22:42.303375437 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
