W1119 16:21:49.609000 2990776 site-packages/torch/distributed/run.py:793] 
W1119 16:21:49.609000 2990776 site-packages/torch/distributed/run.py:793] *****************************************
W1119 16:21:49.609000 2990776 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1119 16:21:49.609000 2990776 site-packages/torch/distributed/run.py:793] *****************************************
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
flash_kv_decode is set to True
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
our world size=4
Using device=cuda
Loading model ...
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
GPTDense(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x DenseTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=2560, bias=False)
        (wo): Linear(in_features=2048, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=14336, bias=False)
        (w2): Linear(in_features=7168, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 1.45 seconds
CUDA_GRAPH are activate
Prefill latency: 0.102667769882828 sec
Decode latency: 0.7173534689936787 sec
Compilation time: 0.85 seconds
Compilation time: 0.91 seconds
Compilation time: 0.82 seconds
Compilation time: 0.82 seconds
Prefill latency: 0.10168416006490588 sec
Decode latency: 0.7170967310667038 sec
Prefill latency: 0.10185723099857569 sec
Decode latency: 0.7168533599469811 sec
Prefill latency: 0.10158363217487931 sec
Decode latency: 0.7167964847758412 sec
Prefill latency: 0.10172363091260195 sec
Decode latency: 0.7170443821232766 sec
Prefill latency: 0.10176381305791438 sec
Decode latency: 0.716734022134915 sec
Time for inference 1: 0.82 sec total, 39.07 tokens/sec
Decode latency: 0.72 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 1419.46 GB/s
FLOPS achieved: 46.84 TF/s

Prefill latency: 0.10157358599826694 sec
Decode latency: 0.7165497720707208 sec
Time for inference 2: 0.82 sec total, 39.09 tokens/sec
Decode latency: 0.72 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 1420.14 GB/s
FLOPS achieved: 46.86 TF/s

Prefill latency: 0.10170125705190003 sec
Decode latency: 0.7171233962289989 sec
Time for inference 3: 0.82 sec total, 39.05 tokens/sec
Decode latency: 0.72 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 1418.77 GB/s
FLOPS achieved: 46.82 TF/s

Prefill latency: 0.10152468807063997 sec
Decode latency: 0.716532435035333 sec
Time for inference 4: 0.82 sec total, 39.09 tokens/sec
Decode latency: 0.72 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 1420.07 GB/s
FLOPS achieved: 46.86 TF/s

Prefill latency: 0.10177725786343217 sec
Decode latency: 0.7164879150222987 sec
Time for inference 5: 0.82 sec total, 39.08 tokens/sec
Decode latency: 0.72 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 1419.82 GB/s
FLOPS achieved: 46.85 TF/s

Prefill latency: 0.10164012899622321 sec
Decode latency: 0.7166216210462153 sec
Time for inference 6: 0.82 sec total, 39.08 tokens/sec
Decode latency: 0.72 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 1419.84 GB/s
FLOPS achieved: 46.85 TF/s

Prefill latency: 0.10154894110746682 sec
Decode latency: 0.7162319968920201 sec
Time for inference 7: 0.82 sec total, 39.10 tokens/sec
Decode latency: 0.72 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 1420.66 GB/s
FLOPS achieved: 46.88 TF/s

Prefill latency: 0.10163803398609161 sec
Decode latency: 0.7168062999844551 sec
Time for inference 8: 0.82 sec total, 39.07 tokens/sec
Decode latency: 0.72 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 1419.47 GB/s
FLOPS achieved: 46.84 TF/s

Prefill latency: 0.10134805901907384 sec
Decode latency: 0.7164275210816413 sec
Time for inference 9: 0.82 sec total, 39.10 tokens/sec
Decode latency: 0.72 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 1420.63 GB/s
FLOPS achieved: 46.88 TF/s

Prefill latency: 0.10206010495312512 sec
Decode latency: 0.7160946330986917 sec
Time for inference 10: 0.82 sec total, 39.09 tokens/sec
Decode latency: 0.72 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 1419.95 GB/s
FLOPS achieved: 46.86 TF/s

==========
Batch Size: 1
Prompt Length: 1024
Generated tokens: 32
Average decode latency: 0.7166 sec
Average prefill latency: 0.1017 sec
Average tokens/sec: 39.08
Memory used: 42.74 GB
Done. we are killing the process
[rank0]:[W1119 16:22:12.326243875 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
