W1118 21:44:24.382000 3637408 site-packages/torch/distributed/run.py:793] 
W1118 21:44:24.382000 3637408 site-packages/torch/distributed/run.py:793] *****************************************
W1118 21:44:24.382000 3637408 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1118 21:44:24.382000 3637408 site-packages/torch/distributed/run.py:793] *****************************************
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
flash_kv_decode is set to True
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
our world size=8
Using device=cuda
Loading model ...
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8rank: 1, global_rank: 1, world_size: 8, global_world_size: 8

rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
GPTParallel(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x ParallelTransformerBlock(
      semi_compiled = False
      (attention): FuseAttentionMLP(
        (wqkv1): Linear(in_features=8192, out_features=8448, bias=False)
        (wo): Linear(in_features=1024, out_features=8192, bias=False)
        (w2): Linear(in_features=3584, out_features=8192, bias=False)
      )
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 1.24 seconds
CUDA_GRAPH are activate
Prefill latency: 0.07865689508616924 sec
Decode latency: 0.47788598015904427 sec
Compilation time: 0.57 seconds
Compilation time: 0.57 seconds
Compilation time: 0.56 seconds
Compilation time: 0.54 secondsCompilation time: 0.56 seconds

Compilation time: 0.59 seconds
Compilation time: 0.60 seconds
Compilation time: 0.56 seconds
Prefill latency: 0.06220471113920212 sec
Decode latency: 0.47749549988657236 sec
Prefill latency: 0.06220032786950469 sec
Decode latency: 0.477261571213603 sec
Prefill latency: 0.06219701422378421 sec
Decode latency: 0.4776399461552501 sec
Prefill latency: 0.0622577709145844 sec
Decode latency: 0.4777595428749919 sec
Prefill latency: 0.0623227609321475 sec
Decode latency: 0.4775819811038673 sec
Time for inference 1: 0.54 sec total, 59.21 tokens/sec
Decode latency: 0.48 sec
Prefill latency: 0.06 sec
Bandwidth achieved: 1137.82 GB/s
FLOPS achieved: 37.55 TF/s

Prefill latency: 0.062205199152231216 sec
Decode latency: 0.4773552590049803 sec
Time for inference 2: 0.54 sec total, 59.25 tokens/sec
Decode latency: 0.48 sec
Prefill latency: 0.06 sec
Bandwidth achieved: 1138.53 GB/s
FLOPS achieved: 37.57 TF/s

Prefill latency: 0.062281076330691576 sec
Decode latency: 0.47758572781458497 sec
Time for inference 3: 0.54 sec total, 59.22 tokens/sec
Decode latency: 0.48 sec
Prefill latency: 0.06 sec
Bandwidth achieved: 1137.95 GB/s
FLOPS achieved: 37.55 TF/s

Prefill latency: 0.0622644629329443 sec
Decode latency: 0.4777012439444661 sec
Time for inference 4: 0.54 sec total, 59.20 tokens/sec
Decode latency: 0.48 sec
Prefill latency: 0.06 sec
Bandwidth achieved: 1137.63 GB/s
FLOPS achieved: 37.54 TF/s

Prefill latency: 0.06218081386759877 sec
Decode latency: 0.4773694211617112 sec
Time for inference 5: 0.54 sec total, 59.23 tokens/sec
Decode latency: 0.48 sec
Prefill latency: 0.06 sec
Bandwidth achieved: 1138.14 GB/s
FLOPS achieved: 37.56 TF/s

Prefill latency: 0.062166281044483185 sec
Decode latency: 0.4772797957994044 sec
Time for inference 6: 0.54 sec total, 59.26 tokens/sec
Decode latency: 0.48 sec
Prefill latency: 0.06 sec
Bandwidth achieved: 1138.68 GB/s
FLOPS achieved: 37.58 TF/s

Prefill latency: 0.06221782881766558 sec
Decode latency: 0.477569333743304 sec
Time for inference 7: 0.54 sec total, 59.23 tokens/sec
Decode latency: 0.48 sec
Prefill latency: 0.06 sec
Bandwidth achieved: 1138.11 GB/s
FLOPS achieved: 37.56 TF/s

Prefill latency: 0.06226949580013752 sec
Decode latency: 0.4773102179169655 sec
Time for inference 8: 0.54 sec total, 59.25 tokens/sec
Decode latency: 0.48 sec
Prefill latency: 0.06 sec
Bandwidth achieved: 1138.57 GB/s
FLOPS achieved: 37.57 TF/s

Prefill latency: 0.062254322692751884 sec
Decode latency: 0.4774255659431219 sec
Time for inference 9: 0.54 sec total, 59.25 tokens/sec
Decode latency: 0.48 sec
Prefill latency: 0.06 sec
Bandwidth achieved: 1138.49 GB/s
FLOPS achieved: 37.57 TF/s

Prefill latency: 0.06220349809154868 sec
Decode latency: 0.4775472292676568 sec
Time for inference 10: 0.54 sec total, 59.24 tokens/sec
Decode latency: 0.48 sec
Prefill latency: 0.06 sec
Bandwidth achieved: 1138.32 GB/s
FLOPS achieved: 37.56 TF/s

==========
Batch Size: 1
Prompt Length: 1024
Generated tokens: 32
Average decode latency: 0.4775 sec
Average prefill latency: 0.0622 sec
Average tokens/sec: 59.23
Memory used: 23.22 GB
Done. we are killing the process
[rank0]:[W1118 21:44:48.848841324 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
