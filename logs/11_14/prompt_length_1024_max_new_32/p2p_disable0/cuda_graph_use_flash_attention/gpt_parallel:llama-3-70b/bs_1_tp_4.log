W1118 21:43:59.182000 3636906 site-packages/torch/distributed/run.py:793] 
W1118 21:43:59.182000 3636906 site-packages/torch/distributed/run.py:793] *****************************************
W1118 21:43:59.182000 3636906 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1118 21:43:59.182000 3636906 site-packages/torch/distributed/run.py:793] *****************************************
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
flash_kv_decode is set to True
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
our world size=4
Using device=cuda
Loading model ...
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
GPTParallel(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x ParallelTransformerBlock(
      semi_compiled = False
      (attention): FuseAttentionMLP(
        (wqkv1): Linear(in_features=8192, out_features=16896, bias=False)
        (wo): Linear(in_features=2048, out_features=8192, bias=False)
        (w2): Linear(in_features=7168, out_features=8192, bias=False)
      )
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 1.31 seconds
CUDA_GRAPH are activate
Prefill latency: 0.11851070588454604 sec
Decode latency: 0.6767573338001966 sec
Compilation time: 0.82 seconds
Compilation time: 0.78 seconds
Compilation time: 0.77 seconds
Compilation time: 0.80 seconds
Prefill latency: 0.09141210606321692 sec
Decode latency: 0.677550442982465 sec
Prefill latency: 0.09156342502683401 sec
Decode latency: 0.6794685553759336 sec
Prefill latency: 0.09163231682032347 sec
Decode latency: 0.6778571349568665 sec
Prefill latency: 0.0917910085991025 sec
Decode latency: 0.6768333371728659 sec
Prefill latency: 0.09173316927626729 sec
Decode latency: 0.6763947582803667 sec
Time for inference 1: 0.77 sec total, 41.63 tokens/sec
Decode latency: 0.68 sec
Prefill latency: 0.09 sec
Bandwidth achieved: 1512.42 GB/s
FLOPS achieved: 49.91 TF/s

Prefill latency: 0.09170009614899755 sec
Decode latency: 0.6777119198814034 sec
Time for inference 2: 0.77 sec total, 41.56 tokens/sec
Decode latency: 0.68 sec
Prefill latency: 0.09 sec
Bandwidth achieved: 1509.91 GB/s
FLOPS achieved: 49.83 TF/s

Prefill latency: 0.09204617934301496 sec
Decode latency: 0.6773800612427294 sec
Time for inference 3: 0.77 sec total, 41.56 tokens/sec
Decode latency: 0.68 sec
Prefill latency: 0.09 sec
Bandwidth achieved: 1509.88 GB/s
FLOPS achieved: 49.83 TF/s

Prefill latency: 0.09187103062868118 sec
Decode latency: 0.6769888140261173 sec
Time for inference 4: 0.77 sec total, 41.59 tokens/sec
Decode latency: 0.68 sec
Prefill latency: 0.09 sec
Bandwidth achieved: 1510.95 GB/s
FLOPS achieved: 49.86 TF/s

Prefill latency: 0.09139654785394669 sec
Decode latency: 0.6784575576893985 sec
Time for inference 5: 0.77 sec total, 41.53 tokens/sec
Decode latency: 0.68 sec
Prefill latency: 0.09 sec
Bandwidth achieved: 1508.89 GB/s
FLOPS achieved: 49.79 TF/s

Prefill latency: 0.09181263810023665 sec
Decode latency: 0.6786603000946343 sec
Time for inference 6: 0.77 sec total, 41.50 tokens/sec
Decode latency: 0.68 sec
Prefill latency: 0.09 sec
Bandwidth achieved: 1507.72 GB/s
FLOPS achieved: 49.75 TF/s

Prefill latency: 0.09132582088932395 sec
Decode latency: 0.6772853098809719 sec
Time for inference 7: 0.77 sec total, 41.61 tokens/sec
Decode latency: 0.68 sec
Prefill latency: 0.09 sec
Bandwidth achieved: 1511.47 GB/s
FLOPS achieved: 49.88 TF/s

Prefill latency: 0.091441101860255 sec
Decode latency: 0.6772302440367639 sec
Time for inference 8: 0.77 sec total, 41.60 tokens/sec
Decode latency: 0.68 sec
Prefill latency: 0.09 sec
Bandwidth achieved: 1511.36 GB/s
FLOPS achieved: 49.87 TF/s

Prefill latency: 0.09126357268542051 sec
Decode latency: 0.6762738423421979 sec
Time for inference 9: 0.77 sec total, 41.66 tokens/sec
Decode latency: 0.68 sec
Prefill latency: 0.09 sec
Bandwidth achieved: 1513.61 GB/s
FLOPS achieved: 49.95 TF/s

Prefill latency: 0.09221929125487804 sec
Decode latency: 0.6784669528715312 sec
Time for inference 10: 0.77 sec total, 41.49 tokens/sec
Decode latency: 0.68 sec
Prefill latency: 0.09 sec
Bandwidth achieved: 1507.36 GB/s
FLOPS achieved: 49.74 TF/s

==========
Batch Size: 1
Prompt Length: 1024
Generated tokens: 32
Average decode latency: 0.6775 sec
Average prefill latency: 0.0917 sec
Average tokens/sec: 41.58
Memory used: 40.43 GB
Done. we are killing the process
[rank0]:[W1118 21:44:21.943075637 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
