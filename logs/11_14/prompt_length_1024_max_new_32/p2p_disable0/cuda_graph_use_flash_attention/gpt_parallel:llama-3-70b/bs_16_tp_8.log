W1118 21:47:21.971000 3641292 site-packages/torch/distributed/run.py:793] 
W1118 21:47:21.971000 3641292 site-packages/torch/distributed/run.py:793] *****************************************
W1118 21:47:21.971000 3641292 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1118 21:47:21.971000 3641292 site-packages/torch/distributed/run.py:793] *****************************************
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
flash_kv_decode is set to True
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
our world size=8
Using device=cuda
Loading model ...
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
GPTParallel(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x ParallelTransformerBlock(
      semi_compiled = False
      (attention): FuseAttentionMLP(
        (wqkv1): Linear(in_features=8192, out_features=8448, bias=False)
        (wo): Linear(in_features=1024, out_features=8192, bias=False)
        (w2): Linear(in_features=3584, out_features=8192, bias=False)
      )
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 1.37 seconds
CUDA_GRAPH are activate
Prefill latency: 0.7891552578657866 sec
Decode latency: 0.5378398830071092 sec
Compilation time: 1.30 secondsCompilation time: 1.29 seconds

Compilation time: 1.33 secondsCompilation time: 1.27 secondsCompilation time: 1.27 seconds


Compilation time: 1.30 seconds
Compilation time: 1.29 seconds
Compilation time: 1.33 seconds
Prefill latency: 0.7307099802419543 sec
Decode latency: 0.537909624632448 sec
Prefill latency: 0.7311548329889774 sec
Decode latency: 0.5390668911859393 sec
Prefill latency: 0.7313021970912814 sec
Decode latency: 0.5367516223341227 sec
Prefill latency: 0.7309080087579787 sec
Decode latency: 0.5387758673168719 sec
Prefill latency: 0.7309983321465552 sec
Decode latency: 0.5374847273342311 sec
Time for inference 1: 1.27 sec total, 403.42 tokens/sec
Decode latency: 0.54 sec
Prefill latency: 0.73 sec
Bandwidth achieved: 7751.97 GB/s
FLOPS achieved: 255.82 TF/s

Prefill latency: 0.7291339938528836 sec
Decode latency: 0.5394705357030034 sec
Time for inference 2: 1.27 sec total, 403.41 tokens/sec
Decode latency: 0.54 sec
Prefill latency: 0.73 sec
Bandwidth achieved: 7751.69 GB/s
FLOPS achieved: 255.81 TF/s

Prefill latency: 0.7307630679570138 sec
Decode latency: 0.5371126849204302 sec
Time for inference 3: 1.27 sec total, 403.63 tokens/sec
Decode latency: 0.54 sec
Prefill latency: 0.73 sec
Bandwidth achieved: 7755.91 GB/s
FLOPS achieved: 255.94 TF/s

Prefill latency: 0.7307460471056402 sec
Decode latency: 0.5380050442181528 sec
Time for inference 4: 1.27 sec total, 403.35 tokens/sec
Decode latency: 0.54 sec
Prefill latency: 0.73 sec
Bandwidth achieved: 7750.55 GB/s
FLOPS achieved: 255.77 TF/s

Prefill latency: 0.7299380851909518 sec
Decode latency: 0.5365375988185406 sec
Time for inference 5: 1.27 sec total, 404.09 tokens/sec
Decode latency: 0.54 sec
Prefill latency: 0.73 sec
Bandwidth achieved: 7764.73 GB/s
FLOPS achieved: 256.24 TF/s

Prefill latency: 0.7316788770258427 sec
Decode latency: 0.5369365899823606 sec
Time for inference 6: 1.27 sec total, 403.42 tokens/sec
Decode latency: 0.54 sec
Prefill latency: 0.73 sec
Bandwidth achieved: 7751.97 GB/s
FLOPS achieved: 255.82 TF/s

Prefill latency: 0.7321004602126777 sec
Decode latency: 0.5395689653232694 sec
Time for inference 7: 1.27 sec total, 402.46 tokens/sec
Decode latency: 0.54 sec
Prefill latency: 0.73 sec
Bandwidth achieved: 7733.46 GB/s
FLOPS achieved: 255.20 TF/s

Prefill latency: 0.7315718429163098 sec
Decode latency: 0.5389463067986071 sec
Time for inference 8: 1.27 sec total, 402.80 tokens/sec
Decode latency: 0.54 sec
Prefill latency: 0.73 sec
Bandwidth achieved: 7739.93 GB/s
FLOPS achieved: 255.42 TF/s

Prefill latency: 0.7322626640088856 sec
Decode latency: 0.5394135643728077 sec
Time for inference 9: 1.27 sec total, 402.45 tokens/sec
Decode latency: 0.54 sec
Prefill latency: 0.73 sec
Bandwidth achieved: 7733.33 GB/s
FLOPS achieved: 255.20 TF/s

Prefill latency: 0.7336639678105712 sec
Decode latency: 0.5389147941023111 sec
Time for inference 10: 1.27 sec total, 402.16 tokens/sec
Decode latency: 0.54 sec
Prefill latency: 0.73 sec
Bandwidth achieved: 7727.77 GB/s
FLOPS achieved: 255.02 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 32
Average decode latency: 0.5382 sec
Average prefill latency: 0.7313 sec
Average tokens/sec: 403.12
Memory used: 49.86 GB
Done. we are killing the process
[rank0]:[W1118 21:47:59.558969367 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
