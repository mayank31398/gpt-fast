W1118 21:45:11.036000 3638827 site-packages/torch/distributed/run.py:793] 
W1118 21:45:11.036000 3638827 site-packages/torch/distributed/run.py:793] *****************************************
W1118 21:45:11.036000 3638827 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1118 21:45:11.036000 3638827 site-packages/torch/distributed/run.py:793] *****************************************
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
flash_kv_decode is set to True
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
our world size=4
Using device=cuda
Loading model ...
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
GPTParallel(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x ParallelTransformerBlock(
      semi_compiled = False
      (attention): FuseAttentionMLP(
        (wqkv1): Linear(in_features=8192, out_features=16896, bias=False)
        (wo): Linear(in_features=2048, out_features=8192, bias=False)
        (w2): Linear(in_features=7168, out_features=8192, bias=False)
      )
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 1.11 seconds
CUDA_GRAPH are activate
Prefill latency: 0.3335239118896425 sec
Decode latency: 0.7052312209270895 sec
Compilation time: 1.11 seconds
Compilation time: 1.06 seconds
Compilation time: 1.07 seconds
Compilation time: 1.04 seconds
Prefill latency: 0.3307403693906963 sec
Decode latency: 0.7062615496106446 sec
Prefill latency: 0.3308209180831909 sec
Decode latency: 0.7056657942011952 sec
Prefill latency: 0.3339055120013654 sec
Decode latency: 0.7065252712927759 sec
Prefill latency: 0.3313294416293502 sec
Decode latency: 0.7056731530465186 sec
Prefill latency: 0.33586342399939895 sec
Decode latency: 0.7057869951240718 sec
Time for inference 1: 1.04 sec total, 122.82 tokens/sec
Decode latency: 0.71 sec
Prefill latency: 0.34 sec
Bandwidth achieved: 4461.97 GB/s
FLOPS achieved: 147.25 TF/s

Prefill latency: 0.335910412017256 sec
Decode latency: 0.7058480265550315 sec
Time for inference 2: 1.04 sec total, 122.81 tokens/sec
Decode latency: 0.71 sec
Prefill latency: 0.34 sec
Bandwidth achieved: 4461.30 GB/s
FLOPS achieved: 147.22 TF/s

Prefill latency: 0.3324312618933618 sec
Decode latency: 0.7066175527870655 sec
Time for inference 3: 1.04 sec total, 123.12 tokens/sec
Decode latency: 0.71 sec
Prefill latency: 0.33 sec
Bandwidth achieved: 4472.89 GB/s
FLOPS achieved: 147.61 TF/s

Prefill latency: 0.33182384772226214 sec
Decode latency: 0.7058063386939466 sec
Time for inference 4: 1.04 sec total, 123.29 tokens/sec
Decode latency: 0.71 sec
Prefill latency: 0.33 sec
Bandwidth achieved: 4479.05 GB/s
FLOPS achieved: 147.81 TF/s

Prefill latency: 0.3319123457185924 sec
Decode latency: 0.705098255071789 sec
Time for inference 5: 1.04 sec total, 123.37 tokens/sec
Decode latency: 0.71 sec
Prefill latency: 0.33 sec
Bandwidth achieved: 4481.83 GB/s
FLOPS achieved: 147.90 TF/s

Prefill latency: 0.3340440960600972 sec
Decode latency: 0.7062050718814135 sec
Time for inference 6: 1.04 sec total, 122.99 tokens/sec
Decode latency: 0.71 sec
Prefill latency: 0.33 sec
Bandwidth achieved: 4467.89 GB/s
FLOPS achieved: 147.44 TF/s

Prefill latency: 0.3332264698110521 sec
Decode latency: 0.7060970012098551 sec
Time for inference 7: 1.04 sec total, 123.10 tokens/sec
Decode latency: 0.71 sec
Prefill latency: 0.33 sec
Bandwidth achieved: 4471.84 GB/s
FLOPS achieved: 147.57 TF/s

Prefill latency: 0.3323700148612261 sec
Decode latency: 0.7061596969142556 sec
Time for inference 8: 1.04 sec total, 123.19 tokens/sec
Decode latency: 0.71 sec
Prefill latency: 0.33 sec
Bandwidth achieved: 4475.27 GB/s
FLOPS achieved: 147.68 TF/s

Prefill latency: 0.33641940681263804 sec
Decode latency: 0.7062235688790679 sec
Time for inference 9: 1.04 sec total, 122.70 tokens/sec
Decode latency: 0.71 sec
Prefill latency: 0.34 sec
Bandwidth achieved: 4457.42 GB/s
FLOPS achieved: 147.09 TF/s

Prefill latency: 0.33347222208976746 sec
Decode latency: 0.7057457701303065 sec
Time for inference 10: 1.04 sec total, 123.11 tokens/sec
Decode latency: 0.71 sec
Prefill latency: 0.33 sec
Bandwidth achieved: 4472.28 GB/s
FLOPS achieved: 147.59 TF/s

==========
Batch Size: 4
Prompt Length: 1024
Generated tokens: 32
Average decode latency: 0.7060 sec
Average prefill latency: 0.3337 sec
Average tokens/sec: 123.05
Memory used: 46.01 GB
Done. we are killing the process
[rank0]:[W1118 21:45:37.678895690 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
