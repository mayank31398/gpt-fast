W1118 21:46:33.096000 3640754 site-packages/torch/distributed/run.py:793] 
W1118 21:46:33.096000 3640754 site-packages/torch/distributed/run.py:793] *****************************************
W1118 21:46:33.096000 3640754 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1118 21:46:33.096000 3640754 site-packages/torch/distributed/run.py:793] *****************************************
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
flash_kv_decode is set to True
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
our world size=4
Using device=cuda
Loading model ...
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
GPTParallel(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x ParallelTransformerBlock(
      semi_compiled = False
      (attention): FuseAttentionMLP(
        (wqkv1): Linear(in_features=8192, out_features=16896, bias=False)
        (wo): Linear(in_features=2048, out_features=8192, bias=False)
        (w2): Linear(in_features=7168, out_features=8192, bias=False)
      )
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 1.45 seconds
CUDA_GRAPH are activate
Prefill latency: 1.3472355497069657 sec
Decode latency: 0.7426674971356988 sec
Compilation time: 2.06 seconds
Compilation time: 2.07 seconds
Compilation time: 2.13 seconds
Compilation time: 2.09 seconds
Prefill latency: 1.3235968393273652 sec
Decode latency: 0.7414688030257821 sec
Prefill latency: 1.3175247288309038 sec
Decode latency: 0.7419121861457825 sec
Prefill latency: 1.312840468250215 sec
Decode latency: 0.7412960263900459 sec
Prefill latency: 1.322058972902596 sec
Decode latency: 0.7413340220227838 sec
Prefill latency: 1.3162694051861763 sec
Decode latency: 0.7416478069499135 sec
Time for inference 1: 2.06 sec total, 248.72 tokens/sec
Decode latency: 0.74 sec
Prefill latency: 1.32 sec
Bandwidth achieved: 9035.70 GB/s
FLOPS achieved: 298.18 TF/s

Prefill latency: 1.3180101551115513 sec
Decode latency: 0.7413900913670659 sec
Time for inference 2: 2.06 sec total, 248.55 tokens/sec
Decode latency: 0.74 sec
Prefill latency: 1.32 sec
Bandwidth achieved: 9029.37 GB/s
FLOPS achieved: 297.97 TF/s

Prefill latency: 1.3184169461019337 sec
Decode latency: 0.7423320766538382 sec
Time for inference 3: 2.06 sec total, 248.38 tokens/sec
Decode latency: 0.74 sec
Prefill latency: 1.32 sec
Bandwidth achieved: 9023.27 GB/s
FLOPS achieved: 297.77 TF/s

Prefill latency: 1.3184791249223053 sec
Decode latency: 0.7409170242026448 sec
Time for inference 4: 2.06 sec total, 248.55 tokens/sec
Decode latency: 0.74 sec
Prefill latency: 1.32 sec
Bandwidth achieved: 9029.22 GB/s
FLOPS achieved: 297.96 TF/s

Prefill latency: 1.322838810738176 sec
Decode latency: 0.7408965118229389 sec
Time for inference 5: 2.06 sec total, 248.02 tokens/sec
Decode latency: 0.74 sec
Prefill latency: 1.32 sec
Bandwidth achieved: 9010.26 GB/s
FLOPS achieved: 297.34 TF/s

Prefill latency: 1.3159150267019868 sec
Decode latency: 0.7421169318258762 sec
Time for inference 6: 2.06 sec total, 248.67 tokens/sec
Decode latency: 0.74 sec
Prefill latency: 1.32 sec
Bandwidth achieved: 9033.90 GB/s
FLOPS achieved: 298.12 TF/s

Prefill latency: 1.318712290842086 sec
Decode latency: 0.7404750250279903 sec
Time for inference 7: 2.06 sec total, 248.57 tokens/sec
Decode latency: 0.74 sec
Prefill latency: 1.32 sec
Bandwidth achieved: 9030.00 GB/s
FLOPS achieved: 297.99 TF/s

Prefill latency: 1.3158717709593475 sec
Decode latency: 0.7417516452260315 sec
Time for inference 8: 2.06 sec total, 248.76 tokens/sec
Decode latency: 0.74 sec
Prefill latency: 1.32 sec
Bandwidth achieved: 9036.87 GB/s
FLOPS achieved: 298.22 TF/s

Prefill latency: 1.3215396492742002 sec
Decode latency: 0.7416490130126476 sec
Time for inference 9: 2.06 sec total, 248.07 tokens/sec
Decode latency: 0.74 sec
Prefill latency: 1.32 sec
Bandwidth achieved: 9011.93 GB/s
FLOPS achieved: 297.39 TF/s

Prefill latency: 1.3160293018445373 sec
Decode latency: 0.7408772972412407 sec
Time for inference 10: 2.06 sec total, 248.82 tokens/sec
Decode latency: 0.74 sec
Prefill latency: 1.32 sec
Bandwidth achieved: 9039.32 GB/s
FLOPS achieved: 298.30 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 32
Average decode latency: 0.7414 sec
Average prefill latency: 1.3182 sec
Average tokens/sec: 248.51
Memory used: 68.59 GB
Done. we are killing the process
[rank0]:[W1118 21:47:18.625062835 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
