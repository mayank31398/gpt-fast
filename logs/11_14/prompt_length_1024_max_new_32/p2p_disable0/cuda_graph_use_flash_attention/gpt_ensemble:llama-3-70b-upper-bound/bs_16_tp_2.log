W1119 16:35:10.279000 3005417 site-packages/torch/distributed/run.py:793] 
W1119 16:35:10.279000 3005417 site-packages/torch/distributed/run.py:793] *****************************************
W1119 16:35:10.279000 3005417 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1119 16:35:10.279000 3005417 site-packages/torch/distributed/run.py:793] *****************************************
rank: 1, global_rank: 1, world_size: 2, global_world_size: 2
rank: 0, global_rank: 0, world_size: 2, global_world_size: 2
rank: 0, global_rank: 0, world_size: 2, global_world_size: 2
rank: 0, global_rank: 0, world_size: 2, global_world_size: 2
rank: 0, global_rank: 0, world_size: 2, global_world_size: 2
rank: 0, global_rank: 0, world_size: 2, global_world_size: 2
flash_kv_decode is set to True
rank: 0, global_rank: 0, world_size: 2, global_world_size: 2
our world size=2
Using device=cuda
Loading model ...
rank: 1, global_rank: 1, world_size: 2, global_world_size: 2
rank: 1, global_rank: 1, world_size: 2, global_world_size: 2
rank: 1, global_rank: 1, world_size: 2, global_world_size: 2
rank: 1, global_rank: 1, world_size: 2, global_world_size: 2
rank: 1, global_rank: 1, world_size: 2, global_world_size: 2
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=5120, bias=False)
        (wo): Linear(in_features=4096, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=28672, bias=False)
        (w2): Linear(in_features=14336, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 1.20 seconds
CUDA_GRAPH are activate
Prefill latency: 2.2719056389760226 sec
Decode latency: 1.0487959240563214 sec
Compilation time: 3.32 seconds
Compilation time: 3.31 seconds
Prefill latency: 2.2697668760083616 sec
Decode latency: 1.0483423911500722 sec
Prefill latency: 2.2661478999070823 sec
Decode latency: 1.0484834390226752 sec
Prefill latency: 2.2707263599149883 sec
Decode latency: 1.0488918079063296 sec
Prefill latency: 2.268570850836113 sec
Decode latency: 1.0498080630786717 sec
Prefill latency: 2.269833202008158 sec
Decode latency: 1.048840937204659 sec
Time for inference 1: 3.32 sec total, 154.25 tokens/sec
Decode latency: 1.05 sec
Prefill latency: 2.27 sec
Bandwidth achieved: 10882.92 GB/s
FLOPS achieved: 359.14 TF/s

Prefill latency: 2.265745420008898 sec
Decode latency: 1.0479210710618645 sec
Time for inference 2: 3.31 sec total, 154.48 tokens/sec
Decode latency: 1.05 sec
Prefill latency: 2.27 sec
Bandwidth achieved: 10899.66 GB/s
FLOPS achieved: 359.69 TF/s

Prefill latency: 2.2687498428858817 sec
Decode latency: 1.0484586439561099 sec
Time for inference 3: 3.32 sec total, 154.32 tokens/sec
Decode latency: 1.05 sec
Prefill latency: 2.27 sec
Bandwidth achieved: 10888.07 GB/s
FLOPS achieved: 359.31 TF/s

Prefill latency: 2.2653905898332596 sec
Decode latency: 1.0485880048945546 sec
Time for inference 4: 3.31 sec total, 154.47 tokens/sec
Decode latency: 1.05 sec
Prefill latency: 2.27 sec
Bandwidth achieved: 10898.49 GB/s
FLOPS achieved: 359.65 TF/s

Prefill latency: 2.2671797298826277 sec
Decode latency: 1.0489710529800504 sec
Time for inference 5: 3.32 sec total, 154.37 tokens/sec
Decode latency: 1.05 sec
Prefill latency: 2.27 sec
Bandwidth achieved: 10891.57 GB/s
FLOPS achieved: 359.42 TF/s

Prefill latency: 2.2627980110701174 sec
Decode latency: 1.0482059670612216 sec
Time for inference 6: 3.31 sec total, 154.61 tokens/sec
Decode latency: 1.05 sec
Prefill latency: 2.26 sec
Bandwidth achieved: 10908.55 GB/s
FLOPS achieved: 359.98 TF/s

Prefill latency: 2.263595717959106 sec
Decode latency: 1.0485513659659773 sec
Time for inference 7: 3.31 sec total, 154.55 tokens/sec
Decode latency: 1.05 sec
Prefill latency: 2.26 sec
Bandwidth achieved: 10904.56 GB/s
FLOPS achieved: 359.85 TF/s

Prefill latency: 2.26621024007909 sec
Decode latency: 1.0485968601424247 sec
Time for inference 8: 3.32 sec total, 154.43 tokens/sec
Decode latency: 1.05 sec
Prefill latency: 2.27 sec
Bandwidth achieved: 10895.49 GB/s
FLOPS achieved: 359.55 TF/s

Prefill latency: 2.26769606792368 sec
Decode latency: 1.049306691158563 sec
Time for inference 9: 3.32 sec total, 154.32 tokens/sec
Decode latency: 1.05 sec
Prefill latency: 2.27 sec
Bandwidth achieved: 10887.91 GB/s
FLOPS achieved: 359.30 TF/s

Prefill latency: 2.2661496340297163 sec
[rank1]:[W1119 16:36:12.446600370 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Decode latency: 1.0472552001010627 sec
Time for inference 10: 3.31 sec total, 154.49 tokens/sec
Decode latency: 1.05 sec
Prefill latency: 2.27 sec
Bandwidth achieved: 10900.29 GB/s
FLOPS achieved: 359.71 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 32
Average decode latency: 1.0485 sec
Average prefill latency: 2.2663 sec
Average tokens/sec: 154.43
Memory used: 83.71 GB
[rank0]:[W1119 16:36:12.473404189 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 492, in <module>
[rank0]:     main(
[rank0]:   File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 463, in main
[rank0]:     dist.barrier()
[rank0]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 4374, in barrier
[rank0]:     work = group.barrier(opts=opts)
[rank0]: torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/NCCLUtils.hpp:328, unhandled cuda error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
[rank0]: ncclUnhandledCudaError: Call to CUDA function failed.
[rank0]: Last error:
[rank0]: Failed to CUDA calloc async 8 bytes
[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 492, in <module>
[rank1]:     main(
[rank1]:   File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 463, in main
[rank1]:     dist.barrier()
[rank1]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
[rank1]:     return func(*args, **kwargs)
[rank1]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 4374, in barrier
[rank1]:     work = group.barrier(opts=opts)
[rank1]: torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/NCCLUtils.hpp:328, unhandled cuda error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
[rank1]: ncclUnhandledCudaError: Call to CUDA function failed.
[rank1]: Last error:
[rank1]: Failed to CUDA calloc async 8 bytes
[rank0]:[W1119 16:36:14.648798498 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W1119 16:36:14.690000 3005417 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 3005490 closing signal SIGTERM
E1119 16:36:14.910000 3005417 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 3005489) of binary: /home/charlie/anaconda3/envs/gpt-fast/bin/python
Traceback (most recent call last):
  File "/home/charlie/anaconda3/envs/gpt-fast/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
benchmark.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-11-19_16:36:14
  host      : mk-xii-15.cloud.together.ai
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 3005489)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
