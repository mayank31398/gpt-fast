W1119 16:39:45.997000 3008238 site-packages/torch/distributed/run.py:793] 
W1119 16:39:45.997000 3008238 site-packages/torch/distributed/run.py:793] *****************************************
W1119 16:39:45.997000 3008238 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1119 16:39:45.997000 3008238 site-packages/torch/distributed/run.py:793] *****************************************
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8rank: 6, global_rank: 6, world_size: 8, global_world_size: 8

rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
flash_kv_decode is set to True
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
our world size=8
Using device=cuda
Loading model ...
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=1280, bias=False)
        (wo): Linear(in_features=1024, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 1.41 seconds
CUDA_GRAPH are activate
Prefill latency: 2.5763432858511806 sec
Decode latency: 0.5462959499564022 sec
Compilation time: 3.13 seconds
Compilation time: 3.12 seconds
Compilation time: 3.13 seconds
Compilation time: 3.12 seconds
Compilation time: 3.11 seconds
Compilation time: 3.10 seconds
Compilation time: 3.15 seconds
Compilation time: 3.16 seconds
Prefill latency: 2.5673932910431176 sec
Decode latency: 0.5464544170536101 sec
Prefill latency: 2.5671555248554796 sec
Decode latency: 0.5462386368308216 sec
Prefill latency: 2.5703584509901702 sec
Decode latency: 0.5460210498422384 sec
Prefill latency: 2.566102714976296 sec
Decode latency: 0.5464510240126401 sec
Prefill latency: 2.567861679941416 sec
Decode latency: 0.5477444061543792 sec
Time for inference 1: 3.12 sec total, 657.23 tokens/sec
Decode latency: 0.55 sec
Prefill latency: 2.57 sec
Bandwidth achieved: 12629.74 GB/s
FLOPS achieved: 416.78 TF/s

Prefill latency: 2.5662152080330998 sec
Decode latency: 0.5466452578548342 sec
Time for inference 2: 3.11 sec total, 657.80 tokens/sec
Decode latency: 0.55 sec
Prefill latency: 2.57 sec
Bandwidth achieved: 12640.80 GB/s
FLOPS achieved: 417.15 TF/s

Prefill latency: 2.5627068988978863 sec
Decode latency: 0.5459786169230938 sec
Time for inference 3: 3.11 sec total, 658.69 tokens/sec
Decode latency: 0.55 sec
Prefill latency: 2.56 sec
Bandwidth achieved: 12657.82 GB/s
FLOPS achieved: 417.71 TF/s

Prefill latency: 2.5658917899709195 sec
Decode latency: 0.546339136082679 sec
Time for inference 4: 3.11 sec total, 657.93 tokens/sec
Decode latency: 0.55 sec
Prefill latency: 2.57 sec
Bandwidth achieved: 12643.30 GB/s
FLOPS achieved: 417.23 TF/s

Prefill latency: 2.56621981295757 sec
Decode latency: 0.5458849549759179 sec
Time for inference 5: 3.11 sec total, 657.96 tokens/sec
Decode latency: 0.55 sec
Prefill latency: 2.57 sec
Bandwidth achieved: 12643.80 GB/s
FLOPS achieved: 417.25 TF/s

Prefill latency: 2.5639742880593985 sec
Decode latency: 0.5467321730684489 sec
Time for inference 6: 3.11 sec total, 658.26 tokens/sec
Decode latency: 0.55 sec
Prefill latency: 2.56 sec
Bandwidth achieved: 12649.64 GB/s
FLOPS achieved: 417.44 TF/s

Prefill latency: 2.5632872320711613 sec
Decode latency: 0.5451313860248774 sec
Time for inference 7: 3.11 sec total, 658.71 tokens/sec
Decode latency: 0.55 sec
Prefill latency: 2.56 sec
Bandwidth achieved: 12658.35 GB/s
FLOPS achieved: 417.73 TF/s

Prefill latency: 2.5644564561080188 sec
Decode latency: 0.5471321099903435 sec
Time for inference 8: 3.11 sec total, 658.05 tokens/sec
Decode latency: 0.55 sec
Prefill latency: 2.56 sec
Bandwidth achieved: 12645.63 GB/s
FLOPS achieved: 417.31 TF/s

Prefill latency: 2.5689674159511924 sec
Decode latency: 0.5476453928276896 sec
Time for inference 9: 3.12 sec total, 656.99 tokens/sec
Decode latency: 0.55 sec
Prefill latency: 2.57 sec
Bandwidth achieved: 12625.24 GB/s
FLOPS achieved: 416.63 TF/s

Prefill latency: 2.5632905538659543 sec
Decode latency: 0.5462277778424323 sec
Time for inference 10: 3.11 sec total, 658.50 tokens/sec
Decode latency: 0.55 sec
Prefill latency: 2.56 sec
Bandwidth achieved: 12654.25 GB/s
FLOPS achieved: 417.59 TF/s

==========
Batch Size: 64
Prompt Length: 1024
Generated tokens: 32
Average decode latency: 0.5465 sec
Average prefill latency: 2.5653 sec
Average tokens/sec: 658.01
Memory used: 48.79 GB
[rank0]:[W1119 16:40:48.533292006 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank1]:[W1119 16:40:49.698223172 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank3]:[W1119 16:40:49.920584088 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank6]:[W1119 16:40:49.099951747 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank4]:[W1119 16:40:49.146425215 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank7]:[W1119 16:40:49.331217770 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank5]:[W1119 16:40:50.907573345 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank2]:[W1119 16:40:50.989976260 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Done. we are killing the process
[rank0]:[W1119 16:40:56.404499713 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
