W1119 16:32:57.195000 3002351 site-packages/torch/distributed/run.py:793] 
W1119 16:32:57.195000 3002351 site-packages/torch/distributed/run.py:793] *****************************************
W1119 16:32:57.195000 3002351 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1119 16:32:57.195000 3002351 site-packages/torch/distributed/run.py:793] *****************************************
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
flash_kv_decode is set to True
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
our world size=8
Using device=cuda
Loading model ...
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=1280, bias=False)
        (wo): Linear(in_features=1024, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 1.28 seconds
CUDA_GRAPH are activate
Prefill latency: 0.04687503399327397 sec
Decode latency: 0.4444005540572107 sec
Compilation time: 0.49 seconds
Prefill latency: 0.046528638107702136 sec
Compilation time: 0.49 seconds
Compilation time: 0.49 seconds
Compilation time: 0.49 seconds
Decode latency: 0.4436532109975815 sec
Prefill latency: 0.046726008178666234 sec
Compilation time: 0.49 seconds
Compilation time: 0.49 seconds
Compilation time: 0.49 seconds
Compilation time: 0.49 seconds
Decode latency: 0.4449098480399698 sec
Prefill latency: 0.04655782715417445 sec
Decode latency: 0.44278818485327065 sec
Prefill latency: 0.04652124806307256 sec
Decode latency: 0.44364180602133274 sec
Prefill latency: 0.046575302025303245 sec
Decode latency: 0.4438618349377066 sec
Time for inference 1: 0.49 sec total, 65.19 tokens/sec
Decode latency: 0.44 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 1252.75 GB/s
FLOPS achieved: 41.34 TF/s

Prefill latency: 0.04663702589459717 sec
Decode latency: 0.4444956169463694 sec
Time for inference 2: 0.49 sec total, 65.11 tokens/sec
Decode latency: 0.44 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 1251.12 GB/s
FLOPS achieved: 41.29 TF/s

Prefill latency: 0.04657063796184957 sec
Decode latency: 0.4437808960210532 sec
Time for inference 3: 0.49 sec total, 65.20 tokens/sec
Decode latency: 0.44 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 1252.98 GB/s
FLOPS achieved: 41.35 TF/s

Prefill latency: 0.04659086698666215 sec
Decode latency: 0.4449795160908252 sec
Time for inference 4: 0.49 sec total, 65.04 tokens/sec
Decode latency: 0.44 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 1249.89 GB/s
FLOPS achieved: 41.25 TF/s

Prefill latency: 0.04658496403135359 sec
Decode latency: 0.4438345110975206 sec
Time for inference 5: 0.49 sec total, 65.18 tokens/sec
Decode latency: 0.44 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 1252.58 GB/s
FLOPS achieved: 41.34 TF/s

Prefill latency: 0.04670411115512252 sec
Decode latency: 0.44459219207055867 sec
Time for inference 6: 0.49 sec total, 65.07 tokens/sec
Decode latency: 0.44 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 1250.42 GB/s
FLOPS achieved: 41.26 TF/s

Prefill latency: 0.04700218699872494 sec
Decode latency: 0.44530590111389756 sec
Time for inference 7: 0.49 sec total, 64.94 tokens/sec
Decode latency: 0.45 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 1247.99 GB/s
FLOPS achieved: 41.18 TF/s

Prefill latency: 0.04666130198165774 sec
Decode latency: 0.4448216010350734 sec
Time for inference 8: 0.49 sec total, 65.06 tokens/sec
Decode latency: 0.44 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 1250.15 GB/s
FLOPS achieved: 41.26 TF/s

Prefill latency: 0.046646476024761796 sec
Decode latency: 0.44376607798039913 sec
Time for inference 9: 0.49 sec total, 65.20 tokens/sec
Decode latency: 0.44 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 1252.99 GB/s
FLOPS achieved: 41.35 TF/s

Prefill latency: 0.04662068281322718 sec
Decode latency: 0.4450589991174638 sec
Time for inference 10: 0.49 sec total, 65.03 tokens/sec
Decode latency: 0.45 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 1249.61 GB/s
FLOPS achieved: 41.24 TF/s

==========
Batch Size: 1
Prompt Length: 1024
Generated tokens: 32
Average decode latency: 0.4444 sec
Average prefill latency: 0.0467 sec
Average tokens/sec: 65.10
Memory used: 21.87 GB
[rank0]:[W1119 16:33:13.793816306 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank4]:[W1119 16:33:13.934737758 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank2]:[W1119 16:33:13.131551523 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank6]:[W1119 16:33:13.202813528 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank3]:[W1119 16:33:13.306192911 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank7]:[W1119 16:33:13.335265190 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank1]:[W1119 16:33:13.342254340 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank5]:[W1119 16:33:13.407059876 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Done. we are killing the process
[rank0]:[W1119 16:33:20.829687131 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
