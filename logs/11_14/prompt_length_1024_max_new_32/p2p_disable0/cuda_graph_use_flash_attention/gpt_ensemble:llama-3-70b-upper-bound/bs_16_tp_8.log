W1119 16:37:02.400000 3006341 site-packages/torch/distributed/run.py:793] 
W1119 16:37:02.400000 3006341 site-packages/torch/distributed/run.py:793] *****************************************
W1119 16:37:02.400000 3006341 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1119 16:37:02.400000 3006341 site-packages/torch/distributed/run.py:793] *****************************************
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
flash_kv_decode is set to True
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
our world size=8
Using device=cuda
Loading model ...
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=1280, bias=False)
        (wo): Linear(in_features=1024, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 1.24 seconds
CUDA_GRAPH are activate
Prefill latency: 0.6307332410942763 sec
Decode latency: 0.4887207190040499 sec
Compilation time: 1.12 seconds
Compilation time: 1.13 seconds
Prefill latency: 0.6349313461687416 sec
Compilation time: 1.13 seconds
Compilation time: 1.14 seconds
Compilation time: 1.12 seconds
Compilation time: 1.14 seconds
Compilation time: 1.12 seconds
Compilation time: 1.12 seconds
Decode latency: 0.4886153591796756 sec
Prefill latency: 0.6350408801808953 sec
Decode latency: 0.48821066902019083 sec
Prefill latency: 0.633351783035323 sec
Decode latency: 0.4889950631186366 sec
Prefill latency: 0.6354494809638709 sec
Decode latency: 0.4883594529237598 sec
Prefill latency: 0.6339166809339076 sec
Decode latency: 0.48825703794136643 sec
Time for inference 1: 1.12 sec total, 456.06 tokens/sec
Decode latency: 0.49 sec
Prefill latency: 0.63 sec
Bandwidth achieved: 8763.98 GB/s
FLOPS achieved: 289.21 TF/s

Prefill latency: 0.6353112771175802 sec
Decode latency: 0.48812757385894656 sec
Time for inference 2: 1.12 sec total, 455.56 tokens/sec
Decode latency: 0.49 sec
Prefill latency: 0.64 sec
Bandwidth achieved: 8754.42 GB/s
FLOPS achieved: 288.90 TF/s

Prefill latency: 0.6344333440065384 sec
Decode latency: 0.4875557681079954 sec
Time for inference 3: 1.12 sec total, 456.14 tokens/sec
Decode latency: 0.49 sec
Prefill latency: 0.63 sec
Bandwidth achieved: 8765.62 GB/s
FLOPS achieved: 289.27 TF/s

Prefill latency: 0.6355695589445531 sec
Decode latency: 0.4878494809381664 sec
Time for inference 4: 1.12 sec total, 455.57 tokens/sec
Decode latency: 0.49 sec
Prefill latency: 0.64 sec
Bandwidth achieved: 8754.51 GB/s
FLOPS achieved: 288.90 TF/s

Prefill latency: 0.6333166309632361 sec
Decode latency: 0.4878048540558666 sec
Time for inference 5: 1.12 sec total, 456.50 tokens/sec
Decode latency: 0.49 sec
Prefill latency: 0.63 sec
Bandwidth achieved: 8772.48 GB/s
FLOPS achieved: 289.49 TF/s

Prefill latency: 0.6359391030855477 sec
Decode latency: 0.48907473194412887 sec
Time for inference 6: 1.13 sec total, 454.93 tokens/sec
Decode latency: 0.49 sec
Prefill latency: 0.64 sec
Bandwidth achieved: 8742.21 GB/s
FLOPS achieved: 288.49 TF/s

Prefill latency: 0.6345566138625145 sec
Decode latency: 0.4880864399019629 sec
Time for inference 7: 1.12 sec total, 455.89 tokens/sec
Decode latency: 0.49 sec
Prefill latency: 0.63 sec
Bandwidth achieved: 8760.75 GB/s
FLOPS achieved: 289.10 TF/s

Prefill latency: 0.6351961921900511 sec
Decode latency: 0.48825942305848 sec
Time for inference 8: 1.12 sec total, 455.56 tokens/sec
Decode latency: 0.49 sec
Prefill latency: 0.64 sec
Bandwidth achieved: 8754.44 GB/s
FLOPS achieved: 288.90 TF/s

Prefill latency: 0.6337005968671292 sec
Decode latency: 0.48809947189874947 sec
Time for inference 9: 1.12 sec total, 456.24 tokens/sec
Decode latency: 0.49 sec
Prefill latency: 0.63 sec
Bandwidth achieved: 8767.47 GB/s
FLOPS achieved: 289.33 TF/s

Prefill latency: 0.6347361910156906 sec
Decode latency: 0.48878950299695134 sec
Time for inference 10: 1.12 sec total, 455.53 tokens/sec
Decode latency: 0.49 sec
Prefill latency: 0.63 sec
Bandwidth achieved: 8753.87 GB/s
FLOPS achieved: 288.88 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 32
Average decode latency: 0.4882 sec
Average prefill latency: 0.6347 sec
Average tokens/sec: 455.80
Memory used: 28.37 GB
[rank0]:[W1119 16:37:28.058009215 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank7]:[W1119 16:37:28.444815733 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank3]:[W1119 16:37:29.776964805 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank6]:[W1119 16:37:29.789904453 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank4]:[W1119 16:37:29.909654241 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank1]:[W1119 16:37:29.925475177 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank2]:[W1119 16:37:29.986029003 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank5]:[W1119 16:37:29.103553865 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Done. we are killing the process
[rank0]:[W1119 16:37:36.065273115 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
