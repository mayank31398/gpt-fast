W1119 16:56:00.716000 3019816 site-packages/torch/distributed/run.py:793] 
W1119 16:56:00.716000 3019816 site-packages/torch/distributed/run.py:793] *****************************************
W1119 16:56:00.716000 3019816 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1119 16:56:00.716000 3019816 site-packages/torch/distributed/run.py:793] *****************************************
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
flash_kv_decode is set to True
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
our world size=8
Using device=cuda
Loading model ...
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
GPTLadder(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x LadderTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=1280, bias=False)
        (wo): Linear(in_features=1024, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 0.66 seconds
CUDA_GRAPH are activate
[rank7]:[W1119 16:56:17.762848026 CUDAGraph.cpp:133] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
Prefill latency: 0.7261479070875794 sec
Decode latency: 0.6627808008342981 sec
Compilation time: 1.50 seconds
Compilation time: 1.55 seconds
Compilation time: 1.39 seconds
Compilation time: 1.39 seconds
Compilation time: 1.49 seconds
Compilation time: 1.39 seconds
Compilation time: 1.49 seconds
Compilation time: 1.39 seconds
Prefill latency: 0.7193079818971455 sec
Decode latency: 0.6606200011447072 sec
Prefill latency: 0.717330572893843 sec
Decode latency: 0.6645167800597847 sec
Prefill latency: 0.7184251579456031 sec
Decode latency: 0.6661214099731296 sec
Prefill latency: 0.720541596179828 sec
Decode latency: 0.6922087438870221 sec
Prefill latency: 0.7196724661625922 sec
Decode latency: 0.6642154199071229 sec
Time for inference 1: 1.38 sec total, 369.76 tokens/sec
Decode latency: 0.66 sec
Prefill latency: 0.72 sec
Bandwidth achieved: 7105.64 GB/s
FLOPS achieved: 234.49 TF/s

Prefill latency: 0.7182572530582547 sec
Decode latency: 0.6767570150550455 sec
Time for inference 2: 1.40 sec total, 366.80 tokens/sec
Decode latency: 0.68 sec
Prefill latency: 0.72 sec
Bandwidth achieved: 7048.77 GB/s
FLOPS achieved: 232.61 TF/s

Prefill latency: 0.7205355959013104 sec
Decode latency: 0.6920806709676981 sec
Time for inference 3: 1.41 sec total, 362.23 tokens/sec
Decode latency: 0.69 sec
Prefill latency: 0.72 sec
Bandwidth achieved: 6960.93 GB/s
FLOPS achieved: 229.71 TF/s

Prefill latency: 0.7204212441574782 sec
Decode latency: 0.6607555060181767 sec
Time for inference 4: 1.38 sec total, 370.50 tokens/sec
Decode latency: 0.66 sec
Prefill latency: 0.72 sec
Bandwidth achieved: 7119.74 GB/s
FLOPS achieved: 234.95 TF/s

Prefill latency: 0.7206580229103565 sec
Decode latency: 0.6687213368713856 sec
Time for inference 5: 1.39 sec total, 368.31 tokens/sec
Decode latency: 0.67 sec
Prefill latency: 0.72 sec
Bandwidth achieved: 7077.67 GB/s
FLOPS achieved: 233.56 TF/s

Prefill latency: 0.7199787839781493 sec
Decode latency: 0.6708940870594233 sec
Time for inference 6: 1.39 sec total, 367.92 tokens/sec
Decode latency: 0.67 sec
Prefill latency: 0.72 sec
Bandwidth achieved: 7070.27 GB/s
FLOPS achieved: 233.32 TF/s

Prefill latency: 0.7202065801247954 sec
Decode latency: 0.6780580210033804 sec
Time for inference 7: 1.40 sec total, 365.97 tokens/sec
Decode latency: 0.68 sec
Prefill latency: 0.72 sec
Bandwidth achieved: 7032.82 GB/s
FLOPS achieved: 232.08 TF/s

Prefill latency: 0.7207040009088814 sec
Decode latency: 0.6712680191267282 sec
Time for inference 8: 1.39 sec total, 367.66 tokens/sec
Decode latency: 0.67 sec
Prefill latency: 0.72 sec
Bandwidth achieved: 7065.15 GB/s
FLOPS achieved: 233.15 TF/s

Prefill latency: 0.7198706450872123 sec
Decode latency: 0.6600610031746328 sec
Time for inference 9: 1.38 sec total, 370.84 tokens/sec
Decode latency: 0.66 sec
Prefill latency: 0.72 sec
Bandwidth achieved: 7126.33 GB/s
FLOPS achieved: 235.17 TF/s

Prefill latency: 0.7186290079262108 sec
Decode latency: 0.6645142040215433 sec
Time for inference 10: 1.38 sec total, 369.89 tokens/sec
Decode latency: 0.66 sec
Prefill latency: 0.72 sec
Bandwidth achieved: 7108.16 GB/s
FLOPS achieved: 234.57 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 32
Average decode latency: 0.6707 sec
Average prefill latency: 0.7199 sec
Average tokens/sec: 367.99
Memory used: 71.09 GB
Done. we are killing the process
[rank0]:[W1119 16:56:51.005705416 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
