W1119 16:54:27.481000 3018724 site-packages/torch/distributed/run.py:793] 
W1119 16:54:27.481000 3018724 site-packages/torch/distributed/run.py:793] *****************************************
W1119 16:54:27.481000 3018724 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1119 16:54:27.481000 3018724 site-packages/torch/distributed/run.py:793] *****************************************
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
flash_kv_decode is set to True
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
our world size=4
Using device=cuda
Loading model ...
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
GPTLadder(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x LadderTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=2560, bias=False)
        (wo): Linear(in_features=2048, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=14336, bias=False)
        (w2): Linear(in_features=7168, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 0.63 seconds
CUDA_GRAPH are activate
[rank2]:[W1119 16:54:37.919779572 CUDAGraph.cpp:133] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
Prefill latency: 0.3468288069125265 sec
Decode latency: 0.6792078309226781 sec
Compilation time: 1.00 secondsCompilation time: 1.03 seconds

Compilation time: 1.04 seconds
Compilation time: 1.03 seconds
Prefill latency: 0.324736031005159 sec
Decode latency: 0.6798555180430412 sec
Prefill latency: 0.3240157279651612 sec
Decode latency: 0.6798810958862305 sec
Prefill latency: 0.32647372991777956 sec
Decode latency: 0.6792048730421811 sec
Prefill latency: 0.3244691831059754 sec
Decode latency: 0.6795316310599446 sec
Prefill latency: 0.32583764009177685 sec
Decode latency: 0.6797212900128216 sec
Time for inference 1: 1.01 sec total, 127.22 tokens/sec
Decode latency: 0.68 sec
Prefill latency: 0.33 sec
Bandwidth achieved: 4621.80 GB/s
FLOPS achieved: 152.52 TF/s

Prefill latency: 0.326115784002468 sec
Decode latency: 0.6799472109414637 sec
Time for inference 2: 1.01 sec total, 127.16 tokens/sec
Decode latency: 0.68 sec
Prefill latency: 0.33 sec
Bandwidth achieved: 4619.82 GB/s
FLOPS achieved: 152.45 TF/s

Prefill latency: 0.3246836659964174 sec
Decode latency: 0.6792579728644341 sec
Time for inference 3: 1.00 sec total, 127.43 tokens/sec
Decode latency: 0.68 sec
Prefill latency: 0.32 sec
Bandwidth achieved: 4629.56 GB/s
FLOPS achieved: 152.78 TF/s

Prefill latency: 0.3251998610794544 sec
Decode latency: 0.6802661609835923 sec
Time for inference 4: 1.01 sec total, 127.24 tokens/sec
Decode latency: 0.68 sec
Prefill latency: 0.33 sec
Bandwidth achieved: 4622.44 GB/s
FLOPS achieved: 152.54 TF/s

Prefill latency: 0.3251445221249014 sec
Decode latency: 0.6792434558738023 sec
Time for inference 5: 1.00 sec total, 127.37 tokens/sec
Decode latency: 0.68 sec
Prefill latency: 0.33 sec
Bandwidth achieved: 4627.47 GB/s
FLOPS achieved: 152.71 TF/s

Prefill latency: 0.3268775139003992 sec
Decode latency: 0.679782714927569 sec
Time for inference 6: 1.01 sec total, 127.08 tokens/sec
Decode latency: 0.68 sec
Prefill latency: 0.33 sec
Bandwidth achieved: 4616.78 GB/s
FLOPS achieved: 152.35 TF/s

Prefill latency: 0.32425176817923784 sec
Decode latency: 0.6802304219454527 sec
Time for inference 7: 1.01 sec total, 127.36 tokens/sec
Decode latency: 0.68 sec
Prefill latency: 0.32 sec
Bandwidth achieved: 4626.82 GB/s
FLOPS achieved: 152.69 TF/s

Prefill latency: 0.3256612201221287 sec
Decode latency: 0.680286173010245 sec
Time for inference 8: 1.01 sec total, 127.16 tokens/sec
Decode latency: 0.68 sec
Prefill latency: 0.33 sec
Bandwidth achieved: 4619.56 GB/s
FLOPS achieved: 152.45 TF/s

Prefill latency: 0.3292695281561464 sec
Decode latency: 0.6799922201316804 sec
Time for inference 9: 1.01 sec total, 126.75 tokens/sec
Decode latency: 0.68 sec
Prefill latency: 0.33 sec
Bandwidth achieved: 4604.59 GB/s
FLOPS achieved: 151.95 TF/s

Prefill latency: 0.3231052979826927 sec
Decode latency: 0.6799249958712608 sec
Time for inference 10: 1.00 sec total, 127.54 tokens/sec
Decode latency: 0.68 sec
Prefill latency: 0.32 sec
Bandwidth achieved: 4633.60 GB/s
FLOPS achieved: 152.91 TF/s

==========
Batch Size: 4
Prompt Length: 1024
Generated tokens: 32
Average decode latency: 0.6799 sec
Average prefill latency: 0.3256 sec
Average tokens/sec: 127.23
Memory used: 55.25 GB
Done. we are killing the process
[rank0]:[W1119 16:54:52.468564999 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
