W1119 16:54:55.956000 3018952 site-packages/torch/distributed/run.py:793] 
W1119 16:54:55.956000 3018952 site-packages/torch/distributed/run.py:793] *****************************************
W1119 16:54:55.956000 3018952 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1119 16:54:55.956000 3018952 site-packages/torch/distributed/run.py:793] *****************************************
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
flash_kv_decode is set to True
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
our world size=8
Using device=cuda
Loading model ...
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
GPTLadder(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x LadderTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=1280, bias=False)
        (wo): Linear(in_features=1024, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 1.01 seconds
CUDA_GRAPH are activate
Prefill latency: 0.24937790003605187 sec
Decode latency: 0.5037547301035374 sec
Compilation time: 0.72 seconds
Compilation time: 0.70 seconds
Compilation time: 0.79 seconds
Compilation time: 0.71 seconds
Compilation time: 0.83 seconds
Compilation time: 0.73 seconds
Compilation time: 0.73 seconds
Compilation time: 0.75 seconds
Prefill latency: 0.19078601687215269 sec
Decode latency: 0.5046571728307754 sec
Prefill latency: 0.1905628889799118 sec
Decode latency: 0.5043867209460586 sec
Prefill latency: 0.18994356109760702 sec
Decode latency: 0.5087948068976402 sec
Prefill latency: 0.1899932969827205 sec
Decode latency: 0.507236260920763 sec
Prefill latency: 0.18997172405943274 sec
Decode latency: 0.5075219629798084 sec
Time for inference 1: 0.70 sec total, 183.33 tokens/sec
Decode latency: 0.51 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 3523.09 GB/s
FLOPS achieved: 116.26 TF/s

Prefill latency: 0.19209092599339783 sec
Decode latency: 0.5096331778913736 sec
Time for inference 2: 0.70 sec total, 182.26 tokens/sec
Decode latency: 0.51 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 3502.42 GB/s
FLOPS achieved: 115.58 TF/s

Prefill latency: 0.1917442660778761 sec
Decode latency: 0.5035670220386237 sec
Time for inference 3: 0.70 sec total, 183.92 tokens/sec
Decode latency: 0.50 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 3534.25 GB/s
FLOPS achieved: 116.63 TF/s

Prefill latency: 0.1909915499854833 sec
Decode latency: 0.5158187800552696 sec
Time for inference 4: 0.71 sec total, 180.94 tokens/sec
Decode latency: 0.52 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 3477.11 GB/s
FLOPS achieved: 114.74 TF/s

Prefill latency: 0.19044570601545274 sec
Decode latency: 0.5053233180660754 sec
Time for inference 5: 0.70 sec total, 183.75 tokens/sec
Decode latency: 0.51 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 3531.07 GB/s
FLOPS achieved: 116.53 TF/s

Prefill latency: 0.18984460295177996 sec
Decode latency: 0.5068440039176494 sec
Time for inference 6: 0.70 sec total, 183.54 tokens/sec
Decode latency: 0.51 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 3527.00 GB/s
FLOPS achieved: 116.39 TF/s

Prefill latency: 0.1914860310498625 sec
Decode latency: 0.5089413200039417 sec
Time for inference 7: 0.70 sec total, 182.57 tokens/sec
Decode latency: 0.51 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 3508.34 GB/s
FLOPS achieved: 115.78 TF/s

Prefill latency: 0.1908332579769194 sec
Decode latency: 0.5094158998690546 sec
Time for inference 8: 0.70 sec total, 182.56 tokens/sec
Decode latency: 0.51 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 3508.30 GB/s
FLOPS achieved: 115.77 TF/s

Prefill latency: 0.1904519919771701 sec
Decode latency: 0.5134396788198501 sec
Time for inference 9: 0.70 sec total, 181.61 tokens/sec
Decode latency: 0.51 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 3489.89 GB/s
FLOPS achieved: 115.17 TF/s

Prefill latency: 0.19062911695800722 sec
Decode latency: 0.5059779509902 sec
Time for inference 10: 0.70 sec total, 183.51 tokens/sec
Decode latency: 0.51 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 3526.43 GB/s
FLOPS achieved: 116.37 TF/s

==========
Batch Size: 4
Prompt Length: 1024
Generated tokens: 32
Average decode latency: 0.5086 sec
Average prefill latency: 0.1908 sec
Average tokens/sec: 182.80
Memory used: 33.84 GB
Done. we are killing the process
[rank0]:[W1119 16:55:21.352799261 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
