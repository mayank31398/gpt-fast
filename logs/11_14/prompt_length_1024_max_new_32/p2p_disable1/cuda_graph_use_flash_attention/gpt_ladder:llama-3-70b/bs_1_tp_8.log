W1119 16:53:44.612000 3018081 site-packages/torch/distributed/run.py:793] 
W1119 16:53:44.612000 3018081 site-packages/torch/distributed/run.py:793] *****************************************
W1119 16:53:44.612000 3018081 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1119 16:53:44.612000 3018081 site-packages/torch/distributed/run.py:793] *****************************************
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
flash_kv_decode is set to True
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
our world size=8
Using device=cuda
Loading model ...
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
GPTLadder(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x LadderTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=1280, bias=False)
        (wo): Linear(in_features=1024, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 0.67 seconds
CUDA_GRAPH are activate
Prefill latency: 0.09095814893953502 sec
Decode latency: 0.4745594868436456 sec
Compilation time: 0.58 seconds
Compilation time: 0.61 seconds
Compilation time: 0.64 seconds
Compilation time: 0.65 seconds
Compilation time: 0.56 seconds
Compilation time: 0.55 seconds
Compilation time: 0.64 seconds
Compilation time: 0.57 seconds
Prefill latency: 0.06878947606310248 sec
Decode latency: 0.4734110308345407 sec
Prefill latency: 0.06893768790178001 sec
Decode latency: 0.47177028795704246 sec
Prefill latency: 0.06882652407512069 sec
Decode latency: 0.4699436060618609 sec
Prefill latency: 0.06861013709567487 sec
Decode latency: 0.4711270199622959 sec
Prefill latency: 0.06783442082814872 sec
Decode latency: 0.4684681680519134 sec
Time for inference 1: 0.54 sec total, 59.60 tokens/sec
Decode latency: 0.47 sec
Prefill latency: 0.07 sec
Bandwidth achieved: 1145.38 GB/s
FLOPS achieved: 37.80 TF/s

Prefill latency: 0.06791091803461313 sec
Decode latency: 0.4706105689983815 sec
Time for inference 2: 0.54 sec total, 59.36 tokens/sec
Decode latency: 0.47 sec
Prefill latency: 0.07 sec
Bandwidth achieved: 1140.76 GB/s
FLOPS achieved: 37.65 TF/s

Prefill latency: 0.06777962390333414 sec
Decode latency: 0.47181113506667316 sec
Time for inference 3: 0.54 sec total, 59.24 tokens/sec
Decode latency: 0.47 sec
Prefill latency: 0.07 sec
Bandwidth achieved: 1138.38 GB/s
FLOPS achieved: 37.57 TF/s

Prefill latency: 0.0697084020357579 sec
Decode latency: 0.47220065887086093 sec
Time for inference 4: 0.54 sec total, 58.99 tokens/sec
Decode latency: 0.47 sec
Prefill latency: 0.07 sec
Bandwidth achieved: 1133.53 GB/s
FLOPS achieved: 37.41 TF/s

Prefill latency: 0.07205297308973968 sec
Decode latency: 0.4675922109745443 sec
Time for inference 5: 0.54 sec total, 59.23 tokens/sec
Decode latency: 0.47 sec
Prefill latency: 0.07 sec
Bandwidth achieved: 1138.17 GB/s
FLOPS achieved: 37.56 TF/s

Prefill latency: 0.06778316502459347 sec
Decode latency: 0.4713212288916111 sec
Time for inference 6: 0.54 sec total, 59.29 tokens/sec
Decode latency: 0.47 sec
Prefill latency: 0.07 sec
Bandwidth achieved: 1139.45 GB/s
FLOPS achieved: 37.60 TF/s

Prefill latency: 0.07189689297229052 sec
Decode latency: 0.46864499105140567 sec
Time for inference 7: 0.54 sec total, 59.11 tokens/sec
Decode latency: 0.47 sec
Prefill latency: 0.07 sec
Bandwidth achieved: 1135.95 GB/s
FLOPS achieved: 37.49 TF/s

Prefill latency: 0.06940895412117243 sec
Decode latency: 0.46897994284518063 sec
Time for inference 8: 0.54 sec total, 59.36 tokens/sec
Decode latency: 0.47 sec
Prefill latency: 0.07 sec
Bandwidth achieved: 1140.72 GB/s
FLOPS achieved: 37.64 TF/s

Prefill latency: 0.06814843299798667 sec
Decode latency: 0.4700174159370363 sec
Time for inference 9: 0.54 sec total, 59.37 tokens/sec
Decode latency: 0.47 sec
Prefill latency: 0.07 sec
Bandwidth achieved: 1140.88 GB/s
FLOPS achieved: 37.65 TF/s

Prefill latency: 0.06800172408111393 sec
Decode latency: 0.46839022007770836 sec
Time for inference 10: 0.54 sec total, 59.57 tokens/sec
Decode latency: 0.47 sec
Prefill latency: 0.07 sec
Bandwidth achieved: 1144.70 GB/s
FLOPS achieved: 37.78 TF/s

==========
Batch Size: 1
Prompt Length: 1024
Generated tokens: 32
Average decode latency: 0.4698 sec
Average prefill latency: 0.0691 sec
Average tokens/sec: 59.31
Memory used: 24.87 GB
Done. we are killing the process
[rank0]:[W1119 16:54:07.300485050 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
