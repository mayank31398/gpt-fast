W1118 21:53:32.200000 3648926 site-packages/torch/distributed/run.py:793] 
W1118 21:53:32.200000 3648926 site-packages/torch/distributed/run.py:793] *****************************************
W1118 21:53:32.200000 3648926 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1118 21:53:32.200000 3648926 site-packages/torch/distributed/run.py:793] *****************************************
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
flash_kv_decode is set to True
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
our world size=8
Using device=cuda
Loading model ...
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
GPTParallel(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x ParallelTransformerBlock(
      semi_compiled = False
      (attention): FuseAttentionMLP(
        (wqkv1): Linear(in_features=8192, out_features=8448, bias=False)
        (wo): Linear(in_features=1024, out_features=8192, bias=False)
        (w2): Linear(in_features=3584, out_features=8192, bias=False)
      )
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 1.26 seconds
CUDA_GRAPH are activate
Prefill latency: 0.7807158678770065 sec
Decode latency: 0.6865470102056861 sec
Compilation time: 1.46 seconds
Compilation time: 1.47 seconds
Compilation time: 1.46 seconds
Compilation time: 1.46 seconds
Compilation time: 1.46 seconds
Compilation time: 1.47 seconds
Compilation time: 1.47 seconds
Compilation time: 1.47 seconds
Prefill latency: 0.7553291800431907 sec
Decode latency: 0.691737306304276 sec
Prefill latency: 0.7540487865917385 sec
Decode latency: 0.6885928777046502 sec
Prefill latency: 0.7545662629418075 sec
Decode latency: 0.6855373815633357 sec
Prefill latency: 0.7519505340605974 sec
Decode latency: 0.6924814931116998 sec
Prefill latency: 0.7548958980478346 sec
Decode latency: 0.6933634099550545 sec
Time for inference 1: 1.45 sec total, 353.39 tokens/sec
Decode latency: 0.69 sec
Prefill latency: 0.75 sec
Bandwidth achieved: 6790.48 GB/s
FLOPS achieved: 224.09 TF/s

Prefill latency: 0.756360047031194 sec
Decode latency: 0.6830176301300526 sec
Time for inference 2: 1.44 sec total, 355.56 tokens/sec
Decode latency: 0.68 sec
Prefill latency: 0.76 sec
Bandwidth achieved: 6832.24 GB/s
FLOPS achieved: 225.46 TF/s

Prefill latency: 0.7562252338975668 sec
Decode latency: 0.6904228078201413 sec
Time for inference 3: 1.45 sec total, 353.75 tokens/sec
Decode latency: 0.69 sec
Prefill latency: 0.76 sec
Bandwidth achieved: 6797.39 GB/s
FLOPS achieved: 224.31 TF/s

Prefill latency: 0.7544465651735663 sec
Decode latency: 0.6837276481091976 sec
Time for inference 4: 1.44 sec total, 355.85 tokens/sec
Decode latency: 0.68 sec
Prefill latency: 0.75 sec
Bandwidth achieved: 6837.79 GB/s
FLOPS achieved: 225.65 TF/s

Prefill latency: 0.7534341737627983 sec
Decode latency: 0.6918552219867706 sec
Time for inference 5: 1.45 sec total, 354.06 tokens/sec
Decode latency: 0.69 sec
Prefill latency: 0.75 sec
Bandwidth achieved: 6803.39 GB/s
FLOPS achieved: 224.51 TF/s

Prefill latency: 0.7567032286897302 sec
Decode latency: 0.6868042140267789 sec
Time for inference 6: 1.44 sec total, 354.51 tokens/sec
Decode latency: 0.69 sec
Prefill latency: 0.76 sec
Bandwidth achieved: 6811.97 GB/s
FLOPS achieved: 224.80 TF/s

Prefill latency: 0.756096834782511 sec
Decode latency: 0.6926295780576766 sec
Time for inference 7: 1.45 sec total, 353.27 tokens/sec
Decode latency: 0.69 sec
Prefill latency: 0.76 sec
Bandwidth achieved: 6788.23 GB/s
FLOPS achieved: 224.01 TF/s

Prefill latency: 0.7557561760768294 sec
Decode latency: 0.6863578190095723 sec
Time for inference 8: 1.44 sec total, 354.88 tokens/sec
Decode latency: 0.69 sec
Prefill latency: 0.76 sec
Bandwidth achieved: 6819.09 GB/s
FLOPS achieved: 225.03 TF/s

Prefill latency: 0.7571599818766117 sec
Decode latency: 0.6759678740054369 sec
Time for inference 9: 1.43 sec total, 357.03 tokens/sec
Decode latency: 0.68 sec
Prefill latency: 0.76 sec
Bandwidth achieved: 6860.50 GB/s
FLOPS achieved: 226.40 TF/s

Prefill latency: 0.7546543278731406 sec
Decode latency: 0.6900967983528972 sec
Time for inference 10: 1.45 sec total, 354.16 tokens/sec
Decode latency: 0.69 sec
Prefill latency: 0.75 sec
Bandwidth achieved: 6805.32 GB/s
FLOPS achieved: 224.58 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 32
Average decode latency: 0.6874 sec
Average prefill latency: 0.7556 sec
Average tokens/sec: 354.64
Memory used: 49.86 GB
Done. we are killing the process
[rank0]:[W1118 21:54:18.609274784 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
