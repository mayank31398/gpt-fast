W1118 21:49:53.090000 3644498 site-packages/torch/distributed/run.py:793] 
W1118 21:49:53.090000 3644498 site-packages/torch/distributed/run.py:793] *****************************************
W1118 21:49:53.090000 3644498 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1118 21:49:53.090000 3644498 site-packages/torch/distributed/run.py:793] *****************************************
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
flash_kv_decode is set to True
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
our world size=4
Using device=cuda
Loading model ...
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
GPTParallel(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x ParallelTransformerBlock(
      semi_compiled = False
      (attention): FuseAttentionMLP(
        (wqkv1): Linear(in_features=8192, out_features=16896, bias=False)
        (wo): Linear(in_features=2048, out_features=8192, bias=False)
        (w2): Linear(in_features=7168, out_features=8192, bias=False)
      )
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 1.19 seconds
CUDA_GRAPH are activate
Prefill latency: 0.0998879843391478 sec
Decode latency: 0.7028343607671559 sec
Compilation time: 0.80 seconds
Compilation time: 0.81 seconds
Compilation time: 0.85 seconds
Compilation time: 0.80 seconds
Prefill latency: 0.09261360811069608 sec
Decode latency: 0.7029735497198999 sec
Prefill latency: 0.09309957176446915 sec
Decode latency: 0.7021437920629978 sec
Prefill latency: 0.0928485388867557 sec
Decode latency: 0.7019516578875482 sec
Prefill latency: 0.0930957910604775 sec
Decode latency: 0.6996177318505943 sec
Prefill latency: 0.09280777862295508 sec
Decode latency: 0.7022659322246909 sec
Time for inference 1: 0.80 sec total, 40.22 tokens/sec
Decode latency: 0.70 sec
Prefill latency: 0.09 sec
Bandwidth achieved: 1461.21 GB/s
FLOPS achieved: 48.22 TF/s

Prefill latency: 0.0932044587098062 sec
Decode latency: 0.7026339489966631 sec
Time for inference 2: 0.80 sec total, 40.18 tokens/sec
Decode latency: 0.70 sec
Prefill latency: 0.09 sec
Bandwidth achieved: 1459.80 GB/s
FLOPS achieved: 48.17 TF/s

Prefill latency: 0.09281936194747686 sec
Decode latency: 0.7026855237782001 sec
Time for inference 3: 0.80 sec total, 40.20 tokens/sec
Decode latency: 0.70 sec
Prefill latency: 0.09 sec
Bandwidth achieved: 1460.39 GB/s
FLOPS achieved: 48.19 TF/s

Prefill latency: 0.09317265870049596 sec
Decode latency: 0.7023561438545585 sec
Time for inference 4: 0.80 sec total, 40.20 tokens/sec
Decode latency: 0.70 sec
Prefill latency: 0.09 sec
Bandwidth achieved: 1460.36 GB/s
FLOPS achieved: 48.19 TF/s

Prefill latency: 0.0933140330016613 sec
Decode latency: 0.7022868869826198 sec
Time for inference 5: 0.80 sec total, 40.19 tokens/sec
Decode latency: 0.70 sec
Prefill latency: 0.09 sec
Bandwidth achieved: 1460.19 GB/s
FLOPS achieved: 48.19 TF/s

Prefill latency: 0.09298670710995793 sec
Decode latency: 0.7024326571263373 sec
Time for inference 6: 0.80 sec total, 40.20 tokens/sec
Decode latency: 0.70 sec
Prefill latency: 0.09 sec
Bandwidth achieved: 1460.52 GB/s
FLOPS achieved: 48.20 TF/s

Prefill latency: 0.09311648504808545 sec
Decode latency: 0.6986397388391197 sec
Time for inference 7: 0.79 sec total, 40.39 tokens/sec
Decode latency: 0.70 sec
Prefill latency: 0.09 sec
Bandwidth achieved: 1467.32 GB/s
FLOPS achieved: 48.42 TF/s

Prefill latency: 0.09310919279232621 sec
Decode latency: 0.702659189235419 sec
Time for inference 8: 0.80 sec total, 40.19 tokens/sec
Decode latency: 0.70 sec
Prefill latency: 0.09 sec
Bandwidth achieved: 1459.92 GB/s
FLOPS achieved: 48.18 TF/s

Prefill latency: 0.09303922206163406 sec
Decode latency: 0.6990648442879319 sec
Time for inference 9: 0.79 sec total, 40.37 tokens/sec
Decode latency: 0.70 sec
Prefill latency: 0.09 sec
Bandwidth achieved: 1466.66 GB/s
FLOPS achieved: 48.40 TF/s

Prefill latency: 0.0928291380405426 sec
Decode latency: 0.7005914528854191 sec
Time for inference 10: 0.79 sec total, 40.31 tokens/sec
Decode latency: 0.70 sec
Prefill latency: 0.09 sec
Bandwidth achieved: 1464.23 GB/s
FLOPS achieved: 48.32 TF/s

==========
Batch Size: 1
Prompt Length: 1024
Generated tokens: 32
Average decode latency: 0.7016 sec
Average prefill latency: 0.0930 sec
Average tokens/sec: 40.25
Memory used: 40.43 GB
Done. we are killing the process
[rank0]:[W1118 21:50:15.505983360 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
