W1118 21:50:18.629000 3645003 site-packages/torch/distributed/run.py:793] 
W1118 21:50:18.629000 3645003 site-packages/torch/distributed/run.py:793] *****************************************
W1118 21:50:18.629000 3645003 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1118 21:50:18.629000 3645003 site-packages/torch/distributed/run.py:793] *****************************************
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
flash_kv_decode is set to True
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
our world size=8
Using device=cuda
Loading model ...
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8rank: 6, global_rank: 6, world_size: 8, global_world_size: 8

rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
GPTParallel(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x ParallelTransformerBlock(
      semi_compiled = False
      (attention): FuseAttentionMLP(
        (wqkv1): Linear(in_features=8192, out_features=8448, bias=False)
        (wo): Linear(in_features=1024, out_features=8192, bias=False)
        (w2): Linear(in_features=3584, out_features=8192, bias=False)
      )
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 1.23 seconds
CUDA_GRAPH are activate
Prefill latency: 0.10656590526923537 sec
Decode latency: 0.570554296951741 sec
Compilation time: 0.67 seconds
Compilation time: 0.66 seconds
Compilation time: 0.65 seconds
Compilation time: 0.71 seconds
Compilation time: 0.68 seconds
Compilation time: 0.65 seconds
Compilation time: 0.73 secondsCompilation time: 0.69 seconds

Prefill latency: 0.07029140228405595 sec
Decode latency: 0.5676397490315139 sec
Prefill latency: 0.06926400680094957 sec
Decode latency: 0.5709630469791591 sec
Prefill latency: 0.06831919495016336 sec
Decode latency: 0.5650382107123733 sec
Prefill latency: 0.06869788281619549 sec
Decode latency: 0.5659310822375119 sec
Prefill latency: 0.07037822622805834 sec
Decode latency: 0.5675979871302843 sec
Time for inference 1: 0.64 sec total, 50.11 tokens/sec
Decode latency: 0.57 sec
Prefill latency: 0.07 sec
Bandwidth achieved: 962.95 GB/s
FLOPS achieved: 31.78 TF/s

Prefill latency: 0.06842008559033275 sec
Decode latency: 0.5631670448929071 sec
Time for inference 2: 0.63 sec total, 50.63 tokens/sec
Decode latency: 0.56 sec
Prefill latency: 0.07 sec
Bandwidth achieved: 972.79 GB/s
FLOPS achieved: 32.10 TF/s

Prefill latency: 0.06809152290225029 sec
Decode latency: 0.5640053427778184 sec
Time for inference 3: 0.63 sec total, 50.58 tokens/sec
Decode latency: 0.56 sec
Prefill latency: 0.07 sec
Bandwidth achieved: 971.91 GB/s
FLOPS achieved: 32.07 TF/s

Prefill latency: 0.06868183892220259 sec
Decode latency: 0.5680037792772055 sec
Time for inference 4: 0.64 sec total, 50.20 tokens/sec
Decode latency: 0.57 sec
Prefill latency: 0.07 sec
Bandwidth achieved: 964.57 GB/s
FLOPS achieved: 31.83 TF/s

Prefill latency: 0.07013051537796855 sec
Decode latency: 0.5657046111300588 sec
Time for inference 5: 0.64 sec total, 50.26 tokens/sec
Decode latency: 0.57 sec
Prefill latency: 0.07 sec
Bandwidth achieved: 965.82 GB/s
FLOPS achieved: 31.87 TF/s

Prefill latency: 0.07145878905430436 sec
Decode latency: 0.5640985728241503 sec
Time for inference 6: 0.64 sec total, 50.30 tokens/sec
Decode latency: 0.56 sec
Prefill latency: 0.07 sec
Bandwidth achieved: 966.58 GB/s
FLOPS achieved: 31.90 TF/s

Prefill latency: 0.07314910599961877 sec
Decode latency: 0.5681364387273788 sec
Time for inference 7: 0.64 sec total, 49.84 tokens/sec
Decode latency: 0.57 sec
Prefill latency: 0.07 sec
Bandwidth achieved: 957.79 GB/s
FLOPS achieved: 31.61 TF/s

Prefill latency: 0.06823813356459141 sec
Decode latency: 0.5651048626750708 sec
Time for inference 8: 0.63 sec total, 50.47 tokens/sec
Decode latency: 0.57 sec
Prefill latency: 0.07 sec
Bandwidth achieved: 969.77 GB/s
FLOPS achieved: 32.00 TF/s

Prefill latency: 0.06878702808171511 sec
Decode latency: 0.5642255782149732 sec
Time for inference 9: 0.63 sec total, 50.50 tokens/sec
Decode latency: 0.56 sec
Prefill latency: 0.07 sec
Bandwidth achieved: 970.47 GB/s
FLOPS achieved: 32.03 TF/s

Prefill latency: 0.0703668687492609 sec
Decode latency: 0.5676119900308549 sec
Time for inference 10: 0.64 sec total, 50.11 tokens/sec
Decode latency: 0.57 sec
Prefill latency: 0.07 sec
Bandwidth achieved: 962.95 GB/s
FLOPS achieved: 31.78 TF/s

==========
Batch Size: 1
Prompt Length: 1024
Generated tokens: 32
Average decode latency: 0.5658 sec
Average prefill latency: 0.0698 sec
Average tokens/sec: 50.30
Memory used: 23.22 GB
Done. we are killing the process
[rank0]:[W1118 21:50:44.173691627 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
