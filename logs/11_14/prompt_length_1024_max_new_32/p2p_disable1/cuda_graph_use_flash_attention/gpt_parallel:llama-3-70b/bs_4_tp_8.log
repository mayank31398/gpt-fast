W1118 21:51:38.261000 3646933 site-packages/torch/distributed/run.py:793] 
W1118 21:51:38.261000 3646933 site-packages/torch/distributed/run.py:793] *****************************************
W1118 21:51:38.261000 3646933 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1118 21:51:38.261000 3646933 site-packages/torch/distributed/run.py:793] *****************************************
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
flash_kv_decode is set to True
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
our world size=8
Using device=cuda
Loading model ...
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8rank: 1, global_rank: 1, world_size: 8, global_world_size: 8

rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
GPTParallel(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x ParallelTransformerBlock(
      semi_compiled = False
      (attention): FuseAttentionMLP(
        (wqkv1): Linear(in_features=8192, out_features=8448, bias=False)
        (wo): Linear(in_features=1024, out_features=8192, bias=False)
        (w2): Linear(in_features=3584, out_features=8192, bias=False)
      )
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 1.57 seconds
CUDA_GRAPH are activate
Prefill latency: 0.21865029307082295 sec
Decode latency: 0.6156762051396072 sec
Compilation time: 0.88 seconds
Compilation time: 0.84 seconds
Compilation time: 0.88 seconds
Compilation time: 0.84 seconds
Compilation time: 0.82 seconds
Compilation time: 0.92 seconds
Compilation time: 0.84 secondsCompilation time: 0.86 seconds

Prefill latency: 0.2000446026213467 sec
Decode latency: 0.6142905778251588 sec
Prefill latency: 0.20054548792541027 sec
Decode latency: 0.6129068681038916 sec
Prefill latency: 0.2005262873135507 sec
Decode latency: 0.6108680367469788 sec
Prefill latency: 0.20117740472778678 sec
Decode latency: 0.6140372599475086 sec
Prefill latency: 0.1996353566646576 sec
Decode latency: 0.6104135941714048 sec
Time for inference 1: 0.81 sec total, 157.88 tokens/sec
Decode latency: 0.61 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 3033.82 GB/s
FLOPS achieved: 100.12 TF/s

Prefill latency: 0.19909802405163646 sec
Decode latency: 0.6269839000888169 sec
Time for inference 2: 0.83 sec total, 154.83 tokens/sec
Decode latency: 0.63 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 2975.03 GB/s
FLOPS achieved: 98.18 TF/s

Prefill latency: 0.20066813612356782 sec
Decode latency: 0.612930053845048 sec
Time for inference 3: 0.81 sec total, 157.17 tokens/sec
Decode latency: 0.61 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 3020.07 GB/s
FLOPS achieved: 99.66 TF/s

Prefill latency: 0.19977955240756273 sec
Decode latency: 0.6105699809268117 sec
Time for inference 4: 0.81 sec total, 157.83 tokens/sec
Decode latency: 0.61 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 3032.76 GB/s
FLOPS achieved: 100.08 TF/s

Prefill latency: 0.20031160535290837 sec
Decode latency: 0.6120420661754906 sec
Time for inference 5: 0.81 sec total, 157.44 tokens/sec
Decode latency: 0.61 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 3025.31 GB/s
FLOPS achieved: 99.84 TF/s

Prefill latency: 0.2022631228901446 sec
Decode latency: 0.613204512745142 sec
Time for inference 6: 0.82 sec total, 156.84 tokens/sec
Decode latency: 0.61 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 3013.67 GB/s
FLOPS achieved: 99.45 TF/s

Prefill latency: 0.20019477093592286 sec
Decode latency: 0.6124690431170166 sec
Time for inference 7: 0.81 sec total, 157.38 tokens/sec
Decode latency: 0.61 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 3024.17 GB/s
FLOPS achieved: 99.80 TF/s

Prefill latency: 0.19997740583494306 sec
Decode latency: 0.6114959581755102 sec
Time for inference 8: 0.81 sec total, 157.54 tokens/sec
Decode latency: 0.61 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 3027.27 GB/s
FLOPS achieved: 99.90 TF/s

Prefill latency: 0.19989255582913756 sec
Decode latency: 0.6169353551231325 sec
Time for inference 9: 0.82 sec total, 156.56 tokens/sec
Decode latency: 0.62 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 3008.43 GB/s
FLOPS achieved: 99.28 TF/s

Prefill latency: 0.2000048952177167 sec
Decode latency: 0.6202418352477252 sec
Time for inference 10: 0.82 sec total, 155.92 tokens/sec
Decode latency: 0.62 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 2996.03 GB/s
FLOPS achieved: 98.87 TF/s

==========
Batch Size: 4
Prompt Length: 1024
Generated tokens: 32
Average decode latency: 0.6147 sec
Average prefill latency: 0.2002 sec
Average tokens/sec: 156.94
Memory used: 28.52 GB
Done. we are killing the process
[rank0]:[W1118 21:52:07.911353383 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
