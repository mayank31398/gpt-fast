W1118 21:49:23.080000 3644172 site-packages/torch/distributed/run.py:793] 
W1118 21:49:23.080000 3644172 site-packages/torch/distributed/run.py:793] *****************************************
W1118 21:49:23.080000 3644172 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1118 21:49:23.080000 3644172 site-packages/torch/distributed/run.py:793] *****************************************
rank: 1, global_rank: 1, world_size: 2, global_world_size: 2
rank: 0, global_rank: 0, world_size: 2, global_world_size: 2
rank: 0, global_rank: 0, world_size: 2, global_world_size: 2
rank: 0, global_rank: 0, world_size: 2, global_world_size: 2
rank: 0, global_rank: 0, world_size: 2, global_world_size: 2
rank: 0, global_rank: 0, world_size: 2, global_world_size: 2
flash_kv_decode is set to True
rank: 0, global_rank: 0, world_size: 2, global_world_size: 2
our world size=2
Using device=cuda
Loading model ...
rank: 1, global_rank: 1, world_size: 2, global_world_size: 2
rank: 1, global_rank: 1, world_size: 2, global_world_size: 2
rank: 1, global_rank: 1, world_size: 2, global_world_size: 2
rank: 1, global_rank: 1, world_size: 2, global_world_size: 2
rank: 1, global_rank: 1, world_size: 2, global_world_size: 2
GPTParallel(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x ParallelTransformerBlock(
      semi_compiled = False
      (attention): FuseAttentionMLP(
        (wqkv1): Linear(in_features=8192, out_features=33792, bias=False)
        (wo): Linear(in_features=4096, out_features=8192, bias=False)
        (w2): Linear(in_features=14336, out_features=8192, bias=False)
      )
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 1.16 seconds
CUDA_GRAPH are activate
Prefill latency: 0.2631540992297232 sec
Decode latency: 1.0288670328445733 sec
Compilation time: 1.28 seconds
Compilation time: 1.29 seconds
Prefill latency: 0.2383938580751419 sec
Decode latency: 1.0281662428751588 sec
Prefill latency: 0.2381707141175866 sec
Decode latency: 1.0270191538147628 sec
Prefill latency: 0.2381685907021165 sec
Decode latency: 1.0274544269777834 sec
Prefill latency: 0.23867205483838916 sec
Decode latency: 1.0279595851898193 sec
Prefill latency: 0.2364102229475975 sec
Decode latency: 1.0270952335558832 sec
Time for inference 1: 1.26 sec total, 25.32 tokens/sec
Decode latency: 1.03 sec
Prefill latency: 0.24 sec
Bandwidth achieved: 1786.13 GB/s
FLOPS achieved: 58.94 TF/s

Prefill latency: 0.2381910178810358 sec
Decode latency: 1.0279663656838238 sec
Time for inference 2: 1.27 sec total, 25.26 tokens/sec
Decode latency: 1.03 sec
Prefill latency: 0.24 sec
Bandwidth achieved: 1782.39 GB/s
FLOPS achieved: 58.82 TF/s

Prefill latency: 0.23908256879076362 sec
Decode latency: 1.0277153099887073 sec
Time for inference 3: 1.27 sec total, 25.25 tokens/sec
Decode latency: 1.03 sec
Prefill latency: 0.24 sec
Bandwidth achieved: 1781.48 GB/s
FLOPS achieved: 58.79 TF/s

Prefill latency: 0.23674484295770526 sec
Decode latency: 1.0273196250200272 sec
Time for inference 4: 1.26 sec total, 25.30 tokens/sec
Decode latency: 1.03 sec
Prefill latency: 0.24 sec
Bandwidth achieved: 1785.32 GB/s
FLOPS achieved: 58.92 TF/s

Prefill latency: 0.23845574306324124 sec
Decode latency: 1.028147695120424 sec
Time for inference 5: 1.27 sec total, 25.25 tokens/sec
Decode latency: 1.03 sec
Prefill latency: 0.24 sec
Bandwidth achieved: 1781.62 GB/s
FLOPS achieved: 58.79 TF/s

Prefill latency: 0.2382259671576321 sec
Decode latency: 1.0275860838592052 sec
Time for inference 6: 1.27 sec total, 25.27 tokens/sec
Decode latency: 1.03 sec
Prefill latency: 0.24 sec
Bandwidth achieved: 1782.65 GB/s
FLOPS achieved: 58.83 TF/s

Prefill latency: 0.23757657501846552 sec
Decode latency: 1.027660075109452 sec
Time for inference 7: 1.27 sec total, 25.28 tokens/sec
Decode latency: 1.03 sec
Prefill latency: 0.24 sec
Bandwidth achieved: 1783.48 GB/s
FLOPS achieved: 58.85 TF/s

Prefill latency: 0.23816403606906533 sec
Decode latency: 1.027820568997413 sec
Time for inference 8: 1.27 sec total, 25.26 tokens/sec
Decode latency: 1.03 sec
Prefill latency: 0.24 sec
Bandwidth achieved: 1782.36 GB/s
FLOPS achieved: 58.82 TF/s

Prefill latency: 0.2374914577230811 sec
Decode latency: 1.0267746271565557 sec
Time for inference 9: 1.26 sec total, 25.30 tokens/sec
Decode latency: 1.03 sec
Prefill latency: 0.24 sec
Bandwidth achieved: 1784.92 GB/s
FLOPS achieved: 58.90 TF/s

Prefill latency: 0.23885591002181172 sec
Decode latency: 1.0277038333006203 sec
Time for inference 10: 1.27 sec total, 25.25 tokens/sec
Decode latency: 1.03 sec
Prefill latency: 0.24 sec
Bandwidth achieved: 1781.76 GB/s
FLOPS achieved: 58.80 TF/s

==========
Batch Size: 1
Prompt Length: 1024
Generated tokens: 32
Average decode latency: 1.0276 sec
Average prefill latency: 0.2379 sec
Average tokens/sec: 25.27
Memory used: 75.72 GB
Done. we are killing the process
[rank0]:[W1118 21:49:50.200620005 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
