W1119 16:42:07.824000 3010273 site-packages/torch/distributed/run.py:793] 
W1119 16:42:07.824000 3010273 site-packages/torch/distributed/run.py:793] *****************************************
W1119 16:42:07.824000 3010273 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1119 16:42:07.824000 3010273 site-packages/torch/distributed/run.py:793] *****************************************
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
flash_kv_decode is set to True
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
our world size=8
Using device=cuda
Loading model ...
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
GPTDense(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x DenseTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=1280, bias=False)
        (wo): Linear(in_features=1024, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 1.46 seconds
CUDA_GRAPH are activate
Prefill latency: 0.12416624720208347 sec
Decode latency: 0.7521751038730145 sec
Compilation time: 0.85 seconds
Compilation time: 0.88 seconds
Compilation time: 0.87 seconds
Compilation time: 0.90 seconds
Compilation time: 0.91 seconds
Compilation time: 0.90 seconds
Compilation time: 0.89 seconds
Compilation time: 0.88 seconds
Prefill latency: 0.08080541016533971 sec
Decode latency: 0.7494857630226761 sec
Prefill latency: 0.0809949969407171 sec
Decode latency: 0.7507140000816435 sec
Prefill latency: 0.07946378504857421 sec
Decode latency: 0.74884122190997 sec
Prefill latency: 0.07880073203705251 sec
Decode latency: 0.7459367248229682 sec
Prefill latency: 0.07920353300869465 sec
Decode latency: 0.7497999640181661 sec
Time for inference 1: 0.83 sec total, 38.57 tokens/sec
Decode latency: 0.75 sec
Prefill latency: 0.08 sec
Bandwidth achieved: 741.27 GB/s
FLOPS achieved: 24.46 TF/s

Prefill latency: 0.07862727390602231 sec
Decode latency: 0.7589955828152597 sec
Time for inference 2: 0.84 sec total, 38.17 tokens/sec
Decode latency: 0.76 sec
Prefill latency: 0.08 sec
Bandwidth achieved: 733.56 GB/s
FLOPS achieved: 24.21 TF/s

Prefill latency: 0.07962529896758497 sec
Decode latency: 0.754376005847007 sec
Time for inference 3: 0.83 sec total, 38.34 tokens/sec
Decode latency: 0.75 sec
Prefill latency: 0.08 sec
Bandwidth achieved: 736.79 GB/s
FLOPS achieved: 24.31 TF/s

Prefill latency: 0.07908587320707738 sec
Decode latency: 0.7551626299973577 sec
Time for inference 4: 0.83 sec total, 38.33 tokens/sec
Decode latency: 0.76 sec
Prefill latency: 0.08 sec
Bandwidth achieved: 736.57 GB/s
FLOPS achieved: 24.31 TF/s

Prefill latency: 0.07843149011023343 sec
Decode latency: 0.7516773480456322 sec
Time for inference 5: 0.83 sec total, 38.52 tokens/sec
Decode latency: 0.75 sec
Prefill latency: 0.08 sec
Bandwidth achieved: 740.26 GB/s
FLOPS achieved: 24.43 TF/s

Prefill latency: 0.07899799104779959 sec
Decode latency: 0.7462585340254009 sec
Time for inference 6: 0.83 sec total, 38.74 tokens/sec
Decode latency: 0.75 sec
Prefill latency: 0.08 sec
Bandwidth achieved: 744.44 GB/s
FLOPS achieved: 24.57 TF/s

Prefill latency: 0.07935491600073874 sec
Decode latency: 0.7463538500014693 sec
Time for inference 7: 0.83 sec total, 38.72 tokens/sec
Decode latency: 0.75 sec
Prefill latency: 0.08 sec
Bandwidth achieved: 744.11 GB/s
FLOPS achieved: 24.56 TF/s

Prefill latency: 0.07995081902481616 sec
Decode latency: 0.7506214431487024 sec
Time for inference 8: 0.83 sec total, 38.48 tokens/sec
Decode latency: 0.75 sec
Prefill latency: 0.08 sec
Bandwidth achieved: 739.48 GB/s
FLOPS achieved: 24.40 TF/s

Prefill latency: 0.07898114994168282 sec
Decode latency: 0.7443976600188762 sec
Time for inference 9: 0.82 sec total, 38.82 tokens/sec
Decode latency: 0.74 sec
Prefill latency: 0.08 sec
Bandwidth achieved: 746.08 GB/s
FLOPS achieved: 24.62 TF/s

Prefill latency: 0.08310360298492014 sec
Decode latency: 0.7493541948497295 sec
Time for inference 10: 0.83 sec total, 38.40 tokens/sec
Decode latency: 0.75 sec
Prefill latency: 0.08 sec
Bandwidth achieved: 737.99 GB/s
FLOPS achieved: 24.35 TF/s

==========
Batch Size: 1
Prompt Length: 1024
Generated tokens: 32
Average decode latency: 0.7507 sec
Average prefill latency: 0.0795 sec
Average tokens/sec: 38.51
Memory used: 24.87 GB
Done. we are killing the process
[rank0]:[W1119 16:42:37.905505829 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
