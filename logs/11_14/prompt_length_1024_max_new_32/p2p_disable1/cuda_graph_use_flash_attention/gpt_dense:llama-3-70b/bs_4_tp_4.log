W1119 16:43:01.738000 3011700 site-packages/torch/distributed/run.py:793] 
W1119 16:43:01.738000 3011700 site-packages/torch/distributed/run.py:793] *****************************************
W1119 16:43:01.738000 3011700 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1119 16:43:01.738000 3011700 site-packages/torch/distributed/run.py:793] *****************************************
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
flash_kv_decode is set to True
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
our world size=4
Using device=cuda
Loading model ...
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
GPTDense(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x DenseTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=2560, bias=False)
        (wo): Linear(in_features=2048, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=14336, bias=False)
        (w2): Linear(in_features=7168, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 1.32 seconds
CUDA_GRAPH are activate
Prefill latency: 0.3551952070556581 sec
Decode latency: 0.8714938340708613 sec
Compilation time: 1.23 seconds
Compilation time: 1.24 seconds
Compilation time: 1.24 seconds
Compilation time: 1.23 seconds
Prefill latency: 0.3534303440246731 sec
Decode latency: 0.8678313521668315 sec
Prefill latency: 0.35494784405454993 sec
Decode latency: 0.8670709789730608 sec
Prefill latency: 0.35479477094486356 sec
Decode latency: 0.8680079549085349 sec
Prefill latency: 0.35490012099035084 sec
Decode latency: 0.868506392929703 sec
Prefill latency: 0.35413408605381846 sec
Decode latency: 0.8680635609198362 sec
Time for inference 1: 1.22 sec total, 104.68 tokens/sec
Decode latency: 0.87 sec
Prefill latency: 0.35 sec
Bandwidth achieved: 3802.95 GB/s
FLOPS achieved: 125.50 TF/s

Prefill latency: 0.35627039801329374 sec
Decode latency: 0.8681929039303213 sec
Time for inference 2: 1.23 sec total, 104.48 tokens/sec
Decode latency: 0.87 sec
Prefill latency: 0.36 sec
Bandwidth achieved: 3795.77 GB/s
FLOPS achieved: 125.26 TF/s

Prefill latency: 0.35540538909845054 sec
Decode latency: 0.8684630021452904 sec
Time for inference 3: 1.22 sec total, 104.54 tokens/sec
Decode latency: 0.87 sec
Prefill latency: 0.36 sec
Bandwidth achieved: 3797.80 GB/s
FLOPS achieved: 125.33 TF/s

Prefill latency: 0.35632779891602695 sec
Decode latency: 0.8671426870860159 sec
Time for inference 4: 1.22 sec total, 104.57 tokens/sec
Decode latency: 0.87 sec
Prefill latency: 0.36 sec
Bandwidth achieved: 3799.13 GB/s
FLOPS achieved: 125.37 TF/s

Prefill latency: 0.3546528611332178 sec
Decode latency: 0.868588644079864 sec
Time for inference 5: 1.22 sec total, 104.59 tokens/sec
Decode latency: 0.87 sec
Prefill latency: 0.35 sec
Bandwidth achieved: 3799.80 GB/s
FLOPS achieved: 125.39 TF/s

Prefill latency: 0.35528801707550883 sec
Decode latency: 0.8677678420208395 sec
Time for inference 6: 1.22 sec total, 104.61 tokens/sec
Decode latency: 0.87 sec
Prefill latency: 0.36 sec
Bandwidth achieved: 3800.33 GB/s
FLOPS achieved: 125.41 TF/s

Prefill latency: 0.3554510159883648 sec
Decode latency: 0.867639719042927 sec
Time for inference 7: 1.22 sec total, 104.61 tokens/sec
Decode latency: 0.87 sec
Prefill latency: 0.36 sec
Bandwidth achieved: 3800.32 GB/s
FLOPS achieved: 125.41 TF/s

Prefill latency: 0.3551112851127982 sec
Decode latency: 0.8681361828930676 sec
Time for inference 8: 1.22 sec total, 104.59 tokens/sec
Decode latency: 0.87 sec
Prefill latency: 0.36 sec
Bandwidth achieved: 3799.79 GB/s
FLOPS achieved: 125.39 TF/s

Prefill latency: 0.3547700811177492 sec
Decode latency: 0.8671258180402219 sec
Time for inference 9: 1.22 sec total, 104.70 tokens/sec
Decode latency: 0.87 sec
Prefill latency: 0.35 sec
Bandwidth achieved: 3803.81 GB/s
FLOPS achieved: 125.53 TF/s

Prefill latency: 0.3559829050209373 sec
Decode latency: 0.8685012559872121 sec
Time for inference 10: 1.23 sec total, 104.49 tokens/sec
Decode latency: 0.87 sec
Prefill latency: 0.36 sec
Bandwidth achieved: 3795.95 GB/s
FLOPS achieved: 125.27 TF/s

==========
Batch Size: 4
Prompt Length: 1024
Generated tokens: 32
Average decode latency: 0.8680 sec
Average prefill latency: 0.3553 sec
Average tokens/sec: 104.59
Memory used: 55.30 GB
Done. we are killing the process
[rank0]:[W1119 16:43:31.822604431 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
