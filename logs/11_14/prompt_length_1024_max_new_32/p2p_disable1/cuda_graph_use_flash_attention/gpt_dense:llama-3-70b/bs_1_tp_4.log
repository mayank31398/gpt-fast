W1119 16:41:40.833000 3009765 site-packages/torch/distributed/run.py:793] 
W1119 16:41:40.833000 3009765 site-packages/torch/distributed/run.py:793] *****************************************
W1119 16:41:40.833000 3009765 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1119 16:41:40.833000 3009765 site-packages/torch/distributed/run.py:793] *****************************************
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
flash_kv_decode is set to True
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
our world size=4
Using device=cuda
Loading model ...
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4rank: 3, global_rank: 3, world_size: 4, global_world_size: 4

rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
GPTDense(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x DenseTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=2560, bias=False)
        (wo): Linear(in_features=2048, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=14336, bias=False)
        (w2): Linear(in_features=7168, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 1.33 seconds
CUDA_GRAPH are activate
Prefill latency: 0.1058065330144018 sec
Decode latency: 0.7680456452071667 sec
Compilation time: 0.89 seconds
Compilation time: 0.91 seconds
Compilation time: 0.87 seconds
Compilation time: 0.87 seconds
Prefill latency: 0.10384900402277708 sec
Decode latency: 0.7650015978142619 sec
Prefill latency: 0.10378238488920033 sec
Decode latency: 0.7640867969021201 sec
Prefill latency: 0.10362882888875902 sec
Decode latency: 0.7640500490088016 sec
Prefill latency: 0.10396695113740861 sec
Decode latency: 0.7643899638205767 sec
Prefill latency: 0.10394447296857834 sec
Decode latency: 0.763221335131675 sec
Time for inference 1: 0.87 sec total, 36.88 tokens/sec
Decode latency: 0.76 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 1339.81 GB/s
FLOPS achieved: 44.21 TF/s

Prefill latency: 0.10372116579674184 sec
Decode latency: 0.7635611491277814 sec
Time for inference 2: 0.87 sec total, 36.87 tokens/sec
Decode latency: 0.76 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 1339.65 GB/s
FLOPS achieved: 44.21 TF/s

Prefill latency: 0.1039352579973638 sec
Decode latency: 0.7635909470263869 sec
Time for inference 3: 0.87 sec total, 36.87 tokens/sec
Decode latency: 0.76 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 1339.32 GB/s
FLOPS achieved: 44.20 TF/s

Prefill latency: 0.10381689597852528 sec
Decode latency: 0.7632950639817864 sec
Time for inference 4: 0.87 sec total, 36.88 tokens/sec
Decode latency: 0.76 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 1339.80 GB/s
FLOPS achieved: 44.21 TF/s

Prefill latency: 0.10386134497821331 sec
Decode latency: 0.7632232431787997 sec
Time for inference 5: 0.87 sec total, 36.88 tokens/sec
Decode latency: 0.76 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 1339.93 GB/s
FLOPS achieved: 44.22 TF/s

Prefill latency: 0.10388671793043613 sec
Decode latency: 0.7630851708818227 sec
Time for inference 6: 0.87 sec total, 36.89 tokens/sec
Decode latency: 0.76 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 1340.03 GB/s
FLOPS achieved: 44.22 TF/s

Prefill latency: 0.10371508705429733 sec
Decode latency: 0.7626407789066434 sec
Time for inference 7: 0.87 sec total, 36.91 tokens/sec
Decode latency: 0.76 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 1340.93 GB/s
FLOPS achieved: 44.25 TF/s

Prefill latency: 0.10379113000817597 sec
Decode latency: 0.7630706049967557 sec
Time for inference 8: 0.87 sec total, 36.89 tokens/sec
Decode latency: 0.76 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 1340.28 GB/s
FLOPS achieved: 44.23 TF/s

Prefill latency: 0.10376505902968347 sec
Decode latency: 0.7631712690927088 sec
Time for inference 9: 0.87 sec total, 36.89 tokens/sec
Decode latency: 0.76 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 1340.11 GB/s
FLOPS achieved: 44.22 TF/s

Prefill latency: 0.10397563502192497 sec
Decode latency: 0.7633147251326591 sec
Time for inference 10: 0.87 sec total, 36.87 tokens/sec
Decode latency: 0.76 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 1339.61 GB/s
FLOPS achieved: 44.21 TF/s

==========
Batch Size: 1
Prompt Length: 1024
Generated tokens: 32
Average decode latency: 0.7632 sec
Average prefill latency: 0.1038 sec
Average tokens/sec: 36.88
Memory used: 42.74 GB
Done. we are killing the process
[rank0]:[W1119 16:42:04.105134901 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
