W1119 16:41:07.657000 3009437 site-packages/torch/distributed/run.py:793] 
W1119 16:41:07.657000 3009437 site-packages/torch/distributed/run.py:793] *****************************************
W1119 16:41:07.657000 3009437 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1119 16:41:07.657000 3009437 site-packages/torch/distributed/run.py:793] *****************************************
rank: 1, global_rank: 1, world_size: 2, global_world_size: 2
rank: 0, global_rank: 0, world_size: 2, global_world_size: 2
rank: 0, global_rank: 0, world_size: 2, global_world_size: 2
rank: 0, global_rank: 0, world_size: 2, global_world_size: 2
rank: 0, global_rank: 0, world_size: 2, global_world_size: 2
rank: 0, global_rank: 0, world_size: 2, global_world_size: 2
flash_kv_decode is set to True
rank: 0, global_rank: 0, world_size: 2, global_world_size: 2
our world size=2
Using device=cuda
Loading model ...
rank: 1, global_rank: 1, world_size: 2, global_world_size: 2
rank: 1, global_rank: 1, world_size: 2, global_world_size: 2
rank: 1, global_rank: 1, world_size: 2, global_world_size: 2
rank: 1, global_rank: 1, world_size: 2, global_world_size: 2
rank: 1, global_rank: 1, world_size: 2, global_world_size: 2
GPTDense(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x DenseTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=5120, bias=False)
        (wo): Linear(in_features=4096, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=28672, bias=False)
        (w2): Linear(in_features=14336, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 1.33 seconds
CUDA_GRAPH are activate
Prefill latency: 0.3646217619534582 sec
Decode latency: 1.080626720096916 sec
Compilation time: 1.45 seconds
Compilation time: 1.45 seconds
Prefill latency: 0.3398611778393388 sec
Decode latency: 1.0901109050028026 sec
Prefill latency: 0.3400727689731866 sec
Decode latency: 1.079183162888512 sec
Prefill latency: 0.33786931610666215 sec
Decode latency: 1.0779592380858958 sec
Prefill latency: 0.3381148201879114 sec
Decode latency: 1.0895726459566504 sec
Prefill latency: 0.33868680195882916 sec
Decode latency: 1.078537205932662 sec
Time for inference 1: 1.42 sec total, 22.57 tokens/sec
Decode latency: 1.08 sec
Prefill latency: 0.34 sec
Bandwidth achieved: 1592.49 GB/s
FLOPS achieved: 52.55 TF/s

Prefill latency: 0.33742478606291115 sec
Decode latency: 1.0783544860314578 sec
Time for inference 2: 1.42 sec total, 22.59 tokens/sec
Decode latency: 1.08 sec
Prefill latency: 0.34 sec
Bandwidth achieved: 1594.06 GB/s
FLOPS achieved: 52.60 TF/s

Prefill latency: 0.3392521778587252 sec
Decode latency: 1.0786188011988997 sec
Time for inference 3: 1.42 sec total, 22.56 tokens/sec
Decode latency: 1.08 sec
Prefill latency: 0.34 sec
Bandwidth achieved: 1591.70 GB/s
FLOPS achieved: 52.53 TF/s

Prefill latency: 0.33868317608721554 sec
Decode latency: 1.0900185690261424 sec
Time for inference 4: 1.43 sec total, 22.39 tokens/sec
Decode latency: 1.09 sec
Prefill latency: 0.34 sec
Bandwidth achieved: 1579.70 GB/s
FLOPS achieved: 52.13 TF/s

Prefill latency: 0.3372486161533743 sec
Decode latency: 1.0784579911269248 sec
Time for inference 5: 1.42 sec total, 22.59 tokens/sec
Decode latency: 1.08 sec
Prefill latency: 0.34 sec
Bandwidth achieved: 1594.14 GB/s
FLOPS achieved: 52.61 TF/s

Prefill latency: 0.33847983716987073 sec
Decode latency: 1.0781389789190143 sec
Time for inference 6: 1.42 sec total, 22.58 tokens/sec
Decode latency: 1.08 sec
Prefill latency: 0.34 sec
Bandwidth achieved: 1593.13 GB/s
FLOPS achieved: 52.57 TF/s

Prefill latency: 0.3378838449716568 sec
Decode latency: 1.0783290041144937 sec
Time for inference 7: 1.42 sec total, 22.59 tokens/sec
Decode latency: 1.08 sec
Prefill latency: 0.34 sec
Bandwidth achieved: 1593.55 GB/s
FLOPS achieved: 52.59 TF/s

Prefill latency: 0.3370502730831504 sec
Decode latency: 1.0784437600523233 sec
Time for inference 8: 1.42 sec total, 22.60 tokens/sec
Decode latency: 1.08 sec
Prefill latency: 0.34 sec
Bandwidth achieved: 1594.43 GB/s
FLOPS achieved: 52.62 TF/s

Prefill latency: 0.3387273079715669 sec
Decode latency: 1.0786381619982421 sec
Time for inference 9: 1.42 sec total, 22.57 tokens/sec
Decode latency: 1.08 sec
Prefill latency: 0.34 sec
Bandwidth achieved: 1592.33 GB/s
FLOPS achieved: 52.55 TF/s

Prefill latency: 0.33797768992371857 sec
Decode latency: 1.0787924379110336 sec
Time for inference 10: 1.42 sec total, 22.58 tokens/sec
Decode latency: 1.08 sec
Prefill latency: 0.34 sec
Bandwidth achieved: 1592.97 GB/s
FLOPS achieved: 52.57 TF/s

==========
Batch Size: 1
Prompt Length: 1024
Generated tokens: 32
Average decode latency: 1.0796 sec
Average prefill latency: 0.3381 sec
Average tokens/sec: 22.56
Memory used: 78.05 GB
Done. we are killing the process
[rank0]:[W1119 16:41:37.526153550 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
