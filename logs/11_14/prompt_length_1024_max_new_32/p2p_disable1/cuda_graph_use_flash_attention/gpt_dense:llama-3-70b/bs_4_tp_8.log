W1119 16:43:34.608000 3012214 site-packages/torch/distributed/run.py:793] 
W1119 16:43:34.608000 3012214 site-packages/torch/distributed/run.py:793] *****************************************
W1119 16:43:34.608000 3012214 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1119 16:43:34.608000 3012214 site-packages/torch/distributed/run.py:793] *****************************************
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
flash_kv_decode is set to True
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
our world size=8
Using device=cuda
Loading model ...
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
GPTDense(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x DenseTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=1280, bias=False)
        (wo): Linear(in_features=1024, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 1.50 seconds
CUDA_GRAPH are activate
Prefill latency: 0.263050336856395 sec
Decode latency: 0.8163198321126401 sec
Compilation time: 1.08 seconds
Compilation time: 1.08 secondsCompilation time: 1.07 seconds

Compilation time: 1.09 seconds
Compilation time: 1.08 seconds
Compilation time: 1.07 seconds
Compilation time: 1.08 seconds
Compilation time: 1.07 seconds
Prefill latency: 0.24451601086184382 sec
Decode latency: 0.8183009868953377 sec
Prefill latency: 0.24273917404934764 sec
Decode latency: 0.8323450868483633 sec
Prefill latency: 0.2418026840314269 sec
Decode latency: 0.8202541929204017 sec
Prefill latency: 0.24163342802785337 sec
Decode latency: 0.8168546948581934 sec
Prefill latency: 0.2421732449438423 sec
Decode latency: 0.8168494990095496 sec
Time for inference 1: 1.06 sec total, 120.81 tokens/sec
Decode latency: 0.82 sec
Prefill latency: 0.24 sec
Bandwidth achieved: 2321.56 GB/s
FLOPS achieved: 76.61 TF/s

Prefill latency: 0.24183155107311904 sec
Decode latency: 0.8132694030646235 sec
Time for inference 2: 1.06 sec total, 121.25 tokens/sec
Decode latency: 0.81 sec
Prefill latency: 0.24 sec
Bandwidth achieved: 2330.10 GB/s
FLOPS achieved: 76.89 TF/s

Prefill latency: 0.24145126598887146 sec
Decode latency: 0.8225250898394734 sec
Time for inference 3: 1.06 sec total, 120.24 tokens/sec
Decode latency: 0.82 sec
Prefill latency: 0.24 sec
Bandwidth achieved: 2310.69 GB/s
FLOPS achieved: 76.25 TF/s

Prefill latency: 0.2425186731852591 sec
Decode latency: 0.8260080609470606 sec
Time for inference 4: 1.07 sec total, 119.73 tokens/sec
Decode latency: 0.83 sec
Prefill latency: 0.24 sec
Bandwidth achieved: 2300.87 GB/s
FLOPS achieved: 75.93 TF/s

Prefill latency: 0.24888103595003486 sec
Decode latency: 0.8177891289815307 sec
Time for inference 5: 1.07 sec total, 119.93 tokens/sec
Decode latency: 0.82 sec
Prefill latency: 0.25 sec
Bandwidth achieved: 2304.69 GB/s
FLOPS achieved: 76.05 TF/s

Prefill latency: 0.2415742101147771 sec
Decode latency: 0.816188951022923 sec
Time for inference 6: 1.06 sec total, 120.92 tokens/sec
Decode latency: 0.82 sec
Prefill latency: 0.24 sec
Bandwidth achieved: 2323.62 GB/s
FLOPS achieved: 76.68 TF/s

Prefill latency: 0.24140928708948195 sec
Decode latency: 0.8161615140270442 sec
Time for inference 7: 1.06 sec total, 120.95 tokens/sec
Decode latency: 0.82 sec
Prefill latency: 0.24 sec
Bandwidth achieved: 2324.35 GB/s
FLOPS achieved: 76.70 TF/s

Prefill latency: 0.24256450799293816 sec
Decode latency: 0.8315012969542295 sec
Time for inference 8: 1.07 sec total, 119.08 tokens/sec
Decode latency: 0.83 sec
Prefill latency: 0.24 sec
Bandwidth achieved: 2288.28 GB/s
FLOPS achieved: 75.51 TF/s

Prefill latency: 0.24399357894435525 sec
Decode latency: 0.8278275979682803 sec
Time for inference 9: 1.07 sec total, 119.34 tokens/sec
Decode latency: 0.83 sec
Prefill latency: 0.24 sec
Bandwidth achieved: 2293.30 GB/s
FLOPS achieved: 75.68 TF/s

Prefill latency: 0.24156096996739507 sec
Decode latency: 0.8188419479411095 sec
Time for inference 10: 1.06 sec total, 120.61 tokens/sec
Decode latency: 0.82 sec
Prefill latency: 0.24 sec
Bandwidth achieved: 2317.73 GB/s
FLOPS achieved: 76.48 TF/s

==========
Batch Size: 4
Prompt Length: 1024
Generated tokens: 32
Average decode latency: 0.8207 sec
Average prefill latency: 0.2428 sec
Average tokens/sec: 120.29
Memory used: 33.84 GB
Done. we are killing the process
[rank0]:[W1119 16:44:07.298561750 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
