W1119 16:59:11.592000 3022095 site-packages/torch/distributed/run.py:793] 
W1119 16:59:11.592000 3022095 site-packages/torch/distributed/run.py:793] *****************************************
W1119 16:59:11.592000 3022095 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1119 16:59:11.592000 3022095 site-packages/torch/distributed/run.py:793] *****************************************
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
flash_kv_decode is set to True
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
our world size=8
Using device=cuda
Loading model ...
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=1280, bias=False)
        (wo): Linear(in_features=1024, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 1.36 seconds
CUDA_GRAPH are activate
Prefill latency: 0.046997813042253256 sec
Decode latency: 0.44483798695728183 sec
Compilation time: 0.49 seconds
Compilation time: 0.49 seconds
Compilation time: 0.49 seconds
Prefill latency: 0.04645874188281596 sec
Compilation time: 0.49 seconds
Compilation time: 0.49 seconds
Compilation time: 0.49 seconds
Compilation time: 0.49 seconds
Compilation time: 0.49 seconds
Decode latency: 0.4437010260298848 sec
Prefill latency: 0.04647021880373359 sec
Decode latency: 0.44384722504764795 sec
Prefill latency: 0.04655725113116205 sec
Decode latency: 0.44494916405528784 sec
Prefill latency: 0.04653658508323133 sec
Decode latency: 0.44471444794908166 sec
Prefill latency: 0.046587579883635044 sec
Decode latency: 0.44390306691639125 sec
Time for inference 1: 0.49 sec total, 65.19 tokens/sec
Decode latency: 0.44 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 1252.67 GB/s
FLOPS achieved: 41.34 TF/s

Prefill latency: 0.04658461990766227 sec
Decode latency: 0.44477178901433945 sec
Time for inference 2: 0.49 sec total, 65.07 tokens/sec
Decode latency: 0.44 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 1250.45 GB/s
FLOPS achieved: 41.26 TF/s

Prefill latency: 0.046712938928976655 sec
Decode latency: 0.4447531560435891 sec
Time for inference 3: 0.49 sec total, 65.06 tokens/sec
Decode latency: 0.44 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 1250.21 GB/s
FLOPS achieved: 41.26 TF/s

Prefill latency: 0.04673753282986581 sec
Decode latency: 0.44503526808694005 sec
Time for inference 4: 0.49 sec total, 65.02 tokens/sec
Decode latency: 0.45 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 1249.41 GB/s
FLOPS achieved: 41.23 TF/s

Prefill latency: 0.04652539803646505 sec
Decode latency: 0.44393132301047444 sec
Time for inference 5: 0.49 sec total, 65.19 tokens/sec
Decode latency: 0.44 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 1252.78 GB/s
FLOPS achieved: 41.34 TF/s

Prefill latency: 0.046602464048191905 sec
Decode latency: 0.4438598449341953 sec
Time for inference 6: 0.49 sec total, 65.19 tokens/sec
Decode latency: 0.44 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 1252.78 GB/s
FLOPS achieved: 41.34 TF/s

Prefill latency: 0.04656311799772084 sec
Decode latency: 0.4447330888360739 sec
Time for inference 7: 0.49 sec total, 65.08 tokens/sec
Decode latency: 0.44 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 1250.62 GB/s
FLOPS achieved: 41.27 TF/s

Prefill latency: 0.046535012079402804 sec
Decode latency: 0.4438519231043756 sec
Time for inference 8: 0.49 sec total, 65.20 tokens/sec
Decode latency: 0.44 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 1252.98 GB/s
FLOPS achieved: 41.35 TF/s

Prefill latency: 0.04660161305218935 sec
Decode latency: 0.44470660109072924 sec
Time for inference 9: 0.49 sec total, 65.07 tokens/sec
Decode latency: 0.44 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 1250.53 GB/s
FLOPS achieved: 41.27 TF/s

Prefill latency: 0.04656240390613675 sec
[rank7]:[W1119 16:59:27.115310909 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank6]:[W1119 16:59:27.141165863 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Decode latency: 0.4437551589217037 sec
Time for inference 10: 0.49 sec total, 65.21 tokens/sec
Decode latency: 0.44 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 1253.12 GB/s
FLOPS achieved: 41.35 TF/s

==========
Batch Size: 1
Prompt Length: 1024
Generated tokens: 32
Average decode latency: 0.4443 sec
Average prefill latency: 0.0466 sec
Average tokens/sec: 65.13
Memory used: 21.87 GB
[rank0]:[W1119 16:59:27.146505946 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank1]:[W1119 16:59:27.175753244 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank2]:[W1119 16:59:27.177904225 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank4]:[W1119 16:59:27.219147096 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank3]:[W1119 16:59:27.243004862 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank5]:[W1119 16:59:27.298454605 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Done. we are killing the process
[rank0]:[W1119 16:59:34.313709273 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
