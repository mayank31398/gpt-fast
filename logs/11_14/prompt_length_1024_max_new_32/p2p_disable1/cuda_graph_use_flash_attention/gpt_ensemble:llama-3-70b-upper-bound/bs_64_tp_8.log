W1119 17:06:00.167000 3027972 site-packages/torch/distributed/run.py:793] 
W1119 17:06:00.167000 3027972 site-packages/torch/distributed/run.py:793] *****************************************
W1119 17:06:00.167000 3027972 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1119 17:06:00.167000 3027972 site-packages/torch/distributed/run.py:793] *****************************************
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
flash_kv_decode is set to True
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
our world size=8
Using device=cuda
Loading model ...
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=1280, bias=False)
        (wo): Linear(in_features=1024, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 1.49 seconds
CUDA_GRAPH are activate
Prefill latency: 2.572587631875649 sec
Decode latency: 0.547130597056821 sec
Compilation time: 3.12 seconds
Compilation time: 3.12 seconds
Compilation time: 3.13 seconds
Compilation time: 3.11 seconds
Compilation time: 3.11 seconds
Compilation time: 3.16 seconds
Compilation time: 3.14 seconds
Compilation time: 3.15 seconds
Prefill latency: 2.5691018619108945 sec
Decode latency: 0.5467053260654211 sec
Prefill latency: 2.566066177096218 sec
Decode latency: 0.5465475041419268 sec
Prefill latency: 2.5653722011484206 sec
Decode latency: 0.5478026939090341 sec
Prefill latency: 2.566051426809281 sec
Decode latency: 0.5466245340649039 sec
Prefill latency: 2.5664523099549115 sec
Decode latency: 0.5461390868294984 sec
Time for inference 1: 3.11 sec total, 657.84 tokens/sec
Decode latency: 0.55 sec
Prefill latency: 2.57 sec
Bandwidth achieved: 12641.59 GB/s
FLOPS achieved: 417.17 TF/s

Prefill latency: 2.568570760078728 sec
Decode latency: 0.5490036499686539 sec
Time for inference 2: 3.12 sec total, 656.76 tokens/sec
Decode latency: 0.55 sec
Prefill latency: 2.57 sec
Bandwidth achieved: 12620.81 GB/s
FLOPS achieved: 416.49 TF/s

Prefill latency: 2.5700771000701934 sec
Decode latency: 0.5472905940841883 sec
Time for inference 3: 3.12 sec total, 656.81 tokens/sec
Decode latency: 0.55 sec
Prefill latency: 2.57 sec
Bandwidth achieved: 12621.82 GB/s
FLOPS achieved: 416.52 TF/s

Prefill latency: 2.566955568967387 sec
Decode latency: 0.5467833511065692 sec
Time for inference 4: 3.11 sec total, 657.60 tokens/sec
Decode latency: 0.55 sec
Prefill latency: 2.57 sec
Bandwidth achieved: 12636.90 GB/s
FLOPS achieved: 417.02 TF/s

Prefill latency: 2.568456898909062 sec
Decode latency: 0.5472643310204148 sec
Time for inference 5: 3.12 sec total, 657.18 tokens/sec
Decode latency: 0.55 sec
Prefill latency: 2.57 sec
Bandwidth achieved: 12628.94 GB/s
FLOPS achieved: 416.76 TF/s

Prefill latency: 2.565490337088704 sec
Decode latency: 0.5472814738750458 sec
Time for inference 6: 3.11 sec total, 657.81 tokens/sec
Decode latency: 0.55 sec
Prefill latency: 2.57 sec
Bandwidth achieved: 12640.98 GB/s
FLOPS achieved: 417.15 TF/s

Prefill latency: 2.565187230007723 sec
Decode latency: 0.5473028181586415 sec
Time for inference 7: 3.11 sec total, 657.84 tokens/sec
Decode latency: 0.55 sec
Prefill latency: 2.57 sec
Bandwidth achieved: 12641.58 GB/s
FLOPS achieved: 417.17 TF/s

Prefill latency: 2.567917119944468 sec
Decode latency: 0.5464775699656457 sec
Time for inference 8: 3.12 sec total, 657.46 tokens/sec
Decode latency: 0.55 sec
Prefill latency: 2.57 sec
Bandwidth achieved: 12634.29 GB/s
FLOPS achieved: 416.93 TF/s

Prefill latency: 2.567817558068782 sec
Decode latency: 0.5460641279350966 sec
Time for inference 9: 3.11 sec total, 657.58 tokens/sec
Decode latency: 0.55 sec
Prefill latency: 2.57 sec
Bandwidth achieved: 12636.62 GB/s
FLOPS achieved: 417.01 TF/s

Prefill latency: 2.567547278944403 sec
Decode latency: 0.5480526119936258 sec
Time for inference 10: 3.12 sec total, 657.21 tokens/sec
Decode latency: 0.55 sec
Prefill latency: 2.57 sec
Bandwidth achieved: 12629.38 GB/s
FLOPS achieved: 416.77 TF/s

==========
Batch Size: 64
Prompt Length: 1024
Generated tokens: 32
Average decode latency: 0.5472 sec
Average prefill latency: 2.5674 sec
Average tokens/sec: 657.41
Memory used: 48.79 GB
[rank0]:[W1119 17:07:02.880193720 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank1]:[W1119 17:07:02.075430413 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank4]:[W1119 17:07:02.238283494 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank7]:[W1119 17:07:02.409476917 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank6]:[W1119 17:07:02.448753530 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank3]:[W1119 17:07:03.928790333 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank2]:[W1119 17:07:03.320819829 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank5]:[W1119 17:07:03.333422884 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Done. we are killing the process
[rank0]:[W1119 17:07:10.595947854 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
