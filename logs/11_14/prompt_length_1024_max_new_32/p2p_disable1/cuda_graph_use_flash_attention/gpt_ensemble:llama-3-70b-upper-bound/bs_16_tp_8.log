W1119 17:03:17.374000 3026082 site-packages/torch/distributed/run.py:793] 
W1119 17:03:17.374000 3026082 site-packages/torch/distributed/run.py:793] *****************************************
W1119 17:03:17.374000 3026082 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1119 17:03:17.374000 3026082 site-packages/torch/distributed/run.py:793] *****************************************
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
flash_kv_decode is set to True
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
our world size=8
Using device=cuda
Loading model ...
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=1280, bias=False)
        (wo): Linear(in_features=1024, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 1.40 seconds
CUDA_GRAPH are activate
Prefill latency: 0.6367667980957776 sec
Compilation time: 1.12 seconds
Compilation time: 1.12 seconds
Decode latency: 0.4890616338234395 sec
Compilation time: 1.13 seconds
Compilation time: 1.12 seconds
Compilation time: 1.12 seconds
Compilation time: 1.13 seconds
Compilation time: 1.14 seconds
Compilation time: 1.13 seconds
Prefill latency: 0.6365564800798893 sec
Decode latency: 0.4878072440624237 sec
Prefill latency: 0.6342045271303505 sec
Decode latency: 0.488492792006582 sec
Prefill latency: 0.6316062361001968 sec
Decode latency: 0.4879915041383356 sec
Prefill latency: 0.6363585251383483 sec
Decode latency: 0.49172517098486423 sec
Prefill latency: 0.6350261089392006 sec
Decode latency: 0.48783295904286206 sec
Time for inference 1: 1.12 sec total, 455.79 tokens/sec
Decode latency: 0.49 sec
Prefill latency: 0.64 sec
Bandwidth achieved: 8758.84 GB/s
FLOPS achieved: 289.04 TF/s

Prefill latency: 0.6361171109601855 sec
Decode latency: 0.4877230890560895 sec
Time for inference 2: 1.12 sec total, 455.40 tokens/sec
Decode latency: 0.49 sec
Prefill latency: 0.64 sec
Bandwidth achieved: 8751.26 GB/s
FLOPS achieved: 288.79 TF/s

Prefill latency: 0.6354886540211737 sec
Decode latency: 0.4879350010305643 sec
Time for inference 3: 1.12 sec total, 455.57 tokens/sec
Decode latency: 0.49 sec
Prefill latency: 0.64 sec
Bandwidth achieved: 8754.49 GB/s
FLOPS achieved: 288.90 TF/s

Prefill latency: 0.6350290637928993 sec
Decode latency: 0.4887110530398786 sec
Time for inference 4: 1.12 sec total, 455.40 tokens/sec
Decode latency: 0.49 sec
Prefill latency: 0.64 sec
Bandwidth achieved: 8751.22 GB/s
FLOPS achieved: 288.79 TF/s

Prefill latency: 0.6344866929575801 sec
Decode latency: 0.4884039747994393 sec
Time for inference 5: 1.12 sec total, 455.76 tokens/sec
Decode latency: 0.49 sec
Prefill latency: 0.63 sec
Bandwidth achieved: 8758.21 GB/s
FLOPS achieved: 289.02 TF/s

Prefill latency: 0.634180705063045 sec
Decode latency: 0.4888310229871422 sec
Time for inference 6: 1.12 sec total, 455.74 tokens/sec
Decode latency: 0.49 sec
Prefill latency: 0.63 sec
Bandwidth achieved: 8757.80 GB/s
FLOPS achieved: 289.01 TF/s

Prefill latency: 0.6325805550441146 sec
Decode latency: 0.4885005869437009 sec
Time for inference 7: 1.12 sec total, 456.50 tokens/sec
Decode latency: 0.49 sec
Prefill latency: 0.63 sec
Bandwidth achieved: 8772.43 GB/s
FLOPS achieved: 289.49 TF/s

Prefill latency: 0.6340343339834362 sec
Decode latency: 0.48804283305071294 sec
Time for inference 8: 1.12 sec total, 456.12 tokens/sec
Decode latency: 0.49 sec
Prefill latency: 0.63 sec
Bandwidth achieved: 8765.05 GB/s
FLOPS achieved: 289.25 TF/s

Prefill latency: 0.6341301340144128 sec
Decode latency: 0.4896604858804494 sec
Time for inference 9: 1.12 sec total, 455.42 tokens/sec
Decode latency: 0.49 sec
Prefill latency: 0.63 sec
Bandwidth achieved: 8751.62 GB/s
FLOPS achieved: 288.80 TF/s

Prefill latency: 0.6329074469394982 sec
[rank4]:[W1119 17:03:44.685896058 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank6]:[W1119 17:03:44.762469924 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Decode latency: 0.48787773214280605 sec
Time for inference 10: 1.12 sec total, 456.65 tokens/sec
Decode latency: 0.49 sec
Prefill latency: 0.63 sec
Bandwidth achieved: 8775.30 GB/s
FLOPS achieved: 289.58 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 32
Average decode latency: 0.4884 sec
Average prefill latency: 0.6344 sec
Average tokens/sec: 455.83
Memory used: 28.37 GB
[rank0]:[W1119 17:03:44.891296144 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank7]:[W1119 17:03:44.045983732 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank1]:[W1119 17:03:44.117996566 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank5]:[W1119 17:03:44.344262446 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank2]:[W1119 17:03:44.367891944 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank3]:[W1119 17:03:44.547356260 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Done. we are killing the process
[rank0]:[W1119 17:03:51.221062193 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
