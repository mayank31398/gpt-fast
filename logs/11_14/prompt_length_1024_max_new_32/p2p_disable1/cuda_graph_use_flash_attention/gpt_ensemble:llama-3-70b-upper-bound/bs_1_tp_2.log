W1119 16:58:19.310000 3021270 site-packages/torch/distributed/run.py:793] 
W1119 16:58:19.310000 3021270 site-packages/torch/distributed/run.py:793] *****************************************
W1119 16:58:19.310000 3021270 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1119 16:58:19.310000 3021270 site-packages/torch/distributed/run.py:793] *****************************************
rank: 1, global_rank: 1, world_size: 2, global_world_size: 2
rank: 0, global_rank: 0, world_size: 2, global_world_size: 2
rank: 0, global_rank: 0, world_size: 2, global_world_size: 2
rank: 0, global_rank: 0, world_size: 2, global_world_size: 2
rank: 0, global_rank: 0, world_size: 2, global_world_size: 2
rank: 0, global_rank: 0, world_size: 2, global_world_size: 2
flash_kv_decode is set to True
rank: 0, global_rank: 0, world_size: 2, global_world_size: 2
our world size=2
Using device=cuda
Loading model ...
rank: 1, global_rank: 1, world_size: 2, global_world_size: 2
rank: 1, global_rank: 1, world_size: 2, global_world_size: 2
rank: 1, global_rank: 1, world_size: 2, global_world_size: 2
rank: 1, global_rank: 1, world_size: 2, global_world_size: 2
rank: 1, global_rank: 1, world_size: 2, global_world_size: 2
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=5120, bias=False)
        (wo): Linear(in_features=4096, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=28672, bias=False)
        (w2): Linear(in_features=14336, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 1.45 seconds
CUDA_GRAPH are activate
Prefill latency: 0.14555009501054883 sec
Compilation time: 1.11 seconds
Decode latency: 0.9789902979973704 sec
Compilation time: 1.13 seconds
Prefill latency: 0.14502763096243143 sec
Decode latency: 0.9793836050666869 sec
Prefill latency: 0.14631598792038858 sec
Decode latency: 0.9792824098840356 sec
Prefill latency: 0.1469948310405016 sec
Decode latency: 0.9792121071368456 sec
Prefill latency: 0.1474265130236745 sec
Decode latency: 0.9791549919173121 sec
Prefill latency: 0.1465217978693545 sec
Decode latency: 0.9788349948357791 sec
Time for inference 1: 1.13 sec total, 28.42 tokens/sec
Decode latency: 0.98 sec
Prefill latency: 0.15 sec
Bandwidth achieved: 2005.33 GB/s
FLOPS achieved: 66.18 TF/s

Prefill latency: 0.14639003900811076 sec
Decode latency: 0.9787226219195873 sec
Time for inference 2: 1.13 sec total, 28.43 tokens/sec
Decode latency: 0.98 sec
Prefill latency: 0.15 sec
Bandwidth achieved: 2005.84 GB/s
FLOPS achieved: 66.19 TF/s

Prefill latency: 0.14641725807450712 sec
Decode latency: 0.9787117750383914 sec
Time for inference 3: 1.13 sec total, 28.43 tokens/sec
Decode latency: 0.98 sec
Prefill latency: 0.15 sec
Bandwidth achieved: 2005.83 GB/s
FLOPS achieved: 66.19 TF/s

Prefill latency: 0.14787529385648668 sec
Decode latency: 0.9790952370967716 sec
Time for inference 4: 1.13 sec total, 28.38 tokens/sec
Decode latency: 0.98 sec
Prefill latency: 0.15 sec
Bandwidth achieved: 2002.53 GB/s
FLOPS achieved: 66.08 TF/s

Prefill latency: 0.14749851101078093 sec
Decode latency: 0.9788766659330577 sec
Time for inference 5: 1.13 sec total, 28.40 tokens/sec
Decode latency: 0.98 sec
Prefill latency: 0.15 sec
Bandwidth achieved: 2003.55 GB/s
FLOPS achieved: 66.12 TF/s

Prefill latency: 0.14706481783650815 sec
Decode latency: 0.9911744149867445 sec
Time for inference 6: 1.14 sec total, 28.10 tokens/sec
Decode latency: 0.99 sec
Prefill latency: 0.15 sec
Bandwidth achieved: 1982.73 GB/s
FLOPS achieved: 65.43 TF/s

Prefill latency: 0.14820506190881133 sec
Decode latency: 0.9790082841645926 sec
Time for inference 7: 1.13 sec total, 28.38 tokens/sec
Decode latency: 0.98 sec
Prefill latency: 0.15 sec
Bandwidth achieved: 2002.08 GB/s
FLOPS achieved: 66.07 TF/s

Prefill latency: 0.14699978800490499 sec
Decode latency: 0.9790958270896226 sec
Time for inference 8: 1.13 sec total, 28.40 tokens/sec
Decode latency: 0.98 sec
Prefill latency: 0.15 sec
Bandwidth achieved: 2004.07 GB/s
FLOPS achieved: 66.13 TF/s

Prefill latency: 0.14667260111309588 sec
Decode latency: 0.9787835020106286 sec
Time for inference 9: 1.13 sec total, 28.42 tokens/sec
Decode latency: 0.98 sec
Prefill latency: 0.15 sec
Bandwidth achieved: 2005.19 GB/s
FLOPS achieved: 66.17 TF/s

Prefill latency: 0.14615003787912428 sec
[rank1]:[W1119 16:58:42.557947563 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Decode latency: 0.9789083180949092 sec
Time for inference 10: 1.13 sec total, 28.43 tokens/sec
Decode latency: 0.98 sec
Prefill latency: 0.15 sec
Bandwidth achieved: 2005.94 GB/s
FLOPS achieved: 66.20 TF/s

==========
Batch Size: 1
Prompt Length: 1024
Generated tokens: 32
Average decode latency: 0.9801 sec
Average prefill latency: 0.1470 sec
Average tokens/sec: 28.38
Memory used: 73.46 GB
[rank0]:[W1119 16:58:43.020923133 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Done. we are killing the process
[rank0]:[W1119 16:58:44.253708898 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
