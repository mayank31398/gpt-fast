W1118 14:26:13.088000 2732637 site-packages/torch/distributed/run.py:793] 
W1118 14:26:13.088000 2732637 site-packages/torch/distributed/run.py:793] *****************************************
W1118 14:26:13.088000 2732637 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1118 14:26:13.088000 2732637 site-packages/torch/distributed/run.py:793] *****************************************
rank: 0, global_rank: 8, world_size: 8, global_world_size: 16
rank: 7, global_rank: 15, world_size: 8, global_world_size: 16
rank: 3, global_rank: 11, world_size: 8, global_world_size: 16rank: 6, global_rank: 14, world_size: 8, global_world_size: 16

rank: 1, global_rank: 9, world_size: 8, global_world_size: 16
rank: 2, global_rank: 10, world_size: 8, global_world_size: 16
rank: 5, global_rank: 13, world_size: 8, global_world_size: 16
rank: 4, global_rank: 12, world_size: 8, global_world_size: 16
rank: 0, global_rank: 8, world_size: 8, global_world_size: 16
rank: 0, global_rank: 8, world_size: 8, global_world_size: 16
rank: 0, global_rank: 8, world_size: 8, global_world_size: 16
rank: 0, global_rank: 8, world_size: 8, global_world_size: 16
rank: 0, global_rank: 8, world_size: 8, global_world_size: 16
rank: 6, global_rank: 14, world_size: 8, global_world_size: 16
rank: 6, global_rank: 14, world_size: 8, global_world_size: 16
rank: 6, global_rank: 14, world_size: 8, global_world_size: 16
rank: 6, global_rank: 14, world_size: 8, global_world_size: 16
rank: 6, global_rank: 14, world_size: 8, global_world_size: 16
rank: 7, global_rank: 15, world_size: 8, global_world_size: 16
rank: 7, global_rank: 15, world_size: 8, global_world_size: 16
rank: 7, global_rank: 15, world_size: 8, global_world_size: 16
rank: 7, global_rank: 15, world_size: 8, global_world_size: 16
rank: 7, global_rank: 15, world_size: 8, global_world_size: 16
rank: 2, global_rank: 10, world_size: 8, global_world_size: 16
rank: 2, global_rank: 10, world_size: 8, global_world_size: 16
rank: 2, global_rank: 10, world_size: 8, global_world_size: 16
rank: 2, global_rank: 10, world_size: 8, global_world_size: 16
rank: 5, global_rank: 13, world_size: 8, global_world_size: 16
rank: 5, global_rank: 13, world_size: 8, global_world_size: 16
rank: 2, global_rank: 10, world_size: 8, global_world_size: 16
rank: 5, global_rank: 13, world_size: 8, global_world_size: 16
rank: 5, global_rank: 13, world_size: 8, global_world_size: 16
rank: 1, global_rank: 9, world_size: 8, global_world_size: 16rank: 3, global_rank: 11, world_size: 8, global_world_size: 16

rank: 5, global_rank: 13, world_size: 8, global_world_size: 16
rank: 1, global_rank: 9, world_size: 8, global_world_size: 16
rank: 3, global_rank: 11, world_size: 8, global_world_size: 16
rank: 1, global_rank: 9, world_size: 8, global_world_size: 16
rank: 3, global_rank: 11, world_size: 8, global_world_size: 16
rank: 1, global_rank: 9, world_size: 8, global_world_size: 16
rank: 1, global_rank: 9, world_size: 8, global_world_size: 16
rank: 3, global_rank: 11, world_size: 8, global_world_size: 16
rank: 3, global_rank: 11, world_size: 8, global_world_size: 16
rank: 4, global_rank: 12, world_size: 8, global_world_size: 16
rank: 4, global_rank: 12, world_size: 8, global_world_size: 16
rank: 4, global_rank: 12, world_size: 8, global_world_size: 16
rank: 4, global_rank: 12, world_size: 8, global_world_size: 16
rank: 4, global_rank: 12, world_size: 8, global_world_size: 16
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manage        (w2): Linear(in_features=3328, out_features=16384, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=16384, out_features=128256, bias=False)
)
Time to load model: 1.80 seconds
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Compiling prefill with dynamic=False
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.vailable but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for)` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
[rank11]: Traceback (most recent call last):
[rank11]:   File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 492, in <module>
[rank11]:     main(
[rank11]:   File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 419, in main
[rank11]:     y, decode_latency, prefill_latency = generate(
[rank11]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank11]:     return func(*args, **kwargs)
[rank11]:   File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 152, in generate
[rank11]:     generated_tokens, _ = decode_n_tokens(model, next_token.view(batch_size, -1), input_pos, max_new_tokens - 1, callback=callback, **sampling_kwargs)
[rank11]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank11]:     return func(*args, **kwargs)
[rank11]:   File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 110, in decode_n_tokens
[rank11]:     new_probs.append(next_prob.clone())
[rank11]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 3 has a total capacity of 79.33 GiB of which 15.44 MiB is free. Including non-PyTorch memory, this process has 79.30 GiB memory in use. Of the allocated memory 75.04 GiB is allocated by PyTorch, with 13.89 GiB allocated in private pools (e.g., CUDA Graphs), and 878.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank9]: Traceback (most recent call last):
[rank9]:   File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 492, in <module>
[rank9]:     main(
[rank9]:   File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 419, in main
[rank9]:     y, decode_latency, prefill_latency = generate(
[rank9]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank9]:     return func(*args, **kwargs)
[rank9]:   File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 152, in generate
[rank9]:     generated_tokens, _ = decode_n_tokens(model, next_token.view(batch_size, -1), input_pos, max_new_tokens - 1, callback=callback, **sampling_kwargs)
[rank9]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank9]:     return func(*args, **kwargs)
[rank9]:   File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 110, in decode_n_tokens
[rank9]:     new_probs.append(next_prob.clone())
[rank9]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 1 has a total capacity of 79.33 GiB of which 3.44 MiB is free. Including non-PyTorch memory, this process has 79.31 GiB memory in use. Of the allocated memory 75.05 GiB is allocated by PyTorch, with 13.88 GiB allocated in private pools (e.g., CUDA Graphs), and 882.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank14]: Traceback (most recent call last):
[rank14]:   File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 492, in <module>
[rank14]:     main(
[rank14]:   File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 419, in main
[rank14]:     y, decode_latency, prefill_latency = generate(
[rank14]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank14]:     return func(*args, **kwargs)
[rank14]:   File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 152, in generate
[rank14]:     generated_tokens, _ = decode_n_tokens(model, next_token.view(batch_size, -1), input_pos, max_new_tokens - 1, callback=callback, **sampling_kwargs)
[rank14]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank14]:     return func(*args, **kwargs)
[rank14]:   File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 110, in decode_n_tokens
[rank14]:     new_probs.append(next_prob.clone())
[rank14]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 6 has a total capacity of 79.33 GiB of which 1.44 MiB is free. Including non-PyTorch memory, this process has 79.31 GiB memory in use. Of the allocated memory 75.05 GiB is allocated by PyTorch, with 13.88 GiB allocated in private pools (e.g., CUDA Graphs), and 882.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank12]: Traceback (most recent call last):
[rank12]:   File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 492, in <module>
[rank12]:     main(
[rank12]:   File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 419, in main
[rank12]:     y, decode_latency, prefill_latency = generate(
[rank12]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank12]:     return func(*args, **kwargs)
[rank12]:   File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 152, in generate
[rank12]:     generated_tokens, _ = decode_n_tokens(model, next_token.view(batch_size, -1), input_pos, max_new_tokens - 1, callback=callback, **sampling_kwargs)
[rank12]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank12]:     return func(*args, **kwargs)
[rank12]:   File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 110, in decode_n_tokens
[rank12]:     new_probs.append(next_prob.clone())
[rank12]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 4 has a total capacity of 79.33 GiB of which 1.44 MiB is free. Including non-PyTorch memory, this process has 79.32 GiB memory in use. Of the allocated memory 75.02 GiB is allocated by PyTorch, with 13.71 GiB allocated in private pools (e.g., CUDA Graphs), and 921.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank8]: Traceback (most recent call last):
[rank8]:   File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 492, in <module>
[rank8]:     main(
[rank8]:   File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 419, in main
[rank8]:     y, decode_latency, prefill_latency = generate(
[rank8]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank8]:     return func(*args, **kwargs)
[rank8]:   File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 152, in generate
[rank8]:     generated_tokens, _ = decode_n_tokens(model, next_token.view(batch_size, -1), input_pos, max_new_tokens - 1, callback=callback, **sampling_kwargs)
[rank8]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank8]:     return func(*args, **kwargs)
[rank8]:   File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 110, in decode_n_tokens
[rank8]:     new_probs.append(next_prob.clone())
[rank8]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.33 GiB of which 7.44 MiB is free. Including non-PyTorch memory, this process has 79.31 GiB memory in use. Of the allocated memory 75.01 GiB is allocated by PyTorch, with 13.71 GiB allocated in private pools (e.g., CUDA Graphs), and 921.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank10]: Traceback (most recent call last):
[rank10]:   File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 492, in <module>
[rank10]:     main(
[rank10]:   File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 419, in main
[rank10]:     y, decode_latency, prefill_latency = generate(
[rank10]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank10]:     return func(*args, **kwargs)
[rank10]:   File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 152, in generate
[rank10]:     generated_tokens, _ = decode_n_tokens(model, next_token.view(batch_size, -1), input_pos, max_new_tokens - 1, callback=callback, **sampling_kwargs)
[rank10]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank10]:     return func(*args, **kwargs)
[rank10]:   File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 110, in decode_n_tokens
[rank10]:     new_probs.append(next_prob.clone())
[rank10]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 2 has a total capacity of 79.33 GiB of which 7.44 MiB is free. Including non-PyTorch memory, this process has 79.31 GiB memory in use. Of the allocated memory 75.01 GiB is allocated by PyTorch, with 13.71 GiB allocated in private pools (e.g., CUDA Graphs), and 921.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank15]: Traceback (most recent call last):
[rank15]:   File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 492, in <module>
[rank15]:     main(
[rank15]:   File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 419, in main
[rank15]:     y, decode_latency, prefill_latency = generate(
[rank15]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank15]:     return func(*args, **kwargs)
[rank15]:   File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 152, in generate
[rank15]:     generated_tokens, _ = decode_n_tokens(model, next_token.view(batch_size, -1), input_pos, max_new_tokens - 1, callback=callback, **sampling_kwargs)
[rank15]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank15]:     return func(*args, **kwargs)
[rank15]:   File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 110, in decode_n_tokens
[rank15]:     new_probs.append(next_prob.clone())
[rank15]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 7 has a total capacity of 79.33 GiB of which 9.44 MiB is free. Including non-PyTorch memory, this process has 79.31 GiB memory in use. Of the allocated memory 75.01 GiB is allocated by PyTorch, with 13.71 GiB allocated in private pools (e.g., CUDA Graphs), and 921.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank13]: Traceback (most recent call last):
[rank13]:   File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 492, in <module>
[rank13]:     main(
[rank13]:   File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 419, in main
[rank13]:     y, decode_latencyis free. Including non-PyTorch memory, this process has 79.31 GiB memory in use. Of the allocated memory 75.05 GiB is allocated by PyTorch, with 13.88 GiB allocated in private pools (e.g., CUDA Graphs), and 882.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank6]: Traceback (most recent call last):
[rank6]:   File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 492, in <module>
[rank6]:     main(
[rank6]:   File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 419, in main
[rank6]:     y, decode_latency, prefill_latency = generate(
[rank6]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank6]:     return func(*args, **kwargs)
[rank6]:   File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 152, in generate
[rank6]:     generated_tokens, _ = decode_n_tokens(model, next_token.view(batch_size, -1), input_pos, max_new_tokens - 1, callback=callback, **sampling_kwargs)
[rank6]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank6]:     return func(*args, **kwargs)
[rank6]:   File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 110, in decode_n_tokens
[rank6]:     new_probs.append(next_prob.clone())
[rank6]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 6 has a total capacity of 79.33 GiB of which 1.44 MiB is free. Including non-PyTorch memory, this process has 79.31 GiB memory in use. Of the allocated memory 75.05 GiB is allocated by PyTorch, with 13.88 GiB allocated in private pools (e.g., CUDA Graphs), and 882.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank5]:[E1118 14:34:01.258434953 ProcessGroupNCCL.cpp:1259] [PG ID 0 PG GUID 0(default_pg) Rank 5] Future for ProcessGroup abort timed out after 60000 ms
[rank4]:[E1118 14:34:01.453663586 ProcessGroupNCCL.cpp:1259] [PG ID 0 PG GUID 0(default_pg) Rank [rank11]:[F1118 14:50:18.125803414 ProcessGroupNCCL.cpp:1432] [PG ID 0 PG GUID 0(default_pg) Rank 11] [PG ID 0 PG GUID 0(default_pg) Rank 11] Terminating the process after attempting to dump debug info, due to ProcessGroupNCCL watchdog hang.
W1118 14:50:19.035000 2732637 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2732705 closing signal SIGTERM
W1118 14:50:19.047000 2732637 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2732706 closing signal SIGTERM
W1118 14:50:19.058000 2732637 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2732707 closing signal SIGTERM
W1118 14:50:19.067000 2732637 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2732709 closing signal SIGTERM
W1118 14:50:19.080000 2732637 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2732710 closing signal SIGTERM
W1118 14:50:19.088000 2732637 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2732711 closing signal SIGTERM
W1118 14:50:19.103000 2732637 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2732712 closing signal SIGTERM
E1118 14:50:23.979000 2732637 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: -6) local_rank: 3 (pid: 2732708) of binary: /home/charlie/anaconda3/envs/gpt-fast/bin/python
Traceback (most recent call last):
  File "/home/charlie/anaconda3/envs/gpt-fast/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/charlie/anacondaps, feel free to file an issue to PyTorch about the short timeout or false positive abort; otherwise, please attempt to debug the hang. 
[rank5]:[F1118 14:50:18.638581630 ProcessGroupNCCL.cpp:1432] [PG ID 0 PG GUID 0(default_pg) Rank 5] [PG ID 0 PG GUID 0(default_pg) Rank 5] Terminating the process after attempting to dump debug info, due to ProcessGroupNCCL watchdog hang.
[rank4]:[F1118 14:50:18.662763402 ProcessGroupNCCL.cpp:1432] [PG ID 0 PG GUID 0(default_pg) Rank 4] [PG ID 0 PG GUID 0(default_pg) Rank 4] Terminating the process after attempting to dump debug info, due to ProcessGroupNCCL watchdog hang.
W1118 14:50:19.135000 2513563 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2513635 closing signal SIGTERM
W1118 14:50:19.146000 2513563 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2513636 closing signal SIGTERM
W1118 14:50:19.155000 2513563 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2513637 closing signal SIGTERM
W1118 14:50:19.163000 2513563 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2513638 closing signal SIGTERM
W1118 14:50:19.171000 2513563 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2513640 closing signal SIGTERM
W1118 14:50:19.174000 2513563 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2513641 closing signal SIGTERM
W1118 14:50:19.197000 2513563 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2513642 closing signal SIGTERM
E1118 14:50:24.959000 2513563 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: -6) local_rank: 4 (pid: 2513639) of binary: /home/charlie/anaconda3/envs/gpt-fast/bin/python
Traceback (most recent call last):
  File "/home/charlie/anaconda3/envs/gpt-fast/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
========================================================
benchmark.py FAILED
--------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
--------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-11-18_14:50:19
  host      : mk-xii-15.cloud.together.ai
  rank      : 4 (local_rank: 4)
  exitcode  : -6 (pid: 2513639)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 2513639
========================================================
