W1118 22:32:40.899000 2769845 site-packages/torch/distributed/run.py:793] 
W1118 22:32:40.899000 2769845 site-packages/torch/distributed/run.py:793] *****************************************
W1118 22:32:40.899000 2769845 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1118 22:32:40.899000 2769845 site-packages/torch/distributed/run.py:793] *****************************************
rank: 1, global_rank: 1, world_size: 8, global_world_size: 16
rank: 2, global_rank: 2, world_size: 8, global_world_size: 16
rank: 5, global_rank: 5, world_size: 8, global_world_size: 16
rank: 0, global_rank: 0, world_size: 8, global_world_size: 16
rank: 6, global_rank: 6, world_size: 8, global_world_size: 16
rank: 3, global_rank: 3, world_size: 8, global_world_size: 16
rank: 7, global_rank: 7, world_size: 8, global_world_size: 16rank: 4, global_rank: 4, world_size: 8, global_world_size: 16

rank: 0, global_rank: 0, world_size: 8, global_world_size: 16
rank: 0, global_rank: 0, world_size: 8, global_world_size: 16
rank: 0, global_rank: 0, world_size: 8, global_world_size: 16
rank: 0, global_rank: 0, world_size: 8, global_world_size: 16
flash_kv_decode is set to False
rank: 0, global_rank: 0, world_size: 8, global_world_size: 16
our world size=16
Using device=cuda
Loading model ...
rank: 2, global_rank: 2, world_size: 8, global_world_size: 16
rank: 2, global_rank: 2, world_size: 8, global_world_size: 16
rank: 2, global_rank: 2, world_size: 8, global_world_size: 16
rank: 2, global_rank: 2, world_size: 8, global_world_size: 16
rank: 2, global_rank: 2, world_size: 8, global_world_size: 16
rank: 5, global_rank: 5, world_size: 8, global_world_size: 16
rank: 5, global_rank: 5, world_size: 8, global_world_size: 16
rank: 5, global_rank: 5, world_size: 8, global_world_size: 16
rank: 5, global_rank: 5, world_size: 8, global_world_size: 16
rank: 5, global_rank: 5, world_size: 8, global_world_size: 16
rank: 1, global_rank: 1, world_size: 8, global_world_size: 16
rank: 7, global_rank: 7, world_size: 8, global_world_size: 16
rank: 1, global_rank: 1, world_size: 8, global_world_size: 16
rank: 7, global_rank: 7, world_size: 8, global_world_size: 16
rank: 1, global_rank: 1, world_size: 8, global_world_size: 16
rank: 7, global_rank: 7, world_size: 8, global_world_size: 16
rank: 1, global_rank: 1, world_size: 8, global_world_size: 16rank: 7, global_rank: 7, world_size: 8, global_world_size: 16

rank: 7, global_rank: 7, world_size: 8, global_world_size: 16
rank: 1, global_rank: 1, world_size: 8, global_world_size: 16
rank: 4, global_rank: 4, world_size: 8, global_world_size: 16
rank: 4, global_rank: 4, world_size: 8, global_world_size: 16
rank: 4, global_rank: 4, world_size: 8, global_world_size: 16
rank: 4, global_rank: 4, world_size: 8, global_world_size: 16
rank: 4, global_rank: 4, world_size: 8, global_world_size: 16
rank: 6, global_rank: 6, world_size: 8, global_world_size: 16
rank: 6, global_rank: 6, world_size: 8, global_world_size: 16
rank: 6, global_rank: 6, world_size: 8, global_world_size: 16
rank: 3, global_rank: 3, world_size: 8, global_world_size: 16
rank: 3, global_rank: 3, world_size: 8, global_world_size: 16
rank: 6, global_rank: 6, world_size: 8, global_world_size: 16
rank: 3, global_rank: 3, world_size: 8, global_world_size: 16
rank: 6, global_rank: 6, world_size: 8, global_world_size: 16
rank: 3, global_rank: 3, world_size: 8, global_world_size: 16
rank: 3, global_rank: 3, world_size: 8, global_world_size: 16
GPTParallel(
  (tok_embeddings): Embedding(128256, 16384)
  (layers): ModuleList(
    (0-125): 126 x ParallelTransformerBlock(
      semi_compiled = False
      (attention): FuseAttentionMLP(
        (wqkv1): Linear(in_features=16384, out_features=7936, bias=False)
        (wo): Linear(in_features=1024, out_features=16384, bias=False)
        (w2): Linear(in_features=3328, out_features=16384, bias=False)
      )
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=16384, out_features=128256, bias=False)
)
Time to load model: 1.51 seconds
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Compiling prefill with dynamic=False
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Prefill latency: 80.42880382784642 sec
/home/charliening: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args,ew context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 196.3942501100246 sec
Compilation time: 276.83 seconds
Compilation time: 276.42 seconds
Compilation time: 276.69 seconds
Compilation time: 276.84 seconds
Compilation time: 276.46 seconds
Compilation time: 276.40 seconds
Compilation time: 276.41 seconds
Compilation time: 276.60 seconds
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Prefill latency: 3.4206663488876075 sec
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecatednature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
[rank8]:[W1118 22:42:02.725742773 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
3: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 18.250044009182602 sec
Prefill latency: 1.2078209188766778 sec
Decode latency: 18.27205791603774 sec
Prefill latency: 1.2044030779507011 sec
Decode latency: 18.25087623600848 sec
Prefill latency: 1.205056888051331 sec
Decode latency: 18.24245141306892 sec
Prefill latency: 1.2066084928810596 sec
Decode latency: 18.27016869490035 sec
Time for inference 1: 19.48 sec total, 210.28 tokens/sec
Decode latency: 18.27 sec
Prefill latency: 1.21 sec
Bandwidth achieved: 11552.82 GB/s
FLOPS achieved: 34.66 TF/s

Prefill latency: 1.2086916349362582 sec
Decode latency: 18.254120917990804 sec
Time for inference 2: 19.47 sec total, 210.43 tokens/sec
Decode latency: 18.25 sec
Prefill latency: 1.21 sec
Bandwidth achieved: 11561.11 GB/s
FLOPS achieved: 34.68 TF/s

Prefill latency: 1.2034316989593208 sec
Decode latency: 18.26747913984582 sec
Time for inference 3: 19.47 sec total, 210.34 tokens/sec
Decode latency: 18.27 sec
Prefill latency: 1.20 sec
Bandwidth achieved: 11556.06 GB/s
FLOPS achieved: 34.67 TF/s

Prefill latency: 1.206550578121096 sec
Decode latency: 18.24381957598962 sec
Time for inference 4: 19.45 sec total, 210.56 tokens/sec
Decode latency: 18.24 sec
Prefill latency: 1.21 sec
Bandwidth achieved: 11568.44 GB/s
FLOPS achieved: 34.71 TF/s

Prefill latency: 1.2091345330700278 sec
Decode latency: 18.263055266812444 sec
Time for inference 5: 19.47 sec total, 210.33 tokens/sec
Decode latency: 18.26 sec
Prefill latency: 1.21 sec
Bandwidth achieved: 11555.55 GB/s
FLOPS achieved: 34.67 TF/s

Prefill latency: 1.2067699651233852 sec
Decode latency: 18.265878569101915 sec
Time for inference 6: 19.48 sec total, 210.32 tokens/sec
Decode latency: 18.27 sec
Prefill latency: 1.21 sec
Bandwidth achieved: 11555.06 GB/s
FLOPS achieved: 34.67 TF/s

Prefill latency: 1.2073824869003147 sec
Decode latency: 18.29630360309966 sec
Time for inference 7: 19.51 sec total, 209.98 tokens/sec
Decode latency: 18.30 sec
Prefill latency: 1.21 sec
Bandwidth achieved: 11536.77 GB/s
FLOPS achieved: 34.61 TF/s

Prefill latency: 1.2113116448745131 sec
Decode latency: 18.242982408963144 sec
Time for inference 8: 19.46 sec total, 210.52 tokens/sec
Decode latency: 18.24 sec
Prefill latency: 1.21 sec
Bandwidth achieved: 11566.04 GB/s
FLOPS achieved: 34.70 TF/s

Prefill latency: 1.2060120298992842 sec
Decode latency: 18.261163752060384 sec
Time for inference 9: 19.47 sec total, 210.38 tokens/sec
Decode latency: 18.26 sec
Prefill latency: 1.21 sec
Bandwidth achieved: 11558.39 GB/s
FLOPS achieved: 34.68 TF/s

Prefill latency: 1.2062146130483598 sec
Decode latency: 18.26209734310396 sec
Time for inference 10: 19.47 sec total, 210.37 tokens/sec
Decode latency: 18.26 sec
Prefill latency: 1.21 sec
Bandwidth achieved: 11557.75 GB/s
FLOPS achieved: 34.67 TF/s

==========
Batch Size: 8
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 18.2627 sec
Average prefill latency: 1.2072 sec
Average tokens/sec: 210.35
Memory used: 65.76 GB
Done. we are killing the process
[rank0]:[W1118 22:42:03.606903755 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
