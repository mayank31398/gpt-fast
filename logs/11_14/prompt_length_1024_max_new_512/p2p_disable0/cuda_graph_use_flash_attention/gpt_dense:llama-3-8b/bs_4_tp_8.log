W1114 02:08:10.454000 2267858 site-packages/torch/distributed/run.py:793] 
W1114 02:08:10.454000 2267858 site-packages/torch/distributed/run.py:793] *****************************************
W1114 02:08:10.454000 2267858 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1114 02:08:10.454000 2267858 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTDense(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x DenseTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=768, bias=False)
        (wo): Linear(in_features=512, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=3584, bias=False)
        (w2): Linear(in_features=1792, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.64 seconds
CUDA_GRAPH are activate
Prefill latency: 0.05479426681995392 sec
Decode latency: 3.2083720825612545 sec
Compilation time: 3.31 seconds
Compilation time: 3.27 seconds
Compilation time: 3.26 seconds
Compilation time: 3.27 seconds
Compilation time: 3.25 seconds
Compilation time: 3.29 seconds
Compilation time: 3.27 secondsCompilation time: 3.29 seconds

Prefill latency: 0.041371025145053864 sec
Decode latency: 3.2056589648127556 sec
Prefill latency: 0.04126998409628868 sec
Decode latency: 3.2059314381331205 sec
Prefill latency: 0.04135710559785366 sec
Decode latency: 3.2064767070114613 sec
Prefill latency: 0.04125134460628033 sec
Decode latency: 3.2071541361510754 sec
Prefill latency: 0.04149426706135273 sec
Decode latency: 3.206473493948579 sec
Time for inference 1: 3.25 sec total, 630.36 tokens/sec
Decode latency: 3.21 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 1762.50 GB/s
FLOPS achieved: 5.29 TF/s

Prefill latency: 0.041343098506331444 sec
Decode latency: 3.2067768462002277 sec
Time for inference 2: 3.25 sec total, 630.28 tokens/sec
Decode latency: 3.21 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 1762.28 GB/s
FLOPS achieved: 5.29 TF/s

Prefill latency: 0.04121941514313221 sec
Decode latency: 3.206553600728512 sec
Time for inference 3: 3.25 sec total, 630.34 tokens/sec
Decode latency: 3.21 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 1762.45 GB/s
FLOPS achieved: 5.29 TF/s

Prefill latency: 0.04131972789764404 sec
Decode latency: 3.2073055524379015 sec
Time for inference 4: 3.25 sec total, 630.19 tokens/sec
Decode latency: 3.21 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 1762.03 GB/s
FLOPS achieved: 5.29 TF/s

Prefill latency: 0.041248640045523643 sec
Decode latency: 3.2051745019853115 sec
Time for inference 5: 3.25 sec total, 630.64 tokens/sec
Decode latency: 3.21 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 1763.28 GB/s
FLOPS achieved: 5.29 TF/s

Prefill latency: 0.04131771996617317 sec
Decode latency: 3.2064757272601128 sec
Time for inference 6: 3.25 sec total, 630.40 tokens/sec
Decode latency: 3.21 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 1762.61 GB/s
FLOPS achieved: 5.29 TF/s

Prefill latency: 0.041589170694351196 sec
Decode latency: 3.208149256184697 sec
Time for inference 7: 3.25 sec total, 630.00 tokens/sec
Decode latency: 3.21 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 1761.50 GB/s
FLOPS achieved: 5.28 TF/s

Prefill latency: 0.04134965501725674 sec
Decode latency: 3.208076035603881 sec
Time for inference 8: 3.25 sec total, 630.07 tokens/sec
Decode latency: 3.21 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 1761.70 GB/s
FLOPS achieved: 5.29 TF/s

Prefill latency: 0.04185299761593342 sec
Decode latency: 3.2061728052794933 sec
Time for inference 9: 3.25 sec total, 630.35 tokens/sec
Decode latency: 3.21 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 1762.49 GB/s
FLOPS achieved: 5.29 TF/s

Prefill latency: 0.04127170331776142 sec
Decode latency: 3.206956561654806 sec
Time for inference 10: 3.25 sec total, 630.30 tokens/sec
Decode latency: 3.21 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 1762.35 GB/s
FLOPS achieved: 5.29 TF/s

==========
Batch Size: 4
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 3.2068 sec
Average prefill latency: 0.0414 sec
Average tokens/sec: 630.29
Memory used: 7.46 GB
Done. we are killing the process
[rank0]:[W1114 02:09:23.037416573 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
