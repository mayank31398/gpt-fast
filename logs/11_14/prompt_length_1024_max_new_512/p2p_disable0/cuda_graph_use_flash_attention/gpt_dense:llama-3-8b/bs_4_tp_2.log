W1114 02:05:49.965000 2258726 site-packages/torch/distributed/run.py:793] 
W1114 02:05:49.965000 2258726 site-packages/torch/distributed/run.py:793] *****************************************
W1114 02:05:49.965000 2258726 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1114 02:05:49.965000 2258726 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTDense(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x DenseTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=3072, bias=False)
        (wo): Linear(in_features=2048, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=14336, bias=False)
        (w2): Linear(in_features=7168, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.00 seconds
CUDA_GRAPH are activate
Prefill latency: 0.08124810829758644 sec
Decode latency: 3.8236495200544596 sec
Compilation time: 3.91 seconds
Compilation time: 3.91 seconds
Prefill latency: 0.08057236485183239 sec
Decode latency: 3.822063524276018 sec
Prefill latency: 0.08068845234811306 sec
Decode latency: 3.822855604812503 sec
Prefill latency: 0.08067608252167702 sec
Decode latency: 3.821941416710615 sec
Prefill latency: 0.08072349242866039 sec
Decode latency: 3.822331504896283 sec
Prefill latency: 0.08069033920764923 sec
Decode latency: 3.8223812375217676 sec
Time for inference 1: 3.90 sec total, 524.60 tokens/sec
Decode latency: 3.82 sec
Prefill latency: 0.08 sec
Bandwidth achieved: 4212.82 GB/s
FLOPS achieved: 12.64 TF/s

Prefill latency: 0.08060192130506039 sec
Decode latency: 3.8236517664045095 sec
Time for inference 2: 3.91 sec total, 524.41 tokens/sec
Decode latency: 3.82 sec
Prefill latency: 0.08 sec
Bandwidth achieved: 4211.26 GB/s
FLOPS achieved: 12.63 TF/s

Prefill latency: 0.08066299743950367 sec
Decode latency: 3.8232032507658005 sec
Time for inference 3: 3.90 sec total, 524.47 tokens/sec
Decode latency: 3.82 sec
Prefill latency: 0.08 sec
Bandwidth achieved: 4211.75 GB/s
FLOPS achieved: 12.64 TF/s

Prefill latency: 0.08089540340006351 sec
Decode latency: 3.8245235458016396 sec
Time for inference 4: 3.91 sec total, 524.27 tokens/sec
Decode latency: 3.82 sec
Prefill latency: 0.08 sec
Bandwidth achieved: 4210.18 GB/s
FLOPS achieved: 12.63 TF/s

Prefill latency: 0.08077417686581612 sec
Decode latency: 3.8225159365683794 sec
Time for inference 5: 3.90 sec total, 524.57 tokens/sec
Decode latency: 3.82 sec
Prefill latency: 0.08 sec
Bandwidth achieved: 4212.56 GB/s
FLOPS achieved: 12.64 TF/s

Prefill latency: 0.08072139695286751 sec
Decode latency: 3.8239491265267134 sec
Time for inference 6: 3.91 sec total, 524.36 tokens/sec
Decode latency: 3.82 sec
Prefill latency: 0.08 sec
Bandwidth achieved: 4210.92 GB/s
FLOPS achieved: 12.63 TF/s

Prefill latency: 0.08082403056323528 sec
Decode latency: 3.822690725326538 sec
Time for inference 7: 3.90 sec total, 524.53 tokens/sec
Decode latency: 3.82 sec
Prefill latency: 0.08 sec
Bandwidth achieved: 4212.25 GB/s
FLOPS achieved: 12.64 TF/s

Prefill latency: 0.08062613382935524 sec
Decode latency: 3.822194207459688 sec
Time for inference 8: 3.90 sec total, 524.63 tokens/sec
Decode latency: 3.82 sec
Prefill latency: 0.08 sec
Bandwidth achieved: 4213.06 GB/s
FLOPS achieved: 12.64 TF/s

Prefill latency: 0.08084447495639324 sec
Decode latency: 3.8227728977799416 sec
Time for inference 9: 3.90 sec total, 524.50 tokens/sec
Decode latency: 3.82 sec
Prefill latency: 0.08 sec
Bandwidth achieved: 4212.05 GB/s
FLOPS achieved: 12.64 TF/s

Prefill latency: 0.08071522787213326 sec
Decode latency: 3.8233713805675507 sec
Time for inference 10: 3.91 sec total, 524.42 tokens/sec
Decode latency: 3.82 sec
Prefill latency: 0.08 sec
Bandwidth achieved: 4211.37 GB/s
FLOPS achieved: 12.63 TF/s

==========
Batch Size: 4
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 3.8231 sec
Average prefill latency: 0.0807 sec
Average tokens/sec: 524.48
Memory used: 14.82 GB
Done. we are killing the process
[rank0]:[W1114 02:06:59.972686690 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
