flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTDense(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x DenseTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
        (wo): Linear(in_features=4096, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=28672, bias=False)
        (w2): Linear(in_features=14336, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 0.93 seconds
CUDA_GRAPH are activate
Prefill latency: 0.03387746773660183 sec
Decode latency: 4.40251300483942 sec
Compilation time: 4.63 seconds
Prefill latency: 0.03362797759473324 sec
Decode latency: 4.3831038400530815 sec
Prefill latency: 0.03363165631890297 sec
Decode latency: 4.3854926116764545 sec
Prefill latency: 0.03376263566315174 sec
Decode latency: 4.4014101680368185 sec
Prefill latency: 0.03371209092438221 sec
Decode latency: 4.379096092656255 sec
Prefill latency: 0.033637531101703644 sec
Decode latency: 4.368727622553706 sec
Time for inference 1: 4.40 sec total, 116.28 tokens/sec
Decode latency: 4.37 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 1745.28 GB/s
FLOPS achieved: 5.24 TF/s

Prefill latency: 0.03363242372870445 sec
Decode latency: 4.405619125813246 sec
Time for inference 2: 4.44 sec total, 115.31 tokens/sec
Decode latency: 4.41 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 1730.81 GB/s
FLOPS achieved: 5.19 TF/s

Prefill latency: 0.03359490446746349 sec
Decode latency: 4.400937270373106 sec
Time for inference 3: 4.44 sec total, 115.43 tokens/sec
Decode latency: 4.40 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 1732.66 GB/s
FLOPS achieved: 5.20 TF/s

Prefill latency: 0.033663367852568626 sec
Decode latency: 4.406705126166344 sec
Time for inference 4: 4.44 sec total, 115.28 tokens/sec
Decode latency: 4.41 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 1730.31 GB/s
FLOPS achieved: 5.19 TF/s

Prefill latency: 0.03365377336740494 sec
Decode latency: 4.328643558546901 sec
Time for inference 5: 4.36 sec total, 117.34 tokens/sec
Decode latency: 4.33 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 1761.28 GB/s
FLOPS achieved: 5.28 TF/s

Prefill latency: 0.033643851056694984 sec
Decode latency: 4.37897115573287 sec
Time for inference 6: 4.41 sec total, 116.01 tokens/sec
Decode latency: 4.38 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 1741.26 GB/s
FLOPS achieved: 5.22 TF/s

Prefill latency: 0.03360922448337078 sec
Decode latency: 4.407316759228706 sec
Time for inference 7: 4.44 sec total, 115.27 tokens/sec
Decode latency: 4.41 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 1730.18 GB/s
FLOPS achieved: 5.19 TF/s

Prefill latency: 0.033625150099396706 sec
Decode latency: 4.436266230419278 sec
Time for inference 8: 4.47 sec total, 114.52 tokens/sec
Decode latency: 4.44 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 1718.98 GB/s
FLOPS achieved: 5.16 TF/s

Prefill latency: 0.03381621651351452 sec
Decode latency: 4.435093952342868 sec
Time for inference 9: 4.47 sec total, 114.55 tokens/sec
Decode latency: 4.44 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 1719.35 GB/s
FLOPS achieved: 5.16 TF/s

Prefill latency: 0.033633532002568245 sec
Decode latency: 4.238541791215539 sec
Time for inference 10: 4.27 sec total, 119.82 tokens/sec
Decode latency: 4.24 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 1798.53 GB/s
FLOPS achieved: 5.40 TF/s

==========
Batch Size: 1
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 4.3807 sec
Average prefill latency: 0.0337 sec
Average tokens/sec: 115.98
Memory used: 17.45 GB
Done. we are killing the process
[rank0]:[W1114 02:00:54.244536230 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
