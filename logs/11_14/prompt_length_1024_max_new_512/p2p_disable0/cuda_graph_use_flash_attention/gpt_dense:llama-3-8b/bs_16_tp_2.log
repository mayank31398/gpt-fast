W1114 02:11:08.760000 2279336 site-packages/torch/distributed/run.py:793] 
W1114 02:11:08.760000 2279336 site-packages/torch/distributed/run.py:793] *****************************************
W1114 02:11:08.760000 2279336 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1114 02:11:08.760000 2279336 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTDense(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x DenseTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=3072, bias=False)
        (wo): Linear(in_features=2048, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=14336, bias=False)
        (w2): Linear(in_features=7168, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.71 seconds
CUDA_GRAPH are activate
Prefill latency: 0.3106249328702688 sec
Decode latency: 4.234120413661003 sec
Compilation time: 4.54 seconds
Compilation time: 4.55 seconds
Prefill latency: 0.3083808422088623 sec
Decode latency: 4.2347930278629065 sec
Prefill latency: 0.30750015564262867 sec
Decode latency: 4.235290607437491 sec
Prefill latency: 0.30998967215418816 sec
Decode latency: 4.235524607822299 sec
Prefill latency: 0.30891369096934795 sec
Decode latency: 4.234132792800665 sec
Prefill latency: 0.30846528708934784 sec
Decode latency: 4.234853286296129 sec
Time for inference 1: 4.54 sec total, 1802.68 tokens/sec
Decode latency: 4.23 sec
Prefill latency: 0.31 sec
Bandwidth achieved: 14476.46 GB/s
FLOPS achieved: 43.43 TF/s

Prefill latency: 0.30918277241289616 sec
Decode latency: 4.235625319182873 sec
Time for inference 2: 4.55 sec total, 1802.02 tokens/sec
Decode latency: 4.24 sec
Prefill latency: 0.31 sec
Bandwidth achieved: 14471.18 GB/s
FLOPS achieved: 43.41 TF/s

Prefill latency: 0.30909975431859493 sec
Decode latency: 4.235365556553006 sec
Time for inference 3: 4.55 sec total, 1802.10 tokens/sec
Decode latency: 4.24 sec
Prefill latency: 0.31 sec
Bandwidth achieved: 14471.82 GB/s
FLOPS achieved: 43.42 TF/s

Prefill latency: 0.30883553996682167 sec
Decode latency: 4.233448261395097 sec
Time for inference 4: 4.54 sec total, 1803.06 tokens/sec
Decode latency: 4.23 sec
Prefill latency: 0.31 sec
Bandwidth achieved: 14479.49 GB/s
FLOPS achieved: 43.44 TF/s

Prefill latency: 0.30864207819104195 sec
Decode latency: 4.237193485721946 sec
Time for inference 5: 4.55 sec total, 1801.57 tokens/sec
Decode latency: 4.24 sec
Prefill latency: 0.31 sec
Bandwidth achieved: 14467.53 GB/s
FLOPS achieved: 43.40 TF/s

Prefill latency: 0.30917105078697205 sec
Decode latency: 4.235850229859352 sec
Time for inference 6: 4.55 sec total, 1801.93 tokens/sec
Decode latency: 4.24 sec
Prefill latency: 0.31 sec
Bandwidth achieved: 14470.47 GB/s
FLOPS achieved: 43.41 TF/s

Prefill latency: 0.30914957262575626 sec
Decode latency: 4.236691256985068 sec
Time for inference 7: 4.55 sec total, 1801.61 tokens/sec
Decode latency: 4.24 sec
Prefill latency: 0.31 sec
Bandwidth achieved: 14467.89 GB/s
FLOPS achieved: 43.40 TF/s

Prefill latency: 0.3084966838359833 sec
Decode latency: 4.234792431816459 sec
Time for inference 8: 4.54 sec total, 1802.63 tokens/sec
Decode latency: 4.23 sec
Prefill latency: 0.31 sec
Bandwidth achieved: 14476.08 GB/s
FLOPS achieved: 43.43 TF/s

Prefill latency: 0.30862474627792835 sec
Decode latency: 4.23670556396246 sec
Time for inference 9: 4.55 sec total, 1801.74 tokens/sec
Decode latency: 4.24 sec
Prefill latency: 0.31 sec
Bandwidth achieved: 14468.90 GB/s
FLOPS achieved: 43.41 TF/s

Prefill latency: 0.30981612391769886 sec
Decode latency: 4.23622702807188 sec
Time for inference 10: 4.55 sec total, 1801.49 tokens/sec
Decode latency: 4.24 sec
Prefill latency: 0.31 sec
Bandwidth achieved: 14466.92 GB/s
FLOPS achieved: 43.40 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 4.2357 sec
Average prefill latency: 0.3089 sec
Average tokens/sec: 1802.08
Memory used: 31.48 GB
Done. we are killing the process
[rank0]:[W1114 02:12:28.312594693 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
