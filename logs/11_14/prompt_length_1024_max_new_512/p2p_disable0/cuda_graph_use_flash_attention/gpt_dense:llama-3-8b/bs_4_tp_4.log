W1114 02:07:02.143000 2262398 site-packages/torch/distributed/run.py:793] 
W1114 02:07:02.143000 2262398 site-packages/torch/distributed/run.py:793] *****************************************
W1114 02:07:02.143000 2262398 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1114 02:07:02.143000 2262398 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTDense(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x DenseTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=1536, bias=False)
        (wo): Linear(in_features=1024, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.39 seconds
CUDA_GRAPH are activate
Prefill latency: 0.05425994656980038 sec
Decode latency: 3.2629871275275946 sec
Compilation time: 3.37 seconds
Compilation time: 3.36 seconds
Compilation time: 3.35 seconds
Compilation time: 3.34 seconds
Prefill latency: 0.054004447534680367 sec
Decode latency: 3.260867891833186 sec
Prefill latency: 0.054099757224321365 sec
Decode latency: 3.2605021446943283 sec
Prefill latency: 0.05401063524186611 sec
Decode latency: 3.261865671724081 sec
Prefill latency: 0.05386984720826149 sec
Decode latency: 3.262325732037425 sec
Prefill latency: 0.05403800867497921 sec
Decode latency: 3.262259751558304 sec
Time for inference 1: 3.32 sec total, 617.36 tokens/sec
Decode latency: 3.26 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 2803.36 GB/s
FLOPS achieved: 8.41 TF/s

Prefill latency: 0.05399053916335106 sec
Decode latency: 3.2619349975138903 sec
Time for inference 2: 3.32 sec total, 617.42 tokens/sec
Decode latency: 3.26 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 2803.64 GB/s
FLOPS achieved: 8.41 TF/s

Prefill latency: 0.05402969568967819 sec
Decode latency: 3.261274067685008 sec
Time for inference 3: 3.32 sec total, 617.56 tokens/sec
Decode latency: 3.26 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 2804.27 GB/s
FLOPS achieved: 8.41 TF/s

Prefill latency: 0.053900787606835365 sec
Decode latency: 3.260545328259468 sec
Time for inference 4: 3.32 sec total, 617.74 tokens/sec
Decode latency: 3.26 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 2805.06 GB/s
FLOPS achieved: 8.42 TF/s

Prefill latency: 0.05394535884261131 sec
Decode latency: 3.261111691594124 sec
Time for inference 5: 3.32 sec total, 617.62 tokens/sec
Decode latency: 3.26 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 2804.52 GB/s
FLOPS achieved: 8.41 TF/s

Prefill latency: 0.053964247927069664 sec
Decode latency: 3.2614337112754583 sec
Time for inference 6: 3.32 sec total, 617.56 tokens/sec
Decode latency: 3.26 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 2804.28 GB/s
FLOPS achieved: 8.41 TF/s

Prefill latency: 0.0540385115891695 sec
Decode latency: 3.262369619682431 sec
Time for inference 7: 3.32 sec total, 617.33 tokens/sec
Decode latency: 3.26 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 2803.22 GB/s
FLOPS achieved: 8.41 TF/s

Prefill latency: 0.05396171659231186 sec
Decode latency: 3.262741582468152 sec
Time for inference 8: 3.32 sec total, 617.28 tokens/sec
Decode latency: 3.26 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 2802.99 GB/s
FLOPS achieved: 8.41 TF/s

Prefill latency: 0.053943783044815063 sec
Decode latency: 3.261652449145913 sec
Time for inference 9: 3.32 sec total, 617.49 tokens/sec
Decode latency: 3.26 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 2803.96 GB/s
FLOPS achieved: 8.41 TF/s

Prefill latency: 0.053977757692337036 sec
Decode latency: 3.260101642459631 sec
Time for inference 10: 3.32 sec total, 617.80 tokens/sec
Decode latency: 3.26 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 2805.33 GB/s
FLOPS achieved: 8.42 TF/s

==========
Batch Size: 4
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 3.2615 sec
Average prefill latency: 0.0540 sec
Average tokens/sec: 617.52
Memory used: 10.24 GB
Done. we are killing the process
[rank0]:[W1114 02:08:07.870105050 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
