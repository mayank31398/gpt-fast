W1114 02:03:12.214000 2247873 site-packages/torch/distributed/run.py:793] 
W1114 02:03:12.214000 2247873 site-packages/torch/distributed/run.py:793] *****************************************
W1114 02:03:12.214000 2247873 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1114 02:03:12.214000 2247873 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTDense(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x DenseTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=768, bias=False)
        (wo): Linear(in_features=512, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=3584, bias=False)
        (w2): Linear(in_features=1792, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.71 seconds
CUDA_GRAPH are activate
Prefill latency: 0.01680075190961361 sec
Decode latency: 2.879634292796254 sec
Compilation time: 2.95 seconds
Compilation time: 2.95 seconds
Compilation time: 2.92 secondsCompilation time: 2.92 seconds

Compilation time: 2.98 secondsCompilation time: 2.93 seconds

Compilation time: 2.90 seconds
Compilation time: 2.92 seconds
Prefill latency: 0.016432588919997215 sec
Decode latency: 2.8804149236530066 sec
Prefill latency: 0.016197318211197853 sec
Decode latency: 2.8811065293848515 sec
Prefill latency: 0.016297515481710434 sec
Decode latency: 2.88011908903718 sec
Prefill latency: 0.016229595988988876 sec
Decode latency: 2.8783251848071814 sec
Prefill latency: 0.016611848026514053 sec
Decode latency: 2.879997557029128 sec
Time for inference 1: 2.90 sec total, 176.68 tokens/sec
Decode latency: 2.88 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 494.00 GB/s
FLOPS achieved: 1.48 TF/s

Prefill latency: 0.01621217466890812 sec
Decode latency: 2.880611242726445 sec
Time for inference 2: 2.90 sec total, 176.67 tokens/sec
Decode latency: 2.88 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 493.97 GB/s
FLOPS achieved: 1.48 TF/s

Prefill latency: 0.01617341674864292 sec
Decode latency: 2.8802060186862946 sec
Time for inference 3: 2.90 sec total, 176.71 tokens/sec
Decode latency: 2.88 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 494.08 GB/s
FLOPS achieved: 1.48 TF/s

Prefill latency: 0.01640339195728302 sec
Decode latency: 2.8798409271985292 sec
Time for inference 4: 2.90 sec total, 176.72 tokens/sec
Decode latency: 2.88 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 494.11 GB/s
FLOPS achieved: 1.48 TF/s

Prefill latency: 0.016258472576737404 sec
Decode latency: 2.8798451758921146 sec
Time for inference 5: 2.90 sec total, 176.73 tokens/sec
Decode latency: 2.88 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 494.13 GB/s
FLOPS achieved: 1.48 TF/s

Prefill latency: 0.016479240730404854 sec
Decode latency: 2.8795158471912146 sec
Time for inference 6: 2.90 sec total, 176.73 tokens/sec
Decode latency: 2.88 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 494.14 GB/s
FLOPS achieved: 1.48 TF/s

Prefill latency: 0.016199583187699318 sec
Decode latency: 2.879554335027933 sec
Time for inference 7: 2.90 sec total, 176.75 tokens/sec
Decode latency: 2.88 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 494.19 GB/s
FLOPS achieved: 1.48 TF/s

Prefill latency: 0.01629207655787468 sec
Decode latency: 2.879998240619898 sec
Time for inference 8: 2.90 sec total, 176.70 tokens/sec
Decode latency: 2.88 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 494.07 GB/s
FLOPS achieved: 1.48 TF/s

Prefill latency: 0.016175968572497368 sec
Decode latency: 2.879313450306654 sec
Time for inference 9: 2.90 sec total, 176.76 tokens/sec
Decode latency: 2.88 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 494.22 GB/s
FLOPS achieved: 1.48 TF/s

Prefill latency: 0.016224727034568787 sec
Decode latency: 2.8790792003273964 sec
Time for inference 10: 2.90 sec total, 176.77 tokens/sec
Decode latency: 2.88 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 494.25 GB/s
FLOPS achieved: 1.48 TF/s

==========
Batch Size: 1
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 2.8798 sec
Average prefill latency: 0.0163 sec
Average tokens/sec: 176.72
Memory used: 4.98 GB
Done. we are killing the process
[rank0]:[W1114 02:04:20.212886705 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
