W1114 02:43:39.022000 2360743 site-packages/torch/distributed/run.py:793] 
W1114 02:43:39.022000 2360743 site-packages/torch/distributed/run.py:793] *****************************************
W1114 02:43:39.022000 2360743 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1114 02:43:39.022000 2360743 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=1536, bias=False)
        (wo): Linear(in_features=1024, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.26 seconds
CUDA_GRAPH are activate
Prefill latency: 0.05183594115078449 sec
Decode latency: 2.9647225700318813 sec
Compilation time: 3.02 seconds
Compilation time: 3.02 seconds
Compilation time: 3.02 seconds
Compilation time: 3.02 seconds
Prefill latency: 0.04915201663970947 sec
Decode latency: 2.964775826781988 sec
Prefill latency: 0.049217768013477325 sec
Decode latency: 2.965167658403516 sec
Prefill latency: 0.049383312463760376 sec
Decode latency: 2.963472407311201 sec
Prefill latency: 0.049313973635435104 sec
Decode latency: 2.96321159042418 sec
Prefill latency: 0.049224043264985085 sec
Decode latency: 2.9656583685427904 sec
Time for inference 1: 3.02 sec total, 679.09 tokens/sec
Decode latency: 2.97 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 3083.64 GB/s
FLOPS achieved: 9.25 TF/s

Prefill latency: 0.0492570698261261 sec
Decode latency: 2.9640629962086678 sec
Time for inference 2: 3.01 sec total, 679.47 tokens/sec
Decode latency: 2.96 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 3085.38 GB/s
FLOPS achieved: 9.26 TF/s

Prefill latency: 0.0492362380027771 sec
Decode latency: 2.963261293247342 sec
Time for inference 3: 3.01 sec total, 679.66 tokens/sec
Decode latency: 2.96 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 3086.22 GB/s
FLOPS achieved: 9.26 TF/s

Prefill latency: 0.049228567630052567 sec
Decode latency: 2.9657268468290567 sec
Time for inference 4: 3.02 sec total, 679.07 tokens/sec
Decode latency: 2.97 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 3083.57 GB/s
FLOPS achieved: 9.25 TF/s

Prefill latency: 0.049230094999074936 sec
Decode latency: 2.9649669639766216 sec
Time for inference 5: 3.02 sec total, 679.24 tokens/sec
Decode latency: 2.96 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 3084.34 GB/s
FLOPS achieved: 9.25 TF/s

Prefill latency: 0.049162816256284714 sec
Decode latency: 2.9639414995908737 sec
Time for inference 6: 3.01 sec total, 679.45 tokens/sec
Decode latency: 2.96 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 3085.30 GB/s
FLOPS achieved: 9.26 TF/s

Prefill latency: 0.049287304282188416 sec
Decode latency: 2.9639985729008913 sec
Time for inference 7: 3.01 sec total, 679.43 tokens/sec
Decode latency: 2.96 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 3085.21 GB/s
FLOPS achieved: 9.26 TF/s

Prefill latency: 0.049159105867147446 sec
Decode latency: 2.9653430804610252 sec
Time for inference 8: 3.02 sec total, 679.16 tokens/sec
Decode latency: 2.97 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 3083.98 GB/s
FLOPS achieved: 9.25 TF/s

Prefill latency: 0.04913542419672012 sec
Decode latency: 2.9639528151601553 sec
Time for inference 9: 3.01 sec total, 679.51 tokens/sec
Decode latency: 2.96 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 3085.57 GB/s
FLOPS achieved: 9.26 TF/s

Prefill latency: 0.049291571602225304 sec
Decode latency: 2.9644763693213463 sec
Time for inference 10: 3.01 sec total, 679.37 tokens/sec
Decode latency: 2.96 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 3084.92 GB/s
FLOPS achieved: 9.25 TF/s

==========
Batch Size: 4
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 2.9645 sec
Average prefill latency: 0.0492 sec
Average tokens/sec: 679.34
Memory used: 9.11 GB
Done. we are killing the process
[rank0]:[W1114 02:44:40.139529189 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
