W1114 02:48:54.321000 2380333 site-packages/torch/distributed/run.py:793] 
W1114 02:48:54.321000 2380333 site-packages/torch/distributed/run.py:793] *****************************************
W1114 02:48:54.321000 2380333 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1114 02:48:54.321000 2380333 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=1536, bias=False)
        (wo): Linear(in_features=1024, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.55 seconds
CUDA_GRAPH are activate
Prefill latency: 0.2006343211978674 sec
Decode latency: 3.2330318838357925 sec
Compilation time: 3.44 secondsCompilation time: 3.42 secondsCompilation time: 3.48 seconds


Compilation time: 3.43 seconds
Prefill latency: 0.19002648815512657 sec
Decode latency: 3.2346425727009773 sec
Prefill latency: 0.19004682451486588 sec
Decode latency: 3.2342251017689705 sec
Prefill latency: 0.19077328220009804 sec
Decode latency: 3.2337491735816 sec
Prefill latency: 0.19060376472771168 sec
Decode latency: 3.231123300269246 sec
Prefill latency: 0.19007575325667858 sec
Decode latency: 3.23337428458035 sec
Time for inference 1: 3.42 sec total, 2392.13 tokens/sec
Decode latency: 3.23 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 10862.32 GB/s
FLOPS achieved: 32.59 TF/s

Prefill latency: 0.19015207141637802 sec
Decode latency: 3.233240343630314 sec
Time for inference 2: 3.42 sec total, 2392.20 tokens/sec
Decode latency: 3.23 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 10862.67 GB/s
FLOPS achieved: 32.59 TF/s

Prefill latency: 0.1908071842044592 sec
Decode latency: 3.233133230358362 sec
Time for inference 3: 3.42 sec total, 2391.91 tokens/sec
Decode latency: 3.23 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 10861.33 GB/s
FLOPS achieved: 32.58 TF/s

Prefill latency: 0.1908571794629097 sec
Decode latency: 3.2324310950934887 sec
Time for inference 4: 3.42 sec total, 2392.31 tokens/sec
Decode latency: 3.23 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 10863.16 GB/s
FLOPS achieved: 32.59 TF/s

Prefill latency: 0.1899932511150837 sec
Decode latency: 3.233584813773632 sec
Time for inference 5: 3.42 sec total, 2392.13 tokens/sec
Decode latency: 3.23 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 10862.35 GB/s
FLOPS achieved: 32.59 TF/s

Prefill latency: 0.1903795786201954 sec
Decode latency: 3.2324166372418404 sec
Time for inference 6: 3.42 sec total, 2392.74 tokens/sec
Decode latency: 3.23 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 10865.13 GB/s
FLOPS achieved: 32.60 TF/s

Prefill latency: 0.1903672907501459 sec
Decode latency: 3.234878296032548 sec
Time for inference 7: 3.43 sec total, 2390.97 tokens/sec
Decode latency: 3.23 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 10857.07 GB/s
FLOPS achieved: 32.57 TF/s

Prefill latency: 0.1907288283109665 sec
Decode latency: 3.233214160427451 sec
Time for inference 8: 3.42 sec total, 2391.87 tokens/sec
Decode latency: 3.23 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 10861.17 GB/s
FLOPS achieved: 32.58 TF/s

Prefill latency: 0.1903131678700447 sec
Decode latency: 3.2341740876436234 sec
Time for inference 9: 3.43 sec total, 2391.37 tokens/sec
Decode latency: 3.23 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 10858.87 GB/s
FLOPS achieved: 32.58 TF/s

Prefill latency: 0.19046124070882797 sec
Decode latency: 3.2345130648463964 sec
Time for inference 10: 3.43 sec total, 2391.17 tokens/sec
Decode latency: 3.23 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 10857.98 GB/s
FLOPS achieved: 32.57 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 3.2335 sec
Average prefill latency: 0.1904 sec
Average tokens/sec: 2391.88
Memory used: 19.15 GB
Done. we are killing the process
[rank0]:[W1114 02:50:01.934069437 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
