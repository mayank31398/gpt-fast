W1114 02:39:58.927000 2347696 site-packages/torch/distributed/run.py:793] 
W1114 02:39:58.927000 2347696 site-packages/torch/distributed/run.py:793] *****************************************
W1114 02:39:58.927000 2347696 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1114 02:39:58.927000 2347696 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=768, bias=False)
        (wo): Linear(in_features=512, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=3584, bias=False)
        (w2): Linear(in_features=1792, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.11 seconds
CUDA_GRAPH are activate
Prefill latency: 0.014843141660094261 sec
Decode latency: 2.488415800035 sec
Compilation time: 2.55 secondsCompilation time: 2.52 secondsCompilation time: 2.52 seconds


Compilation time: 2.50 seconds
Compilation time: 2.51 secondsCompilation time: 2.51 seconds

Compilation time: 2.55 secondsCompilation time: 2.50 seconds

Prefill latency: 0.013589780777692795 sec
Decode latency: 2.488874414935708 sec
Prefill latency: 0.0135395098477602 sec
Decode latency: 2.4882290363311768 sec
Prefill latency: 0.013598902150988579 sec
Decode latency: 2.488471604883671 sec
Prefill latency: 0.01347639411687851 sec
Decode latency: 2.487978246062994 sec
Prefill latency: 0.013621287420392036 sec
Decode latency: 2.488256961107254 sec
Time for inference 1: 2.50 sec total, 204.57 tokens/sec
Decode latency: 2.49 sec
Prefill latency: 0.01 sec
Bandwidth achieved: 571.97 GB/s
FLOPS achieved: 1.72 TF/s

Prefill latency: 0.013482494279742241 sec
Decode latency: 2.4885809551924467 sec
Time for inference 2: 2.50 sec total, 204.56 tokens/sec
Decode latency: 2.49 sec
Prefill latency: 0.01 sec
Bandwidth achieved: 571.95 GB/s
FLOPS achieved: 1.72 TF/s

Prefill latency: 0.013707516714930534 sec
Decode latency: 2.4886374771595 sec
Time for inference 3: 2.50 sec total, 204.53 tokens/sec
Decode latency: 2.49 sec
Prefill latency: 0.01 sec
Bandwidth achieved: 571.86 GB/s
FLOPS achieved: 1.72 TF/s

Prefill latency: 0.013471938669681549 sec
Decode latency: 2.4879580568522215 sec
Time for inference 4: 2.50 sec total, 204.61 tokens/sec
Decode latency: 2.49 sec
Prefill latency: 0.01 sec
Bandwidth achieved: 572.11 GB/s
FLOPS achieved: 1.72 TF/s

Prefill latency: 0.013896187767386436 sec
Decode latency: 2.4877576362341642 sec
Time for inference 5: 2.50 sec total, 204.59 tokens/sec
Decode latency: 2.49 sec
Prefill latency: 0.01 sec
Bandwidth achieved: 572.03 GB/s
FLOPS achieved: 1.72 TF/s

Prefill latency: 0.013720804825425148 sec
Decode latency: 2.4882973097264767 sec
Time for inference 6: 2.50 sec total, 204.54 tokens/sec
Decode latency: 2.49 sec
Prefill latency: 0.01 sec
Bandwidth achieved: 571.91 GB/s
FLOPS achieved: 1.72 TF/s

Prefill latency: 0.013479022309184074 sec
Decode latency: 2.4883315954357386 sec
Time for inference 7: 2.50 sec total, 204.57 tokens/sec
Decode latency: 2.49 sec
Prefill latency: 0.01 sec
Bandwidth achieved: 572.00 GB/s
FLOPS achieved: 1.72 TF/s

Prefill latency: 0.013512929901480675 sec
Decode latency: 2.488168243318796 sec
Time for inference 8: 2.50 sec total, 204.59 tokens/sec
Decode latency: 2.49 sec
Prefill latency: 0.01 sec
Bandwidth achieved: 572.03 GB/s
FLOPS achieved: 1.72 TF/s

Prefill latency: 0.013854136690497398 sec
Decode latency: 2.4882582761347294 sec
Time for inference 9: 2.50 sec total, 204.56 tokens/sec
Decode latency: 2.49 sec
Prefill latency: 0.01 sec
Bandwidth achieved: 571.95 GB/s
FLOPS achieved: 1.72 TF/s

Prefill latency: 0.013590451329946518 sec
Decode latency: 2.4886409770697355 sec
Time for inference 10: 2.50 sec total, 204.53 tokens/sec
Decode latency: 2.49 sec
Prefill latency: 0.01 sec
Bandwidth achieved: 571.86 GB/s
FLOPS achieved: 1.72 TF/s

==========
Batch Size: 1
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 2.4883 sec
Average prefill latency: 0.0136 sec
Average tokens/sec: 204.56
Memory used: 4.65 GB
Done. we are killing the process
[rank0]:[W1114 02:41:01.126583839 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
