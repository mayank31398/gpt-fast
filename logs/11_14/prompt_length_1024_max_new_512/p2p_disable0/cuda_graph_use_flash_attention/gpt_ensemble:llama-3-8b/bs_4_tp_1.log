flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
        (wo): Linear(in_features=4096, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=28672, bias=False)
        (w2): Linear(in_features=14336, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.14 seconds
CUDA_GRAPH are activate
Prefill latency: 0.13090169988572598 sec
Decode latency: 4.729553714394569 sec
Compilation time: 4.86 seconds
Prefill latency: 0.13076733984053135 sec
Decode latency: 4.727142771705985 sec
Prefill latency: 0.1313642505556345 sec
Decode latency: 4.725476216524839 sec
Prefill latency: 0.1323783714324236 sec
Decode latency: 4.725992331281304 sec
Prefill latency: 0.1318782288581133 sec
Decode latency: 4.72427505068481 sec
Prefill latency: 0.13138722069561481 sec
Decode latency: 4.726135611534119 sec
Time for inference 1: 4.86 sec total, 421.53 tokens/sec
Decode latency: 4.73 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6327.12 GB/s
FLOPS achieved: 18.98 TF/s

Prefill latency: 0.1309403870254755 sec
Decode latency: 4.726293783634901 sec
Time for inference 2: 4.86 sec total, 421.57 tokens/sec
Decode latency: 4.73 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6327.66 GB/s
FLOPS achieved: 18.98 TF/s

Prefill latency: 0.13064803183078766 sec
Decode latency: 4.725417044013739 sec
Time for inference 3: 4.86 sec total, 421.67 tokens/sec
Decode latency: 4.73 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6329.20 GB/s
FLOPS achieved: 18.99 TF/s

Prefill latency: 0.13245239667594433 sec
Decode latency: 4.725541656836867 sec
Time for inference 4: 4.86 sec total, 421.49 tokens/sec
Decode latency: 4.73 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6326.55 GB/s
FLOPS achieved: 18.98 TF/s

Prefill latency: 0.13169608265161514 sec
Decode latency: 4.726016292348504 sec
Time for inference 5: 4.86 sec total, 421.51 tokens/sec
Decode latency: 4.73 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6326.87 GB/s
FLOPS achieved: 18.98 TF/s

Prefill latency: 0.13102952390909195 sec
Decode latency: 4.725585842505097 sec
Time for inference 6: 4.86 sec total, 421.62 tokens/sec
Decode latency: 4.73 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6328.42 GB/s
FLOPS achieved: 18.99 TF/s

Prefill latency: 0.13348674587905407 sec
Decode latency: 4.726354705169797 sec
Time for inference 7: 4.86 sec total, 421.33 tokens/sec
Decode latency: 4.73 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6324.05 GB/s
FLOPS achieved: 18.97 TF/s

Prefill latency: 0.13126825168728828 sec
Decode latency: 4.725697714835405 sec
Time for inference 8: 4.86 sec total, 421.59 tokens/sec
Decode latency: 4.73 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6327.99 GB/s
FLOPS achieved: 18.98 TF/s

Prefill latency: 0.1308712400496006 sec
Decode latency: 4.726231351494789 sec
Time for inference 9: 4.86 sec total, 421.58 tokens/sec
Decode latency: 4.73 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6327.89 GB/s
FLOPS achieved: 18.98 TF/s

Prefill latency: 0.13181385584175587 sec
Decode latency: 4.726368460804224 sec
Time for inference 10: 4.86 sec total, 421.47 tokens/sec
Decode latency: 4.73 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6326.21 GB/s
FLOPS achieved: 18.98 TF/s

==========
Batch Size: 4
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 4.7260 sec
Average prefill latency: 0.1316 sec
Average tokens/sec: 421.54
Memory used: 19.87 GB
Done. we are killing the process
[rank0]:[W1114 02:42:27.813162926 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
