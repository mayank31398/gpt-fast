W1114 02:55:48.790000 2403366 site-packages/torch/distributed/run.py:793] 
W1114 02:55:48.790000 2403366 site-packages/torch/distributed/run.py:793] *****************************************
W1114 02:55:48.790000 2403366 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1114 02:55:48.790000 2403366 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=1536, bias=False)
        (wo): Linear(in_features=1024, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.11 seconds
CUDA_GRAPH are activate
[rank3]:[W1114 02:56:06.900185170 CUDAGraph.cpp:133] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
Prefill latency: 0.7276942282915115 sec
Decode latency: 3.8835588302463293 sec
Compilation time: 4.61 secondsCompilation time: 4.61 seconds

Compilation time: 4.61 seconds
Compilation time: 4.64 seconds
Prefill latency: 0.7259532324969769 sec
Decode latency: 3.8864848911762238 sec
Prefill latency: 0.7265480253845453 sec
Decode latency: 3.884363278746605 sec
Prefill latency: 0.7252387944608927 sec
Decode latency: 3.886517586186528 sec
Prefill latency: 0.7260918822139502 sec
Decode latency: 3.886982126161456 sec
Prefill latency: 0.7280267626047134 sec
Decode latency: 3.8859019055962563 sec
Time for inference 1: 4.61 sec total, 7100.39 tokens/sec
Decode latency: 3.89 sec
Prefill latency: 0.73 sec
Bandwidth achieved: 32241.90 GB/s
FLOPS achieved: 96.73 TF/s

Prefill latency: 0.7293121442198753 sec
Decode latency: 3.885226221755147 sec
Time for inference 2: 4.62 sec total, 7099.51 tokens/sec
Decode latency: 3.89 sec
Prefill latency: 0.73 sec
Bandwidth achieved: 32237.92 GB/s
FLOPS achieved: 96.71 TF/s

Prefill latency: 0.7316012121737003 sec
Decode latency: 3.8828381299972534 sec
Time for inference 3: 4.62 sec total, 7099.61 tokens/sec
Decode latency: 3.88 sec
Prefill latency: 0.73 sec
Bandwidth achieved: 32238.38 GB/s
FLOPS achieved: 96.72 TF/s

Prefill latency: 0.7273873016238213 sec
Decode latency: 3.884649321436882 sec
Time for inference 4: 4.61 sec total, 7103.43 tokens/sec
Decode latency: 3.88 sec
Prefill latency: 0.73 sec
Bandwidth achieved: 32255.72 GB/s
FLOPS achieved: 96.77 TF/s

Prefill latency: 0.7272873930633068 sec
Decode latency: 3.8842518385499716 sec
Time for inference 5: 4.61 sec total, 7104.27 tokens/sec
Decode latency: 3.88 sec
Prefill latency: 0.73 sec
Bandwidth achieved: 32259.52 GB/s
FLOPS achieved: 96.78 TF/s

Prefill latency: 0.7238789778202772 sec
Decode latency: 3.888757687062025 sec
Time for inference 6: 4.61 sec total, 7102.37 tokens/sec
Decode latency: 3.89 sec
Prefill latency: 0.72 sec
Bandwidth achieved: 32250.92 GB/s
FLOPS achieved: 96.75 TF/s

Prefill latency: 0.726190472021699 sec
Decode latency: 3.8869231399148703 sec
Time for inference 7: 4.61 sec total, 7101.66 tokens/sec
Decode latency: 3.89 sec
Prefill latency: 0.73 sec
Bandwidth achieved: 32247.68 GB/s
FLOPS achieved: 96.74 TF/s

Prefill latency: 0.725509999319911 sec
Decode latency: 3.885854359716177 sec
Time for inference 8: 4.61 sec total, 7104.45 tokens/sec
Decode latency: 3.89 sec
Prefill latency: 0.73 sec
Bandwidth achieved: 32260.35 GB/s
FLOPS achieved: 96.78 TF/s

Prefill latency: 0.7240920942276716 sec
Decode latency: 3.8872781954705715 sec
Time for inference 9: 4.61 sec total, 7104.62 tokens/sec
Decode latency: 3.89 sec
Prefill latency: 0.72 sec
Bandwidth achieved: 32261.14 GB/s
FLOPS achieved: 96.78 TF/s

Prefill latency: 0.7248380817472935 sec
Decode latency: 3.8866246584802866 sec
Time for inference 10: 4.61 sec total, 7104.30 tokens/sec
Decode latency: 3.89 sec
Prefill latency: 0.72 sec
Bandwidth achieved: 32259.70 GB/s
FLOPS achieved: 96.78 TF/s

==========
Batch Size: 64
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 3.8858 sec
Average prefill latency: 0.7268 sec
Average tokens/sec: 7102.46
Memory used: 59.35 GB
Done. we are killing the process
[rank0]:[W1114 02:57:15.572474105 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
