W1120 02:34:52.995000 3833978 site-packages/torch/distributed/run.py:793] 
W1120 02:34:52.995000 3833978 site-packages/torch/distributed/run.py:793] *****************************************
W1120 02:34:52.995000 3833978 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1120 02:34:52.995000 3833978 site-packages/torch/distributed/run.py:793] *****************************************
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
flash_kv_decode is set to True
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
our world size=8
Using device=cuda
Loading model ...
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8rank: 2, global_rank: 2, world_size: 8, global_world_size: 8

rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
GPTDense(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x DenseTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=1280, bias=False)
        (wo): Linear(in_features=1024, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 1.44 seconds
CUDA_GRAPH are activate
Prefill latency: 0.8638382069766521 sec
Decode latency: 10.43412945093587 sec
Compilation time: 11.30 seconds
Compilation time: 11.30 secondsCompilation time: 11.33 seconds

Compilation time: 11.27 seconds
Compilation time: 11.26 seconds
Compilation time: 11.26 seconds
Compilation time: 11.30 seconds
Compilation time: 11.26 seconds
Prefill latency: 0.8207759591750801 sec
Decode latency: 10.433185098227113 sec
Prefill latency: 0.8209169576875865 sec
Decode latency: 10.432896906044334 sec
Prefill latency: 0.821370757650584 sec
Decode latency: 10.433192019816488 sec
Prefill latency: 0.8239799113944173 sec
Decode latency: 10.4315978679806 sec
Prefill latency: 0.824226058088243 sec
Decode latency: 10.43249950511381 sec
Time for inference 1: 11.26 sec total, 727.65 tokens/sec
Decode latency: 10.43 sec
Prefill latency: 0.82 sec
Bandwidth achieved: 13983.12 GB/s
FLOPS achieved: 41.95 TF/s

Prefill latency: 0.8252773778513074 sec
Decode latency: 10.433717460837215 sec
Time for inference 2: 11.26 sec total, 727.51 tokens/sec
Decode latency: 10.43 sec
Prefill latency: 0.83 sec
Bandwidth achieved: 13980.31 GB/s
FLOPS achieved: 41.94 TF/s

Prefill latency: 0.8247996177524328 sec
Decode latency: 10.43148096697405 sec
Time for inference 3: 11.26 sec total, 727.68 tokens/sec
Decode latency: 10.43 sec
Prefill latency: 0.82 sec
Bandwidth achieved: 13983.65 GB/s
FLOPS achieved: 41.95 TF/s

Prefill latency: 0.824777667876333 sec
Decode latency: 10.433282818179578 sec
Time for inference 4: 11.26 sec total, 727.57 tokens/sec
Decode latency: 10.43 sec
Prefill latency: 0.82 sec
Bandwidth achieved: 13981.57 GB/s
FLOPS achieved: 41.94 TF/s

Prefill latency: 0.8269558758474886 sec
Decode latency: 10.431537544354796 sec
Time for inference 5: 11.26 sec total, 727.54 tokens/sec
Decode latency: 10.43 sec
Prefill latency: 0.83 sec
Bandwidth achieved: 13980.98 GB/s
FLOPS achieved: 41.94 TF/s

Prefill latency: 0.8243311592377722 sec
Decode latency: 10.431489737238735 sec
Time for inference 6: 11.26 sec total, 727.71 tokens/sec
Decode latency: 10.43 sec
Prefill latency: 0.82 sec
Bandwidth achieved: 13984.29 GB/s
FLOPS achieved: 41.95 TF/s

Prefill latency: 0.8266126164235175 sec
Decode latency: 10.431157329119742 sec
Time for inference 7: 11.26 sec total, 727.59 tokens/sec
Decode latency: 10.43 sec
Prefill latency: 0.83 sec
Bandwidth achieved: 13981.92 GB/s
FLOPS achieved: 41.95 TF/s

Prefill latency: 0.8257571868598461 sec
Decode latency: 10.432849451899529 sec
Time for inference 8: 11.26 sec total, 727.54 tokens/sec
Decode latency: 10.43 sec
Prefill latency: 0.83 sec
Bandwidth achieved: 13980.97 GB/s
FLOPS achieved: 41.94 TF/s

Prefill latency: 0.8269985062070191 sec
Decode latency: 10.431781496852636 sec
Time for inference 9: 11.26 sec total, 727.53 tokens/sec
Decode latency: 10.43 sec
Prefill latency: 0.83 sec
Bandwidth achieved: 13980.79 GB/s
FLOPS achieved: 41.94 TF/s

Prefill latency: 0.827418299857527 sec
Decode latency: 10.43096347199753 sec
Time for inference 10: 11.26 sec total, 727.55 tokens/sec
Decode latency: 10.43 sec
Prefill latency: 0.83 sec
Bandwidth achieved: 13981.24 GB/s
FLOPS achieved: 41.94 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 10.4321 sec
Average prefill latency: 0.8257 sec
Average tokens/sec: 727.59
Memory used: 71.38 GB
Done. we are killing the process
[rank0]:[W1120 02:38:01.189903204 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
