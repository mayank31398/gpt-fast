W1114 03:25:32.293000 2300991 site-packages/torch/distributed/run.py:793] 
W1114 03:25:32.293000 2300991 site-packages/torch/distributed/run.py:793] *****************************************
W1114 03:25:32.293000 2300991 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1114 03:25:32.293000 2300991 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTDense(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x DenseTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=2560, bias=False)
        (wo): Linear(in_features=2048, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=14336, bias=False)
        (w2): Linear(in_features=7168, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 1.28 seconds
CUDA_GRAPH are activate
Prefill latency: 0.12640616204589605 sec
Decode latency: 11.794398602098227 sec
Compilation time: 11.93 seconds
Compilation time: 11.90 seconds
Compilation time: 11.92 seconds
Compilation time: 11.92 seconds
Prefill latency: 0.10125739220529795 sec
Decode latency: 11.80071606952697 sec
Prefill latency: 0.1010331166908145 sec
Decode latency: 11.795591454021633 sec
Prefill latency: 0.10112457443028688 sec
Decode latency: 11.80129262059927 sec
Prefill latency: 0.10105228796601295 sec
Decode latency: 11.795374477282166 sec
Prefill latency: 0.10138921439647675 sec
Decode latency: 11.799167476594448 sec
Time for inference 1: 11.90 sec total, 43.02 tokens/sec
Decode latency: 11.80 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 1562.83 GB/s
FLOPS achieved: 4.69 TF/s

Prefill latency: 0.10159135609865189 sec
Decode latency: 11.803015224635601 sec
Time for inference 2: 11.91 sec total, 43.00 tokens/sec
Decode latency: 11.80 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 1562.30 GB/s
FLOPS achieved: 4.69 TF/s

Prefill latency: 0.10157380532473326 sec
Decode latency: 11.795975994318724 sec
Time for inference 3: 11.90 sec total, 43.03 tokens/sec
Decode latency: 11.80 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 1563.23 GB/s
FLOPS achieved: 4.69 TF/s

Prefill latency: 0.1010902626439929 sec
Decode latency: 11.8004019735381 sec
Time for inference 4: 11.90 sec total, 43.01 tokens/sec
Decode latency: 11.80 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 1562.69 GB/s
FLOPS achieved: 4.69 TF/s

Prefill latency: 0.1013823039829731 sec
Decode latency: 11.796659874729812 sec
Time for inference 5: 11.90 sec total, 43.03 tokens/sec
Decode latency: 11.80 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 1563.14 GB/s
FLOPS achieved: 4.69 TF/s

Prefill latency: 0.10169172752648592 sec
Decode latency: 11.79268744867295 sec
Time for inference 6: 11.90 sec total, 43.04 tokens/sec
Decode latency: 11.79 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 1563.65 GB/s
FLOPS achieved: 4.69 TF/s

Prefill latency: 0.10103665105998516 sec
Decode latency: 11.793588986620307 sec
Time for inference 7: 11.90 sec total, 43.04 tokens/sec
Decode latency: 11.79 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 1563.61 GB/s
FLOPS achieved: 4.69 TF/s

Prefill latency: 0.10139401070773602 sec
Decode latency: 11.791892698034644 sec
Time for inference 8: 11.89 sec total, 43.04 tokens/sec
Decode latency: 11.79 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 1563.79 GB/s
FLOPS achieved: 4.69 TF/s

Prefill latency: 0.1011767815798521 sec
Decode latency: 11.794370219111443 sec
Time for inference 9: 11.90 sec total, 43.04 tokens/sec
Decode latency: 11.79 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 1563.48 GB/s
FLOPS achieved: 4.69 TF/s

Prefill latency: 0.1015535295009613 sec
Decode latency: 11.797686973586679 sec
Time for inference 10: 11.90 sec total, 43.02 tokens/sec
Decode latency: 11.80 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 1562.99 GB/s
FLOPS achieved: 4.69 TF/s

==========
Batch Size: 1
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 11.7965 sec
Average prefill latency: 0.1014 sec
Average tokens/sec: 43.03
Memory used: 42.80 GB
Done. we are killing the process
[rank0]:[W1114 03:28:42.815338269 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
