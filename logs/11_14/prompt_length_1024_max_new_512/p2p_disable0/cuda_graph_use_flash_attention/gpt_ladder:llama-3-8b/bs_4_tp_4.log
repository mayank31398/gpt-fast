W1114 02:25:53.805000 2328757 site-packages/torch/distributed/run.py:793] 
W1114 02:25:53.805000 2328757 site-packages/torch/distributed/run.py:793] *****************************************
W1114 02:25:53.805000 2328757 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1114 02:25:53.805000 2328757 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTLadder(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x LadderTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=1536, bias=False)
        (wo): Linear(in_features=1024, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.39 seconds
CUDA_GRAPH are activate
[rank0]:[W1114 02:26:06.762705018 CUDAGraph.cpp:133] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank3]:[W1114 02:26:06.774385348 CUDAGraph.cpp:133] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank2]:[W1114 02:26:07.832400746 CUDAGraph.cpp:133] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
Prefill latency: 0.07865246571600437 sec
Decode latency: 2.7516657765954733 sec
Compilation time: 2.80 secondsCompilation time: 2.83 seconds

Compilation time: 2.84 seconds
Compilation time: 2.83 seconds
Prefill latency: 0.04715432785451412 sec
Decode latency: 2.7541056498885155 sec
Prefill latency: 0.04704051464796066 sec
Decode latency: 2.7537643928080797 sec
Prefill latency: 0.04709180258214474 sec
Decode latency: 2.753983963280916 sec
Prefill latency: 0.047110771760344505 sec
Decode latency: 2.753332633525133 sec
Prefill latency: 0.047133153304457664 sec
Decode latency: 2.7531672697514296 sec
Time for inference 1: 2.80 sec total, 731.13 tokens/sec
Decode latency: 2.75 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 3319.95 GB/s
FLOPS achieved: 9.96 TF/s

Prefill latency: 0.04730517230927944 sec
Decode latency: 2.7529946602880955 sec
Time for inference 2: 2.80 sec total, 731.08 tokens/sec
Decode latency: 2.75 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 3319.75 GB/s
FLOPS achieved: 9.96 TF/s

Prefill latency: 0.04702933691442013 sec
Decode latency: 2.752628903836012 sec
Time for inference 3: 2.80 sec total, 731.26 tokens/sec
Decode latency: 2.75 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 3320.54 GB/s
FLOPS achieved: 9.96 TF/s

Prefill latency: 0.04706618748605251 sec
Decode latency: 2.752979777753353 sec
Time for inference 4: 2.80 sec total, 731.17 tokens/sec
Decode latency: 2.75 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 3320.15 GB/s
FLOPS achieved: 9.96 TF/s

Prefill latency: 0.0470795352011919 sec
Decode latency: 2.7526138108223677 sec
Time for inference 5: 2.80 sec total, 731.29 tokens/sec
Decode latency: 2.75 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 3320.70 GB/s
FLOPS achieved: 9.96 TF/s

Prefill latency: 0.04709291458129883 sec
Decode latency: 2.753217523917556 sec
Time for inference 6: 2.80 sec total, 731.14 tokens/sec
Decode latency: 2.75 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 3319.99 GB/s
FLOPS achieved: 9.96 TF/s

Prefill latency: 0.04717743955552578 sec
Decode latency: 2.7534107845276594 sec
Time for inference 7: 2.80 sec total, 731.06 tokens/sec
Decode latency: 2.75 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 3319.64 GB/s
FLOPS achieved: 9.96 TF/s

Prefill latency: 0.0475015752017498 sec
Decode latency: 2.753678759559989 sec
Time for inference 8: 2.80 sec total, 730.91 tokens/sec
Decode latency: 2.75 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 3318.95 GB/s
FLOPS achieved: 9.96 TF/s

Prefill latency: 0.047320783138275146 sec
Decode latency: 2.7533307764679193 sec
Time for inference 9: 2.80 sec total, 731.05 tokens/sec
Decode latency: 2.75 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 3319.60 GB/s
FLOPS achieved: 9.96 TF/s

Prefill latency: 0.04728434234857559 sec
Decode latency: 2.752772817388177 sec
Time for inference 10: 2.80 sec total, 731.20 tokens/sec
Decode latency: 2.75 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 3320.28 GB/s
FLOPS achieved: 9.96 TF/s

==========
Batch Size: 4
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 2.7531 sec
Average prefill latency: 0.0472 sec
Average tokens/sec: 731.13
Memory used: 10.22 GB
Done. we are killing the process
[rank0]:[W1114 02:26:49.172983584 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
