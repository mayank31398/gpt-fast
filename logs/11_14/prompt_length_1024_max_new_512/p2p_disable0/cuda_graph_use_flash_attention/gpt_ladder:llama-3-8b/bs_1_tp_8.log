W1114 02:22:23.512000 2325686 site-packages/torch/distributed/run.py:793] 
W1114 02:22:23.512000 2325686 site-packages/torch/distributed/run.py:793] *****************************************
W1114 02:22:23.512000 2325686 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1114 02:22:23.512000 2325686 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTLadder(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x LadderTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=768, bias=False)
        (wo): Linear(in_features=512, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=3584, bias=False)
        (w2): Linear(in_features=1792, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.38 seconds
CUDA_GRAPH are activate
[rank7]:[W1114 02:22:44.416273863 CUDAGraph.cpp:133] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank3]:[W1114 02:22:44.419876745 CUDAGraph.cpp:133] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
Prefill latency: 0.01966269686818123 sec
Decode latency: 2.1871998719871044 sec
Compilation time: 2.20 seconds
Compilation time: 2.23 seconds
Compilation time: 2.23 seconds
Compilation time: 2.23 seconds
Compilation time: 2.21 seconds
Compilation time: 2.21 seconds
Compilation time: 2.25 seconds
Compilation time: 2.21 seconds
Prefill latency: 0.014402581378817558 sec
Decode latency: 2.187141753733158 sec
Prefill latency: 0.014132065698504448 sec
Decode latency: 2.1868024971336126 sec
Prefill latency: 0.014119083061814308 sec
Decode latency: 2.186274729669094 sec
Prefill latency: 0.014023475348949432 sec
Decode latency: 2.1855841279029846 sec
Prefill latency: 0.014037743210792542 sec
Decode latency: 2.185955908149481 sec
Time for inference 1: 2.20 sec total, 232.62 tokens/sec
Decode latency: 2.19 sec
Prefill latency: 0.01 sec
Bandwidth achieved: 650.41 GB/s
FLOPS achieved: 1.95 TF/s

Prefill latency: 0.01414337195456028 sec
Decode latency: 2.185711545869708 sec
Time for inference 2: 2.20 sec total, 232.64 tokens/sec
Decode latency: 2.19 sec
Prefill latency: 0.01 sec
Bandwidth achieved: 650.46 GB/s
FLOPS achieved: 1.95 TF/s

Prefill latency: 0.014035718515515327 sec
Decode latency: 2.186045002192259 sec
Time for inference 3: 2.20 sec total, 232.60 tokens/sec
Decode latency: 2.19 sec
Prefill latency: 0.01 sec
Bandwidth achieved: 650.36 GB/s
FLOPS achieved: 1.95 TF/s

Prefill latency: 0.01413661241531372 sec
Decode latency: 2.1857284661382437 sec
Time for inference 4: 2.20 sec total, 232.65 tokens/sec
Decode latency: 2.19 sec
Prefill latency: 0.01 sec
Bandwidth achieved: 650.51 GB/s
FLOPS achieved: 1.95 TF/s

Prefill latency: 0.014210684224963188 sec
Decode latency: 2.1860355716198683 sec
Time for inference 5: 2.20 sec total, 232.58 tokens/sec
Decode latency: 2.19 sec
Prefill latency: 0.01 sec
Bandwidth achieved: 650.31 GB/s
FLOPS achieved: 1.95 TF/s

Prefill latency: 0.014106785878539085 sec
Decode latency: 2.1856181155890226 sec
Time for inference 6: 2.20 sec total, 232.67 tokens/sec
Decode latency: 2.19 sec
Prefill latency: 0.01 sec
Bandwidth achieved: 650.54 GB/s
FLOPS achieved: 1.95 TF/s

Prefill latency: 0.014046693220734596 sec
Decode latency: 2.1859760880470276 sec
Time for inference 7: 2.20 sec total, 232.60 tokens/sec
Decode latency: 2.19 sec
Prefill latency: 0.01 sec
Bandwidth achieved: 650.36 GB/s
FLOPS achieved: 1.95 TF/s

Prefill latency: 0.014075206592679024 sec
Decode latency: 2.185836249962449 sec
Time for inference 8: 2.20 sec total, 232.63 tokens/sec
Decode latency: 2.19 sec
Prefill latency: 0.01 sec
Bandwidth achieved: 650.43 GB/s
FLOPS achieved: 1.95 TF/s

Prefill latency: 0.014100546017289162 sec
Decode latency: 2.1859238538891077 sec
Time for inference 9: 2.20 sec total, 232.62 tokens/sec
Decode latency: 2.19 sec
Prefill latency: 0.01 sec
Bandwidth achieved: 650.43 GB/s
FLOPS achieved: 1.95 TF/s

Prefill latency: 0.014349697157740593 sec
Decode latency: 2.18566819652915 sec
Time for inference 10: 2.20 sec total, 232.62 tokens/sec
Decode latency: 2.19 sec
Prefill latency: 0.01 sec
Bandwidth achieved: 650.42 GB/s
FLOPS achieved: 1.95 TF/s

==========
Batch Size: 1
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 2.1858 sec
Average prefill latency: 0.0141 sec
Average tokens/sec: 232.62
Memory used: 4.98 GB
Done. we are killing the process
[rank0]:[W1114 02:23:18.428255523 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
