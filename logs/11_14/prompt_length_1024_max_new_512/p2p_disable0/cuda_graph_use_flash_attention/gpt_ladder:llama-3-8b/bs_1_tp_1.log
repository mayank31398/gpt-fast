flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTLadder(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x LadderTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
        (wo): Linear(in_features=4096, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=28672, bias=False)
        (w2): Linear(in_features=14336, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 0.47 seconds
CUDA_GRAPH are activate
[rank0]:[W1114 02:19:15.050075001 CUDAGraph.cpp:133] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
Prefill latency: 0.033899422734975815 sec
Decode latency: 4.377506826072931 sec
Compilation time: 4.41 seconds
Prefill latency: 0.03371310420334339 sec
Decode latency: 4.374891709536314 sec
Prefill latency: 0.033604925498366356 sec
Decode latency: 4.372291827574372 sec
Prefill latency: 0.03371976315975189 sec
Decode latency: 4.376803185790777 sec
Prefill latency: 0.03367052413523197 sec
Decode latency: 4.408052682876587 sec
Prefill latency: 0.03364914655685425 sec
Decode latency: 4.371485270559788 sec
Time for inference 1: 4.41 sec total, 116.20 tokens/sec
Decode latency: 4.37 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 1744.19 GB/s
FLOPS achieved: 5.23 TF/s

Prefill latency: 0.0336452592164278 sec
Decode latency: 4.374272337183356 sec
Time for inference 2: 4.41 sec total, 116.13 tokens/sec
Decode latency: 4.37 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 1743.12 GB/s
FLOPS achieved: 5.23 TF/s

Prefill latency: 0.033818986266851425 sec
Decode latency: 4.376566670835018 sec
Time for inference 3: 4.41 sec total, 116.07 tokens/sec
Decode latency: 4.38 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 1742.12 GB/s
FLOPS achieved: 5.23 TF/s

Prefill latency: 0.03364817425608635 sec
Decode latency: 4.407897807657719 sec
Time for inference 4: 4.44 sec total, 115.25 tokens/sec
Decode latency: 4.41 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 1729.89 GB/s
FLOPS achieved: 5.19 TF/s

Prefill latency: 0.03372858837246895 sec
Decode latency: 4.40818827226758 sec
Time for inference 5: 4.44 sec total, 115.24 tokens/sec
Decode latency: 4.41 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 1729.79 GB/s
FLOPS achieved: 5.19 TF/s

Prefill latency: 0.033724524080753326 sec
Decode latency: 4.4078279715031385 sec
Time for inference 6: 4.44 sec total, 115.25 tokens/sec
Decode latency: 4.41 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 1729.90 GB/s
FLOPS achieved: 5.19 TF/s

Prefill latency: 0.03377490118145943 sec
Decode latency: 4.372735509648919 sec
Time for inference 7: 4.41 sec total, 116.17 tokens/sec
Decode latency: 4.37 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 1743.70 GB/s
FLOPS achieved: 5.23 TF/s

Prefill latency: 0.03363613598048687 sec
Decode latency: 4.3750158958137035 sec
Time for inference 8: 4.41 sec total, 116.11 tokens/sec
Decode latency: 4.38 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 1742.80 GB/s
FLOPS achieved: 5.23 TF/s

Prefill latency: 0.033773597329854965 sec
Decode latency: 4.408325385302305 sec
Time for inference 9: 4.44 sec total, 115.24 tokens/sec
Decode latency: 4.41 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 1729.70 GB/s
FLOPS achieved: 5.19 TF/s

Prefill latency: 0.033792419359087944 sec
Decode latency: 4.37619667686522 sec
Time for inference 10: 4.41 sec total, 116.08 tokens/sec
Decode latency: 4.38 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 1742.29 GB/s
FLOPS achieved: 5.23 TF/s

==========
Batch Size: 1
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 4.3879 sec
Average prefill latency: 0.0337 sec
Average tokens/sec: 115.77
Memory used: 17.45 GB
Done. we are killing the process
[rank0]:[W1114 02:20:21.539192054 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
