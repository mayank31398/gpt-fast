W1114 02:29:37.083000 2331851 site-packages/torch/distributed/run.py:793] 
W1114 02:29:37.083000 2331851 site-packages/torch/distributed/run.py:793] *****************************************
W1114 02:29:37.083000 2331851 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1114 02:29:37.083000 2331851 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTLadder(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x LadderTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=3072, bias=False)
        (wo): Linear(in_features=2048, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=14336, bias=False)
        (w2): Linear(in_features=7168, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 0.44 seconds
CUDA_GRAPH are activate
[rank0]:[W1114 02:29:47.188658751 CUDAGraph.cpp:133] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
Prefill latency: 0.29443581961095333 sec
Decode latency: 3.8834938686341047 sec
Compilation time: 4.21 seconds
Compilation time: 4.18 seconds
Prefill latency: 0.2971099838614464 sec
Decode latency: 3.884158931672573 sec
Prefill latency: 0.29718141816556454 sec
Decode latency: 3.883034825325012 sec
Prefill latency: 0.29547314159572124 sec
Decode latency: 3.882977982982993 sec
Prefill latency: 0.29736063070595264 sec
Decode latency: 3.8827947564423084 sec
Prefill latency: 0.29702194035053253 sec
Decode latency: 3.883503697812557 sec
Time for inference 1: 4.18 sec total, 1959.13 tokens/sec
Decode latency: 3.88 sec
Prefill latency: 0.30 sec
Bandwidth achieved: 15732.81 GB/s
FLOPS achieved: 47.20 TF/s

Prefill latency: 0.29691411182284355 sec
Decode latency: 3.8831632770597935 sec
Time for inference 2: 4.18 sec total, 1959.29 tokens/sec
Decode latency: 3.88 sec
Prefill latency: 0.30 sec
Bandwidth achieved: 15734.17 GB/s
FLOPS achieved: 47.20 TF/s

Prefill latency: 0.2976789101958275 sec
Decode latency: 3.882245909422636 sec
Time for inference 3: 4.18 sec total, 1959.39 tokens/sec
Decode latency: 3.88 sec
Prefill latency: 0.30 sec
Bandwidth achieved: 15734.92 GB/s
FLOPS achieved: 47.20 TF/s

Prefill latency: 0.2981389332562685 sec
Decode latency: 3.883185837417841 sec
Time for inference 4: 4.18 sec total, 1958.64 tokens/sec
Decode latency: 3.88 sec
Prefill latency: 0.30 sec
Bandwidth achieved: 15728.90 GB/s
FLOPS achieved: 47.19 TF/s

Prefill latency: 0.2995873000472784 sec
Decode latency: 3.8833541441708803 sec
Time for inference 5: 4.18 sec total, 1957.88 tokens/sec
Decode latency: 3.88 sec
Prefill latency: 0.30 sec
Bandwidth achieved: 15722.81 GB/s
FLOPS achieved: 47.17 TF/s

Prefill latency: 0.2983695790171623 sec
Decode latency: 3.8848004955798388 sec
Time for inference 6: 4.18 sec total, 1957.83 tokens/sec
Decode latency: 3.88 sec
Prefill latency: 0.30 sec
Bandwidth achieved: 15722.44 GB/s
FLOPS achieved: 47.17 TF/s

Prefill latency: 0.29694439470767975 sec
Decode latency: 3.8837423492223024 sec
Time for inference 7: 4.18 sec total, 1959.07 tokens/sec
Decode latency: 3.88 sec
Prefill latency: 0.30 sec
Bandwidth achieved: 15732.33 GB/s
FLOPS achieved: 47.20 TF/s

Prefill latency: 0.2961143981665373 sec
Decode latency: 3.8821870051324368 sec
Time for inference 8: 4.18 sec total, 1960.16 tokens/sec
Decode latency: 3.88 sec
Prefill latency: 0.30 sec
Bandwidth achieved: 15741.16 GB/s
FLOPS achieved: 47.22 TF/s

Prefill latency: 0.29638128913939 sec
Decode latency: 3.884055020287633 sec
Time for inference 9: 4.18 sec total, 1959.20 tokens/sec
Decode latency: 3.88 sec
Prefill latency: 0.30 sec
Bandwidth achieved: 15733.42 GB/s
FLOPS achieved: 47.20 TF/s

Prefill latency: 0.2971797212958336 sec
Decode latency: 3.8825154285877943 sec
Time for inference 10: 4.18 sec total, 1959.55 tokens/sec
Decode latency: 3.88 sec
Prefill latency: 0.30 sec
Bandwidth achieved: 15736.18 GB/s
FLOPS achieved: 47.21 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 3.8833 sec
Average prefill latency: 0.2974 sec
Average tokens/sec: 1959.01
Memory used: 31.18 GB
Done. we are killing the process
[rank0]:[W1114 02:30:50.136614755 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
