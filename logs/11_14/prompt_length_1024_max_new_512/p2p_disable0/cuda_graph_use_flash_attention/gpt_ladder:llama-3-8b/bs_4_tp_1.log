flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTLadder(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x LadderTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
        (wo): Linear(in_features=4096, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=28672, bias=False)
        (w2): Linear(in_features=14336, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 0.25 seconds
CUDA_GRAPH are activate
Prefill latency: 0.1308322288095951 sec
Decode latency: 4.744186758995056 sec
Compilation time: 4.89 seconds
Prefill latency: 0.13179565593600273 sec
Decode latency: 4.744650213047862 sec
Prefill latency: 0.13191779516637325 sec
Decode latency: 4.744452901184559 sec
Prefill latency: 0.1312804389744997 sec
Decode latency: 4.744569832459092 sec
Prefill latency: 0.13136195205152035 sec
Decode latency: 4.745448196306825 sec
Prefill latency: 0.1313883513212204 sec
Decode latency: 4.745288601145148 sec
Time for inference 1: 4.88 sec total, 419.87 tokens/sec
Decode latency: 4.75 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6302.14 GB/s
FLOPS achieved: 18.91 TF/s

Prefill latency: 0.13063033111393452 sec
Decode latency: 4.7445844281464815 sec
Time for inference 2: 4.88 sec total, 419.99 tokens/sec
Decode latency: 4.74 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6304.00 GB/s
FLOPS achieved: 18.91 TF/s

Prefill latency: 0.13030287250876427 sec
Decode latency: 4.744304280728102 sec
Time for inference 3: 4.88 sec total, 420.05 tokens/sec
Decode latency: 4.74 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6304.93 GB/s
FLOPS achieved: 18.91 TF/s

Prefill latency: 0.13097428157925606 sec
Decode latency: 4.744417294859886 sec
Time for inference 4: 4.88 sec total, 419.98 tokens/sec
Decode latency: 4.74 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6303.86 GB/s
FLOPS achieved: 18.91 TF/s

Prefill latency: 0.1308104172348976 sec
Decode latency: 4.744109587743878 sec
Time for inference 5: 4.88 sec total, 420.03 tokens/sec
Decode latency: 4.74 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6304.60 GB/s
FLOPS achieved: 18.91 TF/s

Prefill latency: 0.13216745853424072 sec
Decode latency: 4.744517693296075 sec
Time for inference 6: 4.88 sec total, 419.88 tokens/sec
Decode latency: 4.74 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6302.29 GB/s
FLOPS achieved: 18.91 TF/s

Prefill latency: 0.13171043060719967 sec
Decode latency: 4.744757708162069 sec
Time for inference 7: 4.88 sec total, 419.89 tokens/sec
Decode latency: 4.74 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6302.47 GB/s
FLOPS achieved: 18.91 TF/s

Prefill latency: 0.13052504882216454 sec
Decode latency: 4.744844974949956 sec
Time for inference 8: 4.88 sec total, 419.98 tokens/sec
Decode latency: 4.74 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6303.87 GB/s
FLOPS achieved: 18.91 TF/s

Prefill latency: 0.13103469647467136 sec
Decode latency: 4.744958279654384 sec
Time for inference 9: 4.88 sec total, 419.93 tokens/sec
Decode latency: 4.74 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6303.13 GB/s
FLOPS achieved: 18.91 TF/s

Prefill latency: 0.130610890686512 sec
Decode latency: 4.745009953156114 sec
Time for inference 10: 4.88 sec total, 419.96 tokens/sec
Decode latency: 4.75 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6303.48 GB/s
FLOPS achieved: 18.91 TF/s

==========
Batch Size: 4
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 4.7447 sec
Average prefill latency: 0.1310 sec
Average tokens/sec: 419.96
Memory used: 21.15 GB
Done. we are killing the process
[rank0]:[W1114 02:24:43.369413978 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
