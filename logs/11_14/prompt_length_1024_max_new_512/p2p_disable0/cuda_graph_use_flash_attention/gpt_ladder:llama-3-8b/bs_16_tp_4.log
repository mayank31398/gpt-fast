W1114 02:30:53.646000 2332865 site-packages/torch/distributed/run.py:793] 
W1114 02:30:53.646000 2332865 site-packages/torch/distributed/run.py:793] *****************************************
W1114 02:30:53.646000 2332865 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1114 02:30:53.646000 2332865 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTLadder(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x LadderTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=1536, bias=False)
        (wo): Linear(in_features=1024, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 0.61 seconds
CUDA_GRAPH are activate
[rank1]:[W1114 02:31:07.132861127 CUDAGraph.cpp:133] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank2]:[W1114 02:31:07.137258646 CUDAGraph.cpp:133] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
Prefill latency: 0.18513874523341656 sec
Decode latency: 3.0417758338153362 sec
Compilation time: 3.24 seconds
Compilation time: 3.23 seconds
Compilation time: 3.23 seconds
Compilation time: 3.23 seconds
Prefill latency: 0.18586253561079502 sec
Decode latency: 3.041046306490898 sec
Prefill latency: 0.1857690829783678 sec
Decode latency: 3.0384074207395315 sec
Prefill latency: 0.18581482209265232 sec
Decode latency: 3.0388769414275885 sec
Prefill latency: 0.18505862914025784 sec
Decode latency: 3.0387940276414156 sec
Prefill latency: 0.18596716038882732 sec
Decode latency: 3.03837981633842 sec
Time for inference 1: 3.23 sec total, 2539.73 tokens/sec
Decode latency: 3.04 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 11532.57 GB/s
FLOPS achieved: 34.60 TF/s

Prefill latency: 0.18615041859447956 sec
Decode latency: 3.0397808998823166 sec
Time for inference 2: 3.23 sec total, 2538.53 tokens/sec
Decode latency: 3.04 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 11527.14 GB/s
FLOPS achieved: 34.58 TF/s

Prefill latency: 0.1864529773592949 sec
Decode latency: 3.0379704535007477 sec
Time for inference 3: 3.23 sec total, 2539.68 tokens/sec
Decode latency: 3.04 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 11532.34 GB/s
FLOPS achieved: 34.60 TF/s

Prefill latency: 0.18498182110488415 sec
Decode latency: 3.0388394370675087 sec
Time for inference 4: 3.22 sec total, 2540.26 tokens/sec
Decode latency: 3.04 sec
Prefill latency: 0.18 sec
Bandwidth achieved: 11534.97 GB/s
FLOPS achieved: 34.60 TF/s

Prefill latency: 0.18616507574915886 sec
Decode latency: 3.038402609527111 sec
Time for inference 5: 3.23 sec total, 2539.68 tokens/sec
Decode latency: 3.04 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 11532.36 GB/s
FLOPS achieved: 34.60 TF/s

Prefill latency: 0.18578097596764565 sec
Decode latency: 3.0381074622273445 sec
Time for inference 6: 3.22 sec total, 2540.19 tokens/sec
Decode latency: 3.04 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 11534.65 GB/s
FLOPS achieved: 34.60 TF/s

Prefill latency: 0.1856214925646782 sec
Decode latency: 3.0379557199776173 sec
Time for inference 7: 3.22 sec total, 2540.52 tokens/sec
Decode latency: 3.04 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 11536.14 GB/s
FLOPS achieved: 34.61 TF/s

Prefill latency: 0.1859034113585949 sec
Decode latency: 3.0393958762288094 sec
Time for inference 8: 3.23 sec total, 2539.17 tokens/sec
Decode latency: 3.04 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 11530.01 GB/s
FLOPS achieved: 34.59 TF/s

Prefill latency: 0.1859271414577961 sec
Decode latency: 3.038772288709879 sec
Time for inference 9: 3.23 sec total, 2539.67 tokens/sec
Decode latency: 3.04 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 11532.29 GB/s
FLOPS achieved: 34.60 TF/s

Prefill latency: 0.18747189082205296 sec
Decode latency: 3.0390775203704834 sec
Time for inference 10: 3.23 sec total, 2537.92 tokens/sec
Decode latency: 3.04 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 11524.34 GB/s
FLOPS achieved: 34.57 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 3.0387 sec
Average prefill latency: 0.1860 sec
Average tokens/sec: 2539.53
Memory used: 23.58 GB
Done. we are killing the process
[rank0]:[W1114 02:31:56.770387907 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
