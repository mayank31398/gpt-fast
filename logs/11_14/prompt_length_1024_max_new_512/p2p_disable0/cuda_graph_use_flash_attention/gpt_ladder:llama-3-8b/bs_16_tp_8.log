W1114 02:31:59.809000 2333793 site-packages/torch/distributed/run.py:793] 
W1114 02:31:59.809000 2333793 site-packages/torch/distributed/run.py:793] *****************************************
W1114 02:31:59.809000 2333793 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1114 02:31:59.809000 2333793 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTLadder(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x LadderTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=768, bias=False)
        (wo): Linear(in_features=512, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=3584, bias=False)
        (w2): Linear(in_features=1792, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.41 seconds
CUDA_GRAPH are activate
Prefill latency: 0.12190527096390724 sec
Decode latency: 2.6091651897877455 sec
Compilation time: 2.74 seconds
Compilation time: 2.73 seconds
Compilation time: 2.76 seconds
Compilation time: 2.73 seconds
Compilation time: 2.73 seconds
Compilation time: 2.75 seconds
Compilation time: 2.76 seconds
Compilation time: 2.76 seconds
Prefill latency: 0.11671716161072254 sec
Decode latency: 2.608456574380398 sec
Prefill latency: 0.11606283485889435 sec
Decode latency: 2.6087661683559418 sec
Prefill latency: 0.11599615775048733 sec
Decode latency: 2.60903624817729 sec
Prefill latency: 0.1165343914180994 sec
Decode latency: 2.6085621286183596 sec
Prefill latency: 0.11568325571715832 sec
Decode latency: 2.610227447003126 sec
Time for inference 1: 2.73 sec total, 3004.37 tokens/sec
Decode latency: 2.61 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 8400.32 GB/s
FLOPS achieved: 25.20 TF/s

Prefill latency: 0.11623434349894524 sec
Decode latency: 2.6103656236082315 sec
Time for inference 2: 2.73 sec total, 3003.53 tokens/sec
Decode latency: 2.61 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 8397.97 GB/s
FLOPS achieved: 25.19 TF/s

Prefill latency: 0.11557642929255962 sec
Decode latency: 2.6097596921026707 sec
Time for inference 3: 2.73 sec total, 3004.78 tokens/sec
Decode latency: 2.61 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 8401.47 GB/s
FLOPS achieved: 25.20 TF/s

Prefill latency: 0.116586584597826 sec
Decode latency: 2.6088695600628853 sec
Time for inference 4: 2.73 sec total, 3004.74 tokens/sec
Decode latency: 2.61 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 8401.36 GB/s
FLOPS achieved: 25.20 TF/s

Prefill latency: 0.11539381556212902 sec
Decode latency: 2.6096089612692595 sec
Time for inference 5: 2.73 sec total, 3005.22 tokens/sec
Decode latency: 2.61 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 8402.71 GB/s
FLOPS achieved: 25.21 TF/s

Prefill latency: 0.11633083038032055 sec
Decode latency: 2.609458504244685 sec
Time for inference 6: 2.73 sec total, 3004.32 tokens/sec
Decode latency: 2.61 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 8400.20 GB/s
FLOPS achieved: 25.20 TF/s

Prefill latency: 0.11623303219676018 sec
Decode latency: 2.609055893495679 sec
Time for inference 7: 2.73 sec total, 3004.93 tokens/sec
Decode latency: 2.61 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 8401.89 GB/s
FLOPS achieved: 25.21 TF/s

Prefill latency: 0.11669212952256203 sec
Decode latency: 2.6083013359457254 sec
Time for inference 8: 2.73 sec total, 3005.14 tokens/sec
Decode latency: 2.61 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 8402.47 GB/s
FLOPS achieved: 25.21 TF/s

Prefill latency: 0.11565240286290646 sec
Decode latency: 2.6108383778482676 sec
Time for inference 9: 2.73 sec total, 3003.54 tokens/sec
Decode latency: 2.61 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 8398.01 GB/s
FLOPS achieved: 25.19 TF/s

Prefill latency: 0.11628491245210171 sec
Decode latency: 2.609794622287154 sec
Time for inference 10: 2.73 sec total, 3003.92 tokens/sec
Decode latency: 2.61 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 8399.06 GB/s
FLOPS achieved: 25.20 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 2.6096 sec
Average prefill latency: 0.1161 sec
Average tokens/sec: 3004.45
Memory used: 19.23 GB
Done. we are killing the process
[rank0]:[W1114 02:33:03.737070882 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
