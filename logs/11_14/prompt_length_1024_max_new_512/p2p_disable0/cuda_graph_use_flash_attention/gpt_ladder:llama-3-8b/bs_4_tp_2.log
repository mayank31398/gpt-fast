W1114 02:24:46.773000 2327788 site-packages/torch/distributed/run.py:793] 
W1114 02:24:46.773000 2327788 site-packages/torch/distributed/run.py:793] *****************************************
W1114 02:24:46.773000 2327788 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1114 02:24:46.773000 2327788 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTLadder(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x LadderTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=3072, bias=False)
        (wo): Linear(in_features=2048, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=14336, bias=False)
        (w2): Linear(in_features=7168, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.25 seconds
CUDA_GRAPH are activate
Prefill latency: 0.08031255193054676 sec
Decode latency: 3.4996305909007788 sec
Compilation time: 3.58 seconds
Compilation time: 3.58 seconds
Prefill latency: 0.07696552388370037 sec
Decode latency: 3.497244995087385 sec
Prefill latency: 0.07706941105425358 sec
Decode latency: 3.496625015512109 sec
Prefill latency: 0.07677236013114452 sec
Decode latency: 3.4967460595071316 sec
Prefill latency: 0.07700018212199211 sec
Decode latency: 3.4975696317851543 sec
Prefill latency: 0.07708446495234966 sec
Decode latency: 3.4969759956002235 sec
Time for inference 1: 3.57 sec total, 572.89 tokens/sec
Decode latency: 3.50 sec
Prefill latency: 0.08 sec
Bandwidth achieved: 4600.61 GB/s
FLOPS achieved: 13.80 TF/s

Prefill latency: 0.0769442766904831 sec
Decode latency: 3.496497478336096 sec
Time for inference 2: 3.57 sec total, 572.99 tokens/sec
Decode latency: 3.50 sec
Prefill latency: 0.08 sec
Bandwidth achieved: 4601.41 GB/s
FLOPS achieved: 13.80 TF/s

Prefill latency: 0.07723228819668293 sec
Decode latency: 3.4971615374088287 sec
Time for inference 3: 3.58 sec total, 572.78 tokens/sec
Decode latency: 3.50 sec
Prefill latency: 0.08 sec
Bandwidth achieved: 4599.77 GB/s
FLOPS achieved: 13.80 TF/s

Prefill latency: 0.0771580133587122 sec
Decode latency: 3.497913448140025 sec
Time for inference 4: 3.58 sec total, 572.69 tokens/sec
Decode latency: 3.50 sec
Prefill latency: 0.08 sec
Bandwidth achieved: 4598.97 GB/s
FLOPS achieved: 13.80 TF/s

Prefill latency: 0.07714392803609371 sec
Decode latency: 3.4983042404055595 sec
Time for inference 5: 3.58 sec total, 572.63 tokens/sec
Decode latency: 3.50 sec
Prefill latency: 0.08 sec
Bandwidth achieved: 4598.50 GB/s
FLOPS achieved: 13.80 TF/s

Prefill latency: 0.07706999219954014 sec
Decode latency: 3.4980234280228615 sec
Time for inference 6: 3.58 sec total, 572.70 tokens/sec
Decode latency: 3.50 sec
Prefill latency: 0.08 sec
Bandwidth achieved: 4599.07 GB/s
FLOPS achieved: 13.80 TF/s

Prefill latency: 0.07726557739078999 sec
Decode latency: 3.496612809598446 sec
Time for inference 7: 3.57 sec total, 572.90 tokens/sec
Decode latency: 3.50 sec
Prefill latency: 0.08 sec
Bandwidth achieved: 4600.66 GB/s
FLOPS achieved: 13.80 TF/s

Prefill latency: 0.07711556740105152 sec
Decode latency: 3.4972844310104847 sec
Time for inference 8: 3.58 sec total, 572.81 tokens/sec
Decode latency: 3.50 sec
Prefill latency: 0.08 sec
Bandwidth achieved: 4599.98 GB/s
FLOPS achieved: 13.80 TF/s

Prefill latency: 0.07722426392138004 sec
Decode latency: 3.4975615832954645 sec
Time for inference 9: 3.58 sec total, 572.77 tokens/sec
Decode latency: 3.50 sec
Prefill latency: 0.08 sec
Bandwidth achieved: 4599.61 GB/s
FLOPS achieved: 13.80 TF/s

Prefill latency: 0.07709363661706448 sec
Decode latency: 3.497374389320612 sec
Time for inference 10: 3.58 sec total, 572.82 tokens/sec
Decode latency: 3.50 sec
Prefill latency: 0.08 sec
Bandwidth achieved: 4600.03 GB/s
FLOPS achieved: 13.80 TF/s

==========
Batch Size: 4
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 3.4974 sec
Average prefill latency: 0.0771 sec
Average tokens/sec: 572.80
Memory used: 14.74 GB
Done. we are killing the process
[rank0]:[W1114 02:25:50.401054392 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
