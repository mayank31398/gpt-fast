W1114 03:59:14.024000 2313820 site-packages/torch/distributed/run.py:793] 
W1114 03:59:14.024000 2313820 site-packages/torch/distributed/run.py:793] *****************************************
W1114 03:59:14.024000 2313820 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1114 03:59:14.024000 2313820 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTLadder(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x LadderTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=1280, bias=False)
        (wo): Linear(in_features=1024, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 0.57 seconds
CUDA_GRAPH are activate
Prefill latency: 0.726566618308425 sec
Decode latency: 8.685763824731112 sec
Compilation time: 9.41 seconds
Compilation time: 9.41 seconds
Compilation time: 9.39 seconds
Compilation time: 9.42 seconds
Compilation time: 9.42 seconds
Compilation time: 9.41 secondsCompilation time: 9.40 seconds

Compilation time: 9.41 seconds
Prefill latency: 0.7073188051581383 sec
Decode latency: 8.684611492790282 sec
Prefill latency: 0.7060696929693222 sec
Decode latency: 8.685825992375612 sec
Prefill latency: 0.709029364399612 sec
Decode latency: 8.684890072792768 sec
Prefill latency: 0.7050846833735704 sec
Decode latency: 8.68489870429039 sec
Prefill latency: 0.7056999439373612 sec
Decode latency: 8.685149791650474 sec
Time for inference 1: 9.39 sec total, 872.22 tokens/sec
Decode latency: 8.69 sec
Prefill latency: 0.71 sec
Bandwidth achieved: 16761.17 GB/s
FLOPS achieved: 50.28 TF/s

Prefill latency: 0.7034415137022734 sec
Decode latency: 8.68506103567779 sec
Time for inference 2: 9.39 sec total, 872.43 tokens/sec
Decode latency: 8.69 sec
Prefill latency: 0.70 sec
Bandwidth achieved: 16765.24 GB/s
FLOPS achieved: 50.30 TF/s

Prefill latency: 0.7069428255781531 sec
Decode latency: 8.685023334808648 sec
Time for inference 3: 9.39 sec total, 872.12 tokens/sec
Decode latency: 8.69 sec
Prefill latency: 0.71 sec
Bandwidth achieved: 16759.33 GB/s
FLOPS achieved: 50.28 TF/s

Prefill latency: 0.7055478030815721 sec
Decode latency: 8.684432140551507 sec
Time for inference 4: 9.39 sec total, 872.31 tokens/sec
Decode latency: 8.68 sec
Prefill latency: 0.71 sec
Bandwidth achieved: 16762.91 GB/s
FLOPS achieved: 50.29 TF/s

Prefill latency: 0.7091816570609808 sec
Decode latency: 8.684065380133688 sec
Time for inference 5: 9.39 sec total, 871.99 tokens/sec
Decode latency: 8.68 sec
Prefill latency: 0.71 sec
Bandwidth achieved: 16756.89 GB/s
FLOPS achieved: 50.27 TF/s

Prefill latency: 0.7080409741029143 sec
Decode latency: 8.681490417569876 sec
Time for inference 6: 9.39 sec total, 872.34 tokens/sec
Decode latency: 8.68 sec
Prefill latency: 0.71 sec
Bandwidth achieved: 16763.59 GB/s
FLOPS achieved: 50.29 TF/s

Prefill latency: 0.706908630207181 sec
Decode latency: 8.684116660617292 sec
Time for inference 7: 9.39 sec total, 872.21 tokens/sec
Decode latency: 8.68 sec
Prefill latency: 0.71 sec
Bandwidth achieved: 16761.01 GB/s
FLOPS achieved: 50.28 TF/s

Prefill latency: 0.7096758363768458 sec
Decode latency: 8.681909304112196 sec
Time for inference 8: 9.39 sec total, 872.16 tokens/sec
Decode latency: 8.68 sec
Prefill latency: 0.71 sec
Bandwidth achieved: 16759.99 GB/s
FLOPS achieved: 50.28 TF/s

Prefill latency: 0.7066535139456391 sec
Decode latency: 8.6841613445431 sec
Time for inference 9: 9.39 sec total, 872.22 tokens/sec
Decode latency: 8.68 sec
Prefill latency: 0.71 sec
Bandwidth achieved: 16761.24 GB/s
FLOPS achieved: 50.28 TF/s

Prefill latency: 0.7081630239263177 sec
Decode latency: 8.681988479569554 sec
Time for inference 10: 9.39 sec total, 872.28 tokens/sec
Decode latency: 8.68 sec
Prefill latency: 0.71 sec
Bandwidth achieved: 16762.44 GB/s
FLOPS achieved: 50.29 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 8.6837 sec
Average prefill latency: 0.7070 sec
Average tokens/sec: 872.23
Memory used: 71.38 GB
Done. we are killing the process
[rank0]:[W1114 04:01:52.860368521 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
