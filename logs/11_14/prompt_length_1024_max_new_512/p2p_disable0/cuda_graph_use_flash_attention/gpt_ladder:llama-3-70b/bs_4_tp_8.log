W1114 03:56:18.478000 2312782 site-packages/torch/distributed/run.py:793] 
W1114 03:56:18.478000 2312782 site-packages/torch/distributed/run.py:793] *****************************************
W1114 03:56:18.478000 2312782 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1114 03:56:18.478000 2312782 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTLadder(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x LadderTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=1280, bias=False)
        (wo): Linear(in_features=1024, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 0.65 seconds
CUDA_GRAPH are activate
Prefill latency: 0.2144503490999341 sec
Decode latency: 7.928396039642394 sec
Compilation time: 8.13 seconds
Compilation time: 8.12 seconds
Compilation time: 8.14 seconds
Compilation time: 8.12 seconds
Compilation time: 8.13 secondsCompilation time: 8.13 seconds

Compilation time: 8.13 seconds
Compilation time: 8.14 seconds
Prefill latency: 0.18660877458751202 sec
Decode latency: 7.931111952289939 sec
Prefill latency: 0.18637508619576693 sec
Decode latency: 7.93074806407094 sec
Prefill latency: 0.18718218989670277 sec
Decode latency: 7.929315749555826 sec
Prefill latency: 0.18687897734344006 sec
Decode latency: 7.928675029426813 sec
Prefill latency: 0.18623101990669966 sec
Decode latency: 7.928616208024323 sec
Time for inference 1: 8.12 sec total, 252.33 tokens/sec
Decode latency: 7.93 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 4848.97 GB/s
FLOPS achieved: 14.55 TF/s

Prefill latency: 0.1867960812523961 sec
Decode latency: 7.928485405631363 sec
Time for inference 2: 8.12 sec total, 252.32 tokens/sec
Decode latency: 7.93 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 4848.71 GB/s
FLOPS achieved: 14.55 TF/s

Prefill latency: 0.187483923509717 sec
Decode latency: 7.930155685171485 sec
Time for inference 3: 8.12 sec total, 252.25 tokens/sec
Decode latency: 7.93 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 4847.44 GB/s
FLOPS achieved: 14.54 TF/s

Prefill latency: 0.18752566073089838 sec
Decode latency: 7.929740418680012 sec
Time for inference 4: 8.12 sec total, 252.26 tokens/sec
Decode latency: 7.93 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 4847.65 GB/s
FLOPS achieved: 14.54 TF/s

Prefill latency: 0.18739053886383772 sec
Decode latency: 7.9293287098407745 sec
Time for inference 5: 8.12 sec total, 252.28 tokens/sec
Decode latency: 7.93 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 4847.94 GB/s
FLOPS achieved: 14.54 TF/s

Prefill latency: 0.18735854234546423 sec
Decode latency: 7.929603482596576 sec
Time for inference 6: 8.12 sec total, 252.27 tokens/sec
Decode latency: 7.93 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 4847.76 GB/s
FLOPS achieved: 14.54 TF/s

Prefill latency: 0.18718624580651522 sec
Decode latency: 7.931093154475093 sec
Time for inference 7: 8.12 sec total, 252.23 tokens/sec
Decode latency: 7.93 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 4847.02 GB/s
FLOPS achieved: 14.54 TF/s

Prefill latency: 0.18742104433476925 sec
Decode latency: 7.930983564816415 sec
Time for inference 8: 8.12 sec total, 252.23 tokens/sec
Decode latency: 7.93 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 4846.94 GB/s
FLOPS achieved: 14.54 TF/s

Prefill latency: 0.18745343387126923 sec
Decode latency: 7.930795422755182 sec
Time for inference 9: 8.12 sec total, 252.23 tokens/sec
Decode latency: 7.93 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 4847.07 GB/s
FLOPS achieved: 14.54 TF/s

Prefill latency: 0.18794426042586565 sec
Decode latency: 7.929340528324246 sec
Time for inference 10: 8.12 sec total, 252.26 tokens/sec
Decode latency: 7.93 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 4847.61 GB/s
FLOPS achieved: 14.54 TF/s

==========
Batch Size: 4
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 7.9298 sec
Average prefill latency: 0.1873 sec
Average tokens/sec: 252.27
Memory used: 33.92 GB
Done. we are killing the process
[rank0]:[W1114 03:58:36.905981236 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
