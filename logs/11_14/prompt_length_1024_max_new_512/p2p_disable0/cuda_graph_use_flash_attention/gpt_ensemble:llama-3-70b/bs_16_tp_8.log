W1114 04:19:52.177000 2321463 site-packages/torch/distributed/run.py:793] 
W1114 04:19:52.177000 2321463 site-packages/torch/distributed/run.py:793] *****************************************
W1114 04:19:52.177000 2321463 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1114 04:19:52.177000 2321463 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=1280, bias=False)
        (wo): Linear(in_features=1024, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 1.26 seconds
CUDA_GRAPH are activate
Prefill latency: 0.8115455899387598 sec
Decode latency: 9.214632070623338 sec
Compilation time: 10.06 seconds
Compilation time: 9.99 seconds
Compilation time: 10.01 seconds
Compilation time: 10.00 seconds
Compilation time: 10.03 seconds
Compilation time: 9.98 seconds
Compilation time: 9.96 secondsCompilation time: 10.00 seconds

Prefill latency: 0.7428518738597631 sec
Decode latency: 9.21398360375315 sec
Prefill latency: 0.7433104868978262 sec
Decode latency: 9.213633578270674 sec
Prefill latency: 0.7452897392213345 sec
Decode latency: 9.21376518998295 sec
Prefill latency: 0.7450844757258892 sec
Decode latency: 9.212372720241547 sec
Prefill latency: 0.7456139530986547 sec
Decode latency: 9.213184121064842 sec
Time for inference 1: 9.96 sec total, 822.48 tokens/sec
Decode latency: 9.21 sec
Prefill latency: 0.75 sec
Bandwidth achieved: 15805.45 GB/s
FLOPS achieved: 47.42 TF/s

Prefill latency: 0.7423925194889307 sec
Decode latency: 9.21475141402334 sec
Time for inference 2: 9.96 sec total, 822.62 tokens/sec
Decode latency: 9.21 sec
Prefill latency: 0.74 sec
Bandwidth achieved: 15807.99 GB/s
FLOPS achieved: 47.42 TF/s

Prefill latency: 0.7449138024821877 sec
Decode latency: 9.214920966885984 sec
Time for inference 3: 9.96 sec total, 822.39 tokens/sec
Decode latency: 9.21 sec
Prefill latency: 0.74 sec
Bandwidth achieved: 15803.72 GB/s
FLOPS achieved: 47.41 TF/s

Prefill latency: 0.7460530754178762 sec
Decode latency: 9.212853175587952 sec
Time for inference 4: 9.96 sec total, 822.48 tokens/sec
Decode latency: 9.21 sec
Prefill latency: 0.75 sec
Bandwidth achieved: 15805.34 GB/s
FLOPS achieved: 47.42 TF/s

Prefill latency: 0.7445998517796397 sec
Decode latency: 9.212063703685999 sec
Time for inference 5: 9.96 sec total, 822.66 tokens/sec
Decode latency: 9.21 sec
Prefill latency: 0.74 sec
Bandwidth achieved: 15808.77 GB/s
FLOPS achieved: 47.43 TF/s

Prefill latency: 0.7441166779026389 sec
Decode latency: 9.213704732246697 sec
Time for inference 6: 9.96 sec total, 822.55 tokens/sec
Decode latency: 9.21 sec
Prefill latency: 0.74 sec
Bandwidth achieved: 15806.79 GB/s
FLOPS achieved: 47.42 TF/s

Prefill latency: 0.7438945760950446 sec
Decode latency: 9.21447711251676 sec
Time for inference 7: 9.96 sec total, 822.53 tokens/sec
Decode latency: 9.21 sec
Prefill latency: 0.74 sec
Bandwidth achieved: 15806.28 GB/s
FLOPS achieved: 47.42 TF/s

Prefill latency: 0.7463440569117665 sec
Decode latency: 9.21497749350965 sec
Time for inference 8: 9.96 sec total, 822.28 tokens/sec
Decode latency: 9.21 sec
Prefill latency: 0.75 sec
Bandwidth achieved: 15801.49 GB/s
FLOPS achieved: 47.40 TF/s

Prefill latency: 0.746431184001267 sec
Decode latency: 9.212779563851655 sec
Time for inference 9: 9.96 sec total, 822.45 tokens/sec
Decode latency: 9.21 sec
Prefill latency: 0.75 sec
Bandwidth achieved: 15804.76 GB/s
FLOPS achieved: 47.41 TF/s

Prefill latency: 0.7459625015035272 sec
Decode latency: 9.211846069432795 sec
Time for inference 10: 9.96 sec total, 822.57 tokens/sec
Decode latency: 9.21 sec
Prefill latency: 0.75 sec
Bandwidth achieved: 15807.16 GB/s
FLOPS achieved: 47.42 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 9.2136 sec
Average prefill latency: 0.7450 sec
Average tokens/sec: 822.50
Memory used: 49.88 GB
Done. we are killing the process
[rank0]:[W1114 04:22:39.464055297 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
