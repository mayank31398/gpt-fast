W1114 04:43:32.246000 2330542 site-packages/torch/distributed/run.py:793] 
W1114 04:43:32.246000 2330542 site-packages/torch/distributed/run.py:793] *****************************************
W1114 04:43:32.246000 2330542 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1114 04:43:32.246000 2330542 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=2560, bias=False)
        (wo): Linear(in_features=2048, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=14336, bias=False)
        (w2): Linear(in_features=7168, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 1.52 seconds
CUDA_GRAPH are activate
Prefill latency: 1.184208381921053 sec
Compilation time: 12.01 seconds
Decode latency: 10.84076983295381 sec
Compilation time: 12.03 seconds
Compilation time: 12.63 seconds
Compilation time: 12.89 seconds
Prefill latency: 1.185559967532754 sec
Decode latency: 11.2963074631989 sec
Prefill latency: 1.1813049903139472 sec
Decode latency: 10.879928534850478 sec
Prefill latency: 1.1831527398899198 sec
Decode latency: 11.296842019073665 sec
Prefill latency: 1.1836268827319145 sec
Decode latency: 11.29660468827933 sec
Prefill latency: 1.1853568311780691 sec
Decode latency: 10.935844745486975 sec
Time for inference 1: 12.12 sec total, 675.78 tokens/sec
Decode latency: 10.94 sec
Prefill latency: 1.19 sec
Bandwidth achieved: 24550.83 GB/s
FLOPS achieved: 73.65 TF/s

Prefill latency: 1.1856364384293556 sec
Decode latency: 11.296239134855568 sec
Time for inference 2: 12.48 sec total, 656.25 tokens/sec
Decode latency: 11.30 sec
Prefill latency: 1.19 sec
Bandwidth achieved: 23841.36 GB/s
FLOPS achieved: 71.52 TF/s

Prefill latency: 1.1797637799754739 sec
Decode latency: 11.297623819671571 sec
Time for inference 3: 12.48 sec total, 656.49 tokens/sec
Decode latency: 11.30 sec
Prefill latency: 1.18 sec
Bandwidth achieved: 23849.83 GB/s
FLOPS achieved: 71.55 TF/s

Prefill latency: 1.1850548423826694 sec
Decode latency: 11.294486224651337 sec
Time for inference 4: 12.48 sec total, 656.37 tokens/sec
Decode latency: 11.29 sec
Prefill latency: 1.19 sec
Bandwidth achieved: 23845.71 GB/s
FLOPS achieved: 71.54 TF/s

Prefill latency: 1.1806585369631648 sec
Decode latency: 11.291593678295612 sec
Time for inference 5: 12.47 sec total, 656.76 tokens/sec
Decode latency: 11.29 sec
Prefill latency: 1.18 sec
Bandwidth achieved: 23859.90 GB/s
FLOPS achieved: 71.58 TF/s

Prefill latency: 1.182443143799901 sec
Decode latency: 11.210095286369324 sec
Time for inference 6: 12.39 sec total, 660.98 tokens/sec
Decode latency: 11.21 sec
Prefill latency: 1.18 sec
Bandwidth achieved: 24013.18 GB/s
FLOPS achieved: 72.04 TF/s

Prefill latency: 1.1828749580308795 sec
Decode latency: 11.243809869512916 sec
Time for inference 7: 12.43 sec total, 659.17 tokens/sec
Decode latency: 11.24 sec
Prefill latency: 1.18 sec
Bandwidth achieved: 23947.32 GB/s
FLOPS achieved: 71.84 TF/s

Prefill latency: 1.1831211913377047 sec
Decode latency: 10.900599100627005 sec
Time for inference 8: 12.08 sec total, 677.88 tokens/sec
Decode latency: 10.90 sec
Prefill latency: 1.18 sec
Bandwidth achieved: 24626.99 GB/s
FLOPS achieved: 73.88 TF/s

Prefill latency: 1.1846497729420662 sec
Decode latency: 11.296792266890407 sec
Time for inference 9: 12.48 sec total, 656.28 tokens/sec
Decode latency: 11.30 sec
Prefill latency: 1.18 sec
Bandwidth achieved: 23842.20 GB/s
FLOPS achieved: 71.53 TF/s

Prefill latency: 1.182765713892877 sec
[rank3]:[W1114 04:46:46.809067513 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Decode latency: 10.982413798570633 sec
Time for inference 10: 12.17 sec total, 673.34 tokens/sec
Decode latency: 10.98 sec
Prefill latency: 1.18 sec
Bandwidth achieved: 24462.20 GB/s
FLOPS achieved: 73.39 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 11.1749 sec
Average prefill latency: 1.1832 sec
Average tokens/sec: 662.93
Memory used: 47.45 GB
[rank0]:[W1114 04:46:48.672746513 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank2]:[W1114 04:46:50.695131842 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank1]:[W1114 04:46:55.464372992 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Done. we are killing the process
[rank0]:[W1114 04:46:58.001175808 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
