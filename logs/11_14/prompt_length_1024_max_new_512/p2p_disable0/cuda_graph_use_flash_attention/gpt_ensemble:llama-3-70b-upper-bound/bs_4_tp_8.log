W1114 04:40:58.278000 2328968 site-packages/torch/distributed/run.py:793] 
W1114 04:40:58.278000 2328968 site-packages/torch/distributed/run.py:793] *****************************************
W1114 04:40:58.278000 2328968 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1114 04:40:58.278000 2328968 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=1280, bias=False)
        (wo): Linear(in_features=1024, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 1.32 seconds
CUDA_GRAPH are activate
Prefill latency: 0.1614472158253193 sec
Decode latency: 7.290402187034488 sec
Compilation time: 7.45 seconds
Compilation time: 7.31 seconds
Prefill latency: 0.161118452437222 sec
Compilation time: 7.46 seconds
Compilation time: 7.69 seconds
Compilation time: 7.69 seconds
Compilation time: 7.67 seconds
Compilation time: 7.69 seconds
Compilation time: 7.50 seconds
Decode latency: 7.103776082396507 sec
Prefill latency: 0.16231656167656183 sec
Decode latency: 7.523862883448601 sec
Prefill latency: 0.16174369305372238 sec
Decode latency: 7.52545812446624 sec
Prefill latency: 0.16300634015351534 sec
Decode latency: 7.524446596391499 sec
Prefill latency: 0.16181594505906105 sec
Decode latency: 7.135878814384341 sec
Time for inference 1: 7.30 sec total, 280.60 tokens/sec
Decode latency: 7.14 sec
Prefill latency: 0.16 sec
Bandwidth achieved: 5392.12 GB/s
FLOPS achieved: 16.18 TF/s

Prefill latency: 0.16246674302965403 sec
Decode latency: 7.062791798263788 sec
Time for inference 2: 7.23 sec total, 283.40 tokens/sec
Decode latency: 7.06 sec
Prefill latency: 0.16 sec
Bandwidth achieved: 5446.09 GB/s
FLOPS achieved: 16.34 TF/s

Prefill latency: 0.16312360484153032 sec
Decode latency: 7.526733989827335 sec
Time for inference 3: 7.69 sec total, 266.29 tokens/sec
Decode latency: 7.53 sec
Prefill latency: 0.16 sec
Bandwidth achieved: 5117.17 GB/s
FLOPS achieved: 15.35 TF/s

Prefill latency: 0.1623012199997902 sec
Decode latency: 7.146071373485029 sec
Time for inference 4: 7.31 sec total, 280.18 tokens/sec
Decode latency: 7.15 sec
Prefill latency: 0.16 sec
Bandwidth achieved: 5384.23 GB/s
FLOPS achieved: 16.15 TF/s

Prefill latency: 0.16055265441536903 sec
Decode latency: 7.34339714422822 sec
Time for inference 5: 7.51 sec total, 272.88 tokens/sec
Decode latency: 7.34 sec
Prefill latency: 0.16 sec
Bandwidth achieved: 5243.89 GB/s
FLOPS achieved: 15.73 TF/s

Prefill latency: 0.16246924456208944 sec
Decode latency: 7.147584147751331 sec
Time for inference 6: 7.31 sec total, 280.11 tokens/sec
Decode latency: 7.15 sec
Prefill latency: 0.16 sec
Bandwidth achieved: 5382.86 GB/s
FLOPS achieved: 16.15 TF/s

Prefill latency: 0.16213263291865587 sec
Decode latency: 7.394292532466352 sec
Time for inference 7: 7.56 sec total, 270.99 tokens/sec
Decode latency: 7.39 sec
Prefill latency: 0.16 sec
Bandwidth achieved: 5207.49 GB/s
FLOPS achieved: 15.62 TF/s

Prefill latency: 0.16222057305276394 sec
Decode latency: 7.526954353787005 sec
Time for inference 8: 7.69 sec total, 266.31 tokens/sec
Decode latency: 7.53 sec
Prefill latency: 0.16 sec
Bandwidth achieved: 5117.64 GB/s
FLOPS achieved: 15.35 TF/s

Prefill latency: 0.16221917048096657 sec
Decode latency: 7.489910769276321 sec
Time for inference 9: 7.65 sec total, 267.60 tokens/sec
Decode latency: 7.49 sec
Prefill latency: 0.16 sec
Bandwidth achieved: 5142.38 GB/s
FLOPS achieved: 15.43 TF/s

Prefill latency: 0.16258912160992622 sec
[rank2]:[W1114 04:42:58.379552414 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank7]:[W1114 04:42:59.651738523 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Decode latency: 7.0534457406029105 sec
Time for inference 10: 7.22 sec total, 283.76 tokens/sec
Decode latency: 7.05 sec
Prefill latency: 0.16 sec
Bandwidth achieved: 5452.99 GB/s
FLOPS achieved: 16.36 TF/s

==========
Batch Size: 4
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 7.2827 sec
Average prefill latency: 0.1622 sec
Average tokens/sec: 275.21
Memory used: 23.24 GB
[rank0]:[W1114 04:42:59.015838235 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank3]:[W1114 04:43:00.236312727 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank6]:[W1114 04:43:01.825747078 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank1]:[W1114 04:43:01.872303070 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank4]:[W1114 04:43:01.961692188 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank5]:[W1114 04:43:02.827866464 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Done. we are killing the process
[rank0]:[W1114 04:43:08.953025482 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
