W1114 04:24:06.347000 2324587 site-packages/torch/distributed/run.py:793] 
W1114 04:24:06.347000 2324587 site-packages/torch/distributed/run.py:793] *****************************************
W1114 04:24:06.347000 2324587 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1114 04:24:06.347000 2324587 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=5120, bias=False)
        (wo): Linear(in_features=4096, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=28672, bias=False)
        (w2): Linear(in_features=14336, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 1.32 seconds
CUDA_GRAPH are activate
Prefill latency: 0.14812458772212267 sec
Decode latency: 15.84489372652024 sec
Compilation time: 15.99 seconds
Prefill latency: 0.1499344678595662 sec
Compilation time: 16.01 seconds
Decode latency: 15.753556574694812 sec
Prefill latency: 0.14881619717925787 sec
Decode latency: 15.844486783258617 sec
Prefill latency: 0.14867501519620419 sec
Decode latency: 15.560901457443833 sec
Prefill latency: 0.14803937450051308 sec
Decode latency: 15.844521059654653 sec
Prefill latency: 0.1490734862163663 sec
Decode latency: 15.838265080936253 sec
Time for inference 1: 15.99 sec total, 32.02 tokens/sec
Decode latency: 15.84 sec
Prefill latency: 0.15 sec
Bandwidth achieved: 2259.39 GB/s
FLOPS achieved: 6.78 TF/s

Prefill latency: 0.1484227953478694 sec
Decode latency: 15.596470953896642 sec
Time for inference 2: 15.75 sec total, 32.52 tokens/sec
Decode latency: 15.60 sec
Prefill latency: 0.15 sec
Bandwidth achieved: 2294.18 GB/s
FLOPS achieved: 6.88 TF/s

Prefill latency: 0.14847413077950478 sec
Decode latency: 15.846805813722312 sec
Time for inference 3: 16.00 sec total, 32.01 tokens/sec
Decode latency: 15.85 sec
Prefill latency: 0.15 sec
Bandwidth achieved: 2258.26 GB/s
FLOPS achieved: 6.77 TF/s

Prefill latency: 0.14747802540659904 sec
Decode latency: 15.846196485683322 sec
Time for inference 4: 15.99 sec total, 32.01 tokens/sec
Decode latency: 15.85 sec
Prefill latency: 0.15 sec
Bandwidth achieved: 2258.49 GB/s
FLOPS achieved: 6.78 TF/s

Prefill latency: 0.14902911521494389 sec
Decode latency: 15.846203931607306 sec
Time for inference 5: 16.00 sec total, 32.01 tokens/sec
Decode latency: 15.85 sec
Prefill latency: 0.15 sec
Bandwidth achieved: 2258.27 GB/s
FLOPS achieved: 6.77 TF/s

Prefill latency: 0.14877921901643276 sec
Decode latency: 15.426878743804991 sec
Time for inference 6: 15.58 sec total, 32.87 tokens/sec
Decode latency: 15.43 sec
Prefill latency: 0.15 sec
Bandwidth achieved: 2319.10 GB/s
FLOPS achieved: 6.96 TF/s

Prefill latency: 0.14996725507080555 sec
Decode latency: 15.837803702801466 sec
Time for inference 7: 15.99 sec total, 32.02 tokens/sec
Decode latency: 15.84 sec
Prefill latency: 0.15 sec
Bandwidth achieved: 2259.33 GB/s
FLOPS achieved: 6.78 TF/s

Prefill latency: 0.1481649549677968 sec
Decode latency: 15.769973299466074 sec
Time for inference 8: 15.92 sec total, 32.16 tokens/sec
Decode latency: 15.77 sec
Prefill latency: 0.15 sec
Bandwidth achieved: 2269.21 GB/s
FLOPS achieved: 6.81 TF/s

Prefill latency: 0.1487206667661667 sec
Decode latency: 15.675162505358458 sec
Time for inference 9: 15.83 sec total, 32.35 tokens/sec
Decode latency: 15.68 sec
Prefill latency: 0.15 sec
Bandwidth achieved: 2282.72 GB/s
FLOPS achieved: 6.85 TF/s

Prefill latency: 0.1480491477996111 sec
Decode latency: 15.477079755626619 sec
Time for inference 10: 15.63 sec total, 32.77 tokens/sec
Decode latency: 15.48 sec
Prefill latency: 0.15 sec
Bandwidth achieved: 2311.75 GB/s
FLOPS achieved: 6.94 TF/s

==========
Batch Size: 1
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 15.7161 sec
Average prefill latency: 0.1486 sec
Average tokens/sec: 32.27
Memory used: 73.55 GB
[rank0]:[W1114 04:28:11.821823396 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank1]:[W1114 04:28:13.729069271 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Done. we are killing the process
[rank0]:[W1114 04:28:14.873923293 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
