flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
        (wo): Linear(in_features=4096, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=28672, bias=False)
        (w2): Linear(in_features=14336, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 0.96 seconds
CUDA_GRAPH are activate
Prefill latency: 2.296366775408387 sec
Decode latency: 6.910117369145155 sec
Compilation time: 9.21 seconds
Prefill latency: 2.2956233732402325 sec
Decode latency: 6.910998802632093 sec
Prefill latency: 2.2989899292588234 sec
Decode latency: 6.910004664212465 sec
Prefill latency: 2.297095013782382 sec
Decode latency: 6.900767680257559 sec
Prefill latency: 2.2935965079814196 sec
Decode latency: 6.909591099247336 sec
Prefill latency: 2.2975750975310802 sec
Decode latency: 6.908895846456289 sec
Time for inference 1: 9.21 sec total, 3558.78 tokens/sec
Decode latency: 6.91 sec
Prefill latency: 2.30 sec
Bandwidth achieved: 53416.82 GB/s
FLOPS achieved: 160.25 TF/s

Prefill latency: 2.2986961137503386 sec
Decode latency: 6.798098172992468 sec
Time for inference 2: 9.10 sec total, 3601.69 tokens/sec
Decode latency: 6.80 sec
Prefill latency: 2.30 sec
Bandwidth achieved: 54060.82 GB/s
FLOPS achieved: 162.18 TF/s

Prefill latency: 2.2979509588330984 sec
Decode latency: 6.892554948106408 sec
Time for inference 3: 9.19 sec total, 3564.99 tokens/sec
Decode latency: 6.89 sec
Prefill latency: 2.30 sec
Bandwidth achieved: 53510.00 GB/s
FLOPS achieved: 160.53 TF/s

Prefill latency: 2.296812955290079 sec
Decode latency: 6.7373196836560965 sec
Time for inference 4: 9.04 sec total, 3626.71 tokens/sec
Decode latency: 6.74 sec
Prefill latency: 2.30 sec
Bandwidth achieved: 54436.30 GB/s
FLOPS achieved: 163.31 TF/s

Prefill latency: 2.292468935251236 sec
Decode latency: 6.908881679177284 sec
Time for inference 5: 9.20 sec total, 3560.83 tokens/sec
Decode latency: 6.91 sec
Prefill latency: 2.29 sec
Bandwidth achieved: 53447.55 GB/s
FLOPS achieved: 160.34 TF/s

Prefill latency: 2.2944976165890694 sec
Decode latency: 6.910429608076811 sec
Time for inference 6: 9.21 sec total, 3559.43 tokens/sec
Decode latency: 6.91 sec
Prefill latency: 2.29 sec
Bandwidth achieved: 53426.46 GB/s
FLOPS achieved: 160.28 TF/s

Prefill latency: 2.2962836511433125 sec
Decode latency: 6.902317686006427 sec
Time for inference 7: 9.20 sec total, 3561.84 tokens/sec
Decode latency: 6.90 sec
Prefill latency: 2.30 sec
Bandwidth achieved: 53462.72 GB/s
FLOPS achieved: 160.39 TF/s

Prefill latency: 2.294030264019966 sec
Decode latency: 6.766017768532038 sec
Time for inference 8: 9.06 sec total, 3616.35 tokens/sec
Decode latency: 6.77 sec
Prefill latency: 2.29 sec
Bandwidth achieved: 54280.91 GB/s
FLOPS achieved: 162.84 TF/s

Prefill latency: 2.295583864673972 sec
Decode latency: 6.902152815833688 sec
Time for inference 9: 9.20 sec total, 3562.24 tokens/sec
Decode latency: 6.90 sec
Prefill latency: 2.30 sec
Bandwidth achieved: 53468.68 GB/s
FLOPS achieved: 160.41 TF/s

Prefill latency: 2.2963189259171486 sec
Decode latency: 6.9109244253486395 sec
Time for inference 10: 9.21 sec total, 3558.51 tokens/sec
Decode latency: 6.91 sec
Prefill latency: 2.30 sec
Bandwidth achieved: 53412.68 GB/s
FLOPS achieved: 160.24 TF/s

==========
Batch Size: 64
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 6.8638 sec
Average prefill latency: 2.2960 sec
Average tokens/sec: 3577.14
Memory used: 59.68 GB
[rank0]:[W1114 03:15:01.269058404 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Done. we are killing the process
[rank0]:[W1114 03:15:02.378126036 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
