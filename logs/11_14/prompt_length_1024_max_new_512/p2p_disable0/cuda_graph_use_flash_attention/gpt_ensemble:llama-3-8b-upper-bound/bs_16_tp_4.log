W1114 03:10:20.556000 2462725 site-packages/torch/distributed/run.py:793] 
W1114 03:10:20.556000 2462725 site-packages/torch/distributed/run.py:793] *****************************************
W1114 03:10:20.556000 2462725 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1114 03:10:20.556000 2462725 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=1536, bias=False)
        (wo): Linear(in_features=1024, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.00 seconds
CUDA_GRAPH are activate
Prefill latency: 0.16738273575901985 sec
Decode latency: 2.7048891689628363 sec
Compilation time: 2.87 seconds
Compilation time: 2.92 seconds
Prefill latency: 0.1668537985533476 sec
Compilation time: 3.05 seconds
Compilation time: 2.92 seconds
Decode latency: 2.8674392830580473 sec
Prefill latency: 0.1672748103737831 sec
Decode latency: 2.7407615892589092 sec
Prefill latency: 0.1677878051996231 sec
Decode latency: 2.891915298998356 sec
Prefill latency: 0.1681296732276678 sec
Decode latency: 2.8293791729956865 sec
Prefill latency: 0.16768483258783817 sec
Decode latency: 2.864319095388055 sec
Time for inference 1: 3.03 sec total, 2701.12 tokens/sec
Decode latency: 2.86 sec
Prefill latency: 0.17 sec
Bandwidth achieved: 12265.43 GB/s
FLOPS achieved: 36.80 TF/s

Prefill latency: 0.1668911688029766 sec
Decode latency: 2.827977016568184 sec
Time for inference 2: 3.00 sec total, 2734.57 tokens/sec
Decode latency: 2.83 sec
Prefill latency: 0.17 sec
Bandwidth achieved: 12417.32 GB/s
FLOPS achieved: 37.25 TF/s

Prefill latency: 0.16858340613543987 sec
Decode latency: 2.8942090924829245 sec
Time for inference 3: 3.06 sec total, 2674.00 tokens/sec
Decode latency: 2.89 sec
Prefill latency: 0.17 sec
Bandwidth achieved: 12142.27 GB/s
FLOPS achieved: 36.43 TF/s

Prefill latency: 0.1657711397856474 sec
Decode latency: 2.8954057190567255 sec
Time for inference 4: 3.06 sec total, 2675.30 tokens/sec
Decode latency: 2.90 sec
Prefill latency: 0.17 sec
Bandwidth achieved: 12148.17 GB/s
FLOPS achieved: 36.44 TF/s

Prefill latency: 0.16765873320400715 sec
Decode latency: 2.8271177783608437 sec
Time for inference 5: 3.00 sec total, 2734.66 tokens/sec
Decode latency: 2.83 sec
Prefill latency: 0.17 sec
Bandwidth achieved: 12417.73 GB/s
FLOPS achieved: 37.25 TF/s

Prefill latency: 0.16804541647434235 sec
Decode latency: 2.8561362717300653 sec
Time for inference 6: 3.03 sec total, 2708.08 tokens/sec
Decode latency: 2.86 sec
Prefill latency: 0.17 sec
Bandwidth achieved: 12297.04 GB/s
FLOPS achieved: 36.89 TF/s

Prefill latency: 0.16761930473148823 sec
Decode latency: 2.8954883813858032 sec
Time for inference 7: 3.06 sec total, 2673.68 tokens/sec
Decode latency: 2.90 sec
Prefill latency: 0.17 sec
Bandwidth achieved: 12140.84 GB/s
FLOPS achieved: 36.42 TF/s

Prefill latency: 0.16780903562903404 sec
Decode latency: 2.7197337113320827 sec
Time for inference 8: 2.89 sec total, 2836.26 tokens/sec
Decode latency: 2.72 sec
Prefill latency: 0.17 sec
Bandwidth achieved: 12879.07 GB/s
FLOPS achieved: 38.64 TF/s

Prefill latency: 0.16822785884141922 sec
Decode latency: 2.8656026981770992 sec
Time for inference 9: 3.03 sec total, 2699.56 tokens/sec
Decode latency: 2.87 sec
Prefill latency: 0.17 sec
Bandwidth achieved: 12258.35 GB/s
FLOPS achieved: 36.78 TF/s

Prefill latency: 0.16758781112730503 sec
[rank1]:[W1114 03:11:14.467697205 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Decode latency: 2.8961618039757013 sec
Time for inference 10: 3.06 sec total, 2673.14 tokens/sec
Decode latency: 2.90 sec
Prefill latency: 0.17 sec
Bandwidth achieved: 12138.37 GB/s
FLOPS achieved: 36.42 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 2.8542 sec
Average prefill latency: 0.1676 sec
Average tokens/sec: 2711.04
Memory used: 12.10 GB
[rank0]:[W1114 03:11:15.724411980 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank3]:[W1114 03:11:15.219435902 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank2]:[W1114 03:11:15.402704869 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Done. we are killing the process
[rank0]:[W1114 03:11:19.586232894 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
