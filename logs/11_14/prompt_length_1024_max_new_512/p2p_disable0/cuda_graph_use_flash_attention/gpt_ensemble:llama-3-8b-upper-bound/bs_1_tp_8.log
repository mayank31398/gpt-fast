W1114 03:02:01.960000 2431662 site-packages/torch/distributed/run.py:793] 
W1114 03:02:01.960000 2431662 site-packages/torch/distributed/run.py:793] *****************************************
W1114 03:02:01.960000 2431662 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1114 03:02:01.960000 2431662 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=768, bias=False)
        (wo): Linear(in_features=512, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=3584, bias=False)
        (w2): Linear(in_features=1792, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.44 seconds
CUDA_GRAPH are activate
Prefill latency: 0.010204121470451355 sec
Decode latency: 2.0449882056564093 sec
Compilation time: 2.06 seconds
Prefill latency: 0.010181374847888947 sec
Compilation time: 1.92 seconds
Compilation time: 2.06 seconds
Compilation time: 2.06 seconds
Compilation time: 2.10 seconds
Compilation time: 2.10 seconds
Compilation time: 2.10 seconds
Compilation time: 1.86 seconds
Decode latency: 2.0457101613283157 sec
Prefill latency: 0.01018972136080265 sec
Decode latency: 2.0453637838363647 sec
Prefill latency: 0.010199835523962975 sec
Decode latency: 2.0451328195631504 sec
Prefill latency: 0.010205121710896492 sec
Decode latency: 2.045449199154973 sec
Prefill latency: 0.010175280272960663 sec
Decode latency: 2.024419551715255 sec
Time for inference 1: 2.04 sec total, 251.54 tokens/sec
Decode latency: 2.02 sec
Prefill latency: 0.01 sec
Bandwidth achieved: 703.33 GB/s
FLOPS achieved: 2.11 TF/s

Prefill latency: 0.01018424890935421 sec
Decode latency: 1.979301631450653 sec
Time for inference 2: 1.99 sec total, 257.24 tokens/sec
Decode latency: 1.98 sec
Prefill latency: 0.01 sec
Bandwidth achieved: 719.26 GB/s
FLOPS achieved: 2.16 TF/s

Prefill latency: 0.010172689333558083 sec
Decode latency: 2.0456402879208326 sec
Time for inference 3: 2.06 sec total, 248.96 tokens/sec
Decode latency: 2.05 sec
Prefill latency: 0.01 sec
Bandwidth achieved: 696.11 GB/s
FLOPS achieved: 2.09 TF/s

Prefill latency: 0.010182544589042664 sec
Decode latency: 2.044891944155097 sec
Time for inference 4: 2.06 sec total, 249.05 tokens/sec
Decode latency: 2.04 sec
Prefill latency: 0.01 sec
Bandwidth achieved: 696.35 GB/s
FLOPS achieved: 2.09 TF/s

Prefill latency: 0.01016869768500328 sec
Decode latency: 2.045614829286933 sec
Time for inference 5: 2.06 sec total, 248.95 tokens/sec
Decode latency: 2.05 sec
Prefill latency: 0.01 sec
Bandwidth achieved: 696.07 GB/s
FLOPS achieved: 2.09 TF/s

Prefill latency: 0.01019277423620224 sec
Decode latency: 1.934322314336896 sec
Time for inference 6: 1.95 sec total, 263.19 tokens/sec
Decode latency: 1.93 sec
Prefill latency: 0.01 sec
Bandwidth achieved: 735.89 GB/s
FLOPS achieved: 2.21 TF/s

Prefill latency: 0.010179165750741959 sec
Decode latency: 2.0451707746833563 sec
Time for inference 7: 2.06 sec total, 249.01 tokens/sec
Decode latency: 2.05 sec
Prefill latency: 0.01 sec
Bandwidth achieved: 696.24 GB/s
FLOPS achieved: 2.09 TF/s

Prefill latency: 0.010204911231994629 sec
Decode latency: 2.0450954269617796 sec
Time for inference 8: 2.06 sec total, 249.02 tokens/sec
Decode latency: 2.05 sec
Prefill latency: 0.01 sec
Bandwidth achieved: 696.28 GB/s
FLOPS achieved: 2.09 TF/s

Prefill latency: 0.010205470025539398 sec
Decode latency: 2.038348149508238 sec
Time for inference 9: 2.05 sec total, 249.84 tokens/sec
Decode latency: 2.04 sec
Prefill latency: 0.01 sec
Bandwidth achieved: 698.55 GB/s
FLOPS achieved: 2.10 TF/s

Prefill latency: 0.010174514725804329 sec
Decode latency: 2.044946625828743 sec
Time for inference 10: 2.06 sec total, 249.03 tokens/sec
Decode latency: 2.04 sec
Prefill latency: 0.01 sec
Bandwidth achieved: 696.31 GB/s
FLOPS achieved: 2.09 TF/s

==========
Batch Size: 1
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 2.0248 sec
Average prefill latency: 0.0102 sec
Average tokens/sec: 251.58
Memory used: 4.33 GB
[rank0]:[W1114 03:02:43.952411609 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank4]:[W1114 03:02:43.985704204 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank1]:[W1114 03:02:43.480811983 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank3]:[W1114 03:02:44.779766906 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank2]:[W1114 03:02:44.939148014 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank5]:[W1114 03:02:44.073353223 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank6]:[W1114 03:02:44.466180283 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank7]:[W1114 03:02:44.541396837 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Done. we are killing the process
[rank0]:[W1114 03:02:55.217429965 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
