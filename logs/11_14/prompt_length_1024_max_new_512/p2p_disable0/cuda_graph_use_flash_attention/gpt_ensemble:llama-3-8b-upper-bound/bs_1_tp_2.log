W1114 03:00:08.385000 2423698 site-packages/torch/distributed/run.py:793] 
W1114 03:00:08.385000 2423698 site-packages/torch/distributed/run.py:793] *****************************************
W1114 03:00:08.385000 2423698 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1114 03:00:08.385000 2423698 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=3072, bias=False)
        (wo): Linear(in_features=2048, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=14336, bias=False)
        (w2): Linear(in_features=7168, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.93 seconds
CUDA_GRAPH are activate
Prefill latency: 0.019489463418722153 sec
Decode latency: 3.0031968764960766 sec
Compilation time: 3.02 seconds
Prefill latency: 0.019375009462237358 sec
Compilation time: 3.12 seconds
Decode latency: 3.1923569701611996 sec
Prefill latency: 0.019416097551584244 sec
Decode latency: 3.02373568713665 sec
Prefill latency: 0.0193802323192358 sec
Decode latency: 3.158545669168234 sec
Prefill latency: 0.01936470903456211 sec
Decode latency: 3.1886269357055426 sec
Prefill latency: 0.019378913566470146 sec
Decode latency: 3.182709975168109 sec
Time for inference 1: 3.20 sec total, 159.85 tokens/sec
Decode latency: 3.18 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 1283.71 GB/s
FLOPS achieved: 3.85 TF/s

Prefill latency: 0.019334422424435616 sec
Decode latency: 3.0128893088549376 sec
Time for inference 2: 3.03 sec total, 168.81 tokens/sec
Decode latency: 3.01 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 1355.63 GB/s
FLOPS achieved: 4.07 TF/s

Prefill latency: 0.01935967430472374 sec
Decode latency: 3.1649656537920237 sec
Time for inference 3: 3.19 sec total, 160.75 tokens/sec
Decode latency: 3.16 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 1290.87 GB/s
FLOPS achieved: 3.87 TF/s

Prefill latency: 0.019330816343426704 sec
Decode latency: 3.0602619014680386 sec
Time for inference 4: 3.08 sec total, 166.21 tokens/sec
Decode latency: 3.06 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 1334.79 GB/s
FLOPS achieved: 4.00 TF/s

Prefill latency: 0.019305666908621788 sec
Decode latency: 3.192846857011318 sec
Time for inference 5: 3.21 sec total, 159.36 tokens/sec
Decode latency: 3.19 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 1279.71 GB/s
FLOPS achieved: 3.84 TF/s

Prefill latency: 0.019331511110067368 sec
Decode latency: 3.1927233282476664 sec
Time for inference 6: 3.21 sec total, 159.36 tokens/sec
Decode latency: 3.19 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 1279.73 GB/s
FLOPS achieved: 3.84 TF/s

Prefill latency: 0.019335025921463966 sec
Decode latency: 3.186866929754615 sec
Time for inference 7: 3.21 sec total, 159.65 tokens/sec
Decode latency: 3.19 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 1282.09 GB/s
FLOPS achieved: 3.85 TF/s

Prefill latency: 0.019343623891472816 sec
Decode latency: 3.117389624938369 sec
Time for inference 8: 3.14 sec total, 163.18 tokens/sec
Decode latency: 3.12 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 1310.44 GB/s
FLOPS achieved: 3.93 TF/s

Prefill latency: 0.01931317336857319 sec
Decode latency: 3.1912800427526236 sec
Time for inference 9: 3.21 sec total, 159.43 tokens/sec
Decode latency: 3.19 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 1280.30 GB/s
FLOPS achieved: 3.84 TF/s

Prefill latency: 0.019344447180628777 sec
[rank1]:[W1114 03:01:03.881637877 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Decode latency: 3.1928166281431913 sec
Time for inference 10: 3.21 sec total, 159.35 tokens/sec
Decode latency: 3.19 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 1279.70 GB/s
FLOPS achieved: 3.84 TF/s

==========
Batch Size: 1
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 3.1495 sec
Average prefill latency: 0.0193 sec
Average tokens/sec: 161.60
Memory used: 9.69 GB
[rank0]:[W1114 03:01:04.083575025 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Done. we are killing the process
[rank0]:[W1114 03:01:06.062406162 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
