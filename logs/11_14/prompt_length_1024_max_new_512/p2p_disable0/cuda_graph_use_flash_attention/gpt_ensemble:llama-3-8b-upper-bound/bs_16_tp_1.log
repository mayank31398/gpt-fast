flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
        (wo): Linear(in_features=4096, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=28672, bias=False)
        (w2): Linear(in_features=14336, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 0.91 seconds
CUDA_GRAPH are activate
Prefill latency: 0.5249604843556881 sec
Decode latency: 5.196051927283406 sec
Compilation time: 5.72 seconds
Prefill latency: 0.5255158264189959 sec
Decode latency: 5.19644277356565 sec
Prefill latency: 0.5263285282999277 sec
Decode latency: 5.1955083310604095 sec
Prefill latency: 0.5245109405368567 sec
Decode latency: 5.195228694006801 sec
Prefill latency: 0.5266887489706278 sec
Decode latency: 5.195634223520756 sec
Prefill latency: 0.5262556504458189 sec
Decode latency: 5.195170225575566 sec
Time for inference 1: 5.72 sec total, 1431.58 tokens/sec
Decode latency: 5.20 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 21487.75 GB/s
FLOPS achieved: 64.46 TF/s

Prefill latency: 0.5259650349617004 sec
Decode latency: 5.19563084281981 sec
Time for inference 2: 5.72 sec total, 1431.55 tokens/sec
Decode latency: 5.20 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 21487.28 GB/s
FLOPS achieved: 64.46 TF/s

Prefill latency: 0.5268877260386944 sec
Decode latency: 5.195658724755049 sec
Time for inference 3: 5.72 sec total, 1431.31 tokens/sec
Decode latency: 5.20 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 21483.73 GB/s
FLOPS achieved: 64.45 TF/s

Prefill latency: 0.5252689626067877 sec
Decode latency: 5.194185147061944 sec
Time for inference 4: 5.72 sec total, 1432.07 tokens/sec
Decode latency: 5.19 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 21495.21 GB/s
FLOPS achieved: 64.49 TF/s

Prefill latency: 0.5263630077242851 sec
Decode latency: 5.194561894983053 sec
Time for inference 5: 5.72 sec total, 1431.69 tokens/sec
Decode latency: 5.19 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 21489.49 GB/s
FLOPS achieved: 64.47 TF/s

Prefill latency: 0.5259798336774111 sec
Decode latency: 5.193932414054871 sec
Time for inference 6: 5.72 sec total, 1431.98 tokens/sec
Decode latency: 5.19 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 21493.86 GB/s
FLOPS achieved: 64.48 TF/s

Prefill latency: 0.5247885677963495 sec
Decode latency: 5.195663422346115 sec
Time for inference 7: 5.72 sec total, 1431.83 tokens/sec
Decode latency: 5.20 sec
Prefill latency: 0.52 sec
Bandwidth achieved: 21491.60 GB/s
FLOPS achieved: 64.47 TF/s

Prefill latency: 0.5261667817831039 sec
Decode latency: 5.1937414687126875 sec
Time for inference 8: 5.72 sec total, 1431.97 tokens/sec
Decode latency: 5.19 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 21493.64 GB/s
FLOPS achieved: 64.48 TF/s

Prefill latency: 0.5260487776249647 sec
Decode latency: 5.195930948480964 sec
Time for inference 9: 5.72 sec total, 1431.44 tokens/sec
Decode latency: 5.20 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 21485.76 GB/s
FLOPS achieved: 64.46 TF/s

Prefill latency: 0.5270543619990349 sec
Decode latency: 5.194670727476478 sec
Time for inference 10: 5.72 sec total, 1431.50 tokens/sec
Decode latency: 5.19 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 21486.56 GB/s
FLOPS achieved: 64.46 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 5.1949 sec
Average prefill latency: 0.5261 sec
Average tokens/sec: 1431.69
Memory used: 27.08 GB
[rank0]:[W1114 03:09:02.587898649 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Done. we are killing the process
[rank0]:[W1114 03:09:03.400463585 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
