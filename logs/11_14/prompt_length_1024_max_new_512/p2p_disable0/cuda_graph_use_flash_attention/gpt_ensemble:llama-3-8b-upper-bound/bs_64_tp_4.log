W1114 03:16:49.682000 2484744 site-packages/torch/distributed/run.py:793] 
W1114 03:16:49.682000 2484744 site-packages/torch/distributed/run.py:793] *****************************************
W1114 03:16:49.682000 2484744 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1114 03:16:49.682000 2484744 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=1536, bias=False)
        (wo): Linear(in_features=1024, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.68 seconds
CUDA_GRAPH are activate
Prefill latency: 0.6327441204339266 sec
Decode latency: 3.4831668455153704 sec
Compilation time: 4.12 seconds
Compilation time: 4.09 seconds
Compilation time: 4.10 seconds
Compilation time: 4.11 seconds
Prefill latency: 0.6349221263080835 sec
Decode latency: 3.482077818363905 sec
Prefill latency: 0.6345996763557196 sec
Decode latency: 3.482910443097353 sec
Prefill latency: 0.6348165646195412 sec
Decode latency: 3.4833222664892673 sec
Prefill latency: 0.6336985416710377 sec
Decode latency: 3.482875932008028 sec
Prefill latency: 0.6351299826055765 sec
Decode latency: 3.4831247236579657 sec
Time for inference 1: 4.12 sec total, 7955.02 tokens/sec
Decode latency: 3.48 sec
Prefill latency: 0.64 sec
Bandwidth achieved: 36122.69 GB/s
FLOPS achieved: 108.37 TF/s

Prefill latency: 0.6352503784000874 sec
Decode latency: 3.4825400337576866 sec
Time for inference 2: 4.12 sec total, 7955.88 tokens/sec
Decode latency: 3.48 sec
Prefill latency: 0.64 sec
Bandwidth achieved: 36126.57 GB/s
FLOPS achieved: 108.38 TF/s

Prefill latency: 0.6340655628591776 sec
Decode latency: 3.4818385764956474 sec
Time for inference 3: 4.12 sec total, 7959.72 tokens/sec
Decode latency: 3.48 sec
Prefill latency: 0.63 sec
Bandwidth achieved: 36144.02 GB/s
FLOPS achieved: 108.43 TF/s

Prefill latency: 0.6310334000736475 sec
Decode latency: 3.3853437192738056 sec
Time for inference 4: 4.02 sec total, 8156.91 tokens/sec
Decode latency: 3.39 sec
Prefill latency: 0.63 sec
Bandwidth achieved: 37039.46 GB/s
FLOPS achieved: 111.12 TF/s

Prefill latency: 0.635032195597887 sec
Decode latency: 3.482119418680668 sec
Time for inference 5: 4.12 sec total, 7957.48 tokens/sec
Decode latency: 3.48 sec
Prefill latency: 0.64 sec
Bandwidth achieved: 36133.87 GB/s
FLOPS achieved: 108.40 TF/s

Prefill latency: 0.6348281465470791 sec
Decode latency: 3.48164770193398 sec
Time for inference 6: 4.12 sec total, 7958.79 tokens/sec
Decode latency: 3.48 sec
Prefill latency: 0.63 sec
Bandwidth achieved: 36139.82 GB/s
FLOPS achieved: 108.42 TF/s

Prefill latency: 0.634455006569624 sec
Decode latency: 3.4825105238705873 sec
Time for inference 7: 4.12 sec total, 7957.85 tokens/sec
Decode latency: 3.48 sec
Prefill latency: 0.63 sec
Bandwidth achieved: 36135.52 GB/s
FLOPS achieved: 108.41 TF/s

Prefill latency: 0.6355432402342558 sec
Decode latency: 3.4822077825665474 sec
Time for inference 8: 4.12 sec total, 7956.35 tokens/sec
Decode latency: 3.48 sec
Prefill latency: 0.64 sec
Bandwidth achieved: 36128.72 GB/s
FLOPS achieved: 108.39 TF/s

Prefill latency: 0.6369380410760641 sec
Decode latency: 3.4825328923761845 sec
Time for inference 9: 4.12 sec total, 7952.76 tokens/sec
Decode latency: 3.48 sec
Prefill latency: 0.64 sec
Bandwidth achieved: 36112.42 GB/s
FLOPS achieved: 108.34 TF/s

Prefill latency: 0.6372295711189508 sec
[rank3]:[W1114 03:18:02.714152193 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank1]:[W1114 03:18:02.274842307 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Decode latency: 3.4831004794687033 sec
Time for inference 10: 4.12 sec total, 7951.18 tokens/sec
Decode latency: 3.48 sec
Prefill latency: 0.64 sec
Bandwidth achieved: 36105.23 GB/s
FLOPS achieved: 108.32 TF/s

==========
Batch Size: 64
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 3.4727 sec
Average prefill latency: 0.6350 sec
Average tokens/sec: 7976.19
Memory used: 31.14 GB
[rank0]:[W1114 03:18:02.355754727 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank2]:[W1114 03:18:02.475486408 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Done. we are killing the process
[rank0]:[W1114 03:18:07.853350369 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
