W1114 03:09:06.438000 2459051 site-packages/torch/distributed/run.py:793] 
W1114 03:09:06.438000 2459051 site-packages/torch/distributed/run.py:793] *****************************************
W1114 03:09:06.438000 2459051 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1114 03:09:06.438000 2459051 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=3072, bias=False)
        (wo): Linear(in_features=2048, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=14336, bias=False)
        (w2): Linear(in_features=7168, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.60 seconds
CUDA_GRAPH are activate
Prefill latency: 0.2777267675846815 sec
Decode latency: 3.5538414157927036 sec
Compilation time: 3.83 seconds
Compilation time: 3.89 seconds
Prefill latency: 0.27783037163317204 sec
Decode latency: 3.6967781372368336 sec
Prefill latency: 0.27762393839657307 sec
Decode latency: 3.5964116416871548 sec
Prefill latency: 0.2792475689202547 sec
Decode latency: 3.7064938563853502 sec
Prefill latency: 0.27919367514550686 sec
Decode latency: 3.5991069711744785 sec
Prefill latency: 0.27818547561764717 sec
Decode latency: 3.7197426576167345 sec
Time for inference 1: 4.00 sec total, 2048.60 tokens/sec
Decode latency: 3.72 sec
Prefill latency: 0.28 sec
Bandwidth achieved: 16451.31 GB/s
FLOPS achieved: 49.35 TF/s

Prefill latency: 0.2776887584477663 sec
Decode latency: 3.5408933721482754 sec
Time for inference 2: 3.82 sec total, 2144.83 tokens/sec
Decode latency: 3.54 sec
Prefill latency: 0.28 sec
Bandwidth achieved: 17224.14 GB/s
FLOPS achieved: 51.67 TF/s

Prefill latency: 0.27927379682660103 sec
Decode latency: 3.722705801948905 sec
Time for inference 3: 4.00 sec total, 2046.47 tokens/sec
Decode latency: 3.72 sec
Prefill latency: 0.28 sec
Bandwidth achieved: 16434.25 GB/s
FLOPS achieved: 49.30 TF/s

Prefill latency: 0.2784834895282984 sec
Decode latency: 3.6706285793334246 sec
Time for inference 4: 3.95 sec total, 2073.92 tokens/sec
Decode latency: 3.67 sec
Prefill latency: 0.28 sec
Bandwidth achieved: 16654.64 GB/s
FLOPS achieved: 49.96 TF/s

Prefill latency: 0.2772519588470459 sec
Decode latency: 3.7182520236819983 sec
Time for inference 5: 4.00 sec total, 2049.88 tokens/sec
Decode latency: 3.72 sec
Prefill latency: 0.28 sec
Bandwidth achieved: 16461.63 GB/s
FLOPS achieved: 49.38 TF/s

Prefill latency: 0.2761735040694475 sec
Decode latency: 3.542442850768566 sec
Time for inference 6: 3.82 sec total, 2144.71 tokens/sec
Decode latency: 3.54 sec
Prefill latency: 0.28 sec
Bandwidth achieved: 17223.15 GB/s
FLOPS achieved: 51.67 TF/s

Prefill latency: 0.27811865881085396 sec
Decode latency: 3.697403332218528 sec
Time for inference 7: 3.98 sec total, 2060.02 tokens/sec
Decode latency: 3.70 sec
Prefill latency: 0.28 sec
Bandwidth achieved: 16543.08 GB/s
FLOPS achieved: 49.63 TF/s

Prefill latency: 0.276589697226882 sec
Decode latency: 3.675167912617326 sec
Time for inference 8: 3.95 sec total, 2072.54 tokens/sec
Decode latency: 3.68 sec
Prefill latency: 0.28 sec
Bandwidth achieved: 16643.62 GB/s
FLOPS achieved: 49.93 TF/s

Prefill latency: 0.2796993609517813 sec
Decode latency: 3.701553128659725 sec
Time for inference 9: 3.98 sec total, 2057.22 tokens/sec
Decode latency: 3.70 sec
Prefill latency: 0.28 sec
Bandwidth achieved: 16520.53 GB/s
FLOPS achieved: 49.56 TF/s

Prefill latency: 0.27756323479115963 sec
Decode latency: 3.5697166249156 sec
Time for inference 10: 3.85 sec total, 2128.79 tokens/sec
Decode latency: 3.57 sec
Prefill latency: 0.28 sec
Bandwidth achieved: 17095.32 GB/s
FLOPS achieved: 51.29 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 3.6559 sec
Average prefill latency: 0.2779 sec
Average tokens/sec: 2082.70
Memory used: 16.91 GB
[rank0]:[W1114 03:10:13.537248080 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank1]:[W1114 03:10:14.400059684 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Done. we are killing the process
[rank0]:[W1114 03:10:16.564362656 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
