W1114 05:02:32.041000 2336043 site-packages/torch/distributed/run.py:793] 
W1114 05:02:32.041000 2336043 site-packages/torch/distributed/run.py:793] *****************************************
W1114 05:02:32.041000 2336043 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1114 05:02:32.041000 2336043 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTParallel(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x ParallelTransformerBlock(
      semi_compiled = False
      (attention): FuseAttentionMLP(
        (wqkv1): Linear(in_features=8192, out_features=16896, bias=False)
        (wo): Linear(in_features=2048, out_features=8192, bias=False)
        (w2): Linear(in_features=7168, out_features=8192, bias=False)
      )
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 1.44 seconds
CUDA_GRAPH are activate
Prefill latency: 0.09172278735786676 sec
Decode latency: 11.153703341260552 sec
Compilation time: 11.28 seconds
Compilation time: 11.27 seconds
Compilation time: 11.27 seconds
Compilation time: 11.25 seconds
Prefill latency: 0.09078149031847715 sec
Decode latency: 11.156348746269941 sec
Prefill latency: 0.09043967165052891 sec
Decode latency: 11.126770845614374 sec
Prefill latency: 0.0909779304638505 sec
Decode latency: 11.179801028221846 sec
Prefill latency: 0.0906839007511735 sec
Decode latency: 11.148121600039303 sec
Prefill latency: 0.09101837035268545 sec
Decode latency: 11.163812652230263 sec
Time for inference 1: 11.26 sec total, 45.49 tokens/sec
Decode latency: 11.16 sec
Prefill latency: 0.09 sec
Bandwidth achieved: 1652.45 GB/s
FLOPS achieved: 4.96 TF/s

Prefill latency: 0.09111327677965164 sec
Decode latency: 11.128103130497038 sec
Time for inference 2: 11.22 sec total, 45.63 tokens/sec
Decode latency: 11.13 sec
Prefill latency: 0.09 sec
Bandwidth achieved: 1657.68 GB/s
FLOPS achieved: 4.97 TF/s

Prefill latency: 0.09094583429396152 sec
Decode latency: 11.113706315867603 sec
Time for inference 3: 11.21 sec total, 45.69 tokens/sec
Decode latency: 11.11 sec
Prefill latency: 0.09 sec
Bandwidth achieved: 1659.85 GB/s
FLOPS achieved: 4.98 TF/s

Prefill latency: 0.09060350712388754 sec
Decode latency: 11.180545393377542 sec
Time for inference 4: 11.27 sec total, 45.42 tokens/sec
Decode latency: 11.18 sec
Prefill latency: 0.09 sec
Bandwidth achieved: 1650.05 GB/s
FLOPS achieved: 4.95 TF/s

Prefill latency: 0.09147321246564388 sec
Decode latency: 11.152559872716665 sec
Time for inference 5: 11.25 sec total, 45.53 tokens/sec
Decode latency: 11.15 sec
Prefill latency: 0.09 sec
Bandwidth achieved: 1654.02 GB/s
FLOPS achieved: 4.96 TF/s

Prefill latency: 0.09033833257853985 sec
Decode latency: 11.177384493872523 sec
Time for inference 6: 11.27 sec total, 45.43 tokens/sec
Decode latency: 11.18 sec
Prefill latency: 0.09 sec
Bandwidth achieved: 1650.56 GB/s
FLOPS achieved: 4.95 TF/s

Prefill latency: 0.0906478138640523 sec
Decode latency: 11.171492971479893 sec
Time for inference 7: 11.26 sec total, 45.46 tokens/sec
Decode latency: 11.17 sec
Prefill latency: 0.09 sec
Bandwidth achieved: 1651.37 GB/s
FLOPS achieved: 4.95 TF/s

Prefill latency: 0.09155555721372366 sec
Decode latency: 11.181454669684172 sec
Time for inference 8: 11.27 sec total, 45.41 tokens/sec
Decode latency: 11.18 sec
Prefill latency: 0.09 sec
Bandwidth achieved: 1649.78 GB/s
FLOPS achieved: 4.95 TF/s

Prefill latency: 0.09075951389968395 sec
Decode latency: 11.183494317345321 sec
Time for inference 9: 11.28 sec total, 45.41 tokens/sec
Decode latency: 11.18 sec
Prefill latency: 0.09 sec
Bandwidth achieved: 1649.59 GB/s
FLOPS achieved: 4.95 TF/s

Prefill latency: 0.091215412132442 sec
Decode latency: 11.176467179320753 sec
Time for inference 10: 11.27 sec total, 45.43 tokens/sec
Decode latency: 11.18 sec
Prefill latency: 0.09 sec
Bandwidth achieved: 1650.55 GB/s
FLOPS achieved: 4.95 TF/s

==========
Batch Size: 1
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 11.1629 sec
Average prefill latency: 0.0910 sec
Average tokens/sec: 45.49
Memory used: 40.48 GB
Done. we are killing the process
[rank0]:[W1114 05:05:32.775364871 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
