W1114 05:05:35.884000 2336813 site-packages/torch/distributed/run.py:793] 
W1114 05:05:35.884000 2336813 site-packages/torch/distributed/run.py:793] *****************************************
W1114 05:05:35.884000 2336813 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1114 05:05:35.884000 2336813 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTParallel(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x ParallelTransformerBlock(
      semi_compiled = False
      (attention): FuseAttentionMLP(
        (wqkv1): Linear(in_features=8192, out_features=8448, bias=False)
        (wo): Linear(in_features=1024, out_features=8192, bias=False)
        (w2): Linear(in_features=3584, out_features=8192, bias=False)
      )
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 1.26 seconds
CUDA_GRAPH are activate
Prefill latency: 0.08274491131305695 sec
Decode latency: 7.8134414460510015 sec
Compilation time: 7.89 seconds
Compilation time: 7.88 seconds
Compilation time: 7.92 seconds
Compilation time: 7.93 seconds
Compilation time: 7.90 seconds
Compilation time: 7.93 seconds
Compilation time: 7.89 seconds
Compilation time: 7.89 seconds
Prefill latency: 0.060924872756004333 sec
Decode latency: 7.812114200554788 sec
Prefill latency: 0.060964278876781464 sec
Decode latency: 7.812172842212021 sec
Prefill latency: 0.06098386272788048 sec
Decode latency: 7.813022053800523 sec
Prefill latency: 0.061015764251351357 sec
Decode latency: 7.812220973894 sec
Prefill latency: 0.061254773288965225 sec
Decode latency: 7.8125974871218204 sec
Time for inference 1: 7.87 sec total, 65.02 tokens/sec
Decode latency: 7.81 sec
Prefill latency: 0.06 sec
Bandwidth achieved: 1249.32 GB/s
FLOPS achieved: 3.75 TF/s

Prefill latency: 0.06135640945285559 sec
Decode latency: 7.811215732246637 sec
Time for inference 2: 7.87 sec total, 65.03 tokens/sec
Decode latency: 7.81 sec
Prefill latency: 0.06 sec
Bandwidth achieved: 1249.51 GB/s
FLOPS achieved: 3.75 TF/s

Prefill latency: 0.061076695099473 sec
Decode latency: 7.812248976901174 sec
Time for inference 3: 7.87 sec total, 65.02 tokens/sec
Decode latency: 7.81 sec
Prefill latency: 0.06 sec
Bandwidth achieved: 1249.37 GB/s
FLOPS achieved: 3.75 TF/s

Prefill latency: 0.06096877343952656 sec
Decode latency: 7.812890883535147 sec
Time for inference 4: 7.88 sec total, 65.02 tokens/sec
Decode latency: 7.81 sec
Prefill latency: 0.06 sec
Bandwidth achieved: 1249.29 GB/s
FLOPS achieved: 3.75 TF/s

Prefill latency: 0.06120437290519476 sec
Decode latency: 7.81270398478955 sec
Time for inference 5: 7.88 sec total, 65.01 tokens/sec
Decode latency: 7.81 sec
Prefill latency: 0.06 sec
Bandwidth achieved: 1249.29 GB/s
FLOPS achieved: 3.75 TF/s

Prefill latency: 0.06104687601327896 sec
Decode latency: 7.811618196777999 sec
Time for inference 6: 7.87 sec total, 65.03 tokens/sec
Decode latency: 7.81 sec
Prefill latency: 0.06 sec
Bandwidth achieved: 1249.50 GB/s
FLOPS achieved: 3.75 TF/s

Prefill latency: 0.061186524108052254 sec
Decode latency: 7.813498607836664 sec
Time for inference 7: 7.88 sec total, 65.01 tokens/sec
Decode latency: 7.81 sec
Prefill latency: 0.06 sec
Bandwidth achieved: 1249.18 GB/s
FLOPS achieved: 3.75 TF/s

Prefill latency: 0.06117825768887997 sec
Decode latency: 7.812444641254842 sec
Time for inference 8: 7.87 sec total, 65.02 tokens/sec
Decode latency: 7.81 sec
Prefill latency: 0.06 sec
Bandwidth achieved: 1249.34 GB/s
FLOPS achieved: 3.75 TF/s

Prefill latency: 0.06172339152544737 sec
Decode latency: 7.812250638380647 sec
Time for inference 9: 7.88 sec total, 65.01 tokens/sec
Decode latency: 7.81 sec
Prefill latency: 0.06 sec
Bandwidth achieved: 1249.29 GB/s
FLOPS achieved: 3.75 TF/s

Prefill latency: 0.06131940335035324 sec
Decode latency: 7.810963934287429 sec
Time for inference 10: 7.87 sec total, 65.03 tokens/sec
Decode latency: 7.81 sec
Prefill latency: 0.06 sec
Bandwidth achieved: 1249.55 GB/s
FLOPS achieved: 3.75 TF/s

==========
Batch Size: 1
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 7.8122 sec
Average prefill latency: 0.0612 sec
Average tokens/sec: 65.02
Memory used: 23.24 GB
Done. we are killing the process
[rank0]:[W1114 05:07:50.474188875 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
