flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTParallel(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x ParallelTransformerBlock(
      semi_compiled = False
      (attention): FuseAttentionMLP(
        (wqkv1): Linear(in_features=4096, out_features=34816, bias=False)
        (wo): Linear(in_features=4096, out_features=4096, bias=False)
        (w2): Linear(in_features=14336, out_features=4096, bias=False)
      )
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 0.88 seconds
CUDA_GRAPH are activate
Prefill latency: 2.1754813361912966 sec
Decode latency: 6.842733044177294 sec
Compilation time: 9.02 seconds
Prefill latency: 2.175771689042449 sec
Decode latency: 6.845541771501303 sec
Prefill latency: 2.1782954316586256 sec
Decode latency: 6.748402612283826 sec
Prefill latency: 2.173062624409795 sec
Decode latency: 6.862121723592281 sec
Prefill latency: 2.168455120176077 sec
Decode latency: 6.715523391962051 sec
Prefill latency: 2.1734651252627373 sec
Decode latency: 6.819622145965695 sec
Time for inference 1: 8.99 sec total, 3643.26 tokens/sec
Decode latency: 6.82 sec
Prefill latency: 2.17 sec
Bandwidth achieved: 54683.88 GB/s
FLOPS achieved: 164.05 TF/s

Prefill latency: 2.1750518269836903 sec
Decode latency: 6.723578544333577 sec
Time for inference 2: 8.90 sec total, 3681.96 tokens/sec
Decode latency: 6.72 sec
Prefill latency: 2.18 sec
Bandwidth achieved: 55264.63 GB/s
FLOPS achieved: 165.79 TF/s

Prefill latency: 2.1792559400200844 sec
Decode latency: 6.822371566668153 sec
Time for inference 3: 9.00 sec total, 3639.82 tokens/sec
Decode latency: 6.82 sec
Prefill latency: 2.18 sec
Bandwidth achieved: 54632.16 GB/s
FLOPS achieved: 163.90 TF/s

Prefill latency: 2.1600649133324623 sec
Decode latency: 6.789418384432793 sec
Time for inference 4: 8.95 sec total, 3660.97 tokens/sec
Decode latency: 6.79 sec
Prefill latency: 2.16 sec
Bandwidth achieved: 54949.71 GB/s
FLOPS achieved: 164.85 TF/s

Prefill latency: 2.163134975358844 sec
Decode latency: 6.82641788944602 sec
Time for inference 5: 8.99 sec total, 3644.68 tokens/sec
Decode latency: 6.83 sec
Prefill latency: 2.16 sec
Bandwidth achieved: 54705.12 GB/s
FLOPS achieved: 164.12 TF/s

Prefill latency: 2.1664881464093924 sec
Decode latency: 6.718187175691128 sec
Time for inference 6: 8.89 sec total, 3687.67 tokens/sec
Decode latency: 6.72 sec
Prefill latency: 2.17 sec
Bandwidth achieved: 55350.41 GB/s
FLOPS achieved: 166.05 TF/s

Prefill latency: 2.1737506184726954 sec
Decode latency: 6.834359049797058 sec
Time for inference 7: 9.01 sec total, 3637.15 tokens/sec
Decode latency: 6.83 sec
Prefill latency: 2.17 sec
Bandwidth achieved: 54592.19 GB/s
FLOPS achieved: 163.78 TF/s

Prefill latency: 2.1754696927964687 sec
Decode latency: 6.713262878358364 sec
Time for inference 8: 8.89 sec total, 3685.96 tokens/sec
Decode latency: 6.71 sec
Prefill latency: 2.18 sec
Bandwidth achieved: 55324.77 GB/s
FLOPS achieved: 165.97 TF/s

Prefill latency: 2.175334272906184 sec
Decode latency: 6.846645725890994 sec
Time for inference 9: 9.02 sec total, 3631.57 tokens/sec
Decode latency: 6.85 sec
Prefill latency: 2.18 sec
Bandwidth achieved: 54508.42 GB/s
FLOPS achieved: 163.53 TF/s

Prefill latency: 2.168218668550253 sec
Decode latency: 6.779994543641806 sec
Time for inference 10: 8.95 sec total, 3661.53 tokens/sec
Decode latency: 6.78 sec
Prefill latency: 2.17 sec
Bandwidth achieved: 54958.07 GB/s
FLOPS achieved: 164.87 TF/s

==========
Batch Size: 64
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 6.7874 sec
Average prefill latency: 2.1710 sec
Average tokens/sec: 3657.46
Memory used: 76.61 GB
Done. we are killing the process
[rank0]:[W1114 03:36:21.433926755 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
