W1114 03:20:47.210000 2504785 site-packages/torch/distributed/run.py:793] 
W1114 03:20:47.210000 2504785 site-packages/torch/distributed/run.py:793] *****************************************
W1114 03:20:47.210000 2504785 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1114 03:20:47.210000 2504785 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTParallel(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x ParallelTransformerBlock(
      semi_compiled = False
      (attention): FuseAttentionMLP(
        (wqkv1): Linear(in_features=4096, out_features=17408, bias=False)
        (wo): Linear(in_features=2048, out_features=4096, bias=False)
        (w2): Linear(in_features=7168, out_features=4096, bias=False)
      )
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.09 seconds
CUDA_GRAPH are activate
[rank0]:[W1114 03:20:57.393698365 CUDAGraph.cpp:133] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
Prefill latency: 0.021602796390652657 sec
Decode latency: 3.29085754789412 sec
Compilation time: 3.34 seconds
Compilation time: 3.31 seconds
Prefill latency: 0.02128947153687477 sec
Decode latency: 3.306936701759696 sec
Prefill latency: 0.02134024351835251 sec
Decode latency: 3.2959394194185734 sec
Prefill latency: 0.0212607029825449 sec
Decode latency: 3.2960062194615602 sec
Prefill latency: 0.021273775026202202 sec
Decode latency: 3.30456767603755 sec
Prefill latency: 0.021332109346985817 sec
Decode latency: 3.3037262950092554 sec
Time for inference 1: 3.33 sec total, 153.93 tokens/sec
Decode latency: 3.30 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 1236.08 GB/s
FLOPS achieved: 3.71 TF/s

Prefill latency: 0.02133249118924141 sec
Decode latency: 3.309722799807787 sec
Time for inference 2: 3.33 sec total, 153.65 tokens/sec
Decode latency: 3.31 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 1233.87 GB/s
FLOPS achieved: 3.70 TF/s

Prefill latency: 0.021255653351545334 sec
Decode latency: 3.290453838184476 sec
Time for inference 3: 3.31 sec total, 154.55 tokens/sec
Decode latency: 3.29 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 1241.07 GB/s
FLOPS achieved: 3.72 TF/s

Prefill latency: 0.021327616646885872 sec
Decode latency: 3.304340209811926 sec
Time for inference 4: 3.33 sec total, 153.90 tokens/sec
Decode latency: 3.30 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 1235.87 GB/s
FLOPS achieved: 3.71 TF/s

Prefill latency: 0.02135784551501274 sec
Decode latency: 3.3058836963027716 sec
Time for inference 5: 3.33 sec total, 153.83 tokens/sec
Decode latency: 3.31 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 1235.31 GB/s
FLOPS achieved: 3.71 TF/s

Prefill latency: 0.02136762998998165 sec
Decode latency: 3.296029122546315 sec
Time for inference 6: 3.32 sec total, 154.29 tokens/sec
Decode latency: 3.30 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 1238.99 GB/s
FLOPS achieved: 3.72 TF/s

Prefill latency: 0.02131805568933487 sec
Decode latency: 3.2959486693143845 sec
Time for inference 7: 3.32 sec total, 154.31 tokens/sec
Decode latency: 3.30 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 1239.11 GB/s
FLOPS achieved: 3.72 TF/s

Prefill latency: 0.021331220865249634 sec
Decode latency: 3.296217979863286 sec
Time for inference 8: 3.32 sec total, 154.29 tokens/sec
Decode latency: 3.30 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 1238.99 GB/s
FLOPS achieved: 3.72 TF/s

Prefill latency: 0.021318471059203148 sec
Decode latency: 3.3037702050060034 sec
Time for inference 9: 3.33 sec total, 153.93 tokens/sec
Decode latency: 3.30 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 1236.12 GB/s
FLOPS achieved: 3.71 TF/s

Prefill latency: 0.02135569602251053 sec
Decode latency: 3.3039719238877296 sec
Time for inference 10: 3.33 sec total, 153.93 tokens/sec
Decode latency: 3.30 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 1236.11 GB/s
FLOPS achieved: 3.71 TF/s

==========
Batch Size: 1
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 3.3010 sec
Average prefill latency: 0.0213 sec
Average tokens/sec: 154.06
Memory used: 10.15 GB
Done. we are killing the process
[rank0]:[W1114 03:21:47.631647447 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
