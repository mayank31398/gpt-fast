W1114 03:21:50.548000 2507851 site-packages/torch/distributed/run.py:793] 
W1114 03:21:50.548000 2507851 site-packages/torch/distributed/run.py:793] *****************************************
W1114 03:21:50.548000 2507851 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1114 03:21:50.548000 2507851 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTParallel(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x ParallelTransformerBlock(
      semi_compiled = False
      (attention): FuseAttentionMLP(
        (wqkv1): Linear(in_features=4096, out_features=8704, bias=False)
        (wo): Linear(in_features=1024, out_features=4096, bias=False)
        (w2): Linear(in_features=3584, out_features=4096, bias=False)
      )
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 0.95 seconds
CUDA_GRAPH are activate
Prefill latency: 0.044361138716340065 sec
Decode latency: 2.5618892554193735 sec
Compilation time: 2.58 seconds
Compilation time: 2.59 seconds
Compilation time: 2.60 seconds
Compilation time: 2.61 seconds
Prefill latency: 0.015710407868027687 sec
Decode latency: 2.5590757485479116 sec
Prefill latency: 0.015719443559646606 sec
Decode latency: 2.5619317684322596 sec
Prefill latency: 0.015658363699913025 sec
Decode latency: 2.5572566855698824 sec
Prefill latency: 0.015752825886011124 sec
Decode latency: 2.5586024429649115 sec
Prefill latency: 0.015764718875288963 sec
Decode latency: 2.560656437650323 sec
Time for inference 1: 2.58 sec total, 198.66 tokens/sec
Decode latency: 2.56 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 902.04 GB/s
FLOPS achieved: 2.71 TF/s

Prefill latency: 0.01574741303920746 sec
Decode latency: 2.5593981109559536 sec
Time for inference 2: 2.58 sec total, 198.76 tokens/sec
Decode latency: 2.56 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 902.50 GB/s
FLOPS achieved: 2.71 TF/s

Prefill latency: 0.015718884766101837 sec
Decode latency: 2.5569844394922256 sec
Time for inference 3: 2.57 sec total, 198.93 tokens/sec
Decode latency: 2.56 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 903.28 GB/s
FLOPS achieved: 2.71 TF/s

Prefill latency: 0.015710672363638878 sec
Decode latency: 2.558354288339615 sec
Time for inference 4: 2.58 sec total, 198.83 tokens/sec
Decode latency: 2.56 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 902.83 GB/s
FLOPS achieved: 2.71 TF/s

Prefill latency: 0.01569550111889839 sec
Decode latency: 2.5589928440749645 sec
Time for inference 5: 2.58 sec total, 198.79 tokens/sec
Decode latency: 2.56 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 902.64 GB/s
FLOPS achieved: 2.71 TF/s

Prefill latency: 0.01568959467113018 sec
Decode latency: 2.5596958324313164 sec
Time for inference 6: 2.58 sec total, 198.74 tokens/sec
Decode latency: 2.56 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 902.42 GB/s
FLOPS achieved: 2.71 TF/s

Prefill latency: 0.015764355659484863 sec
Decode latency: 2.5567841716110706 sec
Time for inference 7: 2.57 sec total, 198.96 tokens/sec
Decode latency: 2.56 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 903.42 GB/s
FLOPS achieved: 2.71 TF/s

Prefill latency: 0.015688931569457054 sec
Decode latency: 2.55767472833395 sec
Time for inference 8: 2.57 sec total, 198.89 tokens/sec
Decode latency: 2.56 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 903.07 GB/s
FLOPS achieved: 2.71 TF/s

Prefill latency: 0.015728671103715897 sec
Decode latency: 2.5594120249152184 sec
Time for inference 9: 2.58 sec total, 198.75 tokens/sec
Decode latency: 2.56 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 902.43 GB/s
FLOPS achieved: 2.71 TF/s

Prefill latency: 0.015759622678160667 sec
Decode latency: 2.5588932372629642 sec
Time for inference 10: 2.58 sec total, 198.78 tokens/sec
Decode latency: 2.56 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 902.58 GB/s
FLOPS achieved: 2.71 TF/s

==========
Batch Size: 1
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 2.5587 sec
Average prefill latency: 0.0157 sec
Average tokens/sec: 198.81
Memory used: 6.50 GB
Done. we are killing the process
[rank0]:[W1114 03:22:44.692489635 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
