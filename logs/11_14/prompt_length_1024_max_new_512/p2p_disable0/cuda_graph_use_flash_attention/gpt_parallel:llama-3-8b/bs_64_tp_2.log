W1114 03:36:24.554000 2559875 site-packages/torch/distributed/run.py:793] 
W1114 03:36:24.554000 2559875 site-packages/torch/distributed/run.py:793] *****************************************
W1114 03:36:24.554000 2559875 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1114 03:36:24.554000 2559875 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTParallel(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x ParallelTransformerBlock(
      semi_compiled = False
      (attention): FuseAttentionMLP(
        (wqkv1): Linear(in_features=4096, out_features=17408, bias=False)
        (wo): Linear(in_features=2048, out_features=4096, bias=False)
        (w2): Linear(in_features=7168, out_features=4096, bias=False)
      )
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 2.19 seconds
CUDA_GRAPH are activate
[rank1]:[W1114 03:36:39.340137495 CUDAGraph.cpp:133] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
Prefill latency: 1.2827398348599672 sec
Decode latency: 4.741152664646506 sec
Compilation time: 6.04 seconds
Compilation time: 6.03 seconds
Prefill latency: 1.2800135165452957 sec
Decode latency: 4.7439708430320024 sec
Prefill latency: 1.2900385782122612 sec
Decode latency: 4.741364657878876 sec
Prefill latency: 1.2914547119289637 sec
Decode latency: 4.738662565127015 sec
Prefill latency: 1.2876021154224873 sec
Decode latency: 4.743126139044762 sec
Prefill latency: 1.2952365279197693 sec
Decode latency: 4.7436174508184195 sec
Time for inference 1: 6.04 sec total, 5425.08 tokens/sec
Decode latency: 4.74 sec
Prefill latency: 1.30 sec
Bandwidth achieved: 43564.86 GB/s
FLOPS achieved: 130.69 TF/s

Prefill latency: 1.2943429183214903 sec
Decode latency: 4.742178896442056 sec
Time for inference 2: 6.04 sec total, 5427.35 tokens/sec
Decode latency: 4.74 sec
Prefill latency: 1.29 sec
Bandwidth achieved: 43583.05 GB/s
FLOPS achieved: 130.75 TF/s

Prefill latency: 1.2933349646627903 sec
Decode latency: 4.739575654268265 sec
Time for inference 3: 6.03 sec total, 5430.59 tokens/sec
Decode latency: 4.74 sec
Prefill latency: 1.29 sec
Bandwidth achieved: 43609.08 GB/s
FLOPS achieved: 130.83 TF/s

Prefill latency: 1.2870074361562729 sec
Decode latency: 4.745035171508789 sec
Time for inference 4: 6.03 sec total, 5431.35 tokens/sec
Decode latency: 4.75 sec
Prefill latency: 1.29 sec
Bandwidth achieved: 43615.17 GB/s
FLOPS achieved: 130.85 TF/s

Prefill latency: 1.2887337170541286 sec
Decode latency: 4.743888977915049 sec
Time for inference 5: 6.03 sec total, 5430.68 tokens/sec
Decode latency: 4.74 sec
Prefill latency: 1.29 sec
Bandwidth achieved: 43609.80 GB/s
FLOPS achieved: 130.83 TF/s

Prefill latency: 1.294076669961214 sec
Decode latency: 4.7413538452237844 sec
Time for inference 6: 6.04 sec total, 5428.37 tokens/sec
Decode latency: 4.74 sec
Prefill latency: 1.29 sec
Bandwidth achieved: 43591.27 GB/s
FLOPS achieved: 130.77 TF/s

Prefill latency: 1.2831218373030424 sec
Decode latency: 4.744515677914023 sec
Time for inference 7: 6.03 sec total, 5435.34 tokens/sec
Decode latency: 4.74 sec
Prefill latency: 1.28 sec
Bandwidth achieved: 43647.22 GB/s
FLOPS achieved: 130.94 TF/s

Prefill latency: 1.2892814725637436 sec
Decode latency: 4.743229428306222 sec
Time for inference 8: 6.03 sec total, 5430.89 tokens/sec
Decode latency: 4.74 sec
Prefill latency: 1.29 sec
Bandwidth achieved: 43611.50 GB/s
FLOPS achieved: 130.83 TF/s

Prefill latency: 1.2937016170471907 sec
Decode latency: 4.74327233619988 sec
Time for inference 9: 6.04 sec total, 5426.97 tokens/sec
Decode latency: 4.74 sec
Prefill latency: 1.29 sec
Bandwidth achieved: 43579.98 GB/s
FLOPS achieved: 130.74 TF/s

Prefill latency: 1.2835935093462467 sec
Decode latency: 4.7422971073538065 sec
Time for inference 10: 6.03 sec total, 5436.84 tokens/sec
Decode latency: 4.74 sec
Prefill latency: 1.28 sec
Bandwidth achieved: 43659.30 GB/s
FLOPS achieved: 130.98 TF/s

==========
Batch Size: 64
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 4.7429 sec
Average prefill latency: 1.2902 sec
Average tokens/sec: 5430.35
Memory used: 69.09 GB
Done. we are killing the process
[rank0]:[W1114 03:38:10.235427273 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
