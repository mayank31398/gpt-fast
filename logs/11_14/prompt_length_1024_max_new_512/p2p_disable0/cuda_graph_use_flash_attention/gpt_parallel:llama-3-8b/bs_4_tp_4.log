W1114 03:26:24.480000 2524952 site-packages/torch/distributed/run.py:793] 
W1114 03:26:24.480000 2524952 site-packages/torch/distributed/run.py:793] *****************************************
W1114 03:26:24.480000 2524952 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1114 03:26:24.480000 2524952 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTParallel(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x ParallelTransformerBlock(
      semi_compiled = False
      (attention): FuseAttentionMLP(
        (wqkv1): Linear(in_features=4096, out_features=8704, bias=False)
        (wo): Linear(in_features=1024, out_features=4096, bias=False)
        (w2): Linear(in_features=3584, out_features=4096, bias=False)
      )
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.99 seconds
CUDA_GRAPH are activate
Prefill latency: 0.10532205924391747 sec
Decode latency: 2.794923620298505 sec
Compilation time: 2.90 seconds
Compilation time: 2.84 seconds
Compilation time: 2.87 seconds
Compilation time: 2.85 seconds
Prefill latency: 0.047222865745425224 sec
Decode latency: 2.795155456289649 sec
Prefill latency: 0.04710948094725609 sec
Decode latency: 2.797035528346896 sec
Prefill latency: 0.047280166298151016 sec
Decode latency: 2.7946161162108183 sec
Prefill latency: 0.047138847410678864 sec
Decode latency: 2.79568650200963 sec
Prefill latency: 0.04710911959409714 sec
Decode latency: 2.795458933338523 sec
Time for inference 1: 2.84 sec total, 720.23 tokens/sec
Decode latency: 2.80 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 3270.26 GB/s
FLOPS achieved: 9.81 TF/s

Prefill latency: 0.047241950407624245 sec
Decode latency: 2.794781483709812 sec
Time for inference 2: 2.84 sec total, 720.39 tokens/sec
Decode latency: 2.79 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 3270.99 GB/s
FLOPS achieved: 9.81 TF/s

Prefill latency: 0.04711405001580715 sec
Decode latency: 2.7965803407132626 sec
Time for inference 3: 2.84 sec total, 719.93 tokens/sec
Decode latency: 2.80 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 3268.91 GB/s
FLOPS achieved: 9.81 TF/s

Prefill latency: 0.04718145541846752 sec
Decode latency: 2.796591680496931 sec
Time for inference 4: 2.84 sec total, 719.94 tokens/sec
Decode latency: 2.80 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 3268.94 GB/s
FLOPS achieved: 9.81 TF/s

Prefill latency: 0.0470467247068882 sec
Decode latency: 2.7948430981487036 sec
Time for inference 5: 2.84 sec total, 720.42 tokens/sec
Decode latency: 2.79 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 3271.12 GB/s
FLOPS achieved: 9.81 TF/s

Prefill latency: 0.04727298952639103 sec
Decode latency: 2.796241147443652 sec
Time for inference 6: 2.84 sec total, 720.02 tokens/sec
Decode latency: 2.80 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 3269.33 GB/s
FLOPS achieved: 9.81 TF/s

Prefill latency: 0.04725446738302708 sec
Decode latency: 2.797370620071888 sec
Time for inference 7: 2.85 sec total, 719.71 tokens/sec
Decode latency: 2.80 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 3267.94 GB/s
FLOPS achieved: 9.80 TF/s

Prefill latency: 0.04736967757344246 sec
Decode latency: 2.796737464144826 sec
Time for inference 8: 2.85 sec total, 719.84 tokens/sec
Decode latency: 2.80 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 3268.51 GB/s
FLOPS achieved: 9.81 TF/s

Prefill latency: 0.04722777381539345 sec
Decode latency: 2.7953431885689497 sec
Time for inference 9: 2.84 sec total, 720.26 tokens/sec
Decode latency: 2.80 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 3270.40 GB/s
FLOPS achieved: 9.81 TF/s

Prefill latency: 0.04729202389717102 sec
Decode latency: 2.795527508482337 sec
Time for inference 10: 2.84 sec total, 720.17 tokens/sec
Decode latency: 2.80 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 3270.01 GB/s
FLOPS achieved: 9.81 TF/s

==========
Batch Size: 4
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 2.7959 sec
Average prefill latency: 0.0472 sec
Average tokens/sec: 720.09
Memory used: 8.57 GB
Done. we are killing the process
[rank0]:[W1114 03:27:21.428635177 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
