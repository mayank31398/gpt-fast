W1114 03:38:13.547000 2565238 site-packages/torch/distributed/run.py:793] 
W1114 03:38:13.547000 2565238 site-packages/torch/distributed/run.py:793] *****************************************
W1114 03:38:13.547000 2565238 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1114 03:38:13.547000 2565238 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTParallel(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x ParallelTransformerBlock(
      semi_compiled = False
      (attention): FuseAttentionMLP(
        (wqkv1): Linear(in_features=4096, out_features=8704, bias=False)
        (wo): Linear(in_features=1024, out_features=4096, bias=False)
        (w2): Linear(in_features=3584, out_features=4096, bias=False)
      )
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.42 seconds
CUDA_GRAPH are activate
Prefill latency: 0.6961824670433998 sec
Decode latency: 3.7020813580602407 sec
Compilation time: 4.42 seconds
Compilation time: 4.42 seconds
Compilation time: 4.40 seconds
Compilation time: 4.42 seconds
Prefill latency: 0.6980694308876991 sec
Decode latency: 3.701769817620516 sec
Prefill latency: 0.6987569835036993 sec
Decode latency: 3.7030334435403347 sec
Prefill latency: 0.6977367736399174 sec
Decode latency: 3.7020300179719925 sec
Prefill latency: 0.6993526965379715 sec
Decode latency: 3.7044803481549025 sec
Prefill latency: 0.7000708822160959 sec
Decode latency: 3.7030227575451136 sec
Time for inference 1: 4.40 sec total, 7440.08 tokens/sec
Decode latency: 3.70 sec
Prefill latency: 0.70 sec
Bandwidth achieved: 33782.44 GB/s
FLOPS achieved: 101.35 TF/s

Prefill latency: 0.7002399004995823 sec
Decode latency: 3.6990637462586164 sec
Time for inference 2: 4.40 sec total, 7446.61 tokens/sec
Decode latency: 3.70 sec
Prefill latency: 0.70 sec
Bandwidth achieved: 33812.11 GB/s
FLOPS achieved: 101.44 TF/s

Prefill latency: 0.6984816137701273 sec
Decode latency: 3.7020876519382 sec
Time for inference 3: 4.40 sec total, 7444.77 tokens/sec
Decode latency: 3.70 sec
Prefill latency: 0.70 sec
Bandwidth achieved: 33803.74 GB/s
FLOPS achieved: 101.41 TF/s

Prefill latency: 0.6947314627468586 sec
Decode latency: 3.7040489222854376 sec
Time for inference 4: 4.40 sec total, 7447.54 tokens/sec
Decode latency: 3.70 sec
Prefill latency: 0.69 sec
Bandwidth achieved: 33816.35 GB/s
FLOPS achieved: 101.45 TF/s

Prefill latency: 0.6981479600071907 sec
Decode latency: 3.7036812771111727 sec
Time for inference 5: 4.40 sec total, 7442.38 tokens/sec
Decode latency: 3.70 sec
Prefill latency: 0.70 sec
Bandwidth achieved: 33792.88 GB/s
FLOPS achieved: 101.38 TF/s

Prefill latency: 0.6996412314474583 sec
Decode latency: 3.7017898112535477 sec
Time for inference 6: 4.40 sec total, 7442.87 tokens/sec
Decode latency: 3.70 sec
Prefill latency: 0.70 sec
Bandwidth achieved: 33795.13 GB/s
FLOPS achieved: 101.39 TF/s

Prefill latency: 0.7010604199022055 sec
Decode latency: 3.703012779355049 sec
Time for inference 7: 4.41 sec total, 7438.56 tokens/sec
Decode latency: 3.70 sec
Prefill latency: 0.70 sec
Bandwidth achieved: 33775.56 GB/s
FLOPS achieved: 101.33 TF/s

Prefill latency: 0.7005913332104683 sec
Decode latency: 3.7003095243126154 sec
Time for inference 8: 4.40 sec total, 7444.00 tokens/sec
Decode latency: 3.70 sec
Prefill latency: 0.70 sec
Bandwidth achieved: 33800.26 GB/s
FLOPS achieved: 101.40 TF/s

Prefill latency: 0.6988415885716677 sec
Decode latency: 3.7035676036030054 sec
Time for inference 9: 4.40 sec total, 7441.35 tokens/sec
Decode latency: 3.70 sec
Prefill latency: 0.70 sec
Bandwidth achieved: 33788.23 GB/s
FLOPS achieved: 101.36 TF/s

Prefill latency: 0.6997373756021261 sec
Decode latency: 3.7022265922278166 sec
Time for inference 10: 4.40 sec total, 7442.06 tokens/sec
Decode latency: 3.70 sec
Prefill latency: 0.70 sec
Bandwidth achieved: 33791.44 GB/s
FLOPS achieved: 101.37 TF/s

==========
Batch Size: 64
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 3.7023 sec
Average prefill latency: 0.6992 sec
Average tokens/sec: 7443.02
Memory used: 48.40 GB
Done. we are killing the process
[rank0]:[W1114 03:39:36.555515971 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
