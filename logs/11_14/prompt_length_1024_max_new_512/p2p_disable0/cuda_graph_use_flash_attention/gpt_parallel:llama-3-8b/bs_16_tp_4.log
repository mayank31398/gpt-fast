W1114 03:31:32.124000 2543750 site-packages/torch/distributed/run.py:793] 
W1114 03:31:32.124000 2543750 site-packages/torch/distributed/run.py:793] *****************************************
W1114 03:31:32.124000 2543750 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1114 03:31:32.124000 2543750 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTParallel(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x ParallelTransformerBlock(
      semi_compiled = False
      (attention): FuseAttentionMLP(
        (wqkv1): Linear(in_features=4096, out_features=8704, bias=False)
        (wo): Linear(in_features=1024, out_features=4096, bias=False)
        (w2): Linear(in_features=3584, out_features=4096, bias=False)
      )
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.37 seconds
CUDA_GRAPH are activate
Prefill latency: 0.17785592004656792 sec
Decode latency: 3.081812221556902 sec
Compilation time: 3.27 secondsCompilation time: 3.27 secondsCompilation time: 3.26 seconds


Compilation time: 3.26 seconds
Prefill latency: 0.1773664727807045 sec
Decode latency: 3.0825674813240767 sec
Prefill latency: 0.17771979048848152 sec
Decode latency: 3.0802760683000088 sec
Prefill latency: 0.17822196148335934 sec
Decode latency: 3.0798554196953773 sec
Prefill latency: 0.17817909829318523 sec
Decode latency: 3.080411607399583 sec
Prefill latency: 0.17807051353156567 sec
Decode latency: 3.08013778552413 sec
Time for inference 1: 3.26 sec total, 2513.53 tokens/sec
Decode latency: 3.08 sec
Prefill latency: 0.18 sec
Bandwidth achieved: 11412.95 GB/s
FLOPS achieved: 34.24 TF/s

Prefill latency: 0.17757627554237843 sec
Decode latency: 3.080890752375126 sec
Time for inference 2: 3.26 sec total, 2513.28 tokens/sec
Decode latency: 3.08 sec
Prefill latency: 0.18 sec
Bandwidth achieved: 11411.81 GB/s
FLOPS achieved: 34.24 TF/s

Prefill latency: 0.17803063988685608 sec
Decode latency: 3.081962304189801 sec
Time for inference 3: 3.26 sec total, 2512.08 tokens/sec
Decode latency: 3.08 sec
Prefill latency: 0.18 sec
Bandwidth achieved: 11406.35 GB/s
FLOPS achieved: 34.22 TF/s

Prefill latency: 0.17826608009636402 sec
Decode latency: 3.0810142308473587 sec
Time for inference 4: 3.26 sec total, 2512.57 tokens/sec
Decode latency: 3.08 sec
Prefill latency: 0.18 sec
Bandwidth achieved: 11408.60 GB/s
FLOPS achieved: 34.23 TF/s

Prefill latency: 0.17833558097481728 sec
Decode latency: 3.079809907823801 sec
Time for inference 5: 3.26 sec total, 2513.46 tokens/sec
Decode latency: 3.08 sec
Prefill latency: 0.18 sec
Bandwidth achieved: 11412.62 GB/s
FLOPS achieved: 34.24 TF/s

Prefill latency: 0.17836639285087585 sec
Decode latency: 3.080436335876584 sec
Time for inference 6: 3.26 sec total, 2513.06 tokens/sec
Decode latency: 3.08 sec
Prefill latency: 0.18 sec
Bandwidth achieved: 11410.82 GB/s
FLOPS achieved: 34.23 TF/s

Prefill latency: 0.1776218619197607 sec
Decode latency: 3.081719074398279 sec
Time for inference 7: 3.26 sec total, 2512.51 tokens/sec
Decode latency: 3.08 sec
Prefill latency: 0.18 sec
Bandwidth achieved: 11408.31 GB/s
FLOPS achieved: 34.22 TF/s

Prefill latency: 0.17817173525691032 sec
Decode latency: 3.080567169934511 sec
Time for inference 8: 3.26 sec total, 2513.09 tokens/sec
Decode latency: 3.08 sec
Prefill latency: 0.18 sec
Bandwidth achieved: 11410.94 GB/s
FLOPS achieved: 34.23 TF/s

Prefill latency: 0.177855022251606 sec
Decode latency: 3.0813328865915537 sec
Time for inference 9: 3.26 sec total, 2512.66 tokens/sec
Decode latency: 3.08 sec
Prefill latency: 0.18 sec
Bandwidth achieved: 11409.01 GB/s
FLOPS achieved: 34.23 TF/s

Prefill latency: 0.17855880595743656 sec
Decode latency: 3.0800944045186043 sec
Time for inference 10: 3.26 sec total, 2513.09 tokens/sec
Decode latency: 3.08 sec
Prefill latency: 0.18 sec
Bandwidth achieved: 11410.96 GB/s
FLOPS achieved: 34.23 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 3.0808 sec
Average prefill latency: 0.1781 sec
Average tokens/sec: 2512.93
Memory used: 16.44 GB
Done. we are killing the process
[rank0]:[W1114 03:32:36.592451189 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
