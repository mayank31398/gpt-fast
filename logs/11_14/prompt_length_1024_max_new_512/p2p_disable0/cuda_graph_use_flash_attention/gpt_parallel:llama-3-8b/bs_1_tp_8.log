W1114 03:22:47.792000 2512546 site-packages/torch/distributed/run.py:793] 
W1114 03:22:47.792000 2512546 site-packages/torch/distributed/run.py:793] *****************************************
W1114 03:22:47.792000 2512546 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1114 03:22:47.792000 2512546 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTParallel(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x ParallelTransformerBlock(
      semi_compiled = False
      (attention): FuseAttentionMLP(
        (wqkv1): Linear(in_features=4096, out_features=4352, bias=False)
        (wo): Linear(in_features=512, out_features=4096, bias=False)
        (w2): Linear(in_features=1792, out_features=4096, bias=False)
      )
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 2.22 seconds
CUDA_GRAPH are activate
Prefill latency: 0.03258691541850567 sec
Decode latency: 2.3490636851638556 sec
Compilation time: 2.46 seconds
Compilation time: 2.36 seconds
Compilation time: 2.45 seconds
Compilation time: 2.37 secondsCompilation time: 2.38 seconds

Compilation time: 2.47 seconds
Compilation time: 2.46 seconds
Compilation time: 2.45 seconds
Prefill latency: 0.013810476288199425 sec
Decode latency: 2.348538739606738 sec
Prefill latency: 0.01353118009865284 sec
Decode latency: 2.3475650884211063 sec
Prefill latency: 0.013596301898360252 sec
Decode latency: 2.348753111436963 sec
Prefill latency: 0.013568682596087456 sec
Decode latency: 2.34879245236516 sec
Prefill latency: 0.013469230383634567 sec
Decode latency: 2.349445415660739 sec
Time for inference 1: 2.36 sec total, 216.54 tokens/sec
Decode latency: 2.35 sec
Prefill latency: 0.01 sec
Bandwidth achieved: 605.41 GB/s
FLOPS achieved: 1.82 TF/s

Prefill latency: 0.013491563498973846 sec
Decode latency: 2.3481726720929146 sec
Time for inference 2: 2.36 sec total, 216.69 tokens/sec
Decode latency: 2.35 sec
Prefill latency: 0.01 sec
Bandwidth achieved: 605.81 GB/s
FLOPS achieved: 1.82 TF/s

Prefill latency: 0.01351238414645195 sec
Decode latency: 2.3480993900448084 sec
Time for inference 3: 2.36 sec total, 216.71 tokens/sec
Decode latency: 2.35 sec
Prefill latency: 0.01 sec
Bandwidth achieved: 605.87 GB/s
FLOPS achieved: 1.82 TF/s

Prefill latency: 0.013459999114274979 sec
Decode latency: 2.3486917223781347 sec
Time for inference 4: 2.36 sec total, 216.65 tokens/sec
Decode latency: 2.35 sec
Prefill latency: 0.01 sec
Bandwidth achieved: 605.72 GB/s
FLOPS achieved: 1.82 TF/s

Prefill latency: 0.01358870044350624 sec
Decode latency: 2.3480878062546253 sec
Time for inference 5: 2.36 sec total, 216.70 tokens/sec
Decode latency: 2.35 sec
Prefill latency: 0.01 sec
Bandwidth achieved: 605.85 GB/s
FLOPS achieved: 1.82 TF/s

Prefill latency: 0.013521397486329079 sec
Decode latency: 2.3474103678017855 sec
Time for inference 6: 2.36 sec total, 216.79 tokens/sec
Decode latency: 2.35 sec
Prefill latency: 0.01 sec
Bandwidth achieved: 606.09 GB/s
FLOPS achieved: 1.82 TF/s

Prefill latency: 0.013529384508728981 sec
Decode latency: 2.347836572676897 sec
Time for inference 7: 2.36 sec total, 216.70 tokens/sec
Decode latency: 2.35 sec
Prefill latency: 0.01 sec
Bandwidth achieved: 605.83 GB/s
FLOPS achieved: 1.82 TF/s

Prefill latency: 0.013474933803081512 sec
Decode latency: 2.3485090415924788 sec
Time for inference 8: 2.36 sec total, 216.67 tokens/sec
Decode latency: 2.35 sec
Prefill latency: 0.01 sec
Bandwidth achieved: 605.75 GB/s
FLOPS achieved: 1.82 TF/s

Prefill latency: 0.013497540727257729 sec
Decode latency: 2.3485394958406687 sec
Time for inference 9: 2.36 sec total, 216.67 tokens/sec
Decode latency: 2.35 sec
Prefill latency: 0.01 sec
Bandwidth achieved: 605.75 GB/s
FLOPS achieved: 1.82 TF/s

Prefill latency: 0.01352749951183796 sec
Decode latency: 2.3480778839439154 sec
Time for inference 10: 2.36 sec total, 216.71 tokens/sec
Decode latency: 2.35 sec
Prefill latency: 0.01 sec
Bandwidth achieved: 605.87 GB/s
FLOPS achieved: 1.82 TF/s

==========
Batch Size: 1
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 2.3483 sec
Average prefill latency: 0.0135 sec
Average tokens/sec: 216.68
Memory used: 4.74 GB
Done. we are killing the process
[rank0]:[W1114 03:23:47.745298822 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
