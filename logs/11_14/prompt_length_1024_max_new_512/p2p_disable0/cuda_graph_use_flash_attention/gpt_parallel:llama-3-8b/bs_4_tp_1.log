flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTParallel(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x ParallelTransformerBlock(
      semi_compiled = False
      (attention): FuseAttentionMLP(
        (wqkv1): Linear(in_features=4096, out_features=34816, bias=False)
        (wo): Linear(in_features=4096, out_features=4096, bias=False)
        (w2): Linear(in_features=14336, out_features=4096, bias=False)
      )
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.62 seconds
CUDA_GRAPH are activate
Prefill latency: 0.12880841456353664 sec
Decode latency: 4.779438305646181 sec
Compilation time: 4.91 seconds
Prefill latency: 0.12989026866853237 sec
Decode latency: 4.778286799788475 sec
Prefill latency: 0.12956282682716846 sec
Decode latency: 4.779107317328453 sec
Prefill latency: 0.1292160153388977 sec
Decode latency: 4.77905310690403 sec
Prefill latency: 0.12968100793659687 sec
Decode latency: 4.780411336570978 sec
Prefill latency: 0.12984780594706535 sec
Decode latency: 4.779377613216639 sec
Time for inference 1: 4.91 sec total, 417.09 tokens/sec
Decode latency: 4.78 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6260.37 GB/s
FLOPS achieved: 18.78 TF/s

Prefill latency: 0.12923563830554485 sec
Decode latency: 4.780186118558049 sec
Time for inference 2: 4.91 sec total, 417.08 tokens/sec
Decode latency: 4.78 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6260.15 GB/s
FLOPS achieved: 18.78 TF/s

Prefill latency: 0.12941484153270721 sec
Decode latency: 4.778506446629763 sec
Time for inference 3: 4.91 sec total, 417.21 tokens/sec
Decode latency: 4.78 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6262.20 GB/s
FLOPS achieved: 18.79 TF/s

Prefill latency: 0.13048678264021873 sec
Decode latency: 4.779736619442701 sec
Time for inference 4: 4.91 sec total, 417.01 tokens/sec
Decode latency: 4.78 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6259.08 GB/s
FLOPS achieved: 18.78 TF/s

Prefill latency: 0.12840338423848152 sec
Decode latency: 4.779521480202675 sec
Time for inference 5: 4.91 sec total, 417.21 tokens/sec
Decode latency: 4.78 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6262.11 GB/s
FLOPS achieved: 18.79 TF/s

Prefill latency: 0.12875287607312202 sec
Decode latency: 4.779009191319346 sec
Time for inference 6: 4.91 sec total, 417.22 tokens/sec
Decode latency: 4.78 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6262.24 GB/s
FLOPS achieved: 18.79 TF/s

Prefill latency: 0.13037701696157455 sec
Decode latency: 4.779382787644863 sec
Time for inference 7: 4.91 sec total, 417.04 tokens/sec
Decode latency: 4.78 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6259.66 GB/s
FLOPS achieved: 18.78 TF/s

Prefill latency: 0.1287899799644947 sec
Decode latency: 4.780249210074544 sec
Time for inference 8: 4.91 sec total, 417.10 tokens/sec
Decode latency: 4.78 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6260.56 GB/s
FLOPS achieved: 18.78 TF/s

Prefill latency: 0.12894163094460964 sec
Decode latency: 4.778677187860012 sec
Time for inference 9: 4.91 sec total, 417.24 tokens/sec
Decode latency: 4.78 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6262.54 GB/s
FLOPS achieved: 18.79 TF/s

Prefill latency: 0.12922973185777664 sec
Decode latency: 4.780253391712904 sec
Time for inference 10: 4.91 sec total, 417.07 tokens/sec
Decode latency: 4.78 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6259.98 GB/s
FLOPS achieved: 18.78 TF/s

==========
Batch Size: 4
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 4.7795 sec
Average prefill latency: 0.1293 sec
Average tokens/sec: 417.13
Memory used: 19.99 GB
Done. we are killing the process
[rank0]:[W1114 03:25:13.224750087 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
