W1114 05:34:14.029000 2349058 site-packages/torch/distributed/run.py:793] 
W1114 05:34:14.029000 2349058 site-packages/torch/distributed/run.py:793] *****************************************
W1114 05:34:14.029000 2349058 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1114 05:34:14.029000 2349058 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTDense(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x DenseTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=2560, bias=False)
        (wo): Linear(in_features=2048, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=14336, bias=False)
        (w2): Linear(in_features=7168, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 1.31 seconds
CUDA_GRAPH are activate
Prefill latency: 0.43338220845907927 sec
Decode latency: 14.48784822691232 sec
Compilation time: 14.88 seconds
Compilation time: 14.84 seconds
Compilation time: 14.92 seconds
Compilation time: 14.89 seconds
Prefill latency: 0.35247133765369654 sec
Decode latency: 14.47791978996247 sec
Prefill latency: 0.35147206019610167 sec
Decode latency: 14.485493337735534 sec
Prefill latency: 0.3524454412981868 sec
Decode latency: 14.478128026239574 sec
Prefill latency: 0.3530793320387602 sec
Decode latency: 14.483559731394053 sec
Prefill latency: 0.352403380908072 sec
Decode latency: 14.476039407774806 sec
Time for inference 1: 14.83 sec total, 138.10 tokens/sec
Decode latency: 14.48 sec
Prefill latency: 0.35 sec
Bandwidth achieved: 5017.07 GB/s
FLOPS achieved: 15.05 TF/s

Prefill latency: 0.3527750000357628 sec
Decode latency: 14.478964985348284 sec
Time for inference 2: 14.83 sec total, 138.07 tokens/sec
Decode latency: 14.48 sec
Prefill latency: 0.35 sec
Bandwidth achieved: 5015.98 GB/s
FLOPS achieved: 15.05 TF/s

Prefill latency: 0.3529520211741328 sec
Decode latency: 14.484757183119655 sec
Time for inference 3: 14.84 sec total, 138.01 tokens/sec
Decode latency: 14.48 sec
Prefill latency: 0.35 sec
Bandwidth achieved: 5013.94 GB/s
FLOPS achieved: 15.04 TF/s

Prefill latency: 0.35334125347435474 sec
Decode latency: 14.482395881786942 sec
Time for inference 4: 14.84 sec total, 138.03 tokens/sec
Decode latency: 14.48 sec
Prefill latency: 0.35 sec
Bandwidth achieved: 5014.58 GB/s
FLOPS achieved: 15.04 TF/s

Prefill latency: 0.3539832644164562 sec
Decode latency: 14.475493689067662 sec
Time for inference 5: 14.83 sec total, 138.09 tokens/sec
Decode latency: 14.48 sec
Prefill latency: 0.35 sec
Bandwidth achieved: 5016.70 GB/s
FLOPS achieved: 15.05 TF/s

Prefill latency: 0.3525229748338461 sec
Decode latency: 14.481054725125432 sec
Time for inference 6: 14.84 sec total, 138.05 tokens/sec
Decode latency: 14.48 sec
Prefill latency: 0.35 sec
Bandwidth achieved: 5015.34 GB/s
FLOPS achieved: 15.05 TF/s

Prefill latency: 0.35286472737789154 sec
Decode latency: 14.484920619055629 sec
Time for inference 7: 14.84 sec total, 138.01 tokens/sec
Decode latency: 14.48 sec
Prefill latency: 0.35 sec
Bandwidth achieved: 5013.90 GB/s
FLOPS achieved: 15.04 TF/s

Prefill latency: 0.3528605215251446 sec
Decode latency: 14.480731699615717 sec
Time for inference 8: 14.84 sec total, 138.05 tokens/sec
Decode latency: 14.48 sec
Prefill latency: 0.35 sec
Bandwidth achieved: 5015.31 GB/s
FLOPS achieved: 15.05 TF/s

Prefill latency: 0.35214509535580873 sec
Decode latency: 14.484946058131754 sec
Time for inference 9: 14.84 sec total, 138.02 tokens/sec
Decode latency: 14.48 sec
Prefill latency: 0.35 sec
Bandwidth achieved: 5014.13 GB/s
FLOPS achieved: 15.04 TF/s

Prefill latency: 0.353843679651618 sec
Decode latency: 14.48270289041102 sec
Time for inference 10: 14.84 sec total, 138.02 tokens/sec
Decode latency: 14.48 sec
Prefill latency: 0.35 sec
Bandwidth achieved: 5014.36 GB/s
FLOPS achieved: 15.04 TF/s

==========
Batch Size: 4
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 14.4812 sec
Average prefill latency: 0.3530 sec
Average tokens/sec: 138.05
Memory used: 55.49 GB
Done. we are killing the process
[rank0]:[W1114 05:38:08.723754782 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
