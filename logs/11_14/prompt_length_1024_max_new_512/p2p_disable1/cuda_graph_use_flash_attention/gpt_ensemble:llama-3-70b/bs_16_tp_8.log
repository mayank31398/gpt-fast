W1114 08:39:43.330000 2398725 site-packages/torch/distributed/run.py:793] 
W1114 08:39:43.330000 2398725 site-packages/torch/distributed/run.py:793] *****************************************
W1114 08:39:43.330000 2398725 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1114 08:39:43.330000 2398725 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=1280, bias=False)
        (wo): Linear(in_features=1024, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 1.42 seconds
CUDA_GRAPH are activate
Prefill latency: 0.7979347016662359 sec
Decode latency: 11.675797985866666 sec
Compilation time: 12.46 seconds
Compilation time: 12.46 seconds
Compilation time: 12.46 secondsCompilation time: 12.48 seconds

Compilation time: 12.48 seconds
Compilation time: 12.48 seconds
Compilation time: 12.46 seconds
Compilation time: 12.48 seconds
Prefill latency: 0.7663029236719012 sec
Decode latency: 11.700957335531712 sec
Prefill latency: 0.7677208371460438 sec
Decode latency: 11.625543965958059 sec
Prefill latency: 0.7674043206498027 sec
Decode latency: 11.703367077745497 sec
Prefill latency: 0.7653433559462428 sec
Decode latency: 11.686634016223252 sec
Prefill latency: 0.7667205296456814 sec
Decode latency: 11.677910617552698 sec
Time for inference 1: 12.45 sec total, 658.19 tokens/sec
Decode latency: 11.68 sec
Prefill latency: 0.77 sec
Bandwidth achieved: 12648.29 GB/s
FLOPS achieved: 37.94 TF/s

Prefill latency: 0.7700848858803511 sec
Decode latency: 11.641807608306408 sec
Time for inference 2: 12.41 sec total, 659.93 tokens/sec
Decode latency: 11.64 sec
Prefill latency: 0.77 sec
Bandwidth achieved: 12681.77 GB/s
FLOPS achieved: 38.05 TF/s

Prefill latency: 0.7661675699055195 sec
Decode latency: 11.697413426823914 sec
Time for inference 3: 12.47 sec total, 657.20 tokens/sec
Decode latency: 11.70 sec
Prefill latency: 0.77 sec
Bandwidth achieved: 12629.15 GB/s
FLOPS achieved: 37.89 TF/s

Prefill latency: 0.7680184785276651 sec
Decode latency: 11.674029333516955 sec
Time for inference 4: 12.44 sec total, 658.33 tokens/sec
Decode latency: 11.67 sec
Prefill latency: 0.77 sec
Bandwidth achieved: 12651.04 GB/s
FLOPS achieved: 37.95 TF/s

Prefill latency: 0.7686538053676486 sec
Decode latency: 11.709946828894317 sec
Time for inference 5: 12.48 sec total, 656.41 tokens/sec
Decode latency: 11.71 sec
Prefill latency: 0.77 sec
Bandwidth achieved: 12614.01 GB/s
FLOPS achieved: 37.84 TF/s

Prefill latency: 0.7667417675256729 sec
Decode latency: 11.672713059931993 sec
Time for inference 6: 12.44 sec total, 658.47 tokens/sec
Decode latency: 11.67 sec
Prefill latency: 0.77 sec
Bandwidth achieved: 12653.68 GB/s
FLOPS achieved: 37.96 TF/s

Prefill latency: 0.767516284249723 sec
Decode latency: 11.678287335671484 sec
Time for inference 7: 12.45 sec total, 658.14 tokens/sec
Decode latency: 11.68 sec
Prefill latency: 0.77 sec
Bandwidth achieved: 12647.26 GB/s
FLOPS achieved: 37.94 TF/s

Prefill latency: 0.7666553221642971 sec
Decode latency: 11.647392911836505 sec
Time for inference 8: 12.42 sec total, 659.82 tokens/sec
Decode latency: 11.65 sec
Prefill latency: 0.77 sec
Bandwidth achieved: 12679.67 GB/s
FLOPS achieved: 38.04 TF/s

Prefill latency: 0.7643645359203219 sec
Decode latency: 11.668217182159424 sec
Time for inference 9: 12.43 sec total, 658.82 tokens/sec
Decode latency: 11.67 sec
Prefill latency: 0.76 sec
Bandwidth achieved: 12660.37 GB/s
FLOPS achieved: 37.98 TF/s

Prefill latency: 0.7687122076749802 sec
Decode latency: 11.68831393122673 sec
Time for inference 10: 12.46 sec total, 657.54 tokens/sec
Decode latency: 11.69 sec
Prefill latency: 0.77 sec
Bandwidth achieved: 12635.86 GB/s
FLOPS achieved: 37.91 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 11.6756 sec
Average prefill latency: 0.7674 sec
Average tokens/sec: 658.29
Memory used: 49.88 GB
Done. we are killing the process
[rank0]:[W1114 08:43:14.444597822 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
