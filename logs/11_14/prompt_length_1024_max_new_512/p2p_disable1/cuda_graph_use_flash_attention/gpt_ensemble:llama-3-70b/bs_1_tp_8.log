W1114 06:20:06.647000 2362501 site-packages/torch/distributed/run.py:793] 
W1114 06:20:06.647000 2362501 site-packages/torch/distributed/run.py:793] *****************************************
W1114 06:20:06.647000 2362501 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1114 06:20:06.647000 2362501 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=1280, bias=False)
        (wo): Linear(in_features=1024, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 1.82 seconds
CUDA_GRAPH are activate
Prefill latency: 0.08978214208036661 sec
Decode latency: 9.751441346481442 sec
Compilation time: 9.86 seconds
Compilation time: 9.83 seconds
Compilation time: 9.87 seconds
Compilation time: 9.87 seconds
Compilation time: 9.84 seconds
Compilation time: 9.84 seconds
Compilation time: 9.83 seconds
Compilation time: 9.85 seconds
Prefill latency: 0.06453388091176748 sec
Decode latency: 9.74729315098375 sec
Prefill latency: 0.06393282767385244 sec
Decode latency: 9.751631934195757 sec
Prefill latency: 0.06470366381108761 sec
Decode latency: 9.752265610732138 sec
Prefill latency: 0.06518249586224556 sec
Decode latency: 9.731115101836622 sec
Prefill latency: 0.06414123252034187 sec
Decode latency: 9.760543989948928 sec
Time for inference 1: 9.83 sec total, 52.11 tokens/sec
Decode latency: 9.76 sec
Prefill latency: 0.06 sec
Bandwidth achieved: 1001.33 GB/s
FLOPS achieved: 3.00 TF/s

Prefill latency: 0.06466041319072247 sec
Decode latency: 9.73951571714133 sec
Time for inference 2: 9.81 sec total, 52.22 tokens/sec
Decode latency: 9.74 sec
Prefill latency: 0.06 sec
Bandwidth achieved: 1003.42 GB/s
FLOPS achieved: 3.01 TF/s

Prefill latency: 0.0650236876681447 sec
Decode latency: 9.742930673994124 sec
Time for inference 3: 9.81 sec total, 52.20 tokens/sec
Decode latency: 9.74 sec
Prefill latency: 0.07 sec
Bandwidth achieved: 1003.04 GB/s
FLOPS achieved: 3.01 TF/s

Prefill latency: 0.06492783408612013 sec
Decode latency: 9.754460349678993 sec
Time for inference 4: 9.82 sec total, 52.14 tokens/sec
Decode latency: 9.75 sec
Prefill latency: 0.06 sec
Bandwidth achieved: 1001.87 GB/s
FLOPS achieved: 3.01 TF/s

Prefill latency: 0.06511121895164251 sec
Decode latency: 9.756790866144001 sec
Time for inference 5: 9.82 sec total, 52.12 tokens/sec
Decode latency: 9.76 sec
Prefill latency: 0.07 sec
Bandwidth achieved: 1001.61 GB/s
FLOPS achieved: 3.00 TF/s

Prefill latency: 0.06885821279138327 sec
Decode latency: 9.742066907696426 sec
Time for inference 6: 9.81 sec total, 52.18 tokens/sec
Decode latency: 9.74 sec
Prefill latency: 0.07 sec
Bandwidth achieved: 1002.74 GB/s
FLOPS achieved: 3.01 TF/s

Prefill latency: 0.06879263464361429 sec
Decode latency: 9.739242526702583 sec
Time for inference 7: 9.81 sec total, 52.20 tokens/sec
Decode latency: 9.74 sec
Prefill latency: 0.07 sec
Bandwidth achieved: 1003.03 GB/s
FLOPS achieved: 3.01 TF/s

Prefill latency: 0.06423377897590399 sec
Decode latency: 9.732701260596514 sec
Time for inference 8: 9.80 sec total, 52.25 tokens/sec
Decode latency: 9.73 sec
Prefill latency: 0.06 sec
Bandwidth achieved: 1004.16 GB/s
FLOPS achieved: 3.01 TF/s

Prefill latency: 0.0687814150005579 sec
Decode latency: 9.746938078664243 sec
Time for inference 9: 9.82 sec total, 52.15 tokens/sec
Decode latency: 9.75 sec
Prefill latency: 0.07 sec
Bandwidth achieved: 1002.24 GB/s
FLOPS achieved: 3.01 TF/s

Prefill latency: 0.06480029597878456 sec
Decode latency: 9.736874060705304 sec
Time for inference 10: 9.80 sec total, 52.23 tokens/sec
Decode latency: 9.74 sec
Prefill latency: 0.06 sec
Bandwidth achieved: 1003.67 GB/s
FLOPS achieved: 3.01 TF/s

==========
Batch Size: 1
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 9.7452 sec
Average prefill latency: 0.0659 sec
Average tokens/sec: 52.18
Memory used: 23.20 GB
Done. we are killing the process
[rank0]:[W1114 06:22:51.904020826 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
