W1114 08:45:52.198000 2401145 site-packages/torch/distributed/run.py:793] 
W1114 08:45:52.198000 2401145 site-packages/torch/distributed/run.py:793] *****************************************
W1114 08:45:52.198000 2401145 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1114 08:45:52.198000 2401145 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTParallel(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x ParallelTransformerBlock(
      semi_compiled = False
      (attention): FuseAttentionMLP(
        (wqkv1): Linear(in_features=8192, out_features=8448, bias=False)
        (wo): Linear(in_features=1024, out_features=8192, bias=False)
        (w2): Linear(in_features=3584, out_features=8192, bias=False)
      )
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 1.65 seconds
CUDA_GRAPH are activate
Prefill latency: 0.9801498707383871 sec
Decode latency: 11.363566722720861 sec
Compilation time: 12.31 seconds
Compilation time: 12.35 seconds
Compilation time: 12.13 seconds
Compilation time: 12.13 seconds
Compilation time: 12.13 seconds
Compilation time: 12.35 seconds
Compilation time: 12.12 seconds
Compilation time: 12.31 seconds
Prefill latency: 0.7424822011962533 sec
Decode latency: 11.393469084054232 sec
Prefill latency: 0.7432386092841625 sec
Decode latency: 11.378049280494452 sec
Prefill latency: 0.743881325237453 sec
Decode latency: 11.359340856783092 sec
Prefill latency: 0.7444338612258434 sec
Decode latency: 11.384197449311614 sec
Prefill latency: 0.7419084431603551 sec
Decode latency: 11.374497869051993 sec
Time for inference 1: 12.12 sec total, 676.04 tokens/sec
Decode latency: 11.37 sec
Prefill latency: 0.74 sec
Bandwidth achieved: 12990.40 GB/s
FLOPS achieved: 38.97 TF/s

Prefill latency: 0.7400467302650213 sec
Decode latency: 11.401095489040017 sec
Time for inference 2: 12.14 sec total, 674.64 tokens/sec
Decode latency: 11.40 sec
Prefill latency: 0.74 sec
Bandwidth achieved: 12963.59 GB/s
FLOPS achieved: 38.89 TF/s

Prefill latency: 0.7442982327193022 sec
Decode latency: 11.391827965155244 sec
Time for inference 3: 12.14 sec total, 674.93 tokens/sec
Decode latency: 11.39 sec
Prefill latency: 0.74 sec
Bandwidth achieved: 12969.06 GB/s
FLOPS achieved: 38.91 TF/s

Prefill latency: 0.7450208673253655 sec
Decode latency: 11.41206399910152 sec
Time for inference 4: 12.16 sec total, 673.75 tokens/sec
Decode latency: 11.41 sec
Prefill latency: 0.75 sec
Bandwidth achieved: 12946.39 GB/s
FLOPS achieved: 38.84 TF/s

Prefill latency: 0.7447905279695988 sec
Decode latency: 11.368595825508237 sec
Time for inference 5: 12.11 sec total, 676.19 tokens/sec
Decode latency: 11.37 sec
Prefill latency: 0.74 sec
Bandwidth achieved: 12993.32 GB/s
FLOPS achieved: 38.98 TF/s

Prefill latency: 0.7431334750726819 sec
Decode latency: 11.388699394650757 sec
Time for inference 6: 12.13 sec total, 675.18 tokens/sec
Decode latency: 11.39 sec
Prefill latency: 0.74 sec
Bandwidth achieved: 12973.90 GB/s
FLOPS achieved: 38.92 TF/s

Prefill latency: 0.7421036222949624 sec
Decode latency: 11.404591666534543 sec
Time for inference 7: 12.15 sec total, 674.35 tokens/sec
Decode latency: 11.40 sec
Prefill latency: 0.74 sec
Bandwidth achieved: 12957.84 GB/s
FLOPS achieved: 38.87 TF/s

Prefill latency: 0.7454612040892243 sec
Decode latency: 11.383726549334824 sec
Time for inference 8: 12.13 sec total, 675.32 tokens/sec
Decode latency: 11.38 sec
Prefill latency: 0.75 sec
Bandwidth achieved: 12976.63 GB/s
FLOPS achieved: 38.93 TF/s

Prefill latency: 0.7443149434402585 sec
Decode latency: 11.399471615441144 sec
Time for inference 9: 12.15 sec total, 674.51 tokens/sec
Decode latency: 11.40 sec
Prefill latency: 0.74 sec
Bandwidth achieved: 12961.04 GB/s
FLOPS achieved: 38.88 TF/s

Prefill latency: 0.7441526744514704 sec
Decode latency: 11.359570373781025 sec
Time for inference 10: 12.10 sec total, 676.75 tokens/sec
Decode latency: 11.36 sec
Prefill latency: 0.74 sec
Bandwidth achieved: 13003.96 GB/s
FLOPS achieved: 39.01 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 11.3884 sec
Average prefill latency: 0.7435 sec
Average tokens/sec: 675.17
Memory used: 50.16 GB
Done. we are killing the process
[rank0]:[W1114 08:49:19.819420642 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
