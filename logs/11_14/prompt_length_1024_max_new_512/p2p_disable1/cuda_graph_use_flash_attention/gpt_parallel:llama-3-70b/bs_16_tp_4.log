W1114 07:27:05.039000 2386617 site-packages/torch/distributed/run.py:793] 
W1114 07:27:05.039000 2386617 site-packages/torch/distributed/run.py:793] *****************************************
W1114 07:27:05.039000 2386617 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1114 07:27:05.039000 2386617 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTParallel(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x ParallelTransformerBlock(
      semi_compiled = False
      (attention): FuseAttentionMLP(
        (wqkv1): Linear(in_features=8192, out_features=16896, bias=False)
        (wo): Linear(in_features=2048, out_features=8192, bias=False)
        (w2): Linear(in_features=7168, out_features=8192, bias=False)
      )
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 1.34 seconds
CUDA_GRAPH are activate
Prefill latency: 1.3296774756163359 sec
Decode latency: 13.257431372068822 sec
Compilation time: 14.57 seconds
Compilation time: 14.59 seconds
Compilation time: 14.57 seconds
Compilation time: 14.57 seconds
Prefill latency: 1.3079823469743133 sec
Decode latency: 13.256022060289979 sec
Prefill latency: 1.3076213505119085 sec
Decode latency: 13.2560939732939 sec
Prefill latency: 1.310254099778831 sec
Decode latency: 13.255393367260695 sec
Prefill latency: 1.3096236130222678 sec
Decode latency: 13.254613071680069 sec
Prefill latency: 1.3068557465448976 sec
Decode latency: 13.255102209746838 sec
Time for inference 1: 14.56 sec total, 562.51 tokens/sec
Decode latency: 13.26 sec
Prefill latency: 1.31 sec
Bandwidth achieved: 20435.03 GB/s
FLOPS achieved: 61.31 TF/s

Prefill latency: 1.3091639103367925 sec
Decode latency: 13.255141931585968 sec
Time for inference 2: 14.57 sec total, 562.42 tokens/sec
Decode latency: 13.26 sec
Prefill latency: 1.31 sec
Bandwidth achieved: 20431.61 GB/s
FLOPS achieved: 61.29 TF/s

Prefill latency: 1.312766533344984 sec
Decode latency: 13.256105605512857 sec
Time for inference 3: 14.57 sec total, 562.25 tokens/sec
Decode latency: 13.26 sec
Prefill latency: 1.31 sec
Bandwidth achieved: 20425.37 GB/s
FLOPS achieved: 61.28 TF/s

Prefill latency: 1.309943113476038 sec
Decode latency: 13.260234742425382 sec
Time for inference 4: 14.57 sec total, 562.19 tokens/sec
Decode latency: 13.26 sec
Prefill latency: 1.31 sec
Bandwidth achieved: 20423.42 GB/s
FLOPS achieved: 61.27 TF/s

Prefill latency: 1.3120019407942891 sec
Decode latency: 13.257277743890882 sec
Time for inference 5: 14.57 sec total, 562.23 tokens/sec
Decode latency: 13.26 sec
Prefill latency: 1.31 sec
Bandwidth achieved: 20424.78 GB/s
FLOPS achieved: 61.27 TF/s

Prefill latency: 1.3098132088780403 sec
Decode latency: 13.256602592766285 sec
Time for inference 6: 14.57 sec total, 562.34 tokens/sec
Decode latency: 13.26 sec
Prefill latency: 1.31 sec
Bandwidth achieved: 20428.75 GB/s
FLOPS achieved: 61.29 TF/s

Prefill latency: 1.3091671941801906 sec
Decode latency: 13.255432453006506 sec
Time for inference 7: 14.57 sec total, 562.41 tokens/sec
Decode latency: 13.26 sec
Prefill latency: 1.31 sec
Bandwidth achieved: 20431.31 GB/s
FLOPS achieved: 61.29 TF/s

Prefill latency: 1.3053674288094044 sec
Decode latency: 13.257862895727158 sec
Time for inference 8: 14.56 sec total, 562.46 tokens/sec
Decode latency: 13.26 sec
Prefill latency: 1.31 sec
Bandwidth achieved: 20433.13 GB/s
FLOPS achieved: 61.30 TF/s

Prefill latency: 1.3069833992049098 sec
Decode latency: 13.25490730907768 sec
Time for inference 9: 14.56 sec total, 562.51 tokens/sec
Decode latency: 13.25 sec
Prefill latency: 1.31 sec
Bandwidth achieved: 20435.13 GB/s
FLOPS achieved: 61.31 TF/s

Prefill latency: 1.3052583457902074 sec
Decode latency: 13.257329728454351 sec
Time for inference 10: 14.56 sec total, 562.49 tokens/sec
Decode latency: 13.26 sec
Prefill latency: 1.31 sec
Bandwidth achieved: 20434.07 GB/s
FLOPS achieved: 61.30 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 13.2566 sec
Average prefill latency: 1.3087 sec
Average tokens/sec: 562.38
Memory used: 68.95 GB
Done. we are killing the process
[rank0]:[W1114 07:31:01.508994812 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
