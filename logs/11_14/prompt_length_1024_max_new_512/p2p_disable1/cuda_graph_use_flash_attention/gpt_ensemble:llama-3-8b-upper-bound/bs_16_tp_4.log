W1114 08:14:46.784000 2775897 site-packages/torch/distributed/run.py:793] 
W1114 08:14:46.784000 2775897 site-packages/torch/distributed/run.py:793] *****************************************
W1114 08:14:46.784000 2775897 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1114 08:14:46.784000 2775897 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=1536, bias=False)
        (wo): Linear(in_features=1024, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.10 seconds
CUDA_GRAPH are activate
Prefill latency: 0.16565762273967266 sec
Decode latency: 2.897688314318657 sec
Compilation time: 3.06 seconds
Prefill latency: 0.16738637536764145 sec
Compilation time: 3.05 seconds
Compilation time: 3.03 seconds
Compilation time: 3.02 seconds
Decode latency: 2.897689903154969 sec
Prefill latency: 0.1669467631727457 sec
Decode latency: 2.8963142428547144 sec
Prefill latency: 0.16762301325798035 sec
Decode latency: 2.8966582287102938 sec
Prefill latency: 0.16823049634695053 sec
Decode latency: 2.8966148123145103 sec
Prefill latency: 0.16828975640237331 sec
Decode latency: 2.895938375964761 sec
Time for inference 1: 3.06 sec total, 2672.81 tokens/sec
Decode latency: 2.90 sec
Prefill latency: 0.17 sec
Bandwidth achieved: 12136.88 GB/s
FLOPS achieved: 36.41 TF/s

Prefill latency: 0.16672014445066452 sec
Decode latency: 2.8973404578864574 sec
Time for inference 2: 3.06 sec total, 2672.98 tokens/sec
Decode latency: 2.90 sec
Prefill latency: 0.17 sec
Bandwidth achieved: 12137.66 GB/s
FLOPS achieved: 36.41 TF/s

Prefill latency: 0.16656920686364174 sec
Decode latency: 2.8969784565269947 sec
Time for inference 3: 3.06 sec total, 2673.40 tokens/sec
Decode latency: 2.90 sec
Prefill latency: 0.17 sec
Bandwidth achieved: 12139.55 GB/s
FLOPS achieved: 36.42 TF/s

Prefill latency: 0.16630900651216507 sec
Decode latency: 2.896234340965748 sec
Time for inference 4: 3.06 sec total, 2674.29 tokens/sec
Decode latency: 2.90 sec
Prefill latency: 0.17 sec
Bandwidth achieved: 12143.58 GB/s
FLOPS achieved: 36.43 TF/s

Prefill latency: 0.1673869676887989 sec
Decode latency: 2.898002479225397 sec
Time for inference 5: 3.07 sec total, 2671.64 tokens/sec
Decode latency: 2.90 sec
Prefill latency: 0.17 sec
Bandwidth achieved: 12131.58 GB/s
FLOPS achieved: 36.39 TF/s

Prefill latency: 0.1675912830978632 sec
Decode latency: 2.896835293620825 sec
Time for inference 6: 3.07 sec total, 2672.41 tokens/sec
Decode latency: 2.90 sec
Prefill latency: 0.17 sec
Bandwidth achieved: 12135.06 GB/s
FLOPS achieved: 36.41 TF/s

Prefill latency: 0.16824138723313808 sec
Decode latency: 2.8971863966435194 sec
Time for inference 7: 3.07 sec total, 2671.60 tokens/sec
Decode latency: 2.90 sec
Prefill latency: 0.17 sec
Bandwidth achieved: 12131.38 GB/s
FLOPS achieved: 36.39 TF/s

Prefill latency: 0.16935423389077187 sec
Decode latency: 2.8969835732132196 sec
Time for inference 8: 3.07 sec total, 2670.93 tokens/sec
Decode latency: 2.90 sec
Prefill latency: 0.17 sec
Bandwidth achieved: 12128.34 GB/s
FLOPS achieved: 36.39 TF/s

Prefill latency: 0.1666810717433691 sec
Decode latency: 2.8969201892614365 sec
Time for inference 9: 3.06 sec total, 2673.36 tokens/sec
Decode latency: 2.90 sec
Prefill latency: 0.17 sec
Bandwidth achieved: 12139.38 GB/s
FLOPS achieved: 36.42 TF/s

Prefill latency: 0.1674775332212448 sec
[rank3]:[W1114 08:15:42.912426500 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Decode latency: 2.8971518222242594 sec
Time for inference 10: 3.07 sec total, 2672.47 tokens/sec
Decode latency: 2.90 sec
Prefill latency: 0.17 sec
Bandwidth achieved: 12135.34 GB/s
FLOPS achieved: 36.41 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 2.8970 sec
Average prefill latency: 0.1675 sec
Average tokens/sec: 2672.59
Memory used: 12.10 GB
[rank0]:[W1114 08:15:42.970676025 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank1]:[W1114 08:15:42.165929278 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank2]:[W1114 08:15:42.613531362 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Done. we are killing the process
[rank0]:[W1114 08:15:45.545324192 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
