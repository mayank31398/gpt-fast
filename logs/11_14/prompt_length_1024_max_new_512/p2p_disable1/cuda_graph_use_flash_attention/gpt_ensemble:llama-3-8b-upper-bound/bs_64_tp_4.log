W1114 08:21:09.570000 2779372 site-packages/torch/distributed/run.py:793] 
W1114 08:21:09.570000 2779372 site-packages/torch/distributed/run.py:793] *****************************************
W1114 08:21:09.570000 2779372 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1114 08:21:09.570000 2779372 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=1536, bias=False)
        (wo): Linear(in_features=1024, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.74 seconds
CUDA_GRAPH are activate
Prefill latency: 0.6346849072724581 sec
Compilation time: 3.98 seconds
Decode latency: 3.475270824506879 sec
Compilation time: 4.11 seconds
Compilation time: 4.11 seconds
Compilation time: 4.09 seconds
Prefill latency: 0.6350241601467133 sec
Decode latency: 3.4433588720858097 sec
Prefill latency: 0.6359635181725025 sec
Decode latency: 3.3127418234944344 sec
Prefill latency: 0.6348931584507227 sec
Decode latency: 3.480660241097212 sec
Prefill latency: 0.6353494171053171 sec
Decode latency: 3.481650162488222 sec
Prefill latency: 0.6351828519254923 sec
Decode latency: 3.4811996929347515 sec
Time for inference 1: 4.12 sec total, 7959.00 tokens/sec
Decode latency: 3.48 sec
Prefill latency: 0.64 sec
Bandwidth achieved: 36140.74 GB/s
FLOPS achieved: 108.42 TF/s

Prefill latency: 0.6378542352467775 sec
Decode latency: 3.3299880363047123 sec
Time for inference 2: 3.97 sec total, 8256.90 tokens/sec
Decode latency: 3.33 sec
Prefill latency: 0.64 sec
Bandwidth achieved: 37493.49 GB/s
FLOPS achieved: 112.48 TF/s

Prefill latency: 0.6344238128513098 sec
Decode latency: 3.477972125634551 sec
Time for inference 3: 4.11 sec total, 7966.81 tokens/sec
Decode latency: 3.48 sec
Prefill latency: 0.63 sec
Bandwidth achieved: 36176.22 GB/s
FLOPS achieved: 108.53 TF/s

Prefill latency: 0.636163542047143 sec
Decode latency: 3.3411279749125242 sec
Time for inference 4: 3.98 sec total, 8237.04 tokens/sec
Decode latency: 3.34 sec
Prefill latency: 0.64 sec
Bandwidth achieved: 37403.30 GB/s
FLOPS achieved: 112.21 TF/s

Prefill latency: 0.6320980116724968 sec
Decode latency: 3.472142294049263 sec
Time for inference 5: 4.11 sec total, 7982.34 tokens/sec
Decode latency: 3.47 sec
Prefill latency: 0.63 sec
Bandwidth achieved: 36246.73 GB/s
FLOPS achieved: 108.74 TF/s

Prefill latency: 0.6336044296622276 sec
Decode latency: 3.306392328813672 sec
Time for inference 6: 3.94 sec total, 8315.13 tokens/sec
Decode latency: 3.31 sec
Prefill latency: 0.63 sec
Bandwidth achieved: 37757.88 GB/s
FLOPS achieved: 113.27 TF/s

Prefill latency: 0.6351435277611017 sec
Decode latency: 3.482094557955861 sec
Time for inference 7: 4.12 sec total, 7957.38 tokens/sec
Decode latency: 3.48 sec
Prefill latency: 0.64 sec
Bandwidth achieved: 36133.40 GB/s
FLOPS achieved: 108.40 TF/s

Prefill latency: 0.636232016608119 sec
Decode latency: 3.36464106105268 sec
Time for inference 8: 4.00 sec total, 8188.74 tokens/sec
Decode latency: 3.36 sec
Prefill latency: 0.64 sec
Bandwidth achieved: 37184.00 GB/s
FLOPS achieved: 111.55 TF/s

Prefill latency: 0.636062940582633 sec
Decode latency: 3.430649636313319 sec
Time for inference 9: 4.07 sec total, 8056.27 tokens/sec
Decode latency: 3.43 sec
Prefill latency: 0.64 sec
Bandwidth achieved: 36582.47 GB/s
FLOPS achieved: 109.75 TF/s

Prefill latency: 0.6356297805905342 sec
[rank1]:[W1114 08:22:20.276751525 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Decode latency: 3.4300785288214684 sec
Time for inference 10: 4.07 sec total, 8058.28 tokens/sec
Decode latency: 3.43 sec
Prefill latency: 0.64 sec
Bandwidth achieved: 36591.56 GB/s
FLOPS achieved: 109.77 TF/s

==========
Batch Size: 64
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 3.4116 sec
Average prefill latency: 0.6352 sec
Average tokens/sec: 8097.79
Memory used: 31.14 GB
[rank0]:[W1114 08:22:21.806302746 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank3]:[W1114 08:22:21.461359744 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank2]:[W1114 08:22:21.516749617 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Done. we are killing the process
[rank0]:[W1114 08:22:25.742625270 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
