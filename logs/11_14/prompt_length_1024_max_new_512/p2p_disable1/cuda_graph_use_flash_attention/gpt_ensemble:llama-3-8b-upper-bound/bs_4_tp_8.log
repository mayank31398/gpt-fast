W1114 08:10:58.816000 2773657 site-packages/torch/distributed/run.py:793] 
W1114 08:10:58.816000 2773657 site-packages/torch/distributed/run.py:793] *****************************************
W1114 08:10:58.816000 2773657 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1114 08:10:58.816000 2773657 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=768, bias=False)
        (wo): Linear(in_features=512, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=3584, bias=False)
        (w2): Linear(in_features=1792, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 2.05 seconds
CUDA_GRAPH are activate
Prefill latency: 0.02738644741475582 sec
Compilation time: 2.24 seconds
Decode latency: 2.281256038695574 sec
Compilation time: 2.31 seconds
Prefill latency: 0.027229920029640198 sec
Compilation time: 2.31 seconds
Compilation time: 2.36 seconds
Compilation time: 2.36 seconds
Compilation time: 2.31 seconds
Compilation time: 2.32 seconds
Compilation time: 2.31 seconds
Decode latency: 2.281018190085888 sec
Prefill latency: 0.027269409969449043 sec
Decode latency: 2.2801699079573154 sec
Prefill latency: 0.027239777147769928 sec
Decode latency: 2.2564078625291586 sec
Prefill latency: 0.0272737555205822 sec
Decode latency: 2.2811756879091263 sec
Prefill latency: 0.0272931270301342 sec
Decode latency: 2.2806637324392796 sec
Time for inference 1: 2.31 sec total, 887.10 tokens/sec
Decode latency: 2.28 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 2480.35 GB/s
FLOPS achieved: 7.44 TF/s

Prefill latency: 0.027289709076285362 sec
Decode latency: 2.2192958295345306 sec
Time for inference 2: 2.25 sec total, 911.33 tokens/sec
Decode latency: 2.22 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 2548.12 GB/s
FLOPS achieved: 7.64 TF/s

Prefill latency: 0.02725948579609394 sec
Decode latency: 2.281170468777418 sec
Time for inference 3: 2.31 sec total, 886.89 tokens/sec
Decode latency: 2.28 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 2479.78 GB/s
FLOPS achieved: 7.44 TF/s

Prefill latency: 0.027288038283586502 sec
Decode latency: 2.155630676075816 sec
Time for inference 4: 2.18 sec total, 937.88 tokens/sec
Decode latency: 2.16 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 2622.36 GB/s
FLOPS achieved: 7.87 TF/s

Prefill latency: 0.02731294371187687 sec
Decode latency: 2.281282603740692 sec
Time for inference 5: 2.31 sec total, 886.83 tokens/sec
Decode latency: 2.28 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 2479.62 GB/s
FLOPS achieved: 7.44 TF/s

Prefill latency: 0.027311798185110092 sec
Decode latency: 2.2807370480149984 sec
Time for inference 6: 2.31 sec total, 887.05 tokens/sec
Decode latency: 2.28 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 2480.23 GB/s
FLOPS achieved: 7.44 TF/s

Prefill latency: 0.02731296420097351 sec
Decode latency: 2.281649947166443 sec
Time for inference 7: 2.31 sec total, 886.71 tokens/sec
Decode latency: 2.28 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 2479.27 GB/s
FLOPS achieved: 7.44 TF/s

Prefill latency: 0.02729884162545204 sec
Decode latency: 2.2802776508033276 sec
Time for inference 8: 2.31 sec total, 887.25 tokens/sec
Decode latency: 2.28 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 2480.78 GB/s
FLOPS achieved: 7.44 TF/s

Prefill latency: 0.027298742905259132 sec
Decode latency: 2.281158836558461 sec
Time for inference 9: 2.31 sec total, 886.88 tokens/sec
Decode latency: 2.28 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 2479.74 GB/s
FLOPS achieved: 7.44 TF/s

Prefill latency: 0.027296096086502075 sec
Decode latency: 2.227798819541931 sec
Time for inference 10: 2.26 sec total, 907.83 tokens/sec
Decode latency: 2.23 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 2538.32 GB/s
FLOPS achieved: 7.61 TF/s

==========
Batch Size: 4
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 2.2570 sec
Average prefill latency: 0.0273 sec
Average tokens/sec: 896.57
Memory used: 5.35 GB
[rank0]:[W1114 08:11:43.727270366 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank1]:[W1114 08:11:43.012895463 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank4]:[W1114 08:11:43.132727441 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank5]:[W1114 08:11:43.456573089 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank2]:[W1114 08:11:43.679578059 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank7]:[W1114 08:11:44.821407482 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank6]:[W1114 08:11:44.869774052 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank3]:[W1114 08:11:44.528800873 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Done. we are killing the process
[rank0]:[W1114 08:11:52.643279494 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
