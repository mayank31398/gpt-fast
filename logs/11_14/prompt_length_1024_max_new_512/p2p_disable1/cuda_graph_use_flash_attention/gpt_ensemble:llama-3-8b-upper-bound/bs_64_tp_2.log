W1114 08:19:26.909000 2778586 site-packages/torch/distributed/run.py:793] 
W1114 08:19:26.909000 2778586 site-packages/torch/distributed/run.py:793] *****************************************
W1114 08:19:26.909000 2778586 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1114 08:19:26.909000 2778586 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=3072, bias=False)
        (wo): Linear(in_features=2048, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=14336, bias=False)
        (w2): Linear(in_features=7168, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.23 seconds
CUDA_GRAPH are activate
Prefill latency: 1.2166774701327085 sec
Decode latency: 4.591047191992402 sec
Compilation time: 5.81 seconds
Compilation time: 5.74 seconds
Prefill latency: 1.212993636727333 sec
Decode latency: 4.588070264086127 sec
Prefill latency: 1.2067370042204857 sec
Decode latency: 4.496939500793815 sec
Prefill latency: 1.2122754100710154 sec
Decode latency: 4.589633049443364 sec
Prefill latency: 1.2113373391330242 sec
Decode latency: 4.590233702212572 sec
Prefill latency: 1.2121539674699306 sec
Decode latency: 4.5894508343189955 sec
Time for inference 1: 5.80 sec total, 5647.10 tokens/sec
Decode latency: 4.59 sec
Prefill latency: 1.21 sec
Bandwidth achieved: 45349.21 GB/s
FLOPS achieved: 136.05 TF/s

Prefill latency: 1.2084754072129726 sec
Decode latency: 4.589639676734805 sec
Time for inference 2: 5.80 sec total, 5650.51 tokens/sec
Decode latency: 4.59 sec
Prefill latency: 1.21 sec
Bandwidth achieved: 45376.54 GB/s
FLOPS achieved: 136.13 TF/s

Prefill latency: 1.2096477430313826 sec
Decode latency: 4.589663298800588 sec
Time for inference 3: 5.80 sec total, 5649.30 tokens/sec
Decode latency: 4.59 sec
Prefill latency: 1.21 sec
Bandwidth achieved: 45366.89 GB/s
FLOPS achieved: 136.10 TF/s

Prefill latency: 1.2081068325787783 sec
Decode latency: 4.590845379978418 sec
Time for inference 4: 5.80 sec total, 5649.43 tokens/sec
Decode latency: 4.59 sec
Prefill latency: 1.21 sec
Bandwidth achieved: 45367.92 GB/s
FLOPS achieved: 136.10 TF/s

Prefill latency: 1.208356972783804 sec
Decode latency: 4.589354503899813 sec
Time for inference 5: 5.80 sec total, 5650.93 tokens/sec
Decode latency: 4.59 sec
Prefill latency: 1.21 sec
Bandwidth achieved: 45379.92 GB/s
FLOPS achieved: 136.14 TF/s

Prefill latency: 1.2100156005471945 sec
Decode latency: 4.589223280549049 sec
Time for inference 6: 5.80 sec total, 5649.47 tokens/sec
Decode latency: 4.59 sec
Prefill latency: 1.21 sec
Bandwidth achieved: 45368.25 GB/s
FLOPS achieved: 136.10 TF/s

Prefill latency: 1.2098326552659273 sec
Decode latency: 4.589220670983195 sec
Time for inference 7: 5.80 sec total, 5649.69 tokens/sec
Decode latency: 4.59 sec
Prefill latency: 1.21 sec
Bandwidth achieved: 45369.97 GB/s
FLOPS achieved: 136.11 TF/s

Prefill latency: 1.2104285974055529 sec
Decode latency: 4.589973259717226 sec
Time for inference 8: 5.80 sec total, 5648.15 tokens/sec
Decode latency: 4.59 sec
Prefill latency: 1.21 sec
Bandwidth achieved: 45357.64 GB/s
FLOPS achieved: 136.07 TF/s

Prefill latency: 1.2100547198206186 sec
Decode latency: 4.58958644233644 sec
Time for inference 9: 5.80 sec total, 5648.98 tokens/sec
Decode latency: 4.59 sec
Prefill latency: 1.21 sec
Bandwidth achieved: 45364.27 GB/s
FLOPS achieved: 136.09 TF/s

Prefill latency: 1.2113468144088984 sec
[rank1]:[W1114 08:21:04.301621377 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Decode latency: 4.589814113453031 sec
Time for inference 10: 5.80 sec total, 5647.55 tokens/sec
Decode latency: 4.59 sec
Prefill latency: 1.21 sec
Bandwidth achieved: 45352.84 GB/s
FLOPS achieved: 136.06 TF/s

==========
Batch Size: 64
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 4.5897 sec
Average prefill latency: 1.2098 sec
Average tokens/sec: 5649.11
Memory used: 39.95 GB
[rank0]:[W1114 08:21:05.869372286 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Done. we are killing the process
[rank0]:[W1114 08:21:06.036592849 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
