flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
        (wo): Linear(in_features=4096, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=28672, bias=False)
        (w2): Linear(in_features=14336, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.17 seconds
CUDA_GRAPH are activate
Prefill latency: 0.12948491051793098 sec
Decode latency: 4.677932366728783 sec
Compilation time: 4.81 seconds
Prefill latency: 0.1294783316552639 sec
Decode latency: 4.676152346655726 sec
Prefill latency: 0.13159889541566372 sec
Decode latency: 4.585792848840356 sec
Prefill latency: 0.13022545911371708 sec
Decode latency: 4.674958633258939 sec
Prefill latency: 0.1302531398832798 sec
Decode latency: 4.677098078653216 sec
Prefill latency: 0.12976428866386414 sec
Decode latency: 4.677170563489199 sec
Time for inference 1: 4.81 sec total, 425.97 tokens/sec
Decode latency: 4.68 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6393.72 GB/s
FLOPS achieved: 19.18 TF/s

Prefill latency: 0.1293568890541792 sec
Decode latency: 4.676481522619724 sec
Time for inference 2: 4.81 sec total, 426.07 tokens/sec
Decode latency: 4.68 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6395.29 GB/s
FLOPS achieved: 19.19 TF/s

Prefill latency: 0.131216861307621 sec
Decode latency: 4.662229422479868 sec
Time for inference 3: 4.79 sec total, 427.18 tokens/sec
Decode latency: 4.66 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6411.85 GB/s
FLOPS achieved: 19.24 TF/s

Prefill latency: 0.1300623118877411 sec
Decode latency: 4.479118948802352 sec
Time for inference 4: 4.61 sec total, 444.24 tokens/sec
Decode latency: 4.48 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6668.03 GB/s
FLOPS achieved: 20.00 TF/s

Prefill latency: 0.13102159649133682 sec
Decode latency: 4.653282593935728 sec
Time for inference 5: 4.79 sec total, 427.98 tokens/sec
Decode latency: 4.65 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6423.93 GB/s
FLOPS achieved: 19.27 TF/s

Prefill latency: 0.12990768812596798 sec
Decode latency: 4.51253043487668 sec
Time for inference 6: 4.64 sec total, 441.06 tokens/sec
Decode latency: 4.51 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6620.24 GB/s
FLOPS achieved: 19.86 TF/s

Prefill latency: 0.1297705303877592 sec
Decode latency: 4.65641020052135 sec
Time for inference 7: 4.79 sec total, 427.82 tokens/sec
Decode latency: 4.66 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6421.54 GB/s
FLOPS achieved: 19.26 TF/s

Prefill latency: 0.1297716088593006 sec
Decode latency: 4.677490260452032 sec
Time for inference 8: 4.81 sec total, 425.94 tokens/sec
Decode latency: 4.68 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6393.31 GB/s
FLOPS achieved: 19.18 TF/s

Prefill latency: 0.1302710585296154 sec
Decode latency: 4.631068736314774 sec
Time for inference 9: 4.76 sec total, 430.05 tokens/sec
Decode latency: 4.63 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6455.04 GB/s
FLOPS achieved: 19.37 TF/s

Prefill latency: 0.13139963150024414 sec
Decode latency: 4.557963700965047 sec
Time for inference 10: 4.69 sec total, 436.65 tokens/sec
Decode latency: 4.56 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6554.05 GB/s
FLOPS achieved: 19.66 TF/s

==========
Batch Size: 4
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 4.6184 sec
Average prefill latency: 0.1303 sec
Average tokens/sec: 431.30
Memory used: 18.93 GB
[rank0]:[W1114 08:08:53.542928688 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Done. we are killing the process
[rank0]:[W1114 08:08:55.813725519 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
