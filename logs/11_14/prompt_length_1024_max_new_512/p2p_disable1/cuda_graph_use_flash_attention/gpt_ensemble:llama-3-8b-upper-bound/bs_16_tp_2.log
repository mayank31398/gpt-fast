W1114 08:13:35.402000 2775395 site-packages/torch/distributed/run.py:793] 
W1114 08:13:35.402000 2775395 site-packages/torch/distributed/run.py:793] *****************************************
W1114 08:13:35.402000 2775395 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1114 08:13:35.402000 2775395 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=3072, bias=False)
        (wo): Linear(in_features=2048, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=14336, bias=False)
        (w2): Linear(in_features=7168, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.17 seconds
CUDA_GRAPH are activate
Prefill latency: 0.27717038430273533 sec
Decode latency: 3.7267948146909475 sec
Compilation time: 4.01 seconds
Compilation time: 3.80 seconds
Prefill latency: 0.27713762037456036 sec
Decode latency: 3.7258278895169497 sec
Prefill latency: 0.2776902224868536 sec
Decode latency: 3.633410718291998 sec
Prefill latency: 0.27809674479067326 sec
Decode latency: 3.6029435861855745 sec
Prefill latency: 0.2785721402615309 sec
Decode latency: 3.559488318860531 sec
Prefill latency: 0.2812610901892185 sec
Decode latency: 3.689696427434683 sec
Time for inference 1: 3.97 sec total, 2062.57 tokens/sec
Decode latency: 3.69 sec
Prefill latency: 0.28 sec
Bandwidth achieved: 16563.55 GB/s
FLOPS achieved: 49.69 TF/s

Prefill latency: 0.27781592682003975 sec
Decode latency: 3.616793841123581 sec
Time for inference 2: 3.90 sec total, 2103.01 tokens/sec
Decode latency: 3.62 sec
Prefill latency: 0.28 sec
Bandwidth achieved: 16888.25 GB/s
FLOPS achieved: 50.66 TF/s

Prefill latency: 0.27678741700947285 sec
Decode latency: 3.726557832211256 sec
Time for inference 3: 4.00 sec total, 2045.88 tokens/sec
Decode latency: 3.73 sec
Prefill latency: 0.28 sec
Bandwidth achieved: 16429.53 GB/s
FLOPS achieved: 49.29 TF/s

Prefill latency: 0.2774026822298765 sec
Decode latency: 3.6132601648569107 sec
Time for inference 4: 3.89 sec total, 2105.13 tokens/sec
Decode latency: 3.61 sec
Prefill latency: 0.28 sec
Bandwidth achieved: 16905.31 GB/s
FLOPS achieved: 50.72 TF/s

Prefill latency: 0.27756578102707863 sec
Decode latency: 3.6958906948566437 sec
Time for inference 5: 3.97 sec total, 2061.29 tokens/sec
Decode latency: 3.70 sec
Prefill latency: 0.28 sec
Bandwidth achieved: 16553.26 GB/s
FLOPS achieved: 49.66 TF/s

Prefill latency: 0.27890168875455856 sec
Decode latency: 3.557618772611022 sec
Time for inference 6: 3.84 sec total, 2134.78 tokens/sec
Decode latency: 3.56 sec
Prefill latency: 0.28 sec
Bandwidth achieved: 17143.42 GB/s
FLOPS achieved: 51.43 TF/s

Prefill latency: 0.2779425960034132 sec
Decode latency: 3.656225711107254 sec
Time for inference 7: 3.93 sec total, 2081.84 tokens/sec
Decode latency: 3.66 sec
Prefill latency: 0.28 sec
Bandwidth achieved: 16718.28 GB/s
FLOPS achieved: 50.15 TF/s

Prefill latency: 0.27878059819340706 sec
Decode latency: 3.55807894654572 sec
Time for inference 8: 3.84 sec total, 2134.63 tokens/sec
Decode latency: 3.56 sec
Prefill latency: 0.28 sec
Bandwidth achieved: 17142.17 GB/s
FLOPS achieved: 51.43 TF/s

Prefill latency: 0.2766479756683111 sec
Decode latency: 3.7266589533537626 sec
Time for inference 9: 4.00 sec total, 2045.91 tokens/sec
Decode latency: 3.73 sec
Prefill latency: 0.28 sec
Bandwidth achieved: 16429.73 GB/s
FLOPS achieved: 49.29 TF/s

Prefill latency: 0.27989698201417923 sec
[rank1]:[W1114 08:14:41.661841184 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Decode latency: 3.7072354946285486 sec
Time for inference 10: 3.99 sec total, 2054.21 tokens/sec
Decode latency: 3.71 sec
Prefill latency: 0.28 sec
Bandwidth achieved: 16496.43 GB/s
FLOPS achieved: 49.49 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 3.6548 sec
Average prefill latency: 0.2783 sec
Average tokens/sec: 2082.93
Memory used: 16.91 GB
[rank0]:[W1114 08:14:42.848852457 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Done. we are killing the process
[rank0]:[W1114 08:14:44.790678815 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
