flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
        (wo): Linear(in_features=4096, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=28672, bias=False)
        (w2): Linear(in_features=14336, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.50 seconds
CUDA_GRAPH are activate
Prefill latency: 0.5279071778059006 sec
Decode latency: 5.244017504155636 sec
Compilation time: 5.77 seconds
Prefill latency: 0.5275050364434719 sec
Decode latency: 5.242477091029286 sec
Prefill latency: 0.5283910986036062 sec
Decode latency: 5.070854663848877 sec
Prefill latency: 0.5305537153035402 sec
Decode latency: 5.2432153429836035 sec
Prefill latency: 0.5278639569878578 sec
Decode latency: 5.243143646046519 sec
Prefill latency: 0.5308074746280909 sec
Decode latency: 5.243041444569826 sec
Time for inference 1: 5.77 sec total, 1418.55 tokens/sec
Decode latency: 5.24 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 21292.17 GB/s
FLOPS achieved: 63.88 TF/s

Prefill latency: 0.5293621718883514 sec
Decode latency: 5.206641439348459 sec
Time for inference 2: 5.74 sec total, 1427.94 tokens/sec
Decode latency: 5.21 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 21433.11 GB/s
FLOPS achieved: 64.30 TF/s

Prefill latency: 0.5308133084326982 sec
Decode latency: 5.2434252966195345 sec
Time for inference 3: 5.78 sec total, 1418.47 tokens/sec
Decode latency: 5.24 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 21291.08 GB/s
FLOPS achieved: 63.87 TF/s

Prefill latency: 0.5284147821366787 sec
Decode latency: 5.211276130750775 sec
Time for inference 4: 5.74 sec total, 1427.03 tokens/sec
Decode latency: 5.21 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 21419.47 GB/s
FLOPS achieved: 64.26 TF/s

Prefill latency: 0.529134027659893 sec
Decode latency: 5.242881156504154 sec
Time for inference 5: 5.77 sec total, 1418.99 tokens/sec
Decode latency: 5.24 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 21298.86 GB/s
FLOPS achieved: 63.90 TF/s

Prefill latency: 0.5283060763031244 sec
Decode latency: 5.241093251854181 sec
Time for inference 6: 5.77 sec total, 1419.66 tokens/sec
Decode latency: 5.24 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 21308.81 GB/s
FLOPS achieved: 63.93 TF/s

Prefill latency: 0.527813145890832 sec
Decode latency: 5.243144432082772 sec
Time for inference 7: 5.77 sec total, 1419.31 tokens/sec
Decode latency: 5.24 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 21303.57 GB/s
FLOPS achieved: 63.91 TF/s

Prefill latency: 0.5278179924935102 sec
Decode latency: 5.243030374869704 sec
Time for inference 8: 5.77 sec total, 1419.30 tokens/sec
Decode latency: 5.24 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 21303.49 GB/s
FLOPS achieved: 63.91 TF/s

Prefill latency: 0.5271337330341339 sec
Decode latency: 5.242914108559489 sec
Time for inference 9: 5.77 sec total, 1419.51 tokens/sec
Decode latency: 5.24 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 21306.62 GB/s
FLOPS achieved: 63.92 TF/s

Prefill latency: 0.5279946606606245 sec
Decode latency: 5.2434227261692286 sec
Time for inference 10: 5.77 sec total, 1419.19 tokens/sec
Decode latency: 5.24 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 21301.77 GB/s
FLOPS achieved: 63.91 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 5.2361 sec
Average prefill latency: 0.5288 sec
Average tokens/sec: 1420.79
Memory used: 30.84 GB
Done. we are killing the process
[rank0]:[W1114 07:50:54.421288453 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
