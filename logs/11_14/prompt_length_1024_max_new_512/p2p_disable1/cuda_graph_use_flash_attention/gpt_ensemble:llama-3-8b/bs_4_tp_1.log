flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
        (wo): Linear(in_features=4096, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=28672, bias=False)
        (w2): Linear(in_features=14336, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.03 seconds
CUDA_GRAPH are activate
Prefill latency: 0.1312117837369442 sec
Decode latency: 4.727684108540416 sec
Compilation time: 4.95 seconds
Prefill latency: 0.13071379251778126 sec
Decode latency: 4.72511569596827 sec
Prefill latency: 0.1304742507636547 sec
Decode latency: 4.724585868418217 sec
Prefill latency: 0.1325519122183323 sec
Decode latency: 4.723707903176546 sec
Prefill latency: 0.13084813579916954 sec
Decode latency: 4.724499894306064 sec
Prefill latency: 0.13041573762893677 sec
Decode latency: 4.725933041423559 sec
Time for inference 1: 4.86 sec total, 421.61 tokens/sec
Decode latency: 4.73 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6328.32 GB/s
FLOPS achieved: 18.98 TF/s

Prefill latency: 0.13201632536947727 sec
Decode latency: 4.725383523851633 sec
Time for inference 2: 4.86 sec total, 421.52 tokens/sec
Decode latency: 4.73 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6326.92 GB/s
FLOPS achieved: 18.98 TF/s

Prefill latency: 0.1314379833638668 sec
Decode latency: 4.724175775423646 sec
Time for inference 3: 4.86 sec total, 421.70 tokens/sec
Decode latency: 4.72 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6329.59 GB/s
FLOPS achieved: 18.99 TF/s

Prefill latency: 0.13188037276268005 sec
Decode latency: 4.724917147308588 sec
Time for inference 4: 4.86 sec total, 421.59 tokens/sec
Decode latency: 4.72 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6327.95 GB/s
FLOPS achieved: 18.98 TF/s

Prefill latency: 0.13164491206407547 sec
Decode latency: 4.7247235756367445 sec
Time for inference 5: 4.86 sec total, 421.63 tokens/sec
Decode latency: 4.72 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6328.53 GB/s
FLOPS achieved: 18.99 TF/s

Prefill latency: 0.13221647962927818 sec
Decode latency: 4.724523119628429 sec
Time for inference 6: 4.86 sec total, 421.59 tokens/sec
Decode latency: 4.72 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6327.99 GB/s
FLOPS achieved: 18.98 TF/s

Prefill latency: 0.13121305592358112 sec
Decode latency: 4.725769981741905 sec
Time for inference 7: 4.86 sec total, 421.55 tokens/sec
Decode latency: 4.73 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6327.47 GB/s
FLOPS achieved: 18.98 TF/s

Prefill latency: 0.1344077493995428 sec
Decode latency: 4.724937994033098 sec
Time for inference 8: 4.86 sec total, 421.35 tokens/sec
Decode latency: 4.72 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6324.42 GB/s
FLOPS achieved: 18.97 TF/s

Prefill latency: 0.1325055230408907 sec
Decode latency: 4.724987091496587 sec
Time for inference 9: 4.86 sec total, 421.53 tokens/sec
Decode latency: 4.72 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6327.10 GB/s
FLOPS achieved: 18.98 TF/s

Prefill latency: 0.1309610977768898 sec
Decode latency: 4.724613713100553 sec
Time for inference 10: 4.86 sec total, 421.69 tokens/sec
Decode latency: 4.72 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6329.55 GB/s
FLOPS achieved: 18.99 TF/s

==========
Batch Size: 4
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 4.7250 sec
Average prefill latency: 0.1319 sec
Average tokens/sec: 421.58
Memory used: 19.87 GB
Done. we are killing the process
[rank0]:[W1114 07:45:42.217728478 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
