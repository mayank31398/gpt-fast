W1114 07:52:28.068000 2762045 site-packages/torch/distributed/run.py:793] 
W1114 07:52:28.068000 2762045 site-packages/torch/distributed/run.py:793] *****************************************
W1114 07:52:28.068000 2762045 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1114 07:52:28.068000 2762045 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=1536, bias=False)
        (wo): Linear(in_features=1024, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.09 seconds
CUDA_GRAPH are activate
Prefill latency: 0.1897160690277815 sec
Decode latency: 3.5330089423805475 sec
Compilation time: 3.73 seconds
Compilation time: 3.72 secondsCompilation time: 3.73 seconds

Compilation time: 3.73 seconds
Prefill latency: 0.18961771950125694 sec
Decode latency: 3.5297440961003304 sec
Prefill latency: 0.19037078879773617 sec
Decode latency: 3.5334000904113054 sec
Prefill latency: 0.18928574584424496 sec
Decode latency: 3.5305124260485172 sec
Prefill latency: 0.19031612016260624 sec
Decode latency: 3.5295475628226995 sec
Prefill latency: 0.1896583866328001 sec
Decode latency: 3.529387576505542 sec
Time for inference 1: 3.72 sec total, 2202.20 tokens/sec
Decode latency: 3.53 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 9999.91 GB/s
FLOPS achieved: 30.00 TF/s

Prefill latency: 0.18978544883430004 sec
Decode latency: 3.5303159672766924 sec
Time for inference 2: 3.72 sec total, 2201.61 tokens/sec
Decode latency: 3.53 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 9997.20 GB/s
FLOPS achieved: 29.99 TF/s

Prefill latency: 0.1899262424558401 sec
Decode latency: 3.5301600135862827 sec
Time for inference 3: 3.72 sec total, 2201.50 tokens/sec
Decode latency: 3.53 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 9996.71 GB/s
FLOPS achieved: 29.99 TF/s

Prefill latency: 0.18966803327202797 sec
Decode latency: 3.5299185179173946 sec
Time for inference 4: 3.72 sec total, 2201.86 tokens/sec
Decode latency: 3.53 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 9998.33 GB/s
FLOPS achieved: 29.99 TF/s

Prefill latency: 0.18929260782897472 sec
Decode latency: 3.531622763723135 sec
Time for inference 5: 3.72 sec total, 2201.10 tokens/sec
Decode latency: 3.53 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 9994.91 GB/s
FLOPS achieved: 29.98 TF/s

Prefill latency: 0.190016595646739 sec
Decode latency: 3.529838640242815 sec
Time for inference 6: 3.72 sec total, 2201.72 tokens/sec
Decode latency: 3.53 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 9997.71 GB/s
FLOPS achieved: 29.99 TF/s

Prefill latency: 0.1903142463415861 sec
Decode latency: 3.530237575992942 sec
Time for inference 7: 3.72 sec total, 2201.23 tokens/sec
Decode latency: 3.53 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 9995.51 GB/s
FLOPS achieved: 29.99 TF/s

Prefill latency: 0.1895092260092497 sec
Decode latency: 3.53175157494843 sec
Time for inference 8: 3.72 sec total, 2200.76 tokens/sec
Decode latency: 3.53 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 9993.37 GB/s
FLOPS achieved: 29.98 TF/s

Prefill latency: 0.19047247059643269 sec
Decode latency: 3.5317072719335556 sec
Time for inference 9: 3.72 sec total, 2200.33 tokens/sec
Decode latency: 3.53 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 9991.39 GB/s
FLOPS achieved: 29.97 TF/s

Prefill latency: 0.18993481621146202 sec
Decode latency: 3.5303405970335007 sec
Time for inference 10: 3.72 sec total, 2201.50 tokens/sec
Decode latency: 3.53 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 9996.70 GB/s
FLOPS achieved: 29.99 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 3.5305 sec
Average prefill latency: 0.1899 sec
Average tokens/sec: 2201.38
Memory used: 20.20 GB
Done. we are killing the process
[rank0]:[W1114 07:53:40.766097112 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
