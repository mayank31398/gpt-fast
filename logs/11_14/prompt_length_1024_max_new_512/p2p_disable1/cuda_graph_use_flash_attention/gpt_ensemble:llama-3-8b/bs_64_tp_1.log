flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
        (wo): Linear(in_features=4096, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=28672, bias=False)
        (w2): Linear(in_features=14336, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.79 seconds
CUDA_GRAPH are activate
Prefill latency: 2.3052014522254467 sec
Decode latency: 6.953562751412392 sec
Compilation time: 9.26 seconds
Prefill latency: 2.302408382296562 sec
Decode latency: 6.964816991239786 sec
Prefill latency: 2.3032874409109354 sec
Decode latency: 6.993999237194657 sec
Prefill latency: 2.2965517342090607 sec
Decode latency: 6.992177397012711 sec
Prefill latency: 2.3053029756993055 sec
Decode latency: 6.822884235531092 sec
Prefill latency: 2.2993699312210083 sec
Decode latency: 6.991840882226825 sec
Time for inference 1: 9.29 sec total, 3526.27 tokens/sec
Decode latency: 6.99 sec
Prefill latency: 2.30 sec
Bandwidth achieved: 52928.75 GB/s
FLOPS achieved: 158.79 TF/s

Prefill latency: 2.298775414004922 sec
Decode latency: 6.983449475839734 sec
Time for inference 2: 9.28 sec total, 3529.71 tokens/sec
Decode latency: 6.98 sec
Prefill latency: 2.30 sec
Bandwidth achieved: 52980.34 GB/s
FLOPS achieved: 158.94 TF/s

Prefill latency: 2.315977193415165 sec
Decode latency: 6.993054546415806 sec
Time for inference 3: 9.31 sec total, 3519.57 tokens/sec
Decode latency: 6.99 sec
Prefill latency: 2.32 sec
Bandwidth achieved: 52828.20 GB/s
FLOPS achieved: 158.48 TF/s

Prefill latency: 2.3083453960716724 sec
Decode latency: 6.844593554735184 sec
Time for inference 4: 9.15 sec total, 3579.53 tokens/sec
Decode latency: 6.84 sec
Prefill latency: 2.31 sec
Bandwidth achieved: 53728.28 GB/s
FLOPS achieved: 161.18 TF/s

Prefill latency: 2.300666520372033 sec
Decode latency: 6.99232267215848 sec
Time for inference 5: 9.29 sec total, 3525.63 tokens/sec
Decode latency: 6.99 sec
Prefill latency: 2.30 sec
Bandwidth achieved: 52919.16 GB/s
FLOPS achieved: 158.76 TF/s

Prefill latency: 2.294552920386195 sec
Decode latency: 6.992902558296919 sec
Time for inference 6: 9.29 sec total, 3527.73 tokens/sec
Decode latency: 6.99 sec
Prefill latency: 2.29 sec
Bandwidth achieved: 52950.70 GB/s
FLOPS achieved: 158.85 TF/s

Prefill latency: 2.301707897335291 sec
Decode latency: 6.992061028257012 sec
Time for inference 7: 9.29 sec total, 3525.34 tokens/sec
Decode latency: 6.99 sec
Prefill latency: 2.30 sec
Bandwidth achieved: 52914.89 GB/s
FLOPS achieved: 158.74 TF/s

Prefill latency: 2.311216654255986 sec
Decode latency: 6.862238384783268 sec
Time for inference 8: 9.17 sec total, 3571.57 tokens/sec
Decode latency: 6.86 sec
Prefill latency: 2.31 sec
Bandwidth achieved: 53608.73 GB/s
FLOPS achieved: 160.83 TF/s

Prefill latency: 2.30371274985373 sec
Decode latency: 6.992331363260746 sec
Time for inference 9: 9.30 sec total, 3524.42 tokens/sec
Decode latency: 6.99 sec
Prefill latency: 2.30 sec
Bandwidth achieved: 52901.07 GB/s
FLOPS achieved: 158.70 TF/s

Prefill latency: 2.306638626381755 sec
Decode latency: 6.856390191242099 sec
Time for inference 10: 9.16 sec total, 3575.61 tokens/sec
Decode latency: 6.86 sec
Prefill latency: 2.31 sec
Bandwidth achieved: 53669.38 GB/s
FLOPS achieved: 161.01 TF/s

==========
Batch Size: 64
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 6.9501 sec
Average prefill latency: 2.3041 sec
Average tokens/sec: 3540.54
Memory used: 74.73 GB
Done. we are killing the process
[rank0]:[W1114 07:57:43.752382504 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
