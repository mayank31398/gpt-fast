W1114 07:43:11.304000 2756454 site-packages/torch/distributed/run.py:793] 
W1114 07:43:11.304000 2756454 site-packages/torch/distributed/run.py:793] *****************************************
W1114 07:43:11.304000 2756454 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1114 07:43:11.304000 2756454 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=768, bias=False)
        (wo): Linear(in_features=512, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=3584, bias=False)
        (w2): Linear(in_features=1792, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.49 seconds
CUDA_GRAPH are activate
[rank1]:[W1114 07:43:31.278266052 CUDAGraph.cpp:133] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
Prefill latency: 0.022240908816456795 sec
Decode latency: 2.914073996245861 sec
Compilation time: 2.93 seconds
Compilation time: 2.94 seconds
Compilation time: 2.94 seconds
Compilation time: 2.94 seconds
Compilation time: 2.94 seconds
Compilation time: 2.96 seconds
Compilation time: 3.04 seconds
Compilation time: 2.93 seconds
Prefill latency: 0.016140468418598175 sec
Decode latency: 2.9016036726534367 sec
Prefill latency: 0.015843788161873817 sec
Decode latency: 2.906858369708061 sec
Prefill latency: 0.016591215506196022 sec
Decode latency: 2.909048844128847 sec
Prefill latency: 0.016455896198749542 sec
Decode latency: 2.902750216424465 sec
Prefill latency: 0.01573283225297928 sec
Decode latency: 2.901723315939307 sec
Time for inference 1: 2.92 sec total, 175.42 tokens/sec
Decode latency: 2.90 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 490.48 GB/s
FLOPS achieved: 1.47 TF/s

Prefill latency: 0.015677455812692642 sec
Decode latency: 2.9149863868951797 sec
Time for inference 2: 2.93 sec total, 174.64 tokens/sec
Decode latency: 2.91 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 488.30 GB/s
FLOPS achieved: 1.46 TF/s

Prefill latency: 0.015677623450756073 sec
Decode latency: 2.9016250241547823 sec
Time for inference 3: 2.92 sec total, 175.43 tokens/sec
Decode latency: 2.90 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 490.52 GB/s
FLOPS achieved: 1.47 TF/s

Prefill latency: 0.020337408408522606 sec
Decode latency: 2.904742768034339 sec
Time for inference 4: 2.93 sec total, 174.98 tokens/sec
Decode latency: 2.90 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 489.25 GB/s
FLOPS achieved: 1.47 TF/s

Prefill latency: 0.015744220465421677 sec
Decode latency: 2.9323452208191156 sec
Time for inference 5: 2.95 sec total, 173.61 tokens/sec
Decode latency: 2.93 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 485.42 GB/s
FLOPS achieved: 1.46 TF/s

Prefill latency: 0.01749906875193119 sec
Decode latency: 2.9113222137093544 sec
Time for inference 6: 2.93 sec total, 174.76 tokens/sec
Decode latency: 2.91 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 488.63 GB/s
FLOPS achieved: 1.47 TF/s

Prefill latency: 0.015678711235523224 sec
Decode latency: 2.902758613228798 sec
Time for inference 7: 2.92 sec total, 175.38 tokens/sec
Decode latency: 2.90 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 490.36 GB/s
FLOPS achieved: 1.47 TF/s

Prefill latency: 0.01570945978164673 sec
Decode latency: 2.9138211254030466 sec
Time for inference 8: 2.93 sec total, 174.71 tokens/sec
Decode latency: 2.91 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 488.50 GB/s
FLOPS achieved: 1.47 TF/s

Prefill latency: 0.015638533979654312 sec
Decode latency: 2.9132866840809584 sec
Time for inference 9: 2.93 sec total, 174.75 tokens/sec
Decode latency: 2.91 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 488.60 GB/s
FLOPS achieved: 1.47 TF/s

Prefill latency: 0.015994319692254066 sec
Decode latency: 2.90525826998055 sec
Time for inference 10: 2.92 sec total, 175.21 tokens/sec
Decode latency: 2.91 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 489.89 GB/s
FLOPS achieved: 1.47 TF/s

==========
Batch Size: 1
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 2.9102 sec
Average prefill latency: 0.0164 sec
Average tokens/sec: 174.89
Memory used: 4.65 GB
Done. we are killing the process
[rank0]:[W1114 07:44:17.783647409 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
