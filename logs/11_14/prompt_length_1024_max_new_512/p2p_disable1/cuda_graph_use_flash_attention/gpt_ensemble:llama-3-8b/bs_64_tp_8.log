W1114 08:01:45.762000 2767233 site-packages/torch/distributed/run.py:793] 
W1114 08:01:45.762000 2767233 site-packages/torch/distributed/run.py:793] *****************************************
W1114 08:01:45.762000 2767233 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1114 08:01:45.762000 2767233 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=768, bias=False)
        (wo): Linear(in_features=512, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=3584, bias=False)
        (w2): Linear(in_features=1792, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.21 seconds
CUDA_GRAPH are activate
Prefill latency: 0.5193165615200996 sec
Decode latency: 4.454621171578765 sec
Compilation time: 5.00 seconds
Compilation time: 5.00 seconds
Compilation time: 4.99 seconds
Compilation time: 5.00 seconds
Compilation time: 4.99 seconds
Compilation time: 4.99 secondsCompilation time: 5.00 seconds

Compilation time: 5.00 seconds
Prefill latency: 0.4967360235750675 sec
Decode latency: 4.421324692666531 sec
Prefill latency: 0.49896037578582764 sec
Decode latency: 4.3716368936002254 sec
Prefill latency: 0.5007258635014296 sec
Decode latency: 4.38929957523942 sec
Prefill latency: 0.501359298825264 sec
Decode latency: 4.366965239867568 sec
Prefill latency: 0.5001157857477665 sec
Decode latency: 4.424366103485227 sec
Time for inference 1: 4.93 sec total, 6652.67 tokens/sec
Decode latency: 4.42 sec
Prefill latency: 0.50 sec
Bandwidth achieved: 18601.11 GB/s
FLOPS achieved: 55.80 TF/s

Prefill latency: 0.5016012638807297 sec
Decode latency: 4.363212641328573 sec
Time for inference 2: 4.87 sec total, 6734.08 tokens/sec
Decode latency: 4.36 sec
Prefill latency: 0.50 sec
Bandwidth achieved: 18828.72 GB/s
FLOPS achieved: 56.49 TF/s

Prefill latency: 0.4999247398227453 sec
Decode latency: 4.407150140032172 sec
Time for inference 3: 4.91 sec total, 6676.23 tokens/sec
Decode latency: 4.41 sec
Prefill latency: 0.50 sec
Bandwidth achieved: 18666.98 GB/s
FLOPS achieved: 56.00 TF/s

Prefill latency: 0.5020197946578264 sec
Decode latency: 4.376016987487674 sec
Time for inference 4: 4.88 sec total, 6715.89 tokens/sec
Decode latency: 4.38 sec
Prefill latency: 0.50 sec
Bandwidth achieved: 18777.88 GB/s
FLOPS achieved: 56.33 TF/s

Prefill latency: 0.501165559515357 sec
Decode latency: 4.395814353600144 sec
Time for inference 5: 4.90 sec total, 6689.89 tokens/sec
Decode latency: 4.40 sec
Prefill latency: 0.50 sec
Bandwidth achieved: 18705.17 GB/s
FLOPS achieved: 56.12 TF/s

Prefill latency: 0.5008399002254009 sec
Decode latency: 4.376796409487724 sec
Time for inference 6: 4.88 sec total, 6716.55 tokens/sec
Decode latency: 4.38 sec
Prefill latency: 0.50 sec
Bandwidth achieved: 18779.72 GB/s
FLOPS achieved: 56.34 TF/s

Prefill latency: 0.5000368840992451 sec
Decode latency: 4.399096496403217 sec
Time for inference 7: 4.90 sec total, 6687.00 tokens/sec
Decode latency: 4.40 sec
Prefill latency: 0.50 sec
Bandwidth achieved: 18697.08 GB/s
FLOPS achieved: 56.09 TF/s

Prefill latency: 0.5003005433827639 sec
Decode latency: 4.370079627260566 sec
Time for inference 8: 4.87 sec total, 6726.40 tokens/sec
Decode latency: 4.37 sec
Prefill latency: 0.50 sec
Bandwidth achieved: 18807.27 GB/s
FLOPS achieved: 56.42 TF/s

Prefill latency: 0.4987766891717911 sec
Decode latency: 4.385470090433955 sec
Time for inference 9: 4.89 sec total, 6707.28 tokens/sec
Decode latency: 4.39 sec
Prefill latency: 0.50 sec
Bandwidth achieved: 18753.80 GB/s
FLOPS achieved: 56.26 TF/s

Prefill latency: 0.5014438666403294 sec
Decode latency: 4.368091069161892 sec
Time for inference 10: 4.87 sec total, 6727.32 tokens/sec
Decode latency: 4.37 sec
Prefill latency: 0.50 sec
Bandwidth achieved: 18809.83 GB/s
FLOPS achieved: 56.43 TF/s

==========
Batch Size: 64
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 4.3866 sec
Average prefill latency: 0.5006 sec
Average tokens/sec: 6703.33
Memory used: 42.88 GB
Done. we are killing the process
[rank0]:[W1114 08:03:28.703155830 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
