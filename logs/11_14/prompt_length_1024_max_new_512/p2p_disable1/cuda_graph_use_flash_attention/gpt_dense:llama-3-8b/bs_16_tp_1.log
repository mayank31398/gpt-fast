flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTDense(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x DenseTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
        (wo): Linear(in_features=4096, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=28672, bias=False)
        (w2): Linear(in_features=14336, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 0.96 seconds
CUDA_GRAPH are activate
Prefill latency: 0.5243098642677069 sec
Decode latency: 5.273285239934921 sec
Compilation time: 5.80 seconds
Prefill latency: 0.5248935166746378 sec
Decode latency: 5.228648142889142 sec
Prefill latency: 0.5261389706283808 sec
Decode latency: 5.2705205623060465 sec
Prefill latency: 0.5246311873197556 sec
Decode latency: 5.273246916010976 sec
Prefill latency: 0.52580058388412 sec
Decode latency: 5.2712804693728685 sec
Prefill latency: 0.5253461711108685 sec
Decode latency: 5.272296205163002 sec
Time for inference 1: 5.80 sec total, 1412.77 tokens/sec
Decode latency: 5.27 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 21205.39 GB/s
FLOPS achieved: 63.62 TF/s

Prefill latency: 0.5282408334314823 sec
Decode latency: 5.13890116289258 sec
Time for inference 2: 5.67 sec total, 1445.31 tokens/sec
Decode latency: 5.14 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 21693.86 GB/s
FLOPS achieved: 65.08 TF/s

Prefill latency: 0.526196489110589 sec
Decode latency: 5.270879652351141 sec
Time for inference 3: 5.80 sec total, 1412.90 tokens/sec
Decode latency: 5.27 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 21207.48 GB/s
FLOPS achieved: 63.62 TF/s

Prefill latency: 0.5250196717679501 sec
Decode latency: 5.2712894305586815 sec
Time for inference 4: 5.80 sec total, 1413.07 tokens/sec
Decode latency: 5.27 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 21209.99 GB/s
FLOPS achieved: 63.63 TF/s

Prefill latency: 0.5276571251451969 sec
Decode latency: 5.270881684496999 sec
Time for inference 5: 5.80 sec total, 1412.55 tokens/sec
Decode latency: 5.27 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 21202.17 GB/s
FLOPS achieved: 63.61 TF/s

Prefill latency: 0.5264775101095438 sec
Decode latency: 5.1472499910742044 sec
Time for inference 6: 5.67 sec total, 1443.61 tokens/sec
Decode latency: 5.15 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 21668.32 GB/s
FLOPS achieved: 65.00 TF/s

Prefill latency: 0.5269456394016743 sec
Decode latency: 5.270213592797518 sec
Time for inference 7: 5.80 sec total, 1412.90 tokens/sec
Decode latency: 5.27 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 21207.40 GB/s
FLOPS achieved: 63.62 TF/s

Prefill latency: 0.5254446249455214 sec
Decode latency: 5.27050881460309 sec
Time for inference 8: 5.80 sec total, 1413.18 tokens/sec
Decode latency: 5.27 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 21211.67 GB/s
FLOPS achieved: 63.64 TF/s

Prefill latency: 0.526222288608551 sec
Decode latency: 5.271530766040087 sec
Time for inference 9: 5.80 sec total, 1412.74 tokens/sec
Decode latency: 5.27 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 21204.94 GB/s
FLOPS achieved: 63.61 TF/s

Prefill latency: 0.5260268896818161 sec
Decode latency: 5.270993320271373 sec
Time for inference 10: 5.80 sec total, 1412.92 tokens/sec
Decode latency: 5.27 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 21207.75 GB/s
FLOPS achieved: 63.62 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 5.2455 sec
Average prefill latency: 0.5264 sec
Average tokens/sec: 1419.19
Memory used: 36.01 GB
Done. we are killing the process
[rank0]:[W1114 07:11:06.383706249 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
