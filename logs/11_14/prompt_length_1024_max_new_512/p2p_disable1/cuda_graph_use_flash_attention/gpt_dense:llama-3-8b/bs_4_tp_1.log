flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTDense(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x DenseTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
        (wo): Linear(in_features=4096, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=28672, bias=False)
        (w2): Linear(in_features=14336, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.19 seconds
CUDA_GRAPH are activate
Prefill latency: 0.1283036805689335 sec
Decode latency: 4.638874774798751 sec
Compilation time: 4.77 seconds
Prefill latency: 0.12991898506879807 sec
Decode latency: 4.745393557474017 sec
Prefill latency: 0.12984794192016125 sec
Decode latency: 4.744709152728319 sec
Prefill latency: 0.13014164194464684 sec
Decode latency: 4.745972651988268 sec
Prefill latency: 0.13066683150827885 sec
Decode latency: 4.583659417927265 sec
Prefill latency: 0.13027826510369778 sec
Decode latency: 4.744641195982695 sec
Time for inference 1: 4.88 sec total, 420.02 tokens/sec
Decode latency: 4.74 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6304.47 GB/s
FLOPS achieved: 18.91 TF/s

Prefill latency: 0.1317636240273714 sec
Decode latency: 4.585071859881282 sec
Time for inference 2: 4.72 sec total, 434.11 tokens/sec
Decode latency: 4.59 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6515.91 GB/s
FLOPS achieved: 19.55 TF/s

Prefill latency: 0.13164463825523853 sec
Decode latency: 4.744980126619339 sec
Time for inference 3: 4.88 sec total, 419.88 tokens/sec
Decode latency: 4.74 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6302.30 GB/s
FLOPS achieved: 18.91 TF/s

Prefill latency: 0.13122146390378475 sec
Decode latency: 4.719831144437194 sec
Time for inference 4: 4.85 sec total, 422.10 tokens/sec
Decode latency: 4.72 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6335.64 GB/s
FLOPS achieved: 19.01 TF/s

Prefill latency: 0.130417687818408 sec
Decode latency: 4.744908146560192 sec
Time for inference 5: 4.88 sec total, 420.00 tokens/sec
Decode latency: 4.74 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6304.09 GB/s
FLOPS achieved: 18.91 TF/s

Prefill latency: 0.1307984497398138 sec
Decode latency: 4.745087120682001 sec
Time for inference 6: 4.88 sec total, 419.93 tokens/sec
Decode latency: 4.75 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6303.14 GB/s
FLOPS achieved: 18.91 TF/s

Prefill latency: 0.13122215867042542 sec
Decode latency: 4.6699342876672745 sec
Time for inference 7: 4.80 sec total, 426.47 tokens/sec
Decode latency: 4.67 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6401.19 GB/s
FLOPS achieved: 19.20 TF/s

Prefill latency: 0.13152110017836094 sec
Decode latency: 4.745106415823102 sec
Time for inference 8: 4.88 sec total, 419.88 tokens/sec
Decode latency: 4.75 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6302.34 GB/s
FLOPS achieved: 18.91 TF/s

Prefill latency: 0.13039499148726463 sec
Decode latency: 4.702775912359357 sec
Time for inference 9: 4.83 sec total, 423.64 tokens/sec
Decode latency: 4.70 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6358.85 GB/s
FLOPS achieved: 19.08 TF/s

Prefill latency: 0.13204804435372353 sec
Decode latency: 4.7448582500219345 sec
Time for inference 10: 4.88 sec total, 419.86 tokens/sec
Decode latency: 4.74 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6301.97 GB/s
FLOPS achieved: 18.91 TF/s

==========
Batch Size: 4
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 4.7147 sec
Average prefill latency: 0.1311 sec
Average tokens/sec: 422.59
Memory used: 21.16 GB
Done. we are killing the process
[rank0]:[W1114 07:05:18.014422283 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
