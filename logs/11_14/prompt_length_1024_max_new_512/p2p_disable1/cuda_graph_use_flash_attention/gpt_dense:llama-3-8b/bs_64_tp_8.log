W1114 07:18:49.270000 2741957 site-packages/torch/distributed/run.py:793] 
W1114 07:18:49.270000 2741957 site-packages/torch/distributed/run.py:793] *****************************************
W1114 07:18:49.270000 2741957 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1114 07:18:49.270000 2741957 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTDense(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x DenseTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=768, bias=False)
        (wo): Linear(in_features=512, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=3584, bias=False)
        (w2): Linear(in_features=1792, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.13 seconds
CUDA_GRAPH are activate
Prefill latency: 0.5993372667580843 sec
Decode latency: 6.1001052018254995 sec
Compilation time: 6.71 seconds
Compilation time: 6.80 secondsCompilation time: 6.72 seconds

Compilation time: 6.94 secondsCompilation time: 6.88 seconds

Compilation time: 6.70 seconds
Compilation time: 6.88 seconds
Compilation time: 6.71 seconds
Prefill latency: 0.5798446200788021 sec
Decode latency: 6.004749519750476 sec
Prefill latency: 0.57966055162251 sec
Decode latency: 6.023195140063763 sec
Prefill latency: 0.5803654044866562 sec
Decode latency: 6.004898613318801 sec
Prefill latency: 0.5832632873207331 sec
Decode latency: 6.016207329928875 sec
Prefill latency: 0.5807970240712166 sec
Decode latency: 6.041200924664736 sec
Time for inference 1: 6.62 sec total, 4947.43 tokens/sec
Decode latency: 6.04 sec
Prefill latency: 0.58 sec
Bandwidth achieved: 13833.18 GB/s
FLOPS achieved: 41.50 TF/s

Prefill latency: 0.5794367901980877 sec
Decode latency: 6.0290307607501745 sec
Time for inference 2: 6.61 sec total, 4957.40 tokens/sec
Decode latency: 6.03 sec
Prefill latency: 0.58 sec
Bandwidth achieved: 13861.06 GB/s
FLOPS achieved: 41.58 TF/s

Prefill latency: 0.5806707609444857 sec
Decode latency: 5.997169146314263 sec
Time for inference 3: 6.58 sec total, 4980.70 tokens/sec
Decode latency: 6.00 sec
Prefill latency: 0.58 sec
Bandwidth achieved: 13926.22 GB/s
FLOPS achieved: 41.78 TF/s

Prefill latency: 0.5818165671080351 sec
Decode latency: 5.988595865666866 sec
Time for inference 4: 6.57 sec total, 4986.23 tokens/sec
Decode latency: 5.99 sec
Prefill latency: 0.58 sec
Bandwidth achieved: 13941.67 GB/s
FLOPS achieved: 41.83 TF/s

Prefill latency: 0.5814034696668386 sec
Decode latency: 6.037602772936225 sec
Time for inference 5: 6.62 sec total, 4949.69 tokens/sec
Decode latency: 6.04 sec
Prefill latency: 0.58 sec
Bandwidth achieved: 13839.50 GB/s
FLOPS achieved: 41.52 TF/s

Prefill latency: 0.5798878949135542 sec
Decode latency: 5.996829533949494 sec
Time for inference 6: 6.58 sec total, 4981.50 tokens/sec
Decode latency: 6.00 sec
Prefill latency: 0.58 sec
Bandwidth achieved: 13928.44 GB/s
FLOPS achieved: 41.79 TF/s

Prefill latency: 0.5793074816465378 sec
Decode latency: 6.055195543915033 sec
Time for inference 7: 6.64 sec total, 4938.19 tokens/sec
Decode latency: 6.06 sec
Prefill latency: 0.58 sec
Bandwidth achieved: 13807.35 GB/s
FLOPS achieved: 41.42 TF/s

Prefill latency: 0.5794655233621597 sec
Decode latency: 5.996529418975115 sec
Time for inference 8: 6.58 sec total, 4982.13 tokens/sec
Decode latency: 6.00 sec
Prefill latency: 0.58 sec
Bandwidth achieved: 13930.20 GB/s
FLOPS achieved: 41.79 TF/s

Prefill latency: 0.5813518408685923 sec
Decode latency: 6.005257561802864 sec
Time for inference 9: 6.59 sec total, 4973.99 tokens/sec
Decode latency: 6.01 sec
Prefill latency: 0.58 sec
Bandwidth achieved: 13907.44 GB/s
FLOPS achieved: 41.72 TF/s

Prefill latency: 0.5812888201326132 sec
Decode latency: 6.0096581391990185 sec
Time for inference 10: 6.59 sec total, 4970.73 tokens/sec
Decode latency: 6.01 sec
Prefill latency: 0.58 sec
Bandwidth achieved: 13898.35 GB/s
FLOPS achieved: 41.70 TF/s

==========
Batch Size: 64
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 6.0157 sec
Average prefill latency: 0.5805 sec
Average tokens/sec: 4966.80
Memory used: 60.08 GB
Done. we are killing the process
[rank0]:[W1114 07:21:04.327363075 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
