W1114 07:11:09.617000 2736163 site-packages/torch/distributed/run.py:793] 
W1114 07:11:09.617000 2736163 site-packages/torch/distributed/run.py:793] *****************************************
W1114 07:11:09.617000 2736163 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1114 07:11:09.617000 2736163 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTDense(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x DenseTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=3072, bias=False)
        (wo): Linear(in_features=2048, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=14336, bias=False)
        (w2): Linear(in_features=7168, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.26 seconds
CUDA_GRAPH are activate
Prefill latency: 0.9433306455612183 sec
Decode latency: 5.245288664475083 sec
Compilation time: 6.20 seconds
Compilation time: 6.19 seconds
Prefill latency: 0.876161253079772 sec
Decode latency: 5.241604505106807 sec
Prefill latency: 0.8766678031533957 sec
Decode latency: 5.244109563529491 sec
Prefill latency: 0.8765922337770462 sec
Decode latency: 5.238676348701119 sec
Prefill latency: 0.8750577755272388 sec
Decode latency: 5.238704212009907 sec
Prefill latency: 0.8771102111786604 sec
Decode latency: 5.242677034810185 sec
Time for inference 1: 6.12 sec total, 1338.35 tokens/sec
Decode latency: 5.24 sec
Prefill latency: 0.88 sec
Bandwidth achieved: 10747.68 GB/s
FLOPS achieved: 32.24 TF/s

Prefill latency: 0.8763235155493021 sec
Decode latency: 5.243025779724121 sec
Time for inference 2: 6.12 sec total, 1338.48 tokens/sec
Decode latency: 5.24 sec
Prefill latency: 0.88 sec
Bandwidth achieved: 10748.68 GB/s
FLOPS achieved: 32.25 TF/s

Prefill latency: 0.8770348951220512 sec
Decode latency: 5.239406576380134 sec
Time for inference 3: 6.12 sec total, 1339.10 tokens/sec
Decode latency: 5.24 sec
Prefill latency: 0.88 sec
Bandwidth achieved: 10753.65 GB/s
FLOPS achieved: 32.26 TF/s

Prefill latency: 0.8762013465166092 sec
Decode latency: 5.242253048345447 sec
Time for inference 4: 6.12 sec total, 1338.65 tokens/sec
Decode latency: 5.24 sec
Prefill latency: 0.88 sec
Bandwidth achieved: 10750.03 GB/s
FLOPS achieved: 32.25 TF/s

Prefill latency: 0.878182215616107 sec
Decode latency: 5.241339864209294 sec
Time for inference 5: 6.12 sec total, 1338.42 tokens/sec
Decode latency: 5.24 sec
Prefill latency: 0.88 sec
Bandwidth achieved: 10748.25 GB/s
FLOPS achieved: 32.24 TF/s

Prefill latency: 0.8762716967612505 sec
Decode latency: 5.244820611551404 sec
Time for inference 6: 6.12 sec total, 1338.05 tokens/sec
Decode latency: 5.24 sec
Prefill latency: 0.88 sec
Bandwidth achieved: 10745.23 GB/s
FLOPS achieved: 32.24 TF/s

Prefill latency: 0.8778795432299376 sec
Decode latency: 5.242379814386368 sec
Time for inference 7: 6.12 sec total, 1338.23 tokens/sec
Decode latency: 5.24 sec
Prefill latency: 0.88 sec
Bandwidth achieved: 10746.72 GB/s
FLOPS achieved: 32.24 TF/s

Prefill latency: 0.877861600369215 sec
Decode latency: 5.23947717808187 sec
Time for inference 8: 6.12 sec total, 1338.89 tokens/sec
Decode latency: 5.24 sec
Prefill latency: 0.88 sec
Bandwidth achieved: 10751.96 GB/s
FLOPS achieved: 32.26 TF/s

Prefill latency: 0.8790314923971891 sec
Decode latency: 5.2409766130149364 sec
Time for inference 9: 6.12 sec total, 1338.32 tokens/sec
Decode latency: 5.24 sec
Prefill latency: 0.88 sec
Bandwidth achieved: 10747.39 GB/s
FLOPS achieved: 32.24 TF/s

Prefill latency: 0.8769475165754557 sec
Decode latency: 5.241412928327918 sec
Time for inference 10: 6.12 sec total, 1338.67 tokens/sec
Decode latency: 5.24 sec
Prefill latency: 0.88 sec
Bandwidth achieved: 10750.26 GB/s
FLOPS achieved: 32.25 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 5.2418 sec
Average prefill latency: 0.8773 sec
Average tokens/sec: 1338.52
Memory used: 31.48 GB
Done. we are killing the process
[rank0]:[W1114 07:12:53.515305364 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
