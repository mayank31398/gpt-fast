W1114 06:01:52.088000 2357908 site-packages/torch/distributed/run.py:793] 
W1114 06:01:52.088000 2357908 site-packages/torch/distributed/run.py:793] *****************************************
W1114 06:01:52.088000 2357908 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1114 06:01:52.088000 2357908 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTLadder(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x LadderTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=1280, bias=False)
        (wo): Linear(in_features=1024, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 0.67 seconds
CUDA_GRAPH are activate
Prefill latency: 0.22556774225085974 sec
Decode latency: 8.312634637579322 sec
Compilation time: 8.55 seconds
Compilation time: 8.54 seconds
Compilation time: 8.51 seconds
Compilation time: 8.55 seconds
Compilation time: 8.54 seconds
Compilation time: 8.55 seconds
Compilation time: 8.53 seconds
Compilation time: 8.53 seconds
Prefill latency: 0.18933442328125238 sec
Decode latency: 8.280685759149492 sec
Prefill latency: 0.18982906267046928 sec
Decode latency: 8.273278438486159 sec
Prefill latency: 0.18914704211056232 sec
Decode latency: 8.28012153506279 sec
Prefill latency: 0.18995404057204723 sec
Decode latency: 8.26469000056386 sec
Prefill latency: 0.19018632546067238 sec
Decode latency: 8.29510497301817 sec
Time for inference 1: 8.49 sec total, 241.30 tokens/sec
Decode latency: 8.30 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 4636.96 GB/s
FLOPS achieved: 13.91 TF/s

Prefill latency: 0.1897559342905879 sec
Decode latency: 8.300791165791452 sec
Time for inference 2: 8.49 sec total, 241.16 tokens/sec
Decode latency: 8.30 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 4634.34 GB/s
FLOPS achieved: 13.90 TF/s

Prefill latency: 0.1905780490487814 sec
Decode latency: 8.300043961964548 sec
Time for inference 3: 8.49 sec total, 241.16 tokens/sec
Decode latency: 8.30 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 4634.38 GB/s
FLOPS achieved: 13.90 TF/s

Prefill latency: 0.19001717399805784 sec
Decode latency: 8.288101266138256 sec
Time for inference 4: 8.48 sec total, 241.51 tokens/sec
Decode latency: 8.29 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 4641.11 GB/s
FLOPS achieved: 13.92 TF/s

Prefill latency: 0.18984293844550848 sec
Decode latency: 8.258051581680775 sec
Time for inference 5: 8.45 sec total, 242.38 tokens/sec
Decode latency: 8.26 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 4657.73 GB/s
FLOPS achieved: 13.97 TF/s

Prefill latency: 0.18977174907922745 sec
Decode latency: 8.293614501133561 sec
Time for inference 6: 8.49 sec total, 241.36 tokens/sec
Decode latency: 8.29 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 4638.19 GB/s
FLOPS achieved: 13.91 TF/s

Prefill latency: 0.19058706518262625 sec
Decode latency: 8.288860771805048 sec
Time for inference 7: 8.48 sec total, 241.47 tokens/sec
Decode latency: 8.29 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 4640.30 GB/s
FLOPS achieved: 13.92 TF/s

Prefill latency: 0.1904106428846717 sec
Decode latency: 8.295018628239632 sec
Time for inference 8: 8.49 sec total, 241.30 tokens/sec
Decode latency: 8.30 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 4636.94 GB/s
FLOPS achieved: 13.91 TF/s

Prefill latency: 0.19010125193744898 sec
Decode latency: 8.317671903409064 sec
Time for inference 9: 8.51 sec total, 240.67 tokens/sec
Decode latency: 8.32 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 4624.93 GB/s
FLOPS achieved: 13.87 TF/s

Prefill latency: 0.18958376720547676 sec
Decode latency: 8.311290013603866 sec
Time for inference 10: 8.50 sec total, 240.87 tokens/sec
Decode latency: 8.31 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 4628.68 GB/s
FLOPS achieved: 13.89 TF/s

==========
Batch Size: 4
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 8.2949 sec
Average prefill latency: 0.1901 sec
Average tokens/sec: 241.32
Memory used: 33.92 GB
Done. we are killing the process
[rank0]:[W1114 06:04:15.176219807 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
