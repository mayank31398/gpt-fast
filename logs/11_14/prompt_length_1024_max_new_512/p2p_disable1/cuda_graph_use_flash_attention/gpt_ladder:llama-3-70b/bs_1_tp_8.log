W1114 05:56:07.776000 2356542 site-packages/torch/distributed/run.py:793] 
W1114 05:56:07.776000 2356542 site-packages/torch/distributed/run.py:793] *****************************************
W1114 05:56:07.776000 2356542 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1114 05:56:07.776000 2356542 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTLadder(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x LadderTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=1280, bias=False)
        (wo): Linear(in_features=1024, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 0.65 seconds
CUDA_GRAPH are activate
Prefill latency: 0.10825596377253532 sec
Decode latency: 7.648746337741613 sec
Compilation time: 7.75 secondsCompilation time: 7.79 seconds

Compilation time: 7.77 seconds
Compilation time: 7.77 seconds
Compilation time: 7.73 seconds
Compilation time: 7.76 seconds
Compilation time: 7.73 seconds
Compilation time: 7.75 seconds
Prefill latency: 0.06905398797243834 sec
Decode latency: 7.63552007637918 sec
Prefill latency: 0.06830404233187437 sec
Decode latency: 7.647128921933472 sec
Prefill latency: 0.06892086379230022 sec
Decode latency: 7.655813626945019 sec
Prefill latency: 0.06894779112190008 sec
Decode latency: 7.644943122752011 sec
Prefill latency: 0.06896949652582407 sec
Decode latency: 7.643227536231279 sec
Time for inference 1: 7.71 sec total, 66.38 tokens/sec
Decode latency: 7.64 sec
Prefill latency: 0.07 sec
Bandwidth achieved: 1275.52 GB/s
FLOPS achieved: 3.83 TF/s

Prefill latency: 0.07459568046033382 sec
Decode latency: 7.65730070695281 sec
Time for inference 2: 7.73 sec total, 66.20 tokens/sec
Decode latency: 7.66 sec
Prefill latency: 0.07 sec
Bandwidth achieved: 1272.19 GB/s
FLOPS achieved: 3.82 TF/s

Prefill latency: 0.07155408430844545 sec
Decode latency: 7.651099261827767 sec
Time for inference 3: 7.72 sec total, 66.28 tokens/sec
Decode latency: 7.65 sec
Prefill latency: 0.07 sec
Bandwidth achieved: 1273.77 GB/s
FLOPS achieved: 3.82 TF/s

Prefill latency: 0.06885846517980099 sec
Decode latency: 7.650299409404397 sec
Time for inference 4: 7.72 sec total, 66.31 tokens/sec
Decode latency: 7.65 sec
Prefill latency: 0.07 sec
Bandwidth achieved: 1274.27 GB/s
FLOPS achieved: 3.82 TF/s

Prefill latency: 0.06935761496424675 sec
Decode latency: 7.665444789454341 sec
Time for inference 5: 7.74 sec total, 66.18 tokens/sec
Decode latency: 7.67 sec
Prefill latency: 0.07 sec
Bandwidth achieved: 1271.75 GB/s
FLOPS achieved: 3.82 TF/s

Prefill latency: 0.06820321641862392 sec
Decode latency: 7.647257257252932 sec
Time for inference 6: 7.72 sec total, 66.35 tokens/sec
Decode latency: 7.65 sec
Prefill latency: 0.07 sec
Bandwidth achieved: 1274.99 GB/s
FLOPS achieved: 3.82 TF/s

Prefill latency: 0.06936368718743324 sec
Decode latency: 7.648412828333676 sec
Time for inference 7: 7.72 sec total, 66.33 tokens/sec
Decode latency: 7.65 sec
Prefill latency: 0.07 sec
Bandwidth achieved: 1274.58 GB/s
FLOPS achieved: 3.82 TF/s

Prefill latency: 0.07257669232785702 sec
Decode latency: 7.6476219687610865 sec
Time for inference 8: 7.72 sec total, 66.30 tokens/sec
Decode latency: 7.65 sec
Prefill latency: 0.07 sec
Bandwidth achieved: 1274.12 GB/s
FLOPS achieved: 3.82 TF/s

Prefill latency: 0.07378576975315809 sec
Decode latency: 7.645026845857501 sec
Time for inference 9: 7.72 sec total, 66.32 tokens/sec
Decode latency: 7.65 sec
Prefill latency: 0.07 sec
Bandwidth achieved: 1274.38 GB/s
FLOPS achieved: 3.82 TF/s

Prefill latency: 0.06856843084096909 sec
Decode latency: 7.636577675119042 sec
Time for inference 10: 7.71 sec total, 66.44 tokens/sec
Decode latency: 7.64 sec
Prefill latency: 0.07 sec
Bandwidth achieved: 1276.69 GB/s
FLOPS achieved: 3.83 TF/s

==========
Batch Size: 1
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 7.6492 sec
Average prefill latency: 0.0706 sec
Average tokens/sec: 66.31
Memory used: 24.89 GB
Done. we are killing the process
[rank0]:[W1114 05:58:19.028568054 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
