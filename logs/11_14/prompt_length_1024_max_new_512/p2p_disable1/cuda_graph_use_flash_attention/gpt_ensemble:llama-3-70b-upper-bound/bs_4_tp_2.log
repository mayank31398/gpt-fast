W1114 06:44:53.617000 2373163 site-packages/torch/distributed/run.py:793] 
W1114 06:44:53.617000 2373163 site-packages/torch/distributed/run.py:793] *****************************************
W1114 06:44:53.617000 2373163 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1114 06:44:53.617000 2373163 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=5120, bias=False)
        (wo): Linear(in_features=4096, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=28672, bias=False)
        (w2): Linear(in_features=14336, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 1.32 seconds
CUDA_GRAPH are activate
Prefill latency: 0.5574513804167509 sec
Decode latency: 16.444583890028298 sec
Compilation time: 17.00 seconds
Compilation time: 17.02 seconds
Prefill latency: 0.5525011057034135 sec
Decode latency: 16.445645034313202 sec
Prefill latency: 0.5531055433675647 sec
Decode latency: 16.445353559218347 sec
Prefill latency: 0.5541042443364859 sec
Decode latency: 16.445140168070793 sec
Prefill latency: 0.5553728928789496 sec
Decode latency: 16.444751244038343 sec
Prefill latency: 0.554759668186307 sec
Decode latency: 16.445262706838548 sec
Time for inference 1: 17.00 sec total, 120.46 tokens/sec
Decode latency: 16.45 sec
Prefill latency: 0.55 sec
Bandwidth achieved: 8499.24 GB/s
FLOPS achieved: 25.50 TF/s

Prefill latency: 0.5526386592537165 sec
Decode latency: 16.389752261340618 sec
Time for inference 2: 16.94 sec total, 120.87 tokens/sec
Decode latency: 16.39 sec
Prefill latency: 0.55 sec
Bandwidth achieved: 8528.14 GB/s
FLOPS achieved: 25.58 TF/s

Prefill latency: 0.5525060715153813 sec
Decode latency: 16.445103728212416 sec
Time for inference 3: 17.00 sec total, 120.48 tokens/sec
Decode latency: 16.45 sec
Prefill latency: 0.55 sec
Bandwidth achieved: 8500.45 GB/s
FLOPS achieved: 25.50 TF/s

Prefill latency: 0.5523776914924383 sec
Decode latency: 16.444887573830783 sec
Time for inference 4: 17.00 sec total, 120.48 tokens/sec
Decode latency: 16.44 sec
Prefill latency: 0.55 sec
Bandwidth achieved: 8500.57 GB/s
FLOPS achieved: 25.50 TF/s

Prefill latency: 0.5520528564229608 sec
Decode latency: 16.445225448347628 sec
Time for inference 5: 17.00 sec total, 120.48 tokens/sec
Decode latency: 16.45 sec
Prefill latency: 0.55 sec
Bandwidth achieved: 8500.58 GB/s
FLOPS achieved: 25.50 TF/s

Prefill latency: 0.5518471794202924 sec
Decode latency: 16.444856454618275 sec
Time for inference 6: 17.00 sec total, 120.49 tokens/sec
Decode latency: 16.44 sec
Prefill latency: 0.55 sec
Bandwidth achieved: 8500.88 GB/s
FLOPS achieved: 25.50 TF/s

Prefill latency: 0.553084172308445 sec
Decode latency: 16.444943712092936 sec
Time for inference 7: 17.00 sec total, 120.48 tokens/sec
Decode latency: 16.44 sec
Prefill latency: 0.55 sec
Bandwidth achieved: 8500.23 GB/s
FLOPS achieved: 25.50 TF/s

Prefill latency: 0.5534241683781147 sec
Decode latency: 16.44385894574225 sec
Time for inference 8: 17.00 sec total, 120.48 tokens/sec
Decode latency: 16.44 sec
Prefill latency: 0.55 sec
Bandwidth achieved: 8500.61 GB/s
FLOPS achieved: 25.50 TF/s

Prefill latency: 0.5533986641094089 sec
Decode latency: 16.445366827771068 sec
Time for inference 9: 17.00 sec total, 120.47 tokens/sec
Decode latency: 16.45 sec
Prefill latency: 0.55 sec
Bandwidth achieved: 8499.85 GB/s
FLOPS achieved: 25.50 TF/s

Prefill latency: 0.5522403744980693 sec
Decode latency: 16.442166784778237 sec
Time for inference 10: 17.00 sec total, 120.50 tokens/sec
Decode latency: 16.44 sec
Prefill latency: 0.55 sec
Bandwidth achieved: 8502.03 GB/s
FLOPS achieved: 25.51 TF/s

==========
Batch Size: 4
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 16.4391 sec
Average prefill latency: 0.5528 sec
Average tokens/sec: 120.52
Memory used: 75.89 GB
[rank0]:[W1114 06:49:16.106199071 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank1]:[W1114 06:49:16.222099133 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Done. we are killing the process
[rank0]:[W1114 06:49:17.414289719 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
