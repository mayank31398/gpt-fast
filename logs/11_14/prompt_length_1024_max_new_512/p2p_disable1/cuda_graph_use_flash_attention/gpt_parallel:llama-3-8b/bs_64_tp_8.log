W1114 08:45:21.630000 2793660 site-packages/torch/distributed/run.py:793] 
W1114 08:45:21.630000 2793660 site-packages/torch/distributed/run.py:793] *****************************************
W1114 08:45:21.630000 2793660 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1114 08:45:21.630000 2793660 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTParallel(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x ParallelTransformerBlock(
      semi_compiled = False
      (attention): FuseAttentionMLP(
        (wqkv1): Linear(in_features=4096, out_features=4352, bias=False)
        (wo): Linear(in_features=512, out_features=4096, bias=False)
        (w2): Linear(in_features=1792, out_features=4096, bias=False)
      )
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.06 seconds
CUDA_GRAPH are activate
Prefill latency: 0.48641328141093254 sec
Decode latency: 4.2955998703837395 sec
Compilation time: 4.78 secondsCompilation time: 4.79 seconds
Compilation time: 4.78 seconds

Compilation time: 4.78 seconds
Compilation time: 4.78 seconds
Compilation time: 4.78 seconds
Compilation time: 4.78 seconds
Compilation time: 4.78 seconds
Prefill latency: 0.4777946751564741 sec
Decode latency: 4.173386907204986 sec
Prefill latency: 0.47574077546596527 sec
Decode latency: 4.186947455629706 sec
Prefill latency: 0.47757067903876305 sec
Decode latency: 4.157767524942756 sec
Prefill latency: 0.47771719843149185 sec
Decode latency: 4.17272780276835 sec
Prefill latency: 0.47644514217972755 sec
Decode latency: 4.175346951931715 sec
Time for inference 1: 4.65 sec total, 7042.26 tokens/sec
Decode latency: 4.18 sec
Prefill latency: 0.48 sec
Bandwidth achieved: 19688.57 GB/s
FLOPS achieved: 59.07 TF/s

Prefill latency: 0.47776287235319614 sec
Decode latency: 4.151927176862955 sec
Time for inference 2: 4.63 sec total, 7076.09 tokens/sec
Decode latency: 4.15 sec
Prefill latency: 0.48 sec
Bandwidth achieved: 19783.14 GB/s
FLOPS achieved: 59.35 TF/s

Prefill latency: 0.47791074216365814 sec
Decode latency: 4.160318544134498 sec
Time for inference 3: 4.64 sec total, 7063.02 tokens/sec
Decode latency: 4.16 sec
Prefill latency: 0.48 sec
Bandwidth achieved: 19746.62 GB/s
FLOPS achieved: 59.24 TF/s

Prefill latency: 0.47531446255743504 sec
Decode latency: 4.1643914468586445 sec
Time for inference 4: 4.64 sec total, 7060.83 tokens/sec
Decode latency: 4.16 sec
Prefill latency: 0.48 sec
Bandwidth achieved: 19740.47 GB/s
FLOPS achieved: 59.22 TF/s

Prefill latency: 0.4793681539595127 sec
Decode latency: 4.150597771629691 sec
Time for inference 5: 4.63 sec total, 7075.01 tokens/sec
Decode latency: 4.15 sec
Prefill latency: 0.48 sec
Bandwidth achieved: 19780.13 GB/s
FLOPS achieved: 59.34 TF/s

Prefill latency: 0.47775269486010075 sec
Decode latency: 4.172606253996491 sec
Time for inference 6: 4.65 sec total, 7044.55 tokens/sec
Decode latency: 4.17 sec
Prefill latency: 0.48 sec
Bandwidth achieved: 19694.96 GB/s
FLOPS achieved: 59.08 TF/s

Prefill latency: 0.47725870460271835 sec
Decode latency: 4.148979991674423 sec
Time for inference 7: 4.63 sec total, 7081.20 tokens/sec
Decode latency: 4.15 sec
Prefill latency: 0.48 sec
Bandwidth achieved: 19797.44 GB/s
FLOPS achieved: 59.39 TF/s

Prefill latency: 0.47687449865043163 sec
Decode latency: 4.163817722350359 sec
Time for inference 8: 4.64 sec total, 7059.40 tokens/sec
Decode latency: 4.16 sec
Prefill latency: 0.48 sec
Bandwidth achieved: 19736.48 GB/s
FLOPS achieved: 59.21 TF/s

Prefill latency: 0.4775167237967253 sec
Decode latency: 4.197234222665429 sec
Time for inference 9: 4.68 sec total, 7007.97 tokens/sec
Decode latency: 4.20 sec
Prefill latency: 0.48 sec
Bandwidth achieved: 19592.69 GB/s
FLOPS achieved: 58.78 TF/s

Prefill latency: 0.4774216916412115 sec
Decode latency: 4.15899240411818 sec
Time for inference 10: 4.64 sec total, 7065.99 tokens/sec
Decode latency: 4.16 sec
Prefill latency: 0.48 sec
Bandwidth achieved: 19754.92 GB/s
FLOPS achieved: 59.26 TF/s

==========
Batch Size: 64
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 4.1644 sec
Average prefill latency: 0.4774 sec
Average tokens/sec: 7057.63
Memory used: 43.48 GB
Done. we are killing the process
[rank0]:[W1114 08:46:59.347152414 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
