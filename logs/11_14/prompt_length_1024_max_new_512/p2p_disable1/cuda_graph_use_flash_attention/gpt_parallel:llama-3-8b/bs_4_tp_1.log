flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTParallel(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x ParallelTransformerBlock(
      semi_compiled = False
      (attention): FuseAttentionMLP(
        (wqkv1): Linear(in_features=4096, out_features=34816, bias=False)
        (wo): Linear(in_features=4096, out_features=4096, bias=False)
        (w2): Linear(in_features=14336, out_features=4096, bias=False)
      )
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.12 seconds
CUDA_GRAPH are activate
[rank0]:[W1114 08:28:23.123473924 CUDAGraph.cpp:133] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
Prefill latency: 0.12908321619033813 sec
Decode latency: 4.7758315410465 sec
Compilation time: 5.06 seconds
Prefill latency: 0.12871996499598026 sec
Decode latency: 4.7763362396508455 sec
Prefill latency: 0.12868909537792206 sec
Decode latency: 4.774373486638069 sec
Prefill latency: 0.12887770682573318 sec
Decode latency: 4.774420939385891 sec
Prefill latency: 0.12809832021594048 sec
Decode latency: 4.7752783838659525 sec
Prefill latency: 0.12890649400651455 sec
Decode latency: 4.7754512168467045 sec
Time for inference 1: 4.91 sec total, 417.52 tokens/sec
Decode latency: 4.78 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6266.84 GB/s
FLOPS achieved: 18.80 TF/s

Prefill latency: 0.12848464399576187 sec
Decode latency: 4.774287350475788 sec
Time for inference 2: 4.90 sec total, 417.65 tokens/sec
Decode latency: 4.77 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6268.71 GB/s
FLOPS achieved: 18.81 TF/s

Prefill latency: 0.1305624358355999 sec
Decode latency: 4.77560380846262 sec
Time for inference 3: 4.91 sec total, 417.36 tokens/sec
Decode latency: 4.78 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6264.33 GB/s
FLOPS achieved: 18.79 TF/s

Prefill latency: 0.13008540496230125 sec
Decode latency: 4.77551956474781 sec
Time for inference 4: 4.91 sec total, 417.42 tokens/sec
Decode latency: 4.78 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6265.23 GB/s
FLOPS achieved: 18.80 TF/s

Prefill latency: 0.12990246899425983 sec
Decode latency: 4.774876207113266 sec
Time for inference 5: 4.91 sec total, 417.48 tokens/sec
Decode latency: 4.77 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6266.27 GB/s
FLOPS achieved: 18.80 TF/s

Prefill latency: 0.1292971521615982 sec
Decode latency: 4.775314791128039 sec
Time for inference 6: 4.91 sec total, 417.50 tokens/sec
Decode latency: 4.78 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6266.51 GB/s
FLOPS achieved: 18.80 TF/s

Prefill latency: 0.1294251959770918 sec
Decode latency: 4.775558223947883 sec
Time for inference 7: 4.91 sec total, 417.47 tokens/sec
Decode latency: 4.78 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6266.05 GB/s
FLOPS achieved: 18.80 TF/s

Prefill latency: 0.129395242780447 sec
Decode latency: 4.775050844997168 sec
Time for inference 8: 4.91 sec total, 417.50 tokens/sec
Decode latency: 4.78 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6266.47 GB/s
FLOPS achieved: 18.80 TF/s

Prefill latency: 0.12914320454001427 sec
Decode latency: 4.774616057053208 sec
Time for inference 9: 4.90 sec total, 417.56 tokens/sec
Decode latency: 4.77 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6267.39 GB/s
FLOPS achieved: 18.80 TF/s

Prefill latency: 0.12927203625440598 sec
Decode latency: 4.775166288018227 sec
Time for inference 10: 4.91 sec total, 417.51 tokens/sec
Decode latency: 4.78 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6266.68 GB/s
FLOPS achieved: 18.80 TF/s

==========
Batch Size: 4
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 4.7751 sec
Average prefill latency: 0.1294 sec
Average tokens/sec: 417.50
Memory used: 19.99 GB
Done. we are killing the process
[rank0]:[W1114 08:29:37.087608246 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
