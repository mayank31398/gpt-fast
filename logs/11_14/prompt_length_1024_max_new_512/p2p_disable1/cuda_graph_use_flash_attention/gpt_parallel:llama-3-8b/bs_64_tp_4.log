W1114 08:43:47.201000 2792931 site-packages/torch/distributed/run.py:793] 
W1114 08:43:47.201000 2792931 site-packages/torch/distributed/run.py:793] *****************************************
W1114 08:43:47.201000 2792931 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1114 08:43:47.201000 2792931 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTParallel(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x ParallelTransformerBlock(
      semi_compiled = False
      (attention): FuseAttentionMLP(
        (wqkv1): Linear(in_features=4096, out_features=8704, bias=False)
        (wo): Linear(in_features=1024, out_features=4096, bias=False)
        (w2): Linear(in_features=3584, out_features=4096, bias=False)
      )
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 0.98 seconds
CUDA_GRAPH are activate
Prefill latency: 0.8372342027723789 sec
Decode latency: 4.028020879253745 sec
Compilation time: 4.73 seconds
Compilation time: 4.93 seconds
Compilation time: 5.01 seconds
Compilation time: 4.87 seconds
Prefill latency: 0.6963129956275225 sec
Decode latency: 4.02758982218802 sec
Prefill latency: 0.6961497664451599 sec
Decode latency: 4.0299737975001335 sec
Prefill latency: 0.6958372965455055 sec
Decode latency: 4.028010146692395 sec
Prefill latency: 0.6970716863870621 sec
Decode latency: 4.027953717857599 sec
Prefill latency: 0.6939120776951313 sec
Decode latency: 4.033223764970899 sec
Time for inference 1: 4.73 sec total, 6930.45 tokens/sec
Decode latency: 4.03 sec
Prefill latency: 0.69 sec
Bandwidth achieved: 31468.43 GB/s
FLOPS achieved: 94.41 TF/s

Prefill latency: 0.6962976139038801 sec
Decode latency: 4.030592320486903 sec
Time for inference 2: 4.73 sec total, 6930.94 tokens/sec
Decode latency: 4.03 sec
Prefill latency: 0.70 sec
Bandwidth achieved: 31470.67 GB/s
FLOPS achieved: 94.41 TF/s

Prefill latency: 0.6966416127979755 sec
Decode latency: 4.030873246490955 sec
Time for inference 3: 4.73 sec total, 6929.79 tokens/sec
Decode latency: 4.03 sec
Prefill latency: 0.70 sec
Bandwidth achieved: 31465.44 GB/s
FLOPS achieved: 94.40 TF/s

Prefill latency: 0.6963500939309597 sec
Decode latency: 4.029976526275277 sec
Time for inference 4: 4.73 sec total, 6931.78 tokens/sec
Decode latency: 4.03 sec
Prefill latency: 0.70 sec
Bandwidth achieved: 31474.48 GB/s
FLOPS achieved: 94.42 TF/s

Prefill latency: 0.695743627846241 sec
Decode latency: 4.031577616930008 sec
Time for inference 5: 4.73 sec total, 6930.23 tokens/sec
Decode latency: 4.03 sec
Prefill latency: 0.70 sec
Bandwidth achieved: 31467.44 GB/s
FLOPS achieved: 94.40 TF/s

Prefill latency: 0.6965796146541834 sec
Decode latency: 4.029644252732396 sec
Time for inference 6: 4.73 sec total, 6931.76 tokens/sec
Decode latency: 4.03 sec
Prefill latency: 0.70 sec
Bandwidth achieved: 31474.39 GB/s
FLOPS achieved: 94.42 TF/s

Prefill latency: 0.6956052649766207 sec
Decode latency: 4.030226930975914 sec
Time for inference 7: 4.73 sec total, 6932.26 tokens/sec
Decode latency: 4.03 sec
Prefill latency: 0.70 sec
Bandwidth achieved: 31476.65 GB/s
FLOPS achieved: 94.43 TF/s

Prefill latency: 0.6956465449184179 sec
Decode latency: 4.029008070006967 sec
Time for inference 8: 4.73 sec total, 6934.17 tokens/sec
Decode latency: 4.03 sec
Prefill latency: 0.70 sec
Bandwidth achieved: 31485.32 GB/s
FLOPS achieved: 94.46 TF/s

Prefill latency: 0.6960219275206327 sec
Decode latency: 4.030946360900998 sec
Time for inference 9: 4.73 sec total, 6930.79 tokens/sec
Decode latency: 4.03 sec
Prefill latency: 0.70 sec
Bandwidth achieved: 31469.98 GB/s
FLOPS achieved: 94.41 TF/s

Prefill latency: 0.6968176886439323 sec
Decode latency: 4.030260747298598 sec
Time for inference 10: 4.73 sec total, 6930.62 tokens/sec
Decode latency: 4.03 sec
Prefill latency: 0.70 sec
Bandwidth achieved: 31469.22 GB/s
FLOPS achieved: 94.41 TF/s

==========
Batch Size: 64
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 4.0306 sec
Average prefill latency: 0.6960 sec
Average tokens/sec: 6931.28
Memory used: 48.40 GB
Done. we are killing the process
[rank0]:[W1114 08:45:16.368286094 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
