flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTParallel(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x ParallelTransformerBlock(
      semi_compiled = False
      (attention): FuseAttentionMLP(
        (wqkv1): Linear(in_features=4096, out_features=34816, bias=False)
        (wo): Linear(in_features=4096, out_features=4096, bias=False)
        (w2): Linear(in_features=14336, out_features=4096, bias=False)
      )
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.02 seconds
CUDA_GRAPH are activate
Prefill latency: 0.03325996361672878 sec
Decode latency: 4.406611517071724 sec
Compilation time: 4.44 seconds
Prefill latency: 0.03294268064200878 sec
Decode latency: 4.4059670474380255 sec
Prefill latency: 0.032932231202721596 sec
Decode latency: 4.405491195619106 sec
Prefill latency: 0.03292791359126568 sec
Decode latency: 4.405703397467732 sec
Prefill latency: 0.03293163329362869 sec
Decode latency: 4.406011136248708 sec
Prefill latency: 0.032878145575523376 sec
Decode latency: 4.405534479767084 sec
Time for inference 1: 4.44 sec total, 115.34 tokens/sec
Decode latency: 4.41 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 1731.15 GB/s
FLOPS achieved: 5.19 TF/s

Prefill latency: 0.03291292116045952 sec
Decode latency: 4.40570386312902 sec
Time for inference 2: 4.44 sec total, 115.33 tokens/sec
Decode latency: 4.41 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 1731.06 GB/s
FLOPS achieved: 5.19 TF/s

Prefill latency: 0.0329024363309145 sec
Decode latency: 4.40599287673831 sec
Time for inference 3: 4.44 sec total, 115.32 tokens/sec
Decode latency: 4.41 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 1730.96 GB/s
FLOPS achieved: 5.19 TF/s

Prefill latency: 0.0329297985881567 sec
Decode latency: 4.405765311792493 sec
Time for inference 4: 4.44 sec total, 115.33 tokens/sec
Decode latency: 4.41 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 1731.02 GB/s
FLOPS achieved: 5.19 TF/s

Prefill latency: 0.03289628215134144 sec
Decode latency: 4.405568530783057 sec
Time for inference 5: 4.44 sec total, 115.34 tokens/sec
Decode latency: 4.41 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 1731.13 GB/s
FLOPS achieved: 5.19 TF/s

Prefill latency: 0.03285398706793785 sec
Decode latency: 4.4056933503597975 sec
Time for inference 6: 4.44 sec total, 115.33 tokens/sec
Decode latency: 4.41 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 1731.12 GB/s
FLOPS achieved: 5.19 TF/s

Prefill latency: 0.03290628641843796 sec
Decode latency: 4.406410524621606 sec
Time for inference 7: 4.44 sec total, 115.31 tokens/sec
Decode latency: 4.41 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 1730.77 GB/s
FLOPS achieved: 5.19 TF/s

Prefill latency: 0.03292694129049778 sec
Decode latency: 4.406077923253179 sec
Time for inference 8: 4.44 sec total, 115.32 tokens/sec
Decode latency: 4.41 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 1730.92 GB/s
FLOPS achieved: 5.19 TF/s

Prefill latency: 0.03289102762937546 sec
Decode latency: 4.406216768547893 sec
Time for inference 9: 4.44 sec total, 115.31 tokens/sec
Decode latency: 4.41 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 1730.82 GB/s
FLOPS achieved: 5.19 TF/s

Prefill latency: 0.03289366886019707 sec
Decode latency: 4.405515816062689 sec
Time for inference 10: 4.44 sec total, 115.33 tokens/sec
Decode latency: 4.41 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 1731.13 GB/s
FLOPS achieved: 5.19 TF/s

==========
Batch Size: 1
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 4.4058 sec
Average prefill latency: 0.0329 sec
Average tokens/sec: 115.33
Memory used: 17.15 GB
Done. we are killing the process
[rank0]:[W1114 08:24:59.525043513 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
