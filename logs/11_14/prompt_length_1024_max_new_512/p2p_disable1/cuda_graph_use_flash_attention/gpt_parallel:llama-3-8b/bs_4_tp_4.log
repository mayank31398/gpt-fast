W1114 08:30:51.746000 2786131 site-packages/torch/distributed/run.py:793] 
W1114 08:30:51.746000 2786131 site-packages/torch/distributed/run.py:793] *****************************************
W1114 08:30:51.746000 2786131 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1114 08:30:51.746000 2786131 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTParallel(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x ParallelTransformerBlock(
      semi_compiled = False
      (attention): FuseAttentionMLP(
        (wqkv1): Linear(in_features=4096, out_features=8704, bias=False)
        (wo): Linear(in_features=1024, out_features=4096, bias=False)
        (w2): Linear(in_features=3584, out_features=4096, bias=False)
      )
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.04 seconds
CUDA_GRAPH are activate
Prefill latency: 0.047850536182522774 sec
Decode latency: 3.0190095137804747 sec
Compilation time: 3.07 seconds
Compilation time: 3.09 seconds
Compilation time: 3.11 secondsCompilation time: 3.09 seconds

Prefill latency: 0.047690583392977715 sec
Decode latency: 3.0165084078907967 sec
Prefill latency: 0.047608183696866035 sec
Decode latency: 3.016304386779666 sec
Prefill latency: 0.047585127875208855 sec
Decode latency: 3.0154979210346937 sec
Prefill latency: 0.04772409424185753 sec
Decode latency: 3.015282655134797 sec
Prefill latency: 0.047653913497924805 sec
Decode latency: 3.0165941305458546 sec
Time for inference 1: 3.07 sec total, 668.16 tokens/sec
Decode latency: 3.02 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 3033.85 GB/s
FLOPS achieved: 9.10 TF/s

Prefill latency: 0.04768608324229717 sec
Decode latency: 3.0162024591118097 sec
Time for inference 2: 3.06 sec total, 668.21 tokens/sec
Decode latency: 3.02 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 3034.06 GB/s
FLOPS achieved: 9.10 TF/s

Prefill latency: 0.04766012355685234 sec
Decode latency: 3.0173290632665157 sec
Time for inference 3: 3.07 sec total, 667.98 tokens/sec
Decode latency: 3.02 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 3033.05 GB/s
FLOPS achieved: 9.10 TF/s

Prefill latency: 0.047650376334786415 sec
Decode latency: 3.015894964337349 sec
Time for inference 4: 3.06 sec total, 668.29 tokens/sec
Decode latency: 3.02 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 3034.43 GB/s
FLOPS achieved: 9.10 TF/s

Prefill latency: 0.04779903590679169 sec
Decode latency: 3.0170043352991343 sec
Time for inference 5: 3.07 sec total, 667.93 tokens/sec
Decode latency: 3.02 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 3032.81 GB/s
FLOPS achieved: 9.10 TF/s

Prefill latency: 0.047589948400855064 sec
Decode latency: 3.015843376517296 sec
Time for inference 6: 3.06 sec total, 668.30 tokens/sec
Decode latency: 3.02 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 3034.47 GB/s
FLOPS achieved: 9.10 TF/s

Prefill latency: 0.047703707590699196 sec
Decode latency: 3.0169826932251453 sec
Time for inference 7: 3.07 sec total, 668.03 tokens/sec
Decode latency: 3.02 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 3033.28 GB/s
FLOPS achieved: 9.10 TF/s

Prefill latency: 0.04759972542524338 sec
Decode latency: 3.016664432361722 sec
Time for inference 8: 3.07 sec total, 668.12 tokens/sec
Decode latency: 3.02 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 3033.69 GB/s
FLOPS achieved: 9.10 TF/s

Prefill latency: 0.047667378559708595 sec
Decode latency: 3.0144164245575666 sec
Time for inference 9: 3.06 sec total, 668.63 tokens/sec
Decode latency: 3.01 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 3035.97 GB/s
FLOPS achieved: 9.11 TF/s

Prefill latency: 0.047756968066096306 sec
Decode latency: 3.0186482965946198 sec
Time for inference 10: 3.07 sec total, 667.67 tokens/sec
Decode latency: 3.02 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 3031.63 GB/s
FLOPS achieved: 9.09 TF/s

==========
Batch Size: 4
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 3.0166 sec
Average prefill latency: 0.0477 sec
Average tokens/sec: 668.13
Memory used: 8.43 GB
Done. we are killing the process
[rank0]:[W1114 08:31:50.120836209 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
