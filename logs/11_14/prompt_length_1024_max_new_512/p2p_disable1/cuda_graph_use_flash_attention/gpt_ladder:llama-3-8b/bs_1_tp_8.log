W1114 07:24:24.804000 2745209 site-packages/torch/distributed/run.py:793] 
W1114 07:24:24.804000 2745209 site-packages/torch/distributed/run.py:793] *****************************************
W1114 07:24:24.804000 2745209 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1114 07:24:24.804000 2745209 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTLadder(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x LadderTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=768, bias=False)
        (wo): Linear(in_features=512, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=3584, bias=False)
        (w2): Linear(in_features=1792, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.12 seconds
CUDA_GRAPH are activate
Prefill latency: 0.046437567099928856 sec
Decode latency: 2.521132431924343 sec
Compilation time: 2.54 seconds
Compilation time: 2.55 seconds
Compilation time: 2.57 seconds
Compilation time: 2.55 seconds
Compilation time: 2.54 seconds
Compilation time: 2.57 seconds
Compilation time: 2.56 seconds
Compilation time: 2.54 seconds
Prefill latency: 0.017828544601798058 sec
Decode latency: 2.4983366932719946 sec
Prefill latency: 0.015322966501116753 sec
Decode latency: 2.550371779128909 sec
Prefill latency: 0.015067877247929573 sec
Decode latency: 2.52650972828269 sec
Prefill latency: 0.015089014545083046 sec
Decode latency: 2.5432539749890566 sec
Prefill latency: 0.019651135429739952 sec
Decode latency: 2.5087637323886156 sec
Time for inference 1: 2.53 sec total, 202.40 tokens/sec
Decode latency: 2.51 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 565.93 GB/s
FLOPS achieved: 1.70 TF/s

Prefill latency: 0.031175583600997925 sec
Decode latency: 2.526540467515588 sec
Time for inference 2: 2.56 sec total, 200.06 tokens/sec
Decode latency: 2.53 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 559.37 GB/s
FLOPS achieved: 1.68 TF/s

Prefill latency: 0.01550031453371048 sec
Decode latency: 2.50404910184443 sec
Time for inference 3: 2.52 sec total, 203.12 tokens/sec
Decode latency: 2.50 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 567.92 GB/s
FLOPS achieved: 1.70 TF/s

Prefill latency: 0.017241213470697403 sec
Decode latency: 2.5182916689664125 sec
Time for inference 4: 2.54 sec total, 201.79 tokens/sec
Decode latency: 2.52 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 564.21 GB/s
FLOPS achieved: 1.69 TF/s

Prefill latency: 0.014775114133954048 sec
Decode latency: 2.5143312700092793 sec
Time for inference 5: 2.53 sec total, 202.32 tokens/sec
Decode latency: 2.51 sec
Prefill latency: 0.01 sec
Bandwidth achieved: 565.69 GB/s
FLOPS achieved: 1.70 TF/s

Prefill latency: 0.021302785724401474 sec
Decode latency: 2.5367239452898502 sec
Time for inference 6: 2.56 sec total, 200.06 tokens/sec
Decode latency: 2.54 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 559.36 GB/s
FLOPS achieved: 1.68 TF/s

Prefill latency: 0.01629607565701008 sec
Decode latency: 2.5072552785277367 sec
Time for inference 7: 2.52 sec total, 202.80 tokens/sec
Decode latency: 2.51 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 567.02 GB/s
FLOPS achieved: 1.70 TF/s

Prefill latency: 0.01527186669409275 sec
Decode latency: 2.514647651463747 sec
Time for inference 8: 2.53 sec total, 202.28 tokens/sec
Decode latency: 2.51 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 565.60 GB/s
FLOPS achieved: 1.70 TF/s

Prefill latency: 0.015145987272262573 sec
Decode latency: 2.527363210916519 sec
Time for inference 9: 2.54 sec total, 201.29 tokens/sec
Decode latency: 2.53 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 562.80 GB/s
FLOPS achieved: 1.69 TF/s

Prefill latency: 0.0183400958776474 sec
Decode latency: 2.5439262203872204 sec
Time for inference 10: 2.56 sec total, 199.73 tokens/sec
Decode latency: 2.54 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 558.46 GB/s
FLOPS achieved: 1.68 TF/s

==========
Batch Size: 1
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 2.5202 sec
Average prefill latency: 0.0185 sec
Average tokens/sec: 201.58
Memory used: 4.98 GB
Done. we are killing the process
[rank0]:[W1114 07:25:23.092733974 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
