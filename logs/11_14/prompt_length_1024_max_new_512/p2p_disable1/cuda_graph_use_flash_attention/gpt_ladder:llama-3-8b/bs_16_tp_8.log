W1114 07:34:23.650000 2750572 site-packages/torch/distributed/run.py:793] 
W1114 07:34:23.650000 2750572 site-packages/torch/distributed/run.py:793] *****************************************
W1114 07:34:23.650000 2750572 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1114 07:34:23.650000 2750572 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTLadder(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x LadderTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=768, bias=False)
        (wo): Linear(in_features=512, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=3584, bias=False)
        (w2): Linear(in_features=1792, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.36 seconds
CUDA_GRAPH are activate
[rank4]:[W1114 07:34:44.830205678 CUDAGraph.cpp:133] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
Prefill latency: 0.1812783796340227 sec
Decode latency: 3.3482312224805355 sec
Compilation time: 3.51 seconds
Compilation time: 3.47 seconds
Compilation time: 3.53 seconds
Compilation time: 3.50 seconds
Compilation time: 3.47 seconds
Compilation time: 3.48 seconds
Compilation time: 3.48 seconds
Compilation time: 3.53 seconds
Prefill latency: 0.11778043583035469 sec
Decode latency: 3.3547364939004183 sec
Prefill latency: 0.11843476258218288 sec
Decode latency: 3.3449598476290703 sec
Prefill latency: 0.1177283339202404 sec
Decode latency: 3.363475736230612 sec
Prefill latency: 0.11795943044126034 sec
Decode latency: 3.3997968677431345 sec
Prefill latency: 0.11850181967020035 sec
Decode latency: 3.3417602851986885 sec
Time for inference 1: 3.46 sec total, 2366.40 tokens/sec
Decode latency: 3.34 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 6616.55 GB/s
FLOPS achieved: 19.85 TF/s

Prefill latency: 0.11869018338620663 sec
Decode latency: 3.335864968597889 sec
Time for inference 2: 3.46 sec total, 2370.46 tokens/sec
Decode latency: 3.34 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 6627.88 GB/s
FLOPS achieved: 19.88 TF/s

Prefill latency: 0.11795816197991371 sec
Decode latency: 3.3859145045280457 sec
Time for inference 3: 3.51 sec total, 2337.11 tokens/sec
Decode latency: 3.39 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 6534.65 GB/s
FLOPS achieved: 19.60 TF/s

Prefill latency: 0.11766252480447292 sec
Decode latency: 3.3679357450455427 sec
Time for inference 4: 3.49 sec total, 2349.38 tokens/sec
Decode latency: 3.37 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 6568.95 GB/s
FLOPS achieved: 19.71 TF/s

Prefill latency: 0.11772135831415653 sec
Decode latency: 3.3531429562717676 sec
Time for inference 5: 3.47 sec total, 2359.21 tokens/sec
Decode latency: 3.35 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 6596.43 GB/s
FLOPS achieved: 19.79 TF/s

Prefill latency: 0.1192995235323906 sec
Decode latency: 3.362574240192771 sec
Time for inference 6: 3.48 sec total, 2351.91 tokens/sec
Decode latency: 3.36 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 6576.02 GB/s
FLOPS achieved: 19.73 TF/s

Prefill latency: 0.11864051967859268 sec
Decode latency: 3.340685084462166 sec
Time for inference 7: 3.46 sec total, 2367.15 tokens/sec
Decode latency: 3.34 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 6618.64 GB/s
FLOPS achieved: 19.86 TF/s

Prefill latency: 0.1184851061552763 sec
Decode latency: 3.3426275718957186 sec
Time for inference 8: 3.46 sec total, 2366.06 tokens/sec
Decode latency: 3.34 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 6615.60 GB/s
FLOPS achieved: 19.85 TF/s

Prefill latency: 0.11810176260769367 sec
Decode latency: 3.3618160001933575 sec
Time for inference 9: 3.48 sec total, 2353.28 tokens/sec
Decode latency: 3.36 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 6579.86 GB/s
FLOPS achieved: 19.74 TF/s

Prefill latency: 0.11813025735318661 sec
Decode latency: 3.3402163553982973 sec
Time for inference 10: 3.46 sec total, 2367.82 tokens/sec
Decode latency: 3.34 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 6620.52 GB/s
FLOPS achieved: 19.86 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 3.3533 sec
Average prefill latency: 0.1183 sec
Average tokens/sec: 2358.88
Memory used: 20.04 GB
Done. we are killing the process
[rank0]:[W1114 07:35:36.389464589 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
