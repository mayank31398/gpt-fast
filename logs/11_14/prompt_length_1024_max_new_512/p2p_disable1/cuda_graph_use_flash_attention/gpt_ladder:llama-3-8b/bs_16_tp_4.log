W1114 07:33:10.998000 2749972 site-packages/torch/distributed/run.py:793] 
W1114 07:33:10.998000 2749972 site-packages/torch/distributed/run.py:793] *****************************************
W1114 07:33:10.998000 2749972 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1114 07:33:10.998000 2749972 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTLadder(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x LadderTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=1536, bias=False)
        (wo): Linear(in_features=1024, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.26 seconds
CUDA_GRAPH are activate
Prefill latency: 0.46708477288484573 sec
Decode latency: 3.1797667406499386 sec
Compilation time: 3.79 secondsCompilation time: 3.37 seconds

Compilation time: 3.65 seconds
Compilation time: 3.65 seconds
Prefill latency: 0.1880177967250347 sec
Decode latency: 3.177482483908534 sec
Prefill latency: 0.18789668940007687 sec
Decode latency: 3.1784246042370796 sec
Prefill latency: 0.18822655640542507 sec
Decode latency: 3.177692510187626 sec
Prefill latency: 0.18771460093557835 sec
Decode latency: 3.1779946871101856 sec
Prefill latency: 0.1879744678735733 sec
Decode latency: 3.177693562582135 sec
Time for inference 1: 3.37 sec total, 2433.23 tokens/sec
Decode latency: 3.18 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 11048.97 GB/s
FLOPS achieved: 33.15 TF/s

Prefill latency: 0.1882445253431797 sec
Decode latency: 3.1769683975726366 sec
Time for inference 2: 3.37 sec total, 2433.60 tokens/sec
Decode latency: 3.18 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 11050.68 GB/s
FLOPS achieved: 33.15 TF/s

Prefill latency: 0.18793809227645397 sec
Decode latency: 3.177209373563528 sec
Time for inference 3: 3.37 sec total, 2433.73 tokens/sec
Decode latency: 3.18 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 11051.25 GB/s
FLOPS achieved: 33.15 TF/s

Prefill latency: 0.18856229819357395 sec
Decode latency: 3.1783281788229942 sec
Time for inference 4: 3.37 sec total, 2432.33 tokens/sec
Decode latency: 3.18 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 11044.88 GB/s
FLOPS achieved: 33.13 TF/s

Prefill latency: 0.1882161907851696 sec
Decode latency: 3.1775538083165884 sec
Time for inference 5: 3.37 sec total, 2432.97 tokens/sec
Decode latency: 3.18 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 11047.80 GB/s
FLOPS achieved: 33.14 TF/s

Prefill latency: 0.18742401711642742 sec
Decode latency: 3.177409417927265 sec
Time for inference 6: 3.37 sec total, 2433.78 tokens/sec
Decode latency: 3.18 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 11051.48 GB/s
FLOPS achieved: 33.15 TF/s

Prefill latency: 0.18843924440443516 sec
Decode latency: 3.178038589656353 sec
Time for inference 7: 3.37 sec total, 2432.77 tokens/sec
Decode latency: 3.18 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 11046.87 GB/s
FLOPS achieved: 33.14 TF/s

Prefill latency: 0.18883722089231014 sec
Decode latency: 3.1759608313441277 sec
Time for inference 8: 3.37 sec total, 2433.94 tokens/sec
Decode latency: 3.18 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 11052.19 GB/s
FLOPS achieved: 33.16 TF/s

Prefill latency: 0.18748986162245274 sec
Decode latency: 3.176015133038163 sec
Time for inference 9: 3.36 sec total, 2434.86 tokens/sec
Decode latency: 3.18 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 11056.40 GB/s
FLOPS achieved: 33.17 TF/s

Prefill latency: 0.18838358856737614 sec
Decode latency: 3.1758721098303795 sec
Time for inference 10: 3.37 sec total, 2434.38 tokens/sec
Decode latency: 3.18 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 11054.20 GB/s
FLOPS achieved: 33.16 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 3.1771 sec
Average prefill latency: 0.1882 sec
Average tokens/sec: 2433.56
Memory used: 23.58 GB
Done. we are killing the process
[rank0]:[W1114 07:34:18.983614936 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
