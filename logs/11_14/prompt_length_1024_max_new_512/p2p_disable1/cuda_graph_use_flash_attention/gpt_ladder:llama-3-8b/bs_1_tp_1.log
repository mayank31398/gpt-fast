flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTLadder(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x LadderTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
        (wo): Linear(in_features=4096, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=28672, bias=False)
        (w2): Linear(in_features=14336, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 0.26 seconds
CUDA_GRAPH are activate
[rank0]:[W1114 07:21:19.886434199 CUDAGraph.cpp:133] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
Prefill latency: 0.0339293759316206 sec
Decode latency: 4.3782771322876215 sec
Compilation time: 4.41 seconds
Prefill latency: 0.03370005264878273 sec
Decode latency: 4.37558108381927 sec
Prefill latency: 0.03367413766682148 sec
Decode latency: 4.374575925990939 sec
Prefill latency: 0.03371809050440788 sec
Decode latency: 4.371565161272883 sec
Prefill latency: 0.03362385742366314 sec
Decode latency: 4.27845318056643 sec
Prefill latency: 0.03362763486802578 sec
Decode latency: 4.374246606603265 sec
Time for inference 1: 4.41 sec total, 116.13 tokens/sec
Decode latency: 4.37 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 1743.15 GB/s
FLOPS achieved: 5.23 TF/s

Prefill latency: 0.033623043447732925 sec
Decode latency: 4.376598581671715 sec
Time for inference 2: 4.41 sec total, 116.07 tokens/sec
Decode latency: 4.38 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 1742.23 GB/s
FLOPS achieved: 5.23 TF/s

Prefill latency: 0.0337341483682394 sec
Decode latency: 4.371951488777995 sec
Time for inference 3: 4.41 sec total, 116.19 tokens/sec
Decode latency: 4.37 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 1744.03 GB/s
FLOPS achieved: 5.23 TF/s

Prefill latency: 0.0337371900677681 sec
Decode latency: 4.3723345920443535 sec
Time for inference 4: 4.41 sec total, 116.18 tokens/sec
Decode latency: 4.37 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 1743.84 GB/s
FLOPS achieved: 5.23 TF/s

Prefill latency: 0.03363847732543945 sec
Decode latency: 4.373056830838323 sec
Time for inference 5: 4.41 sec total, 116.16 tokens/sec
Decode latency: 4.37 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 1743.57 GB/s
FLOPS achieved: 5.23 TF/s

Prefill latency: 0.03361840732395649 sec
Decode latency: 4.3761069774627686 sec
Time for inference 6: 4.41 sec total, 116.09 tokens/sec
Decode latency: 4.38 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 1742.42 GB/s
FLOPS achieved: 5.23 TF/s

Prefill latency: 0.03363102674484253 sec
Decode latency: 4.3719657100737095 sec
Time for inference 7: 4.41 sec total, 116.19 tokens/sec
Decode latency: 4.37 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 1744.06 GB/s
FLOPS achieved: 5.23 TF/s

Prefill latency: 0.033705223351716995 sec
Decode latency: 4.407496927306056 sec
Time for inference 8: 4.44 sec total, 115.26 tokens/sec
Decode latency: 4.41 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 1730.09 GB/s
FLOPS achieved: 5.19 TF/s

Prefill latency: 0.03372894413769245 sec
Decode latency: 4.3604886047542095 sec
Time for inference 9: 4.40 sec total, 116.50 tokens/sec
Decode latency: 4.36 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 1748.58 GB/s
FLOPS achieved: 5.25 TF/s

Prefill latency: 0.03369978815317154 sec
Decode latency: 4.37275061942637 sec
Time for inference 10: 4.41 sec total, 116.17 tokens/sec
Decode latency: 4.37 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 1743.65 GB/s
FLOPS achieved: 5.23 TF/s

==========
Batch Size: 1
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 4.3757 sec
Average prefill latency: 0.0337 sec
Average tokens/sec: 116.09
Memory used: 17.45 GB
Done. we are killing the process
[rank0]:[W1114 07:22:25.166663279 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
