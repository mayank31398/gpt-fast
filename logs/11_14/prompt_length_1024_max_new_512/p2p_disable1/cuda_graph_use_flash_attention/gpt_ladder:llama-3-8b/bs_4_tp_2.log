W1114 07:26:52.843000 2746495 site-packages/torch/distributed/run.py:793] 
W1114 07:26:52.843000 2746495 site-packages/torch/distributed/run.py:793] *****************************************
W1114 07:26:52.843000 2746495 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1114 07:26:52.843000 2746495 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTLadder(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x LadderTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=3072, bias=False)
        (wo): Linear(in_features=2048, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=14336, bias=False)
        (w2): Linear(in_features=7168, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 0.39 seconds
CUDA_GRAPH are activate
Prefill latency: 0.17769812233746052 sec
Decode latency: 3.5100335255265236 sec
Compilation time: 3.69 seconds
Compilation time: 3.69 seconds
Prefill latency: 0.16890372149646282 sec
Decode latency: 3.509049743413925 sec
Prefill latency: 0.1685707475990057 sec
Decode latency: 3.5095905885100365 sec
Prefill latency: 0.16844877041876316 sec
Decode latency: 3.5093464888632298 sec
Prefill latency: 0.168675746768713 sec
Decode latency: 3.509878473356366 sec
Prefill latency: 0.16932630352675915 sec
Decode latency: 3.508958315476775 sec
Time for inference 1: 3.68 sec total, 556.63 tokens/sec
Decode latency: 3.51 sec
Prefill latency: 0.17 sec
Bandwidth achieved: 4470.05 GB/s
FLOPS achieved: 13.41 TF/s

Prefill latency: 0.16921737603843212 sec
Decode latency: 3.508897464722395 sec
Time for inference 2: 3.68 sec total, 556.63 tokens/sec
Decode latency: 3.51 sec
Prefill latency: 0.17 sec
Bandwidth achieved: 4470.06 GB/s
FLOPS achieved: 13.41 TF/s

Prefill latency: 0.16870468482375145 sec
Decode latency: 3.5093521513044834 sec
Time for inference 3: 3.68 sec total, 556.63 tokens/sec
Decode latency: 3.51 sec
Prefill latency: 0.17 sec
Bandwidth achieved: 4470.05 GB/s
FLOPS achieved: 13.41 TF/s

Prefill latency: 0.16866703890264034 sec
Decode latency: 3.509699923917651 sec
Time for inference 4: 3.68 sec total, 556.60 tokens/sec
Decode latency: 3.51 sec
Prefill latency: 0.17 sec
Bandwidth achieved: 4469.80 GB/s
FLOPS achieved: 13.41 TF/s

Prefill latency: 0.16837612353265285 sec
Decode latency: 3.5100984182208776 sec
Time for inference 5: 3.68 sec total, 556.59 tokens/sec
Decode latency: 3.51 sec
Prefill latency: 0.17 sec
Bandwidth achieved: 4469.74 GB/s
FLOPS achieved: 13.41 TF/s

Prefill latency: 0.16859967075288296 sec
Decode latency: 3.509748863056302 sec
Time for inference 6: 3.68 sec total, 556.59 tokens/sec
Decode latency: 3.51 sec
Prefill latency: 0.17 sec
Bandwidth achieved: 4469.69 GB/s
FLOPS achieved: 13.41 TF/s

Prefill latency: 0.16879015788435936 sec
Decode latency: 3.5099418312311172 sec
Time for inference 7: 3.68 sec total, 556.56 tokens/sec
Decode latency: 3.51 sec
Prefill latency: 0.17 sec
Bandwidth achieved: 4469.44 GB/s
FLOPS achieved: 13.41 TF/s

Prefill latency: 0.16843384504318237 sec
Decode latency: 3.5098173078149557 sec
Time for inference 8: 3.68 sec total, 556.64 tokens/sec
Decode latency: 3.51 sec
Prefill latency: 0.17 sec
Bandwidth achieved: 4470.15 GB/s
FLOPS achieved: 13.41 TF/s

Prefill latency: 0.16898063011467457 sec
Decode latency: 3.509754730388522 sec
Time for inference 9: 3.68 sec total, 556.55 tokens/sec
Decode latency: 3.51 sec
Prefill latency: 0.17 sec
Bandwidth achieved: 4469.38 GB/s
FLOPS achieved: 13.41 TF/s

Prefill latency: 0.16833669506013393 sec
Decode latency: 3.509852599352598 sec
Time for inference 10: 3.68 sec total, 556.62 tokens/sec
Decode latency: 3.51 sec
Prefill latency: 0.17 sec
Bandwidth achieved: 4469.94 GB/s
FLOPS achieved: 13.41 TF/s

==========
Batch Size: 4
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 3.5096 sec
Average prefill latency: 0.1687 sec
Average tokens/sec: 556.60
Memory used: 14.74 GB
Done. we are killing the process
[rank0]:[W1114 07:27:57.015311872 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
