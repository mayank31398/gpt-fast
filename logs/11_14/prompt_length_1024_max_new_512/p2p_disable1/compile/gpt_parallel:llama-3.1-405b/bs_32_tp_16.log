W1118 17:36:27.349000 2904866 site-packages/torch/distributed/run.py:793] 
W1118 17:36:27.349000 2904866 site-packages/torch/distributed/run.py:793] *****************************************
W1118 17:36:27.349000 2904866 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1118 17:36:27.349000 2904866 site-packages/torch/distributed/run.py:793] *****************************************
rank: 1, global_rank: 9, world_size: 8, global_world_size: 16
rank: 0, global_rank: 8, world_size: 8, global_world_size: 16
rank: 6, global_rank: 14, world_size: 8, global_world_size: 16
rank: 2, global_rank: 10, world_size: 8, global_world_size: 16
rank: 3, global_rank: 11, world_size: 8, global_world_size: 16
rank: 5, global_rank: 13, world_size: 8, global_world_size: 16
rank: 7, global_rank: 15, world_size: 8, global_world_size: 16
rank: 4, global_rank: 12, world_size: 8, global_world_size: 16
rank: 0, global_rank: 8, world_size: 8, global_world_size: 16
rank: 0, global_rank: 8, world_size: 8, global_world_size: 16
rank: 0, global_rank: 8, world_size: 8, global_world_size: 16
rank: 0, global_rank: 8, world_size: 8, global_world_size: 16
rank: 0, global_rank: 8, world_size: 8, global_world_size: 16
rank: 1, global_rank: 9, world_size: 8, global_world_size: 16
rank: 1, global_rank: 9, world_size: 8, global_world_size: 16
rank: 1, global_rank: 9, world_size: 8, global_world_size: 16
rank: 1, global_rank: 9, world_size: 8, global_world_size: 16
rank: 1, global_rank: 9, world_size: 8, global_world_size: 16
rank: 5, global_rank: 13, world_size: 8, global_world_size: 16
rank: 5, global_rank: 13, world_size: 8, global_world_size: 16
rank: 5, global_rank: 13, world_size: 8, global_world_size: 16
rank: 2, global_rank: 10, world_size: 8, global_world_size: 16
rank: 5, global_rank: 13, world_size: 8, global_world_size: 16
rank: 2, global_rank: 10, world_size: 8, global_world_size: 16
rank: 2, global_rank: 10, world_size: 8, global_world_size: 16
rank: 5, global_rank: 13, world_size: 8, global_world_size: 16
rank: 2, global_rank: 10, world_size: 8, global_world_size: 16
rank: 2, global_rank: 10, world_size: 8, global_world_size: 16
rank: 6, global_rank: 14, world_size: 8, global_world_size: 16
rank: 6, global_rank: 14, world_size: 8, global_world_size: 16
rank: 3, global_rank: 11, world_size: 8, global_world_size: 16
rank: 6, global_rank: 14, world_size: 8, global_world_size: 16
rank: 3, global_rank: 11, world_size: 8, global_world_size: 16
rank: 6, global_rank: 14, world_size: 8, global_world_size: 16
rank: 3, global_rank: 11, world_size: 8, global_world_size: 16
rank: 3, global_rank: 11, world_size: 8, global_world_size: 16
rank: 6, global_rank: 14, world_size: 8, global_world_size: 16
rank: 3, global_rank: 11, world_size: 8, global_world_size: 16
rank: 7, global_rank: 15, world_size: 8, global_world_size: 16
rank: 7, global_rank: 15, world_size: 8, global_world_size: 16
rank: 7, global_rank: 15, world_size: 8, global_world_size: 16
rank: 4, global_rank: 12, world_size: 8, global_world_size: 16
rank: 4, global_rank: 12, world_size: 8, global_world_size: 16
rank: 7, global_rank: 15, world_size: 8, global_world_size: 16
rank: 4, global_rank: 12, world_size: 8, global_world_size: 16
rank: 7, global_rank: 15, world_size: 8, global_world_size: 16
rank: 4, global_rank: 12, world_size: 8, global_world_size: 16
rank: 4, global_rank: 12, world_size: 8, global_world_size: 16
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
[rank10]:[E1118 17:38:47.337933592 ProcessGroupNCCL.cpp:627] [Rank 10] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=64, OpType=ALLREDUCE, NumelIn=536870912, NumelOut=536870912, Timeout(ms)=60000) ran for 73022 milliseconds before timing out.
[rank15]:[E1118 17:38:47.337934292 ProcessGroupNCCL.cpp:627] [Rank 15] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=64, OpType=ALLREDUCE, NumelIn=536870912, NumelOut=536870912, Timeout(ms)=60000) ran for 73048 milliseconds before timing out.
[rank14]:[E1118 17:38:47.337925359 ProcessGroupNCCL.cpp:627] [Rank 14] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=64, OpType=ALLREDUCE, NumelIn=536870912, NumelOut=536870912, Timeout(ms)=60000) ran for 73048 milliseconds before timing out.
[rank8]:[E1118 17:38:47.337968730 ProcessGroupNCCL.cpp:627] [Rank 8] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=64, OpType=ALLREDUCE, NumelIn=536870912, NumelOut=536870912, Timeout(ms)=60000) ran for 73022 milliseconds before timing out.
[rank11]:[E1118 17:38:47.338026751 ProcessGroupNCCL.cpp:627] [Rank 11] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=64, OpType=ALLREDUCE, NumelIn=536870912, NumelOut=536870912, Timeout(ms)=60000) ran for 73022 milliseconds before timing out.
[rank13]:[E1118 17:38:47.338027818 ProcessGroupNCCL.cpp:627] [Rank 13] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=64, OpType=ALLREDUCE, NumelIn=536870912, NumelOut=536870912, Timeout(ms)=60000) ran for 73048 milliseconds before timing out.
[rank12]:[E1118 17:38:47.338073321 ProcessGroupNCCL.cpp:627] [Rank 12] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=64, OpType=ALLREDUCE, NumelIn=536870912, NumelOut=536870912, Timeout(ms)=60000) ran for 73048 milliseconds before timing out.
[rank10]:[E1118 17:38:47.338099401 ProcessGroupNCCL.cpp:2007] [PG ID 0 PG GUID 0(default_pg) Rank 10]  failure detected by watchdog at work sequence id: 64 PG status: last enqueued work: 120, last completed work: 63
[rank14]:[E1118 17:38:47.338102191 ProcessGroupNCCL.cpp:2007] [PG ID 0 PG GUID 0(default_pg) Rank 14]  failure detected by watchdog at work sequence id: 64 PG status: last enqueued work: 120, last completed work: 63
[rank10]:[E1118 17:38:47.338105195 ProcessGroupNCCL.cpp:665] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank14]:[E1118 17:38:47.338107746 ProcessGroupNCCL.cpp:665] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank15]:[E1118 17:38:47.338109302 ProcessGroupNCCL.cpp:2007] [PG ID 0 PG GUID 0(default_pg) Rank 15]  failure detected by watchdog at work sequence id: 64 PG status: last enqueued work: 120, last completed work: 63
[rank15]:[E1118 17:38:47.338120088 ProcessGroupNCCL.cpp:665] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank8]:[E1118 17:38:47.338126801 ProcessGroupNCCL.cpp:2007] [PG ID 0 PG GUID 0(default_pg) Rank 8]  failure detected by watchdog at work sequence id: 64 PG status: last enqueued work: 120, last completed work: 63
[rank8]:[E1118 17:38:47.338134262 ProcessGroupNCCL.cpp:665] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank11]:[E1118 17:38:47.338173585 ProcessGroupNCCL.cpp:2007] [PG ID 0 PG GUID 0(default_pg) Rank 11]  failure detected by watchdog at work sequence id: 64 PG status: last enqueued work: 120, last completed work: 63
[rank11]:[E1118 17:38:47.338178975 ProcessGroupNCCL.cpp:665] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank13]:[E1118 17:38:47.338189630 ProcessGroupNCCL.cpp:2007] [PG ID 0 PG GUID 0(default_pg) Rank 13]  failure detected by watchdog at work sequence id: 64 PG status: last enqueued work: 120, last completed work: 63
[rank13]:[E1118 17:38:47.338197603 ProcessGroupNCCL.cpp:665] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank12]:[E1118 17:38:47.338221012 ProcessGroupNCCL.cpp:2007] [PG ID 0 PG GUID 0(default_pg) Rank 12]  failure detected by watchdog at work sequence id: 64 PG status: last enqueued work: 120, last completed work: 63
[rank12]:[E1118 17:38:47.338227282 ProcessGroupNCCL.cpp:665] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank9]:[E1118 17:38:47.338373780 ProcessGroupNCCL.cpp:627] [Rank 9] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=64, OpType=ALLREDUCE, NumelIn=536870912, NumelOut=536870912, Timeout(ms)=60000) ran for 73023 milliseconds before timing out.
[rank9]:[E1118 17:38:47.338511219 ProcessGroupNCCL.cpp:2007] [PG ID 0 PG GUID 0(default_pg) Rank 9]  failure detected by watchdog at work sequence id: 64 PG status: last enqueued work: 120, last completed work: 63
[rank9]:[E1118 17:38:47.338521586 ProcessGroupNCCL.cpp:665] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank15]:[E1118 17:40:14.337836615 ProcessGroupNCCL.cpp:679] [Rank 15] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank15]:[E1118 17:40:14.337910749 ProcessGroupNCCL.cpp:693] [Rank 15] To avoid data inconsistency, we are taking the entire process down.
[rank15]:[E1118 17:40:14.339014389 ProcessGroupNCCL.cpp:1737] [PG ID 0 PG GUID 0(default_pg) Rank 15] Process group watchdog thread terminated with exception: [Rank 15] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=64, OpType=ALLREDUCE, NumelIn=536870912, NumelOut=536870912, Timeout(ms)=60000) ran for 73048 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:630 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x14c3230f9106 in /home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x14c3243dfa54 in /home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x14c3243e15b0 in /home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x14c3243e24cd in /home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x14c36f5195c0 in /home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x14c370075ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x14c370107850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 15] Process group watchdog thread terminated with exception: [Rank 15] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=64, OpType=ALLREDUCE, NumelIn=536870912, NumelOut=536870912, Timeout(ms)=60000) ran for 73048 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:630 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x14c3230f9106 in /home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x14c3243dfa54 in /home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x14c3243e15b0 in /home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x14c3243e24cd in /home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x14c36f5195c0 in /home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x14c370075ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x14c370107850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1743 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x14c3230f9106 in /home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xebe69e (0x14c32404b69e in /home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x14c36f5195c0 in /home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x14c370075ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x14c370107850 in /lib/x86_64-linux-gnu/libc.so.6)

[rank14]:[E1118 17:40:14.384284296 ProcessGroupNCCL.cpp:679] [Rank 14] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank14]:[E1118 17:40:14.384371224 ProcessGroupNCCL.cpp:693] [Rank 14] To avoid data inconsistency, we are taking the entire process down.
[rank14]:[E1118 17:40:14.385777598 ProcessGroupNCCL.cpp:1737] [PG ID 0 PG GUID 0(default_pg) Rank 14] Process group watchdog thread terminated with exception: [Rank 14] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=64, OpType=ALLREDUCE, NumelIn=536870912, NumelOut=536870912, Timeout(ms)=60000) ran for 73048 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:630 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x14721eaf9106 in /home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x14721fddfa54 in /home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x14721fde15b0 in /home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x14721fde24cd in /home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x14726abe85c0 in /home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x14726b9e6ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x14726ba78850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 14] Process group watchdog thread terminated with exception: [Rank 14] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=64, OpType=ALLREDUCE, NumelIn=536870912, NumelOut=536870912, Timeout(ms)=60000) ran for 73048 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:630 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x14721eaf9106 in /home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x14721fddfa54 in /home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x14721fde15b0 in /home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x14721fde24cd in /home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x14726abe85c0 in /home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x14726b9e6ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x14726ba78850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1743 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x14721eaf9106 in /home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xebe69e (0x14721fa4b69e in /home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x14726abe85c0 in /home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x14726b9e6ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x14726ba78850 in /lib/x86_64-linux-gnu/libc.so.6)

[rank13]:[E1118 17:40:14.442706974 ProcessGroupNCCL.cpp:679] [Rank 13] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank13]:[E1118 17:40:14.442770424 ProcessGroupNCCL.cpp:693] [Rank 13] To avoid data inconsistency, we are taking the entire process down.
[rank13]:[E1118 17:40:14.443795155 ProcessGroupNCCL.cpp:1737] [PG ID 0 PG GUID 0(default_pg) Rank 13] Process group watchdog thread terminated with exception: [Rank 13] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=64, OpType=ALLREDUCE, NumelIn=536870912, NumelOut=536870912, Timeout(ms)=60000) ran for 73048 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:630 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x14e78b8f9106 in /home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x14e78cbdfa54 in /home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x14e78cbe15b0 in /home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x14e78cbe24cd in /home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x14e7d79e95c0 in /home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x14e7d87efac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x14e7d8881850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 13] Process group watchdog thread terminated with exception: [Rank 13] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=64, OpType=ALLREDUCE, NumelIn=536870912, NumelOut=536870912, Timeout(ms)=60000) ran for 73048 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:630 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x14e78b8f9106 in /home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x14e78cbdfa54 in /home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x14e78cbe15b0 in /home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x14e78cbe24cd in /home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x14e7d79e95c0 in /home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x14e7d87efac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x14e7d8881850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1743 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x14e78b8f9106 in /home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xebe69e (0x14e78c84b69e in /home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x14e7d79e95c0 in /home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x14e7d87efac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x14e7d8881850 in /lib/x86_64-linux-gnu/libc.so.6)

[rank12]:[E1118 17:40:14.524405214 ProcessGroupNCCL.cpp:679] [Rank 12] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank12]:[E1118 17:40:14.524464596 ProcessGroupNCCL.cpp:693] [Rank 12] To avoid data inconsistency, we are taking the entire process down.
[rank12]:[E1118 17:40:14.525515112 ProcessGroupNCCL.cpp:1737] [PG ID 0 PG GUID 0(default_pg) Rank 12] Process group watchdog thread terminated with exception: [Rank 12] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=64, OpType=ALLREDUCE, NumelIn=536870912, NumelOut=536870912, Timeout(ms)=60000) ran for 73048 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:630 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x1462d0b6c106 in /home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x146285ddfa54 in /home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x146285de15b0 in /home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x146285de24cd in /home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x1462d10095c0 in /home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x1462d1b65ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x1462d1bf7850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 12] Process group watchdog thread terminated with exception: [Rank 12] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=64, OpType=ALLREDUCE, NumelIn=536870912, NumelOut=536870912, Timeout(ms)=60000) ran for 73048 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:630 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x1462d0b6c106 in /home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x146285ddfa54 in /home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x146285de15b0 in /home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x146285de24cd in /home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x1462d10095c0 in /home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x1462d1b65ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x1462d1bf7850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1743 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x1462d0b6c106 in /home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xebe69e (0x146285a4b69e in /home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x1462d10095c0 in /home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x1462d1b65ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x1462d1bf7850 in /lib/x86_64-linux-gnu/libc.so.6)

W1118 17:40:14.732000 2904866 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2904932 closing signal SIGTERM
W1118 17:40:14.736000 2904866 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2904933 closing signal SIGTERM
W1118 17:40:14.742000 2904866 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2904934 closing signal SIGTERM
W1118 17:40:14.745000 2904866 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2904935 closing signal SIGTERM
W1118 17:40:14.749000 2904866 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2904936 closing signal SIGTERM
W1118 17:40:14.753000 2904866 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2904937 closing signal SIGTERM
W1118 17:40:14.754000 2904866 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2904938 closing signal SIGTERM
E1118 17:40:17.455000 2904866 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: -6) local_rank: 7 (pid: 2904939) of binary: /home/charlie/anaconda3/envs/gpt-fast/bin/python
Traceback (most recent call last):
  File "/home/charlie/anaconda3/envs/gpt-fast/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
========================================================
benchmark.py FAILED
--------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
--------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-11-18_17:40:14
  host      : mk-xii-06.cloud.together.ai
  rank      : 15 (local_rank: 7)
  exitcode  : -6 (pid: 2904939)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 2904939
========================================================
