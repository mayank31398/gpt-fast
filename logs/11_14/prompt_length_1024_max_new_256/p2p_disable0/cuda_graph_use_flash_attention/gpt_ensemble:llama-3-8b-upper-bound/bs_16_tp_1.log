flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
        (wo): Linear(in_features=4096, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=28672, bias=False)
        (w2): Linear(in_features=14336, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 0.92 seconds
CUDA_GRAPH are activate
Prefill latency: 0.5240173190832138 sec
Decode latency: 2.5457439366728067 sec
Compilation time: 3.07 seconds
Prefill latency: 0.5256306082010269 sec
Decode latency: 2.5118877813220024 sec
Prefill latency: 0.5248110331594944 sec
Decode latency: 2.5024179611355066 sec
Prefill latency: 0.527440307661891 sec
Decode latency: 2.5458555445075035 sec
Prefill latency: 0.5270021874457598 sec
Decode latency: 2.546345006674528 sec
Prefill latency: 0.5274331253021955 sec
Decode latency: 2.5450073070824146 sec
Time for inference 1: 3.07 sec total, 1332.86 tokens/sec
Decode latency: 2.55 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 20005.95 GB/s
FLOPS achieved: 100.03 TF/s

Prefill latency: 0.527785224840045 sec
Decode latency: 2.457134509459138 sec
Time for inference 2: 2.99 sec total, 1371.89 tokens/sec
Decode latency: 2.46 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 20591.81 GB/s
FLOPS achieved: 102.96 TF/s

Prefill latency: 0.526356989517808 sec
Decode latency: 2.510170178487897 sec
Time for inference 3: 3.04 sec total, 1348.56 tokens/sec
Decode latency: 2.51 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 20241.73 GB/s
FLOPS achieved: 101.21 TF/s

Prefill latency: 0.5268033631145954 sec
Decode latency: 2.545734029263258 sec
Time for inference 4: 3.07 sec total, 1332.72 tokens/sec
Decode latency: 2.55 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 20003.97 GB/s
FLOPS achieved: 100.02 TF/s

Prefill latency: 0.5254743117839098 sec
Decode latency: 2.546309854835272 sec
Time for inference 5: 3.07 sec total, 1333.11 tokens/sec
Decode latency: 2.55 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 20009.83 GB/s
FLOPS achieved: 100.05 TF/s

Prefill latency: 0.5254727583378553 sec
Decode latency: 2.5459609758108854 sec
Time for inference 6: 3.07 sec total, 1333.25 tokens/sec
Decode latency: 2.55 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 20011.86 GB/s
FLOPS achieved: 100.06 TF/s

Prefill latency: 0.5248630847781897 sec
Decode latency: 2.544568357989192 sec
Time for inference 7: 3.07 sec total, 1334.10 tokens/sec
Decode latency: 2.54 sec
Prefill latency: 0.52 sec
Bandwidth achieved: 20024.67 GB/s
FLOPS achieved: 100.12 TF/s

Prefill latency: 0.5282230116426945 sec
Decode latency: 2.5458767488598824 sec
Time for inference 8: 3.07 sec total, 1332.07 tokens/sec
Decode latency: 2.55 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 19994.24 GB/s
FLOPS achieved: 99.97 TF/s

Prefill latency: 0.5241630636155605 sec
Decode latency: 2.5159573312848806 sec
Time for inference 9: 3.04 sec total, 1346.98 tokens/sec
Decode latency: 2.52 sec
Prefill latency: 0.52 sec
Bandwidth achieved: 20217.91 GB/s
FLOPS achieved: 101.09 TF/s

Prefill latency: 0.5262369718402624 sec
Decode latency: 2.5455345679074526 sec
Time for inference 10: 3.07 sec total, 1333.15 tokens/sec
Decode latency: 2.55 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 20010.37 GB/s
FLOPS achieved: 100.05 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 2.5302 sec
Average prefill latency: 0.5263 sec
Average tokens/sec: 1339.87
Memory used: 26.54 GB
[rank0]:[W1113 11:52:16.416423778 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Done. we are killing the process
[rank0]:[W1113 11:52:17.178184747 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
