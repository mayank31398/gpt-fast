W1113 12:56:26.187000 1541267 site-packages/torch/distributed/run.py:793] 
W1113 12:56:26.187000 1541267 site-packages/torch/distributed/run.py:793] *****************************************
W1113 12:56:26.187000 1541267 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1113 12:56:26.187000 1541267 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=3072, bias=False)
        (wo): Linear(in_features=2048, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=14336, bias=False)
        (w2): Linear(in_features=7168, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.70 seconds
CUDA_GRAPH are activate
Prefill latency: 1.2124960757791996 sec
Compilation time: 3.45 seconds
Decode latency: 2.2745373249053955 sec
Compilation time: 3.49 seconds
Prefill latency: 1.2087134197354317 sec
Decode latency: 2.2739219944924116 sec
Prefill latency: 1.2065597996115685 sec
Decode latency: 2.2738320156931877 sec
Prefill latency: 1.2100905273109674 sec
Decode latency: 2.273715579882264 sec
Prefill latency: 1.2133185360580683 sec
Decode latency: 2.274368664249778 sec
Prefill latency: 1.2095317393541336 sec
Decode latency: 2.2740411311388016 sec
Time for inference 1: 3.48 sec total, 4702.11 tokens/sec
Decode latency: 2.27 sec
Prefill latency: 1.21 sec
Bandwidth achieved: 37760.40 GB/s
FLOPS achieved: 188.80 TF/s

Prefill latency: 1.2094694264233112 sec
Decode latency: 2.274701427668333 sec
Time for inference 2: 3.48 sec total, 4701.35 tokens/sec
Decode latency: 2.27 sec
Prefill latency: 1.21 sec
Bandwidth achieved: 37754.35 GB/s
FLOPS achieved: 188.77 TF/s

Prefill latency: 1.210316201671958 sec
Decode latency: 2.274251591414213 sec
Time for inference 3: 3.49 sec total, 4700.82 tokens/sec
Decode latency: 2.27 sec
Prefill latency: 1.21 sec
Bandwidth achieved: 37750.09 GB/s
FLOPS achieved: 188.75 TF/s

Prefill latency: 1.2137415315955877 sec
Decode latency: 2.2745152823626995 sec
Time for inference 4: 3.49 sec total, 4695.91 tokens/sec
Decode latency: 2.27 sec
Prefill latency: 1.21 sec
Bandwidth achieved: 37710.63 GB/s
FLOPS achieved: 188.55 TF/s

Prefill latency: 1.20940906368196 sec
Decode latency: 2.2745015248656273 sec
Time for inference 5: 3.48 sec total, 4701.46 tokens/sec
Decode latency: 2.27 sec
Prefill latency: 1.21 sec
Bandwidth achieved: 37755.17 GB/s
FLOPS achieved: 188.78 TF/s

Prefill latency: 1.210494427010417 sec
Decode latency: 2.2744150552898645 sec
Time for inference 6: 3.49 sec total, 4700.31 tokens/sec
Decode latency: 2.27 sec
Prefill latency: 1.21 sec
Bandwidth achieved: 37745.98 GB/s
FLOPS achieved: 188.73 TF/s

Prefill latency: 1.2131071295589209 sec
Decode latency: 2.273981835693121 sec
Time for inference 7: 3.49 sec total, 4697.47 tokens/sec
Decode latency: 2.27 sec
Prefill latency: 1.21 sec
Bandwidth achieved: 37723.15 GB/s
FLOPS achieved: 188.62 TF/s

Prefill latency: 1.210044588893652 sec
Decode latency: 2.2739325873553753 sec
Time for inference 8: 3.48 sec total, 4701.59 tokens/sec
Decode latency: 2.27 sec
Prefill latency: 1.21 sec
Bandwidth achieved: 37756.24 GB/s
FLOPS achieved: 188.78 TF/s

Prefill latency: 1.2067221403121948 sec
Decode latency: 2.273760709911585 sec
Time for inference 9: 3.48 sec total, 4706.22 tokens/sec
Decode latency: 2.27 sec
Prefill latency: 1.21 sec
Bandwidth achieved: 37793.46 GB/s
FLOPS achieved: 188.97 TF/s

Prefill latency: 1.2158664129674435 sec
[rank1]:[W1113 12:57:29.084342276 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Decode latency: 2.2747745104134083 sec
Time for inference 10: 3.49 sec total, 4692.74 tokens/sec
Decode latency: 2.27 sec
Prefill latency: 1.22 sec
Bandwidth achieved: 37685.17 GB/s
FLOPS achieved: 188.43 TF/s

==========
Batch Size: 64
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 2.2743 sec
Average prefill latency: 1.2109 sec
Average tokens/sec: 4700.00
Memory used: 38.87 GB
[rank0]:[W1113 12:57:29.632319846 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Done. we are killing the process
[rank0]:[W1113 12:57:32.022047328 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
