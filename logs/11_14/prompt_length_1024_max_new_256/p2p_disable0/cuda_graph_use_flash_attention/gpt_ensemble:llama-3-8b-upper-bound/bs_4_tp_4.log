W1113 11:49:56.632000 1380166 site-packages/torch/distributed/run.py:793] 
W1113 11:49:56.632000 1380166 site-packages/torch/distributed/run.py:793] *****************************************
W1113 11:49:56.632000 1380166 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1113 11:49:56.632000 1380166 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=1536, bias=False)
        (wo): Linear(in_features=1024, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.71 seconds
CUDA_GRAPH are activate
Prefill latency: 0.04142521321773529 sec
Compilation time: 1.35 seconds
Compilation time: 1.35 seconds
Compilation time: 1.35 seconds
Decode latency: 1.308112246915698 sec
Compilation time: 1.35 seconds
Prefill latency: 0.04123225435614586 sec
Decode latency: 1.3080993574112654 sec
Prefill latency: 0.041166093200445175 sec
Decode latency: 1.308045368641615 sec
Prefill latency: 0.04137669876217842 sec
Decode latency: 1.3099538311362267 sec
Prefill latency: 0.041235703974962234 sec
Decode latency: 1.308642491698265 sec
Prefill latency: 0.04133458621799946 sec
Decode latency: 1.3098247181624174 sec
Time for inference 1: 1.35 sec total, 757.56 tokens/sec
Decode latency: 1.31 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 3439.96 GB/s
FLOPS achieved: 17.20 TF/s

Prefill latency: 0.04129248671233654 sec
Decode latency: 1.3081225790083408 sec
Time for inference 2: 1.35 sec total, 758.54 tokens/sec
Decode latency: 1.31 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 3444.44 GB/s
FLOPS achieved: 17.22 TF/s

Prefill latency: 0.04130125977098942 sec
Decode latency: 1.309694368392229 sec
Time for inference 3: 1.35 sec total, 757.65 tokens/sec
Decode latency: 1.31 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 3440.40 GB/s
FLOPS achieved: 17.20 TF/s

Prefill latency: 0.04125306196510792 sec
Decode latency: 1.3077242374420166 sec
Time for inference 4: 1.35 sec total, 758.80 tokens/sec
Decode latency: 1.31 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 3445.59 GB/s
FLOPS achieved: 17.23 TF/s

Prefill latency: 0.04135877639055252 sec
Decode latency: 1.3092075865715742 sec
Time for inference 5: 1.35 sec total, 757.89 tokens/sec
Decode latency: 1.31 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 3441.47 GB/s
FLOPS achieved: 17.21 TF/s

Prefill latency: 0.04129226319491863 sec
Decode latency: 1.3082245085388422 sec
Time for inference 6: 1.35 sec total, 758.46 tokens/sec
Decode latency: 1.31 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 3444.08 GB/s
FLOPS achieved: 17.22 TF/s

Prefill latency: 0.04129869490861893 sec
Decode latency: 1.3094574883580208 sec
Time for inference 7: 1.35 sec total, 757.78 tokens/sec
Decode latency: 1.31 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 3440.96 GB/s
FLOPS achieved: 17.20 TF/s

Prefill latency: 0.041339848190546036 sec
Decode latency: 1.3083140589296818 sec
Time for inference 8: 1.35 sec total, 758.38 tokens/sec
Decode latency: 1.31 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 3443.72 GB/s
FLOPS achieved: 17.22 TF/s

Prefill latency: 0.041312508285045624 sec
Decode latency: 1.3084587585180998 sec
Time for inference 9: 1.35 sec total, 758.31 tokens/sec
Decode latency: 1.31 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 3443.40 GB/s
FLOPS achieved: 17.22 TF/s

Prefill latency: 0.04146406054496765 sec
[rank1]:[W1113 11:50:26.804776968 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank2]:[W1113 11:50:26.948766253 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank3]:[W1113 11:50:26.966502014 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Decode latency: 1.3085002023726702 sec
Time for inference 10: 1.35 sec total, 758.17 tokens/sec
Decode latency: 1.31 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 3442.74 GB/s
FLOPS achieved: 17.21 TF/s

==========
Batch Size: 4
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 1.3088 sec
Average prefill latency: 0.0413 sec
Average tokens/sec: 758.15
Memory used: 7.29 GB
[rank0]:[W1113 11:50:26.027028328 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Done. we are killing the process
[rank0]:[W1113 11:50:32.735287318 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
