flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTLadder(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x LadderTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
        (wo): Linear(in_features=4096, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=28672, bias=False)
        (w2): Linear(in_features=14336, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 0.27 seconds
CUDA_GRAPH are activate
Prefill latency: 0.5241501033306122 sec
Decode latency: 2.588220862671733 sec
Compilation time: 3.11 seconds
Prefill latency: 0.5283352266997099 sec
Decode latency: 2.586301438510418 sec
Prefill latency: 0.5278531834483147 sec
Decode latency: 2.5852703899145126 sec
Prefill latency: 0.5266404468566179 sec
Decode latency: 2.5881571397185326 sec
Prefill latency: 0.5271088480949402 sec
Decode latency: 2.585239225998521 sec
Prefill latency: 0.5278302449733019 sec
Decode latency: 2.586263792589307 sec
Time for inference 1: 3.11 sec total, 1315.00 tokens/sec
Decode latency: 2.59 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 19737.97 GB/s
FLOPS achieved: 98.69 TF/s

Prefill latency: 0.5264245290309191 sec
Decode latency: 2.5866267196834087 sec
Time for inference 2: 3.11 sec total, 1315.47 tokens/sec
Decode latency: 2.59 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 19745.05 GB/s
FLOPS achieved: 98.73 TF/s

Prefill latency: 0.5281925443559885 sec
Decode latency: 2.586828203871846 sec
Time for inference 3: 3.12 sec total, 1314.62 tokens/sec
Decode latency: 2.59 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 19732.20 GB/s
FLOPS achieved: 98.66 TF/s

Prefill latency: 0.526580948382616 sec
Decode latency: 2.5859066657721996 sec
Time for inference 4: 3.11 sec total, 1315.69 tokens/sec
Decode latency: 2.59 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 19748.32 GB/s
FLOPS achieved: 98.74 TF/s

Prefill latency: 0.5281603392213583 sec
Decode latency: 2.5877103321254253 sec
Time for inference 5: 3.12 sec total, 1314.29 tokens/sec
Decode latency: 2.59 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 19727.33 GB/s
FLOPS achieved: 98.64 TF/s

Prefill latency: 0.5260481927543879 sec
Decode latency: 2.585719348862767 sec
Time for inference 6: 3.11 sec total, 1315.98 tokens/sec
Decode latency: 2.59 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 19752.64 GB/s
FLOPS achieved: 98.76 TF/s

Prefill latency: 0.5274587646126747 sec
Decode latency: 2.5864622835069895 sec
Time for inference 7: 3.11 sec total, 1315.06 tokens/sec
Decode latency: 2.59 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 19738.82 GB/s
FLOPS achieved: 98.69 TF/s

Prefill latency: 0.5290597043931484 sec
Decode latency: 2.586745562031865 sec
Time for inference 8: 3.12 sec total, 1314.29 tokens/sec
Decode latency: 2.59 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 19727.34 GB/s
FLOPS achieved: 98.64 TF/s

Prefill latency: 0.5268894154578447 sec
Decode latency: 2.5874461997300386 sec
Time for inference 9: 3.12 sec total, 1314.87 tokens/sec
Decode latency: 2.59 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 19736.07 GB/s
FLOPS achieved: 98.68 TF/s

Prefill latency: 0.5255833398550749 sec
Decode latency: 2.5856946241110563 sec
Time for inference 10: 3.11 sec total, 1316.17 tokens/sec
Decode latency: 2.59 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 19755.51 GB/s
FLOPS achieved: 98.78 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 2.5865 sec
Average prefill latency: 0.5272 sec
Average tokens/sec: 1315.14
Memory used: 35.41 GB
Done. we are killing the process
[rank0]:[W1113 11:39:45.309258776 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
