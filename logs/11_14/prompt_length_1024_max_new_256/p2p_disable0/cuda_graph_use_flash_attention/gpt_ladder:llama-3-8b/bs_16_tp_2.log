W1113 11:39:48.412000 1350982 site-packages/torch/distributed/run.py:793] 
W1113 11:39:48.412000 1350982 site-packages/torch/distributed/run.py:793] *****************************************
W1113 11:39:48.412000 1350982 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1113 11:39:48.412000 1350982 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTLadder(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x LadderTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=3072, bias=False)
        (wo): Linear(in_features=2048, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=14336, bias=False)
        (w2): Linear(in_features=7168, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 0.37 seconds
CUDA_GRAPH are activate
Prefill latency: 0.29441062733531 sec
Decode latency: 1.9156336914747953 sec
Compilation time: 2.21 seconds
Compilation time: 2.22 seconds
Prefill latency: 0.2969650197774172 sec
Decode latency: 1.915385328233242 sec
Prefill latency: 0.29671910032629967 sec
Decode latency: 1.9158183056861162 sec
Prefill latency: 0.2965251822024584 sec
Decode latency: 1.916429813951254 sec
Prefill latency: 0.29768134094774723 sec
Decode latency: 1.9152972511947155 sec
Prefill latency: 0.2966763209551573 sec
Decode latency: 1.9152170158922672 sec
Time for inference 1: 2.21 sec total, 1851.07 tokens/sec
Decode latency: 1.92 sec
Prefill latency: 0.30 sec
Bandwidth achieved: 14865.06 GB/s
FLOPS achieved: 74.33 TF/s

Prefill latency: 0.296736815944314 sec
Decode latency: 1.9147493857890368 sec
Time for inference 2: 2.21 sec total, 1851.47 tokens/sec
Decode latency: 1.91 sec
Prefill latency: 0.30 sec
Bandwidth achieved: 14868.32 GB/s
FLOPS achieved: 74.34 TF/s

Prefill latency: 0.29596675746142864 sec
Decode latency: 1.9153674244880676 sec
Time for inference 3: 2.21 sec total, 1851.60 tokens/sec
Decode latency: 1.92 sec
Prefill latency: 0.30 sec
Bandwidth achieved: 14869.29 GB/s
FLOPS achieved: 74.35 TF/s

Prefill latency: 0.2954676207154989 sec
Decode latency: 1.9160676822066307 sec
Time for inference 4: 2.21 sec total, 1851.47 tokens/sec
Decode latency: 1.92 sec
Prefill latency: 0.30 sec
Bandwidth achieved: 14868.24 GB/s
FLOPS achieved: 74.34 TF/s

Prefill latency: 0.296882264316082 sec
Decode latency: 1.915670057758689 sec
Time for inference 5: 2.21 sec total, 1850.68 tokens/sec
Decode latency: 1.92 sec
Prefill latency: 0.30 sec
Bandwidth achieved: 14861.90 GB/s
FLOPS achieved: 74.31 TF/s

Prefill latency: 0.2971119601279497 sec
Decode latency: 1.9165268205106258 sec
Time for inference 6: 2.21 sec total, 1849.68 tokens/sec
Decode latency: 1.92 sec
Prefill latency: 0.30 sec
Bandwidth achieved: 14853.93 GB/s
FLOPS achieved: 74.27 TF/s

Prefill latency: 0.29620867781341076 sec
Decode latency: 1.9155865926295519 sec
Time for inference 7: 2.21 sec total, 1851.02 tokens/sec
Decode latency: 1.92 sec
Prefill latency: 0.30 sec
Bandwidth achieved: 14864.65 GB/s
FLOPS achieved: 74.32 TF/s

Prefill latency: 0.2960616871714592 sec
Decode latency: 1.916228137910366 sec
Time for inference 8: 2.21 sec total, 1850.59 tokens/sec
Decode latency: 1.92 sec
Prefill latency: 0.30 sec
Bandwidth achieved: 14861.19 GB/s
FLOPS achieved: 74.31 TF/s

Prefill latency: 0.29669657349586487 sec
Decode latency: 1.9159504026174545 sec
Time for inference 9: 2.21 sec total, 1850.49 tokens/sec
Decode latency: 1.92 sec
Prefill latency: 0.30 sec
Bandwidth achieved: 14860.44 GB/s
FLOPS achieved: 74.30 TF/s

Prefill latency: 0.29512327909469604 sec
Decode latency: 1.915987879037857 sec
Time for inference 10: 2.21 sec total, 1851.75 tokens/sec
Decode latency: 1.92 sec
Prefill latency: 0.30 sec
Bandwidth achieved: 14870.49 GB/s
FLOPS achieved: 74.35 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 1.9157 sec
Average prefill latency: 0.2963 sec
Average tokens/sec: 1850.98
Memory used: 30.91 GB
Done. we are killing the process
[rank0]:[W1113 11:40:32.790306727 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
