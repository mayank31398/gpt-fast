W1113 11:40:34.976000 1351725 site-packages/torch/distributed/run.py:793] 
W1113 11:40:34.976000 1351725 site-packages/torch/distributed/run.py:793] *****************************************
W1113 11:40:34.976000 1351725 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1113 11:40:34.976000 1351725 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTLadder(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x LadderTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=1536, bias=False)
        (wo): Linear(in_features=1024, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 0.91 seconds
CUDA_GRAPH are activate
Prefill latency: 0.20131928101181984 sec
Decode latency: 1.5053516998887062 sec
Compilation time: 1.70 seconds
Compilation time: 1.69 seconds
Compilation time: 1.70 seconds
Compilation time: 1.71 seconds
Prefill latency: 0.18616200052201748 sec
Decode latency: 1.5056679733097553 sec
Prefill latency: 0.1855042800307274 sec
Decode latency: 1.5072677824646235 sec
Prefill latency: 0.18545265309512615 sec
Decode latency: 1.506093891337514 sec
Prefill latency: 0.18569443188607693 sec
Decode latency: 1.505312129855156 sec
Prefill latency: 0.18532005324959755 sec
Decode latency: 1.506670305505395 sec
Time for inference 1: 1.69 sec total, 2419.80 tokens/sec
Decode latency: 1.51 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 10987.98 GB/s
FLOPS achieved: 54.94 TF/s

Prefill latency: 0.18599192425608635 sec
Decode latency: 1.506463186815381 sec
Time for inference 2: 1.69 sec total, 2419.14 tokens/sec
Decode latency: 1.51 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 10984.99 GB/s
FLOPS achieved: 54.92 TF/s

Prefill latency: 0.18514573574066162 sec
Decode latency: 1.5067995842546225 sec
Time for inference 3: 1.69 sec total, 2419.65 tokens/sec
Decode latency: 1.51 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 10987.33 GB/s
FLOPS achieved: 54.94 TF/s

Prefill latency: 0.1857538241893053 sec
Decode latency: 1.5049335975199938 sec
Time for inference 4: 1.69 sec total, 2421.62 tokens/sec
Decode latency: 1.50 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 10996.25 GB/s
FLOPS achieved: 54.98 TF/s

Prefill latency: 0.18517588078975677 sec
Decode latency: 1.5057284086942673 sec
Time for inference 5: 1.69 sec total, 2421.38 tokens/sec
Decode latency: 1.51 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 10995.15 GB/s
FLOPS achieved: 54.98 TF/s

Prefill latency: 0.18654312938451767 sec
Decode latency: 1.5052559357136488 sec
Time for inference 6: 1.69 sec total, 2420.01 tokens/sec
Decode latency: 1.51 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 10988.93 GB/s
FLOPS achieved: 54.94 TF/s

Prefill latency: 0.18540772050619125 sec
Decode latency: 1.5045333933085203 sec
Time for inference 7: 1.69 sec total, 2422.76 tokens/sec
Decode latency: 1.50 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 11001.41 GB/s
FLOPS achieved: 55.01 TF/s

Prefill latency: 0.18575305677950382 sec
Decode latency: 1.5057670548558235 sec
Time for inference 8: 1.69 sec total, 2420.50 tokens/sec
Decode latency: 1.51 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 10991.15 GB/s
FLOPS achieved: 54.96 TF/s

Prefill latency: 0.18570753373205662 sec
Decode latency: 1.5051388908177614 sec
Time for inference 9: 1.69 sec total, 2421.35 tokens/sec
Decode latency: 1.51 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 10995.01 GB/s
FLOPS achieved: 54.98 TF/s

Prefill latency: 0.18733108788728714 sec
Decode latency: 1.505016403272748 sec
Time for inference 10: 1.69 sec total, 2419.29 tokens/sec
Decode latency: 1.51 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 10985.69 GB/s
FLOPS achieved: 54.93 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 1.5056 sec
Average prefill latency: 0.1858 sec
Average tokens/sec: 2420.55
Memory used: 24.23 GB
Done. we are killing the process
[rank0]:[W1113 11:41:14.724689666 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
