flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTLadder(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x LadderTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
        (wo): Linear(in_features=4096, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=28672, bias=False)
        (w2): Linear(in_features=14336, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 0.25 seconds
CUDA_GRAPH are activate
Prefill latency: 0.1332847997546196 sec
Decode latency: 2.353035679087043 sec
Compilation time: 2.49 seconds
Prefill latency: 0.12880349718034267 sec
Decode latency: 2.351517101749778 sec
Prefill latency: 0.130594864487648 sec
Decode latency: 2.352278022095561 sec
Prefill latency: 0.13060834631323814 sec
Decode latency: 2.3534252047538757 sec
Prefill latency: 0.13124733604490757 sec
Decode latency: 2.352696191519499 sec
Prefill latency: 0.1313422191888094 sec
Decode latency: 2.3528773300349712 sec
Time for inference 1: 2.48 sec total, 412.11 tokens/sec
Decode latency: 2.35 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6185.69 GB/s
FLOPS achieved: 30.93 TF/s

Prefill latency: 0.13111747801303864 sec
Decode latency: 2.353069022297859 sec
Time for inference 2: 2.49 sec total, 412.07 tokens/sec
Decode latency: 2.35 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6185.12 GB/s
FLOPS achieved: 30.93 TF/s

Prefill latency: 0.13080577552318573 sec
Decode latency: 2.352389372885227 sec
Time for inference 3: 2.48 sec total, 412.26 tokens/sec
Decode latency: 2.35 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6187.94 GB/s
FLOPS achieved: 30.94 TF/s

Prefill latency: 0.1328644473105669 sec
Decode latency: 2.3285207469016314 sec
Time for inference 4: 2.46 sec total, 415.93 tokens/sec
Decode latency: 2.33 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6243.01 GB/s
FLOPS achieved: 31.22 TF/s

Prefill latency: 0.13079076446592808 sec
Decode latency: 2.3521955627948046 sec
Time for inference 5: 2.48 sec total, 412.29 tokens/sec
Decode latency: 2.35 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6188.35 GB/s
FLOPS achieved: 30.94 TF/s

Prefill latency: 0.13134681060910225 sec
Decode latency: 2.352363768965006 sec
Time for inference 6: 2.48 sec total, 412.18 tokens/sec
Decode latency: 2.35 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6186.80 GB/s
FLOPS achieved: 30.93 TF/s

Prefill latency: 0.13086478039622307 sec
Decode latency: 2.3514048904180527 sec
Time for inference 7: 2.48 sec total, 412.43 tokens/sec
Decode latency: 2.35 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6190.48 GB/s
FLOPS achieved: 30.95 TF/s

Prefill latency: 0.1304818708449602 sec
Decode latency: 2.3526424784213305 sec
Time for inference 8: 2.48 sec total, 412.28 tokens/sec
Decode latency: 2.35 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6188.30 GB/s
FLOPS achieved: 30.94 TF/s

Prefill latency: 0.13117706775665283 sec
Decode latency: 2.352459080517292 sec
Time for inference 9: 2.48 sec total, 412.19 tokens/sec
Decode latency: 2.35 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6186.98 GB/s
FLOPS achieved: 30.93 TF/s

Prefill latency: 0.1303564291447401 sec
Decode latency: 2.3531480710953474 sec
Time for inference 10: 2.48 sec total, 412.22 tokens/sec
Decode latency: 2.35 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6187.39 GB/s
FLOPS achieved: 30.94 TF/s

==========
Batch Size: 4
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 2.3501 sec
Average prefill latency: 0.1311 sec
Average tokens/sec: 412.60
Memory used: 21.01 GB
Done. we are killing the process
[rank0]:[W1113 11:36:45.448599262 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
