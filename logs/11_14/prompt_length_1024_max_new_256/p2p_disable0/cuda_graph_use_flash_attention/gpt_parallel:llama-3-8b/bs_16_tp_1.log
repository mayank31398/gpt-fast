flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTParallel(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x ParallelTransformerBlock(
      semi_compiled = False
      (attention): FuseAttentionMLP(
        (wqkv1): Linear(in_features=4096, out_features=34816, bias=False)
        (wo): Linear(in_features=4096, out_features=4096, bias=False)
        (w2): Linear(in_features=14336, out_features=4096, bias=False)
      )
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.13 seconds
CUDA_GRAPH are activate
[rank0]:[W1113 11:57:05.369579794 CUDAGraph.cpp:133] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
Prefill latency: 0.5157566480338573 sec
Decode latency: 2.566877957433462 sec
Compilation time: 3.08 seconds
Prefill latency: 0.5190094597637653 sec
Decode latency: 2.56699113920331 sec
Prefill latency: 0.5172426551580429 sec
Decode latency: 2.566218199208379 sec
Prefill latency: 0.518403135240078 sec
Decode latency: 2.5663192998617887 sec
Prefill latency: 0.5186227075755596 sec
Decode latency: 2.566681884229183 sec
Prefill latency: 0.5196630675345659 sec
Decode latency: 2.565628372132778 sec
Time for inference 1: 3.09 sec total, 1327.29 tokens/sec
Decode latency: 2.57 sec
Prefill latency: 0.52 sec
Bandwidth achieved: 19922.10 GB/s
FLOPS achieved: 99.61 TF/s

Prefill latency: 0.5188487805426121 sec
Decode latency: 2.566591741517186 sec
Time for inference 2: 3.09 sec total, 1327.20 tokens/sec
Decode latency: 2.57 sec
Prefill latency: 0.52 sec
Bandwidth achieved: 19920.78 GB/s
FLOPS achieved: 99.60 TF/s

Prefill latency: 0.5182719752192497 sec
Decode latency: 2.566037779673934 sec
Time for inference 3: 3.09 sec total, 1327.69 tokens/sec
Decode latency: 2.57 sec
Prefill latency: 0.52 sec
Bandwidth achieved: 19928.05 GB/s
FLOPS achieved: 99.64 TF/s

Prefill latency: 0.5178538355976343 sec
Decode latency: 2.5663690455257893 sec
Time for inference 4: 3.08 sec total, 1327.75 tokens/sec
Decode latency: 2.57 sec
Prefill latency: 0.52 sec
Bandwidth achieved: 19928.95 GB/s
FLOPS achieved: 99.64 TF/s

Prefill latency: 0.5178216639906168 sec
Decode latency: 2.5666082240641117 sec
Time for inference 5: 3.09 sec total, 1327.68 tokens/sec
Decode latency: 2.57 sec
Prefill latency: 0.52 sec
Bandwidth achieved: 19927.86 GB/s
FLOPS achieved: 99.64 TF/s

Prefill latency: 0.519169794395566 sec
Decode latency: 2.566232342272997 sec
Time for inference 6: 3.09 sec total, 1327.28 tokens/sec
Decode latency: 2.57 sec
Prefill latency: 0.52 sec
Bandwidth achieved: 19921.96 GB/s
FLOPS achieved: 99.61 TF/s

Prefill latency: 0.5181420724838972 sec
Decode latency: 2.5658004321157932 sec
Time for inference 7: 3.08 sec total, 1327.91 tokens/sec
Decode latency: 2.57 sec
Prefill latency: 0.52 sec
Bandwidth achieved: 19931.36 GB/s
FLOPS achieved: 99.66 TF/s

Prefill latency: 0.5193186663091183 sec
Decode latency: 2.566800406202674 sec
Time for inference 8: 3.09 sec total, 1326.99 tokens/sec
Decode latency: 2.57 sec
Prefill latency: 0.52 sec
Bandwidth achieved: 19917.62 GB/s
FLOPS achieved: 99.59 TF/s

Prefill latency: 0.5187074150890112 sec
Decode latency: 2.5667545218020678 sec
Time for inference 9: 3.09 sec total, 1327.27 tokens/sec
Decode latency: 2.57 sec
Prefill latency: 0.52 sec
Bandwidth achieved: 19921.80 GB/s
FLOPS achieved: 99.61 TF/s

Prefill latency: 0.5171386580914259 sec
Decode latency: 2.5663050767034292 sec
Time for inference 10: 3.08 sec total, 1328.12 tokens/sec
Decode latency: 2.57 sec
Prefill latency: 0.52 sec
Bandwidth achieved: 19934.54 GB/s
FLOPS achieved: 99.67 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 2.5663 sec
Average prefill latency: 0.5185 sec
Average tokens/sec: 1327.52
Memory used: 30.77 GB
Done. we are killing the process
[rank0]:[W1113 11:57:52.923343382 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
