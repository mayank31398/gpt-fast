flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTParallel(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x ParallelTransformerBlock(
      semi_compiled = False
      (attention): FuseAttentionMLP(
        (wqkv1): Linear(in_features=4096, out_features=34816, bias=False)
        (wo): Linear(in_features=4096, out_features=4096, bias=False)
        (w2): Linear(in_features=14336, out_features=4096, bias=False)
      )
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.23 seconds
CUDA_GRAPH are activate
Prefill latency: 0.1294794548302889 sec
Decode latency: 2.3499827813357115 sec
Compilation time: 2.58 seconds
Prefill latency: 0.1284323688596487 sec
Decode latency: 2.3494452070444822 sec
Prefill latency: 0.1313429307192564 sec
Decode latency: 2.3484356328845024 sec
Prefill latency: 0.12910914421081543 sec
Decode latency: 2.348697042092681 sec
Prefill latency: 0.12833746522665024 sec
Decode latency: 2.34870439209044 sec
Prefill latency: 0.12938934937119484 sec
Decode latency: 2.3493185713887215 sec
Time for inference 1: 2.48 sec total, 413.01 tokens/sec
Decode latency: 2.35 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6199.17 GB/s
FLOPS achieved: 31.00 TF/s

Prefill latency: 0.1298871971666813 sec
Decode latency: 2.348685020580888 sec
Time for inference 2: 2.48 sec total, 413.04 tokens/sec
Decode latency: 2.35 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6199.49 GB/s
FLOPS achieved: 31.00 TF/s

Prefill latency: 0.1312132440507412 sec
Decode latency: 2.349021365866065 sec
Time for inference 3: 2.48 sec total, 412.76 tokens/sec
Decode latency: 2.35 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6195.32 GB/s
FLOPS achieved: 30.98 TF/s

Prefill latency: 0.13136368244886398 sec
Decode latency: 2.349378263577819 sec
Time for inference 4: 2.48 sec total, 412.68 tokens/sec
Decode latency: 2.35 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6194.20 GB/s
FLOPS achieved: 30.97 TF/s

Prefill latency: 0.12994085438549519 sec
Decode latency: 2.3487336300313473 sec
Time for inference 5: 2.48 sec total, 413.01 tokens/sec
Decode latency: 2.35 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6199.07 GB/s
FLOPS achieved: 31.00 TF/s

Prefill latency: 0.12910006754100323 sec
Decode latency: 2.348945677280426 sec
Time for inference 6: 2.48 sec total, 413.12 tokens/sec
Decode latency: 2.35 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6200.79 GB/s
FLOPS achieved: 31.00 TF/s

Prefill latency: 0.13154779002070427 sec
Decode latency: 2.349282205104828 sec
Time for inference 7: 2.48 sec total, 412.66 tokens/sec
Decode latency: 2.35 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6193.83 GB/s
FLOPS achieved: 30.97 TF/s

Prefill latency: 0.12933555245399475 sec
Decode latency: 2.3488704916089773 sec
Time for inference 8: 2.48 sec total, 413.10 tokens/sec
Decode latency: 2.35 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6200.51 GB/s
FLOPS achieved: 31.00 TF/s

Prefill latency: 0.13002575747668743 sec
Decode latency: 2.3497376181185246 sec
Time for inference 9: 2.48 sec total, 412.83 tokens/sec
Decode latency: 2.35 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6196.43 GB/s
FLOPS achieved: 30.98 TF/s

Prefill latency: 0.12993639707565308 sec
Decode latency: 2.3491813857108355 sec
Time for inference 10: 2.48 sec total, 412.94 tokens/sec
Decode latency: 2.35 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6198.06 GB/s
FLOPS achieved: 30.99 TF/s

==========
Batch Size: 4
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 2.3491 sec
Average prefill latency: 0.1302 sec
Average tokens/sec: 412.92
Memory used: 19.85 GB
Done. we are killing the process
[rank0]:[W1113 11:54:45.861896769 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
