W1113 11:56:08.765000 1405684 site-packages/torch/distributed/run.py:793] 
W1113 11:56:08.765000 1405684 site-packages/torch/distributed/run.py:793] *****************************************
W1113 11:56:08.765000 1405684 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1113 11:56:08.765000 1405684 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTParallel(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x ParallelTransformerBlock(
      semi_compiled = False
      (attention): FuseAttentionMLP(
        (wqkv1): Linear(in_features=4096, out_features=4352, bias=False)
        (wo): Linear(in_features=512, out_features=4096, bias=False)
        (w2): Linear(in_features=1792, out_features=4096, bias=False)
      )
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.80 seconds
CUDA_GRAPH are activate
Prefill latency: 0.050380297005176544 sec
Decode latency: 1.3029159475117922 sec
Compilation time: 1.34 seconds
Compilation time: 1.35 seconds
Compilation time: 1.34 seconds
Compilation time: 1.35 seconds
Compilation time: 1.35 seconds
Compilation time: 1.34 seconds
Compilation time: 1.36 seconds
Compilation time: 1.35 seconds
Prefill latency: 0.033717118203639984 sec
Decode latency: 1.3032963145524263 sec
Prefill latency: 0.03358719125390053 sec
Decode latency: 1.302706141024828 sec
Prefill latency: 0.03354648873209953 sec
Decode latency: 1.302664978429675 sec
Prefill latency: 0.033534059301018715 sec
Decode latency: 1.3025377821177244 sec
Prefill latency: 0.0335233174264431 sec
Decode latency: 1.3022378757596016 sec
Time for inference 1: 1.34 sec total, 766.09 tokens/sec
Decode latency: 1.30 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 2141.80 GB/s
FLOPS achieved: 10.71 TF/s

Prefill latency: 0.03341952711343765 sec
Decode latency: 1.303162967786193 sec
Time for inference 2: 1.34 sec total, 765.55 tokens/sec
Decode latency: 1.30 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 2140.31 GB/s
FLOPS achieved: 10.70 TF/s

Prefill latency: 0.03343256376683712 sec
Decode latency: 1.3021425046026707 sec
Time for inference 3: 1.34 sec total, 766.22 tokens/sec
Decode latency: 1.30 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 2142.18 GB/s
FLOPS achieved: 10.71 TF/s

Prefill latency: 0.03345082141458988 sec
Decode latency: 1.3029063530266285 sec
Time for inference 4: 1.34 sec total, 765.84 tokens/sec
Decode latency: 1.30 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 2141.10 GB/s
FLOPS achieved: 10.71 TF/s

Prefill latency: 0.033487992361187935 sec
Decode latency: 1.3023692443966866 sec
Time for inference 5: 1.34 sec total, 766.18 tokens/sec
Decode latency: 1.30 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 2142.08 GB/s
FLOPS achieved: 10.71 TF/s

Prefill latency: 0.033516427502036095 sec
Decode latency: 1.302418477833271 sec
Time for inference 6: 1.34 sec total, 766.16 tokens/sec
Decode latency: 1.30 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 2142.00 GB/s
FLOPS achieved: 10.71 TF/s

Prefill latency: 0.03347388096153736 sec
Decode latency: 1.3027325123548508 sec
Time for inference 7: 1.34 sec total, 765.97 tokens/sec
Decode latency: 1.30 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 2141.47 GB/s
FLOPS achieved: 10.71 TF/s

Prefill latency: 0.0335311908274889 sec
Decode latency: 1.3022229094058275 sec
Time for inference 8: 1.34 sec total, 766.25 tokens/sec
Decode latency: 1.30 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 2142.27 GB/s
FLOPS achieved: 10.71 TF/s

Prefill latency: 0.033570440486073494 sec
Decode latency: 1.3019505552947521 sec
Time for inference 9: 1.34 sec total, 766.35 tokens/sec
Decode latency: 1.30 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 2142.53 GB/s
FLOPS achieved: 10.71 TF/s

Prefill latency: 0.033444955945014954 sec
Decode latency: 1.303747171536088 sec
Time for inference 10: 1.34 sec total, 765.40 tokens/sec
Decode latency: 1.30 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 2139.90 GB/s
FLOPS achieved: 10.70 TF/s

==========
Batch Size: 4
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 1.3026 sec
Average prefill latency: 0.0335 sec
Average tokens/sec: 766.00
Memory used: 6.43 GB
Done. we are killing the process
[rank0]:[W1113 11:56:52.815525998 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
