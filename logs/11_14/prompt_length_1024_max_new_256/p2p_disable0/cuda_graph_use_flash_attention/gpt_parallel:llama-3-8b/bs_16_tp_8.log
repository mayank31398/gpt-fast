W1113 11:58:52.977000 1415843 site-packages/torch/distributed/run.py:793] 
W1113 11:58:52.977000 1415843 site-packages/torch/distributed/run.py:793] *****************************************
W1113 11:58:52.977000 1415843 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1113 11:58:52.977000 1415843 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTParallel(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x ParallelTransformerBlock(
      semi_compiled = False
      (attention): FuseAttentionMLP(
        (wqkv1): Linear(in_features=4096, out_features=4352, bias=False)
        (wo): Linear(in_features=512, out_features=4096, bias=False)
        (w2): Linear(in_features=1792, out_features=4096, bias=False)
      )
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.44 seconds
CUDA_GRAPH are activate
Prefill latency: 0.1382574811577797 sec
Decode latency: 1.385039933025837 sec
Compilation time: 1.54 seconds
Compilation time: 1.51 seconds
Compilation time: 1.51 seconds
Compilation time: 1.53 seconds
Compilation time: 1.51 seconds
Compilation time: 1.52 secondsCompilation time: 1.50 seconds

Compilation time: 1.55 seconds
Prefill latency: 0.116382060572505 sec
Decode latency: 1.3841184042394161 sec
Prefill latency: 0.11656041815876961 sec
Decode latency: 1.3831622321158648 sec
Prefill latency: 0.11653118021786213 sec
Decode latency: 1.382898872718215 sec
Prefill latency: 0.11661653965711594 sec
Decode latency: 1.3826278243213892 sec
Prefill latency: 0.11673755571246147 sec
Decode latency: 1.382351852953434 sec
Time for inference 1: 1.50 sec total, 2730.81 tokens/sec
Decode latency: 1.38 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 7634.72 GB/s
FLOPS achieved: 38.17 TF/s

Prefill latency: 0.1167738065123558 sec
Decode latency: 1.3827968370169401 sec
Time for inference 2: 1.50 sec total, 2730.12 tokens/sec
Decode latency: 1.38 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 7632.81 GB/s
FLOPS achieved: 38.16 TF/s

Prefill latency: 0.11665499024093151 sec
Decode latency: 1.3830903563648462 sec
Time for inference 3: 1.50 sec total, 2729.83 tokens/sec
Decode latency: 1.38 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 7631.98 GB/s
FLOPS achieved: 38.16 TF/s

Prefill latency: 0.11694015190005302 sec
Decode latency: 1.383602499961853 sec
Time for inference 4: 1.50 sec total, 2728.25 tokens/sec
Decode latency: 1.38 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 7627.56 GB/s
FLOPS achieved: 38.14 TF/s

Prefill latency: 0.11702068708837032 sec
Decode latency: 1.3823532275855541 sec
Time for inference 5: 1.50 sec total, 2730.50 tokens/sec
Decode latency: 1.38 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 7633.86 GB/s
FLOPS achieved: 38.17 TF/s

Prefill latency: 0.1165317427366972 sec
Decode latency: 1.3835869077593088 sec
Time for inference 6: 1.50 sec total, 2729.03 tokens/sec
Decode latency: 1.38 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 7629.76 GB/s
FLOPS achieved: 38.15 TF/s

Prefill latency: 0.11688674613833427 sec
Decode latency: 1.38225581869483 sec
Time for inference 7: 1.50 sec total, 2730.96 tokens/sec
Decode latency: 1.38 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 7635.15 GB/s
FLOPS achieved: 38.18 TF/s

Prefill latency: 0.11686430685222149 sec
Decode latency: 1.383656507357955 sec
Time for inference 8: 1.50 sec total, 2728.59 tokens/sec
Decode latency: 1.38 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 7628.51 GB/s
FLOPS achieved: 38.14 TF/s

Prefill latency: 0.11688188463449478 sec
Decode latency: 1.383054468780756 sec
Time for inference 9: 1.50 sec total, 2729.63 tokens/sec
Decode latency: 1.38 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 7631.43 GB/s
FLOPS achieved: 38.16 TF/s

Prefill latency: 0.1169914435595274 sec
Decode latency: 1.381466107442975 sec
Time for inference 10: 1.50 sec total, 2732.28 tokens/sec
Decode latency: 1.38 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 7638.85 GB/s
FLOPS achieved: 38.19 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 1.3828 sec
Average prefill latency: 0.1168 sec
Average tokens/sec: 2730.00
Memory used: 15.80 GB
Done. we are killing the process
[rank0]:[W1113 11:59:40.886704176 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
