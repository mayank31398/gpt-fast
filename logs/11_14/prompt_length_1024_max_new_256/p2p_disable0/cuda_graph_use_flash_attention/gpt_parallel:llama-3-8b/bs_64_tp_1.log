flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTParallel(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x ParallelTransformerBlock(
      semi_compiled = False
      (attention): FuseAttentionMLP(
        (wqkv1): Linear(in_features=4096, out_features=34816, bias=False)
        (wo): Linear(in_features=4096, out_features=4096, bias=False)
        (w2): Linear(in_features=14336, out_features=4096, bias=False)
      )
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.17 seconds
CUDA_GRAPH are activate
Prefill latency: 2.1702442411333323 sec
Decode latency: 3.3922735266387463 sec
Compilation time: 5.56 seconds
Prefill latency: 2.160011852160096 sec
Decode latency: 3.390388196334243 sec
Prefill latency: 2.1784431505948305 sec
Decode latency: 3.3911125492304564 sec
Prefill latency: 2.1569702476263046 sec
Decode latency: 3.389772407710552 sec
Prefill latency: 2.1619506794959307 sec
Decode latency: 3.390941735357046 sec
Prefill latency: 2.1667014583945274 sec
Decode latency: 3.3907477110624313 sec
Time for inference 1: 5.56 sec total, 2947.58 tokens/sec
Decode latency: 3.39 sec
Prefill latency: 2.17 sec
Bandwidth achieved: 44241.89 GB/s
FLOPS achieved: 221.21 TF/s

Prefill latency: 2.166802767664194 sec
Decode latency: 3.3919199500232935 sec
Time for inference 2: 5.56 sec total, 2946.90 tokens/sec
Decode latency: 3.39 sec
Prefill latency: 2.17 sec
Bandwidth achieved: 44231.71 GB/s
FLOPS achieved: 221.16 TF/s

Prefill latency: 2.1608181837946177 sec
Decode latency: 3.3908217139542103 sec
Time for inference 3: 5.55 sec total, 2950.64 tokens/sec
Decode latency: 3.39 sec
Prefill latency: 2.16 sec
Bandwidth achieved: 44287.92 GB/s
FLOPS achieved: 221.44 TF/s

Prefill latency: 2.1683937050402164 sec
Decode latency: 3.3909387327730656 sec
Time for inference 4: 5.56 sec total, 2946.65 tokens/sec
Decode latency: 3.39 sec
Prefill latency: 2.17 sec
Bandwidth achieved: 44227.96 GB/s
FLOPS achieved: 221.14 TF/s

Prefill latency: 2.172962237149477 sec
Decode latency: 3.3903905116021633 sec
Time for inference 5: 5.56 sec total, 2944.52 tokens/sec
Decode latency: 3.39 sec
Prefill latency: 2.17 sec
Bandwidth achieved: 44196.10 GB/s
FLOPS achieved: 220.98 TF/s

Prefill latency: 2.1635094564408064 sec
Decode latency: 3.391969798132777 sec
Time for inference 6: 5.56 sec total, 2948.69 tokens/sec
Decode latency: 3.39 sec
Prefill latency: 2.16 sec
Bandwidth achieved: 44258.62 GB/s
FLOPS achieved: 221.29 TF/s

Prefill latency: 2.1652289517223835 sec
Decode latency: 3.392420094460249 sec
Time for inference 7: 5.56 sec total, 2947.53 tokens/sec
Decode latency: 3.39 sec
Prefill latency: 2.17 sec
Bandwidth achieved: 44241.14 GB/s
FLOPS achieved: 221.21 TF/s

Prefill latency: 2.178109522908926 sec
Decode latency: 3.3902968280017376 sec
Time for inference 8: 5.57 sec total, 2941.84 tokens/sec
Decode latency: 3.39 sec
Prefill latency: 2.18 sec
Bandwidth achieved: 44155.79 GB/s
FLOPS achieved: 220.78 TF/s

Prefill latency: 2.1635276917368174 sec
Decode latency: 3.390683436766267 sec
Time for inference 9: 5.56 sec total, 2949.31 tokens/sec
Decode latency: 3.39 sec
Prefill latency: 2.16 sec
Bandwidth achieved: 44267.95 GB/s
FLOPS achieved: 221.34 TF/s

Prefill latency: 2.162218324840069 sec
Decode latency: 3.391628924757242 sec
Time for inference 10: 5.55 sec total, 2949.42 tokens/sec
Decode latency: 3.39 sec
Prefill latency: 2.16 sec
Bandwidth achieved: 44269.60 GB/s
FLOPS achieved: 221.35 TF/s

==========
Batch Size: 64
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 3.3912 sec
Average prefill latency: 2.1668 sec
Average tokens/sec: 2947.31
Memory used: 74.46 GB
Done. we are killing the process
[rank0]:[W1113 12:59:14.010217411 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
