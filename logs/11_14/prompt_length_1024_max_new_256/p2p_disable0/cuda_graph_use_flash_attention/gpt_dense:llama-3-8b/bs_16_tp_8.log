W1113 11:35:05.070000 1340646 site-packages/torch/distributed/run.py:793] 
W1113 11:35:05.070000 1340646 site-packages/torch/distributed/run.py:793] *****************************************
W1113 11:35:05.070000 1340646 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1113 11:35:05.070000 1340646 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTDense(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x DenseTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=768, bias=False)
        (wo): Linear(in_features=512, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=3584, bias=False)
        (w2): Linear(in_features=1792, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.88 seconds
CUDA_GRAPH are activate
Prefill latency: 0.1866443231701851 sec
Decode latency: 1.68935277312994 sec
Compilation time: 1.85 seconds
Compilation time: 1.83 seconds
Compilation time: 1.85 secondsCompilation time: 1.83 seconds

Compilation time: 1.90 seconds
Compilation time: 1.85 seconds
Compilation time: 1.88 seconds
Compilation time: 1.87 seconds
Prefill latency: 0.13807685673236847 sec
Decode latency: 1.6887365020811558 sec
Prefill latency: 0.138310132548213 sec
Decode latency: 1.6875417046248913 sec
Prefill latency: 0.13818697445094585 sec
Decode latency: 1.688264099881053 sec
Prefill latency: 0.13810228556394577 sec
Decode latency: 1.68777571991086 sec
Prefill latency: 0.13810975663363934 sec
Decode latency: 1.6899762898683548 sec
Time for inference 1: 1.83 sec total, 2239.58 tokens/sec
Decode latency: 1.69 sec
Prefill latency: 0.14 sec
Bandwidth achieved: 6261.94 GB/s
FLOPS achieved: 31.31 TF/s

Prefill latency: 0.13806233741343021 sec
Decode latency: 1.6885628681629896 sec
Time for inference 2: 1.83 sec total, 2241.32 tokens/sec
Decode latency: 1.69 sec
Prefill latency: 0.14 sec
Bandwidth achieved: 6266.82 GB/s
FLOPS achieved: 31.33 TF/s

Prefill latency: 0.1380893737077713 sec
Decode latency: 1.689036175608635 sec
Time for inference 3: 1.83 sec total, 2240.89 tokens/sec
Decode latency: 1.69 sec
Prefill latency: 0.14 sec
Bandwidth achieved: 6265.62 GB/s
FLOPS achieved: 31.33 TF/s

Prefill latency: 0.13819437846541405 sec
Decode latency: 1.689215574413538 sec
Time for inference 4: 1.83 sec total, 2240.45 tokens/sec
Decode latency: 1.69 sec
Prefill latency: 0.14 sec
Bandwidth achieved: 6264.39 GB/s
FLOPS achieved: 31.32 TF/s

Prefill latency: 0.13814121671020985 sec
Decode latency: 1.6892183814197779 sec
Time for inference 5: 1.83 sec total, 2240.55 tokens/sec
Decode latency: 1.69 sec
Prefill latency: 0.14 sec
Bandwidth achieved: 6264.67 GB/s
FLOPS achieved: 31.32 TF/s

Prefill latency: 0.13840345479547977 sec
Decode latency: 1.6887706350535154 sec
Time for inference 6: 1.83 sec total, 2240.92 tokens/sec
Decode latency: 1.69 sec
Prefill latency: 0.14 sec
Bandwidth achieved: 6265.68 GB/s
FLOPS achieved: 31.33 TF/s

Prefill latency: 0.13816107623279095 sec
Decode latency: 1.6898213326931 sec
Time for inference 7: 1.83 sec total, 2239.51 tokens/sec
Decode latency: 1.69 sec
Prefill latency: 0.14 sec
Bandwidth achieved: 6261.74 GB/s
FLOPS achieved: 31.31 TF/s

Prefill latency: 0.1381034329533577 sec
Decode latency: 1.688947394490242 sec
Time for inference 8: 1.83 sec total, 2240.80 tokens/sec
Decode latency: 1.69 sec
Prefill latency: 0.14 sec
Bandwidth achieved: 6265.35 GB/s
FLOPS achieved: 31.33 TF/s

Prefill latency: 0.13815517909824848 sec
Decode latency: 1.6890441291034222 sec
Time for inference 9: 1.83 sec total, 2240.82 tokens/sec
Decode latency: 1.69 sec
Prefill latency: 0.14 sec
Bandwidth achieved: 6265.43 GB/s
FLOPS achieved: 31.33 TF/s

Prefill latency: 0.1381746493279934 sec
Decode latency: 1.6891435757279396 sec
Time for inference 10: 1.83 sec total, 2240.55 tokens/sec
Decode latency: 1.69 sec
Prefill latency: 0.14 sec
Bandwidth achieved: 6264.67 GB/s
FLOPS achieved: 31.32 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 1.6892 sec
Average prefill latency: 0.1382 sec
Average tokens/sec: 2240.54
Memory used: 17.93 GB
Done. we are killing the process
[rank0]:[W1113 11:35:57.877834671 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
