W1113 11:31:34.588000 1326817 site-packages/torch/distributed/run.py:793] 
W1113 11:31:34.588000 1326817 site-packages/torch/distributed/run.py:793] *****************************************
W1113 11:31:34.588000 1326817 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1113 11:31:34.588000 1326817 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTDense(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x DenseTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=768, bias=False)
        (wo): Linear(in_features=512, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=3584, bias=False)
        (w2): Linear(in_features=1792, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 2.46 seconds
CUDA_GRAPH are activate
Prefill latency: 0.10053963959217072 sec
Decode latency: 1.599049188196659 sec
Compilation time: 1.65 seconds
Compilation time: 1.64 seconds
Compilation time: 1.70 seconds
Compilation time: 1.64 seconds
Compilation time: 1.71 seconds
Compilation time: 1.66 secondsCompilation time: 1.66 seconds

Compilation time: 1.67 seconds
Prefill latency: 0.041513971984386444 sec
Decode latency: 1.5990589689463377 sec
Prefill latency: 0.04133186116814613 sec
Decode latency: 1.5988619644194841 sec
Prefill latency: 0.04127419926226139 sec
Decode latency: 1.5970982480794191 sec
Prefill latency: 0.04144736938178539 sec
Decode latency: 1.5975949186831713 sec
Prefill latency: 0.041320571675896645 sec
Decode latency: 1.598056273534894 sec
Time for inference 1: 1.64 sec total, 624.40 tokens/sec
Decode latency: 1.60 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 1745.85 GB/s
FLOPS achieved: 8.73 TF/s

Prefill latency: 0.04130980372428894 sec
Decode latency: 1.5979019813239574 sec
Time for inference 2: 1.64 sec total, 624.44 tokens/sec
Decode latency: 1.60 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 1745.96 GB/s
FLOPS achieved: 8.73 TF/s

Prefill latency: 0.04127075336873531 sec
Decode latency: 1.5980988964438438 sec
Time for inference 3: 1.64 sec total, 624.32 tokens/sec
Decode latency: 1.60 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 1745.62 GB/s
FLOPS achieved: 8.73 TF/s

Prefill latency: 0.041226960718631744 sec
Decode latency: 1.5986348520964384 sec
Time for inference 4: 1.64 sec total, 624.13 tokens/sec
Decode latency: 1.60 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 1745.09 GB/s
FLOPS achieved: 8.73 TF/s

Prefill latency: 0.041231825947761536 sec
Decode latency: 1.5978274960070848 sec
Time for inference 5: 1.64 sec total, 624.42 tokens/sec
Decode latency: 1.60 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 1745.90 GB/s
FLOPS achieved: 8.73 TF/s

Prefill latency: 0.041190242394804955 sec
Decode latency: 1.597310185432434 sec
Time for inference 6: 1.64 sec total, 624.63 tokens/sec
Decode latency: 1.60 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 1746.48 GB/s
FLOPS achieved: 8.73 TF/s

Prefill latency: 0.041151322424411774 sec
Decode latency: 1.597862597554922 sec
Time for inference 7: 1.64 sec total, 624.48 tokens/sec
Decode latency: 1.60 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 1746.07 GB/s
FLOPS achieved: 8.73 TF/s

Prefill latency: 0.0412471741437912 sec
Decode latency: 1.5973181836307049 sec
Time for inference 8: 1.64 sec total, 624.65 tokens/sec
Decode latency: 1.60 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 1746.54 GB/s
FLOPS achieved: 8.73 TF/s

Prefill latency: 0.041260916739702225 sec
Decode latency: 1.5986011195927858 sec
Time for inference 9: 1.64 sec total, 624.11 tokens/sec
Decode latency: 1.60 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 1745.02 GB/s
FLOPS achieved: 8.73 TF/s

Prefill latency: 0.041408101096749306 sec
Decode latency: 1.5988527666777372 sec
Time for inference 10: 1.64 sec total, 624.01 tokens/sec
Decode latency: 1.60 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 1744.75 GB/s
FLOPS achieved: 8.72 TF/s

==========
Batch Size: 4
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 1.5980 sec
Average prefill latency: 0.0413 sec
Average tokens/sec: 624.36
Memory used: 7.44 GB
Done. we are killing the process
[rank0]:[W1113 11:32:23.996703084 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
