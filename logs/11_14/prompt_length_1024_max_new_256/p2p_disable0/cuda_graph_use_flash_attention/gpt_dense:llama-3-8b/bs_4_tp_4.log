W1113 11:30:51.884000 1323623 site-packages/torch/distributed/run.py:793] 
W1113 11:30:51.884000 1323623 site-packages/torch/distributed/run.py:793] *****************************************
W1113 11:30:51.884000 1323623 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1113 11:30:51.884000 1323623 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTDense(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x DenseTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=1536, bias=False)
        (wo): Linear(in_features=1024, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.09 seconds
CUDA_GRAPH are activate
Prefill latency: 0.08636725880205631 sec
Decode latency: 1.6172711458057165 sec
Compilation time: 1.74 seconds
Compilation time: 1.67 seconds
Compilation time: 1.72 seconds
Compilation time: 1.71 seconds
Prefill latency: 0.0539863221347332 sec
Decode latency: 1.6162589229643345 sec
Prefill latency: 0.05381733365356922 sec
Decode latency: 1.6161356009542942 sec
Prefill latency: 0.053832028061151505 sec
Decode latency: 1.6158203724771738 sec
Prefill latency: 0.05390055663883686 sec
Decode latency: 1.6178007330745459 sec
Prefill latency: 0.05399151146411896 sec
Decode latency: 1.6173977851867676 sec
Time for inference 1: 1.67 sec total, 612.41 tokens/sec
Decode latency: 1.62 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 2780.88 GB/s
FLOPS achieved: 13.90 TF/s

Prefill latency: 0.05411346070468426 sec
Decode latency: 1.6174124125391245 sec
Time for inference 2: 1.67 sec total, 612.36 tokens/sec
Decode latency: 1.62 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 2780.65 GB/s
FLOPS achieved: 13.90 TF/s

Prefill latency: 0.0539421122521162 sec
Decode latency: 1.6158298589289188 sec
Time for inference 3: 1.67 sec total, 612.99 tokens/sec
Decode latency: 1.62 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 2783.49 GB/s
FLOPS achieved: 13.92 TF/s

Prefill latency: 0.05395921505987644 sec
Decode latency: 1.6175630204379559 sec
Time for inference 4: 1.67 sec total, 612.36 tokens/sec
Decode latency: 1.62 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 2780.67 GB/s
FLOPS achieved: 13.90 TF/s

Prefill latency: 0.05381828173995018 sec
Decode latency: 1.6166460625827312 sec
Time for inference 5: 1.67 sec total, 612.72 tokens/sec
Decode latency: 1.62 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 2782.26 GB/s
FLOPS achieved: 13.91 TF/s

Prefill latency: 0.05404076725244522 sec
Decode latency: 1.6164665054529905 sec
Time for inference 6: 1.67 sec total, 612.67 tokens/sec
Decode latency: 1.62 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 2782.06 GB/s
FLOPS achieved: 13.91 TF/s

Prefill latency: 0.05395469069480896 sec
Decode latency: 1.6169273741543293 sec
Time for inference 7: 1.67 sec total, 612.50 tokens/sec
Decode latency: 1.62 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 2781.29 GB/s
FLOPS achieved: 13.91 TF/s

Prefill latency: 0.053949179127812386 sec
Decode latency: 1.6172046139836311 sec
Time for inference 8: 1.67 sec total, 612.45 tokens/sec
Decode latency: 1.62 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 2781.04 GB/s
FLOPS achieved: 13.91 TF/s

Prefill latency: 0.05395179241895676 sec
Decode latency: 1.6162978131324053 sec
Time for inference 9: 1.67 sec total, 612.78 tokens/sec
Decode latency: 1.62 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 2782.57 GB/s
FLOPS achieved: 13.91 TF/s

Prefill latency: 0.053984252735972404 sec
Decode latency: 1.6175232082605362 sec
Time for inference 10: 1.67 sec total, 612.35 tokens/sec
Decode latency: 1.62 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 2780.59 GB/s
FLOPS achieved: 13.90 TF/s

==========
Batch Size: 4
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 1.6169 sec
Average prefill latency: 0.0540 sec
Average tokens/sec: 612.56
Memory used: 10.18 GB
Done. we are killing the process
[rank0]:[W1113 11:31:31.310598825 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
