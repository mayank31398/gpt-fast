flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTDense(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x DenseTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
        (wo): Linear(in_features=4096, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=28672, bias=False)
        (w2): Linear(in_features=14336, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.30 seconds
CUDA_GRAPH are activate
Prefill latency: 0.13084795325994492 sec
Decode latency: 2.3620470203459263 sec
Compilation time: 2.49 seconds
Prefill latency: 0.1304118800908327 sec
Decode latency: 2.3606552314013243 sec
Prefill latency: 0.13027400709688663 sec
Decode latency: 2.3605281319469213 sec
Prefill latency: 0.1313215233385563 sec
Decode latency: 2.3601927403360605 sec
Prefill latency: 0.13057390972971916 sec
Decode latency: 2.359573131427169 sec
Prefill latency: 0.1308360993862152 sec
Decode latency: 2.359221575781703 sec
Time for inference 1: 2.49 sec total, 411.12 tokens/sec
Decode latency: 2.36 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6170.87 GB/s
FLOPS achieved: 30.85 TF/s

Prefill latency: 0.1298016868531704 sec
Decode latency: 2.359744645655155 sec
Time for inference 2: 2.49 sec total, 411.21 tokens/sec
Decode latency: 2.36 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6172.20 GB/s
FLOPS achieved: 30.86 TF/s

Prefill latency: 0.1305751260370016 sec
Decode latency: 2.3597972318530083 sec
Time for inference 3: 2.49 sec total, 411.09 tokens/sec
Decode latency: 2.36 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6170.34 GB/s
FLOPS achieved: 30.85 TF/s

Prefill latency: 0.13096691481769085 sec
Decode latency: 2.359309295192361 sec
Time for inference 4: 2.49 sec total, 411.11 tokens/sec
Decode latency: 2.36 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6170.66 GB/s
FLOPS achieved: 30.85 TF/s

Prefill latency: 0.12976746633648872 sec
Decode latency: 2.3587452489882708 sec
Time for inference 5: 2.49 sec total, 411.39 tokens/sec
Decode latency: 2.36 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6174.85 GB/s
FLOPS achieved: 30.87 TF/s

Prefill latency: 0.13034620508551598 sec
Decode latency: 2.360731028020382 sec
Time for inference 6: 2.49 sec total, 410.95 tokens/sec
Decode latency: 2.36 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6168.32 GB/s
FLOPS achieved: 30.84 TF/s

Prefill latency: 0.130928510800004 sec
Decode latency: 2.3592458497732878 sec
Time for inference 7: 2.49 sec total, 411.09 tokens/sec
Decode latency: 2.36 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6170.41 GB/s
FLOPS achieved: 30.85 TF/s

Prefill latency: 0.13024098984897137 sec
Decode latency: 2.3604601956903934 sec
Time for inference 8: 2.49 sec total, 411.02 tokens/sec
Decode latency: 2.36 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6169.30 GB/s
FLOPS achieved: 30.85 TF/s

Prefill latency: 0.13070974871516228 sec
Decode latency: 2.359770020470023 sec
Time for inference 9: 2.49 sec total, 411.04 tokens/sec
Decode latency: 2.36 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6169.59 GB/s
FLOPS achieved: 30.85 TF/s

Prefill latency: 0.1304665319621563 sec
Decode latency: 2.3599639236927032 sec
Time for inference 10: 2.49 sec total, 411.06 tokens/sec
Decode latency: 2.36 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6169.93 GB/s
FLOPS achieved: 30.85 TF/s

==========
Batch Size: 4
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 2.3597 sec
Average prefill latency: 0.1305 sec
Average tokens/sec: 411.11
Memory used: 21.03 GB
Done. we are killing the process
[rank0]:[W1113 11:30:04.670866352 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
