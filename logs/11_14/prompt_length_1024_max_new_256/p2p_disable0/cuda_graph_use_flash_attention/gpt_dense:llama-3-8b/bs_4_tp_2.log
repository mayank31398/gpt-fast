W1113 11:30:08.071000 1321770 site-packages/torch/distributed/run.py:793] 
W1113 11:30:08.071000 1321770 site-packages/torch/distributed/run.py:793] *****************************************
W1113 11:30:08.071000 1321770 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1113 11:30:08.071000 1321770 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTDense(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x DenseTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=3072, bias=False)
        (wo): Linear(in_features=2048, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=14336, bias=False)
        (w2): Linear(in_features=7168, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 0.90 seconds
CUDA_GRAPH are activate
Prefill latency: 0.09487421065568924 sec
Decode latency: 1.9138405285775661 sec
Compilation time: 2.01 seconds
Compilation time: 2.00 seconds
Prefill latency: 0.08054237067699432 sec
Decode latency: 1.9133417718112469 sec
Prefill latency: 0.08028149977326393 sec
Decode latency: 1.9131100717931986 sec
Prefill latency: 0.08042342774569988 sec
Decode latency: 1.9136986043304205 sec
Prefill latency: 0.08009624108672142 sec
Decode latency: 1.9127134941518307 sec
Prefill latency: 0.0803907997906208 sec
Decode latency: 1.9136014841496944 sec
Time for inference 1: 1.99 sec total, 513.34 tokens/sec
Decode latency: 1.91 sec
Prefill latency: 0.08 sec
Bandwidth achieved: 4122.42 GB/s
FLOPS achieved: 20.61 TF/s

Prefill latency: 0.08036826364696026 sec
Decode latency: 1.913123020902276 sec
Time for inference 2: 1.99 sec total, 513.48 tokens/sec
Decode latency: 1.91 sec
Prefill latency: 0.08 sec
Bandwidth achieved: 4123.50 GB/s
FLOPS achieved: 20.62 TF/s

Prefill latency: 0.08040975965559483 sec
Decode latency: 1.9135193172842264 sec
Time for inference 3: 1.99 sec total, 513.39 tokens/sec
Decode latency: 1.91 sec
Prefill latency: 0.08 sec
Bandwidth achieved: 4122.75 GB/s
FLOPS achieved: 20.61 TF/s

Prefill latency: 0.08067790791392326 sec
Decode latency: 1.9134487118571997 sec
Time for inference 4: 1.99 sec total, 513.32 tokens/sec
Decode latency: 1.91 sec
Prefill latency: 0.08 sec
Bandwidth achieved: 4122.24 GB/s
FLOPS achieved: 20.61 TF/s

Prefill latency: 0.08090903609991074 sec
Decode latency: 1.9140040446072817 sec
Time for inference 5: 2.00 sec total, 513.11 tokens/sec
Decode latency: 1.91 sec
Prefill latency: 0.08 sec
Bandwidth achieved: 4120.57 GB/s
FLOPS achieved: 20.60 TF/s

Prefill latency: 0.08077497594058514 sec
Decode latency: 1.9130807965993881 sec
Time for inference 6: 1.99 sec total, 513.41 tokens/sec
Decode latency: 1.91 sec
Prefill latency: 0.08 sec
Bandwidth achieved: 4122.98 GB/s
FLOPS achieved: 20.61 TF/s

Prefill latency: 0.08066960424184799 sec
Decode latency: 1.9139664210379124 sec
Time for inference 7: 2.00 sec total, 513.19 tokens/sec
Decode latency: 1.91 sec
Prefill latency: 0.08 sec
Bandwidth achieved: 4121.22 GB/s
FLOPS achieved: 20.61 TF/s

Prefill latency: 0.08034199476242065 sec
Decode latency: 1.912318592891097 sec
Time for inference 8: 1.99 sec total, 513.73 tokens/sec
Decode latency: 1.91 sec
Prefill latency: 0.08 sec
Bandwidth achieved: 4125.53 GB/s
FLOPS achieved: 20.63 TF/s

Prefill latency: 0.08046970330178738 sec
Decode latency: 1.9131079651415348 sec
Time for inference 9: 1.99 sec total, 513.46 tokens/sec
Decode latency: 1.91 sec
Prefill latency: 0.08 sec
Bandwidth achieved: 4123.34 GB/s
FLOPS achieved: 20.62 TF/s

Prefill latency: 0.08048086799681187 sec
Decode latency: 1.9143931567668915 sec
Time for inference 10: 2.00 sec total, 513.12 tokens/sec
Decode latency: 1.91 sec
Prefill latency: 0.08 sec
Bandwidth achieved: 4120.60 GB/s
FLOPS achieved: 20.60 TF/s

==========
Batch Size: 4
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 1.9135 sec
Average prefill latency: 0.0805 sec
Average tokens/sec: 513.36
Memory used: 14.71 GB
Done. we are killing the process
[rank0]:[W1113 11:30:48.416653835 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
