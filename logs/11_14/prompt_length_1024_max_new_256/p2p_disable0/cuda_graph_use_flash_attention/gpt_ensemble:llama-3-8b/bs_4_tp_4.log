W1113 11:43:36.260000 1356350 site-packages/torch/distributed/run.py:793] 
W1113 11:43:36.260000 1356350 site-packages/torch/distributed/run.py:793] *****************************************
W1113 11:43:36.260000 1356350 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1113 11:43:36.260000 1356350 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=1536, bias=False)
        (wo): Linear(in_features=1024, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.61 seconds
CUDA_GRAPH are activate
Prefill latency: 0.08538486249744892 sec
Decode latency: 1.4705753810703754 sec
Compilation time: 1.68 seconds
Compilation time: 1.68 seconds
Compilation time: 1.56 seconds
Compilation time: 1.52 seconds
Prefill latency: 0.0492617879062891 sec
Decode latency: 1.4699366446584463 sec
Prefill latency: 0.04908364824950695 sec
Decode latency: 1.470508612692356 sec
Prefill latency: 0.04896448738873005 sec
Decode latency: 1.4703359715640545 sec
Prefill latency: 0.04919886402785778 sec
Decode latency: 1.4701239950954914 sec
Prefill latency: 0.04905933886766434 sec
Decode latency: 1.4696894884109497 sec
Time for inference 1: 1.52 sec total, 673.88 tokens/sec
Decode latency: 1.47 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 3060.00 GB/s
FLOPS achieved: 15.30 TF/s

Prefill latency: 0.049123095348477364 sec
Decode latency: 1.4702267292886972 sec
Time for inference 2: 1.52 sec total, 673.63 tokens/sec
Decode latency: 1.47 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 3058.86 GB/s
FLOPS achieved: 15.29 TF/s

Prefill latency: 0.04913376830518246 sec
Decode latency: 1.4699649792164564 sec
Time for inference 3: 1.52 sec total, 673.79 tokens/sec
Decode latency: 1.47 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 3059.57 GB/s
FLOPS achieved: 15.30 TF/s

Prefill latency: 0.04910399205982685 sec
Decode latency: 1.4704797677695751 sec
Time for inference 4: 1.52 sec total, 673.57 tokens/sec
Decode latency: 1.47 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 3058.60 GB/s
FLOPS achieved: 15.29 TF/s

Prefill latency: 0.04919543117284775 sec
Decode latency: 1.4699154123663902 sec
Time for inference 5: 1.52 sec total, 673.79 tokens/sec
Decode latency: 1.47 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 3059.58 GB/s
FLOPS achieved: 15.30 TF/s

Prefill latency: 0.04904131591320038 sec
Decode latency: 1.4706115573644638 sec
Time for inference 6: 1.52 sec total, 673.58 tokens/sec
Decode latency: 1.47 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 3058.62 GB/s
FLOPS achieved: 15.29 TF/s

Prefill latency: 0.049036454409360886 sec
Decode latency: 1.4697779342532158 sec
Time for inference 7: 1.52 sec total, 673.93 tokens/sec
Decode latency: 1.47 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 3060.21 GB/s
FLOPS achieved: 15.30 TF/s

Prefill latency: 0.04928387142717838 sec
Decode latency: 1.4703255016356707 sec
Time for inference 8: 1.52 sec total, 673.59 tokens/sec
Decode latency: 1.47 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 3058.70 GB/s
FLOPS achieved: 15.29 TF/s

Prefill latency: 0.049140412360429764 sec
Decode latency: 1.469675425440073 sec
Time for inference 9: 1.52 sec total, 673.93 tokens/sec
Decode latency: 1.47 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 3060.21 GB/s
FLOPS achieved: 15.30 TF/s

Prefill latency: 0.04918370209634304 sec
Decode latency: 1.470791282132268 sec
Time for inference 10: 1.52 sec total, 673.38 tokens/sec
Decode latency: 1.47 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 3057.75 GB/s
FLOPS achieved: 15.29 TF/s

==========
Batch Size: 4
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 1.4701 sec
Average prefill latency: 0.0491 sec
Average tokens/sec: 673.71
Memory used: 9.05 GB
Done. we are killing the process
[rank0]:[W1113 11:44:13.952749738 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
