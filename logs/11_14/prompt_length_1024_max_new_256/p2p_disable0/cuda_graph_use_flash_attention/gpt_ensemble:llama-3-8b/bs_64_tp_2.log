W1113 12:53:28.346000 1536115 site-packages/torch/distributed/run.py:793] 
W1113 12:53:28.346000 1536115 site-packages/torch/distributed/run.py:793] *****************************************
W1113 12:53:28.346000 1536115 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1113 12:53:28.346000 1536115 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=3072, bias=False)
        (wo): Linear(in_features=2048, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=14336, bias=False)
        (w2): Linear(in_features=7168, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.21 seconds
CUDA_GRAPH are activate
Prefill latency: 1.3316679205745459 sec
Decode latency: 2.432285286486149 sec
Compilation time: 3.77 seconds
Compilation time: 3.73 seconds
Prefill latency: 1.296342920511961 sec
Decode latency: 2.4309685546904802 sec
Prefill latency: 1.2995984051376581 sec
Decode latency: 2.4306594617664814 sec
Prefill latency: 1.2976261414587498 sec
Decode latency: 2.434565095230937 sec
Prefill latency: 1.2915522679686546 sec
Decode latency: 2.4326333720237017 sec
Prefill latency: 1.2893451824784279 sec
Decode latency: 2.4326657615602016 sec
Time for inference 1: 3.72 sec total, 4401.01 tokens/sec
Decode latency: 2.43 sec
Prefill latency: 1.29 sec
Bandwidth achieved: 35342.46 GB/s
FLOPS achieved: 176.71 TF/s

Prefill latency: 1.2966009229421616 sec
Decode latency: 2.4324969239532948 sec
Time for inference 2: 3.73 sec total, 4392.76 tokens/sec
Decode latency: 2.43 sec
Prefill latency: 1.30 sec
Bandwidth achieved: 35276.19 GB/s
FLOPS achieved: 176.38 TF/s

Prefill latency: 1.2943140529096127 sec
Decode latency: 2.4332910366356373 sec
Time for inference 3: 3.73 sec total, 4394.56 tokens/sec
Decode latency: 2.43 sec
Prefill latency: 1.29 sec
Bandwidth achieved: 35290.67 GB/s
FLOPS achieved: 176.45 TF/s

Prefill latency: 1.2968341894447803 sec
Decode latency: 2.431015556678176 sec
Time for inference 4: 3.73 sec total, 4394.35 tokens/sec
Decode latency: 2.43 sec
Prefill latency: 1.30 sec
Bandwidth achieved: 35288.93 GB/s
FLOPS achieved: 176.44 TF/s

Prefill latency: 1.2951703686267138 sec
Decode latency: 2.434375060722232 sec
Time for inference 5: 3.73 sec total, 4392.17 tokens/sec
Decode latency: 2.43 sec
Prefill latency: 1.30 sec
Bandwidth achieved: 35271.41 GB/s
FLOPS achieved: 176.36 TF/s

Prefill latency: 1.3004273474216461 sec
Decode latency: 2.433413490653038 sec
Time for inference 6: 3.73 sec total, 4387.21 tokens/sec
Decode latency: 2.43 sec
Prefill latency: 1.30 sec
Bandwidth achieved: 35231.63 GB/s
FLOPS achieved: 176.16 TF/s

Prefill latency: 1.3025943245738745 sec
Decode latency: 2.4326416812837124 sec
Time for inference 7: 3.74 sec total, 4385.25 tokens/sec
Decode latency: 2.43 sec
Prefill latency: 1.30 sec
Bandwidth achieved: 35215.90 GB/s
FLOPS achieved: 176.08 TF/s

Prefill latency: 1.295153945684433 sec
Decode latency: 2.4340270161628723 sec
Time for inference 8: 3.73 sec total, 4392.47 tokens/sec
Decode latency: 2.43 sec
Prefill latency: 1.30 sec
Bandwidth achieved: 35273.81 GB/s
FLOPS achieved: 176.37 TF/s

Prefill latency: 1.3007933162152767 sec
Decode latency: 2.430705450475216 sec
Time for inference 9: 3.73 sec total, 4389.84 tokens/sec
Decode latency: 2.43 sec
Prefill latency: 1.30 sec
Bandwidth achieved: 35252.75 GB/s
FLOPS achieved: 176.26 TF/s

Prefill latency: 1.2928048726171255 sec
Decode latency: 2.433934336528182 sec
Time for inference 10: 3.73 sec total, 4395.55 tokens/sec
Decode latency: 2.43 sec
Prefill latency: 1.29 sec
Bandwidth achieved: 35298.56 GB/s
FLOPS achieved: 176.49 TF/s

==========
Batch Size: 64
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 2.4329 sec
Average prefill latency: 1.2964 sec
Average tokens/sec: 4392.52
Memory used: 67.08 GB
Done. we are killing the process
[rank0]:[W1113 12:54:39.138823910 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
