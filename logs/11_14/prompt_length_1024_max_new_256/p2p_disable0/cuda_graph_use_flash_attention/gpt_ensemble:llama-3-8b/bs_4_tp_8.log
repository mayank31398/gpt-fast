W1113 11:44:16.271000 1359419 site-packages/torch/distributed/run.py:793] 
W1113 11:44:16.271000 1359419 site-packages/torch/distributed/run.py:793] *****************************************
W1113 11:44:16.271000 1359419 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1113 11:44:16.271000 1359419 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=768, bias=False)
        (wo): Linear(in_features=512, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=3584, bias=False)
        (w2): Linear(in_features=1792, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.47 seconds
CUDA_GRAPH are activate
Prefill latency: 0.14666777290403843 sec
Decode latency: 1.3799473270773888 sec
Compilation time: 1.49 seconds
Compilation time: 1.53 seconds
Compilation time: 1.54 seconds
Compilation time: 1.44 seconds
Compilation time: 1.42 seconds
Compilation time: 1.46 secondsCompilation time: 1.44 seconds

Compilation time: 1.42 seconds
Prefill latency: 0.035506077110767365 sec
Decode latency: 1.3791865054517984 sec
Prefill latency: 0.03544689156115055 sec
Decode latency: 1.3783640768378973 sec
Prefill latency: 0.03545967675745487 sec
Decode latency: 1.3789398148655891 sec
Prefill latency: 0.035448092967271805 sec
Decode latency: 1.3783834464848042 sec
Prefill latency: 0.03541050665080547 sec
Decode latency: 1.3789092171937227 sec
Time for inference 1: 1.42 sec total, 723.59 tokens/sec
Decode latency: 1.38 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 2023.18 GB/s
FLOPS achieved: 10.12 TF/s

Prefill latency: 0.03543558903038502 sec
Decode latency: 1.3780907709151506 sec
Time for inference 2: 1.41 sec total, 723.99 tokens/sec
Decode latency: 1.38 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 2024.31 GB/s
FLOPS achieved: 10.12 TF/s

Prefill latency: 0.035405442118644714 sec
Decode latency: 1.3793501779437065 sec
Time for inference 3: 1.42 sec total, 723.35 tokens/sec
Decode latency: 1.38 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 2022.52 GB/s
FLOPS achieved: 10.11 TF/s

Prefill latency: 0.035370804369449615 sec
Decode latency: 1.3786258436739445 sec
Time for inference 4: 1.41 sec total, 723.74 tokens/sec
Decode latency: 1.38 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 2023.60 GB/s
FLOPS achieved: 10.12 TF/s

Prefill latency: 0.035386333242058754 sec
Decode latency: 1.379008887335658 sec
Time for inference 5: 1.42 sec total, 723.57 tokens/sec
Decode latency: 1.38 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 2023.12 GB/s
FLOPS achieved: 10.12 TF/s

Prefill latency: 0.03541197068989277 sec
Decode latency: 1.3782807048410177 sec
Time for inference 6: 1.41 sec total, 723.86 tokens/sec
Decode latency: 1.38 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 2023.94 GB/s
FLOPS achieved: 10.12 TF/s

Prefill latency: 0.03536202944815159 sec
Decode latency: 1.3790448755025864 sec
Time for inference 7: 1.42 sec total, 723.57 tokens/sec
Decode latency: 1.38 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 2023.12 GB/s
FLOPS achieved: 10.12 TF/s

Prefill latency: 0.03545293025672436 sec
Decode latency: 1.3780801240354776 sec
Time for inference 8: 1.41 sec total, 724.00 tokens/sec
Decode latency: 1.38 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 2024.33 GB/s
FLOPS achieved: 10.12 TF/s

Prefill latency: 0.035369839519262314 sec
Decode latency: 1.3789428360760212 sec
Time for inference 9: 1.42 sec total, 723.59 tokens/sec
Decode latency: 1.38 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 2023.18 GB/s
FLOPS achieved: 10.12 TF/s

Prefill latency: 0.035406025126576424 sec
Decode latency: 1.3786378968507051 sec
Time for inference 10: 1.41 sec total, 723.71 tokens/sec
Decode latency: 1.38 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 2023.52 GB/s
FLOPS achieved: 10.12 TF/s

==========
Batch Size: 4
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 1.3787 sec
Average prefill latency: 0.0354 sec
Average tokens/sec: 723.70
Memory used: 6.36 GB
Done. we are killing the process
[rank0]:[W1113 11:45:02.769376394 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
