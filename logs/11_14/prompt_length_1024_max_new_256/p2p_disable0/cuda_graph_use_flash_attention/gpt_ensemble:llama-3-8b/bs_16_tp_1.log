flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
        (wo): Linear(in_features=4096, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=28672, bias=False)
        (w2): Linear(in_features=14336, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.69 seconds
CUDA_GRAPH are activate
[rank0]:[W1113 11:45:15.489098862 CUDAGraph.cpp:133] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
Prefill latency: 0.5267824288457632 sec
Decode latency: 2.590894967317581 sec
Compilation time: 3.12 seconds
Prefill latency: 0.5277845878154039 sec
Decode latency: 2.5890339259058237 sec
Prefill latency: 0.5287384651601315 sec
Decode latency: 2.537112131714821 sec
Prefill latency: 0.5305015332996845 sec
Decode latency: 2.589795082807541 sec
Prefill latency: 0.5297568254172802 sec
Decode latency: 2.588917961344123 sec
Prefill latency: 0.5286586303263903 sec
Decode latency: 2.588431440293789 sec
Time for inference 1: 3.12 sec total, 1313.69 tokens/sec
Decode latency: 2.59 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 19718.25 GB/s
FLOPS achieved: 98.59 TF/s

Prefill latency: 0.5301565267145634 sec
Decode latency: 2.5883984435349703 sec
Time for inference 2: 3.12 sec total, 1313.14 tokens/sec
Decode latency: 2.59 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 19710.01 GB/s
FLOPS achieved: 98.55 TF/s

Prefill latency: 0.5287639647722244 sec
Decode latency: 2.5870484318584204 sec
Time for inference 3: 3.12 sec total, 1314.31 tokens/sec
Decode latency: 2.59 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 19727.63 GB/s
FLOPS achieved: 98.64 TF/s

Prefill latency: 0.5286234114319086 sec
Decode latency: 2.588501639664173 sec
Time for inference 4: 3.12 sec total, 1313.73 tokens/sec
Decode latency: 2.59 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 19718.93 GB/s
FLOPS achieved: 98.59 TF/s

Prefill latency: 0.5270324926823378 sec
Decode latency: 2.586710572242737 sec
Time for inference 5: 3.11 sec total, 1315.16 tokens/sec
Decode latency: 2.59 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 19740.29 GB/s
FLOPS achieved: 98.70 TF/s

Prefill latency: 0.5290462300181389 sec
Decode latency: 2.5390123203396797 sec
Time for inference 6: 3.07 sec total, 1334.75 tokens/sec
Decode latency: 2.54 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 20034.44 GB/s
FLOPS achieved: 100.17 TF/s

Prefill latency: 0.5284408777952194 sec
Decode latency: 2.589245431125164 sec
Time for inference 7: 3.12 sec total, 1313.48 tokens/sec
Decode latency: 2.59 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 19715.14 GB/s
FLOPS achieved: 98.58 TF/s

Prefill latency: 0.5291209500283003 sec
Decode latency: 2.5647935960441828 sec
Time for inference 8: 3.09 sec total, 1323.53 tokens/sec
Decode latency: 2.56 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 19866.05 GB/s
FLOPS achieved: 99.33 TF/s

Prefill latency: 0.5273021962493658 sec
Decode latency: 2.5866229329258204 sec
Time for inference 9: 3.11 sec total, 1315.07 tokens/sec
Decode latency: 2.59 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 19739.04 GB/s
FLOPS achieved: 98.70 TF/s

Prefill latency: 0.5311441589146852 sec
Decode latency: 2.58684435300529 sec
Time for inference 10: 3.12 sec total, 1313.31 tokens/sec
Decode latency: 2.59 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 19712.54 GB/s
FLOPS achieved: 98.56 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 2.5806 sec
Average prefill latency: 0.5288 sec
Average tokens/sec: 1317.02
Memory used: 30.31 GB
Done. we are killing the process
[rank0]:[W1113 11:46:02.392617639 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
