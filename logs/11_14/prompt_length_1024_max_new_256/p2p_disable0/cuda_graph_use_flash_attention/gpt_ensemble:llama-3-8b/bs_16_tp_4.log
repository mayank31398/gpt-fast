W1113 11:46:53.514000 1368187 site-packages/torch/distributed/run.py:793] 
W1113 11:46:53.514000 1368187 site-packages/torch/distributed/run.py:793] *****************************************
W1113 11:46:53.514000 1368187 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1113 11:46:53.514000 1368187 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=1536, bias=False)
        (wo): Linear(in_features=1024, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 0.90 seconds
CUDA_GRAPH are activate
Prefill latency: 0.20887155830860138 sec
Decode latency: 1.606873705983162 sec
Compilation time: 1.80 seconds
Compilation time: 1.82 seconds
Compilation time: 1.80 seconds
Compilation time: 1.82 seconds
Prefill latency: 0.18961816653609276 sec
Decode latency: 1.6078930161893368 sec
Prefill latency: 0.19065441377460957 sec
Decode latency: 1.608509935438633 sec
Prefill latency: 0.1902891844511032 sec
Decode latency: 1.6073328219354153 sec
Prefill latency: 0.18993081711232662 sec
Decode latency: 1.6084803249686956 sec
Prefill latency: 0.18985012359917164 sec
Decode latency: 1.607849234715104 sec
Time for inference 1: 1.80 sec total, 2277.46 tokens/sec
Decode latency: 1.61 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 10341.64 GB/s
FLOPS achieved: 51.71 TF/s

Prefill latency: 0.19051911123096943 sec
Decode latency: 1.6078533362597227 sec
Time for inference 2: 1.80 sec total, 2276.57 tokens/sec
Decode latency: 1.61 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 10337.62 GB/s
FLOPS achieved: 51.69 TF/s

Prefill latency: 0.19087367691099644 sec
Decode latency: 1.6094642635434866 sec
Time for inference 3: 1.80 sec total, 2274.13 tokens/sec
Decode latency: 1.61 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 10326.54 GB/s
FLOPS achieved: 51.63 TF/s

Prefill latency: 0.1904362179338932 sec
Decode latency: 1.607003103941679 sec
Time for inference 4: 1.80 sec total, 2277.95 tokens/sec
Decode latency: 1.61 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 10343.85 GB/s
FLOPS achieved: 51.72 TF/s

Prefill latency: 0.1904469169676304 sec
Decode latency: 1.607897199690342 sec
Time for inference 5: 1.80 sec total, 2276.59 tokens/sec
Decode latency: 1.61 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 10337.69 GB/s
FLOPS achieved: 51.69 TF/s

Prefill latency: 0.1902003213763237 sec
Decode latency: 1.6083042938262224 sec
Time for inference 6: 1.80 sec total, 2276.38 tokens/sec
Decode latency: 1.61 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 10336.76 GB/s
FLOPS achieved: 51.68 TF/s

Prefill latency: 0.19012905471026897 sec
Decode latency: 1.6076449546962976 sec
Time for inference 7: 1.80 sec total, 2277.49 tokens/sec
Decode latency: 1.61 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 10341.76 GB/s
FLOPS achieved: 51.71 TF/s

Prefill latency: 0.19047971069812775 sec
Decode latency: 1.608070032671094 sec
Time for inference 8: 1.80 sec total, 2276.47 tokens/sec
Decode latency: 1.61 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 10337.14 GB/s
FLOPS achieved: 51.69 TF/s

Prefill latency: 0.19027693569660187 sec
Decode latency: 1.6076798848807812 sec
Time for inference 9: 1.80 sec total, 2277.13 tokens/sec
Decode latency: 1.61 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 10340.13 GB/s
FLOPS achieved: 51.70 TF/s

Prefill latency: 0.19043114967644215 sec
Decode latency: 1.6079557929188013 sec
Time for inference 10: 1.80 sec total, 2276.68 tokens/sec
Decode latency: 1.61 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 10338.11 GB/s
FLOPS achieved: 51.69 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 1.6080 sec
Average prefill latency: 0.1904 sec
Average tokens/sec: 2276.69
Memory used: 19.13 GB
Done. we are killing the process
[rank0]:[W1113 11:47:35.746674329 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
