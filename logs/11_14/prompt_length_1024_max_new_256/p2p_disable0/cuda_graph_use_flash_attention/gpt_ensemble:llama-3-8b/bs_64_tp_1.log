flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
        (wo): Linear(in_features=4096, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=28672, bias=False)
        (w2): Linear(in_features=14336, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 0.99 seconds
CUDA_GRAPH are activate
Prefill latency: 2.3101944625377655 sec
Decode latency: 3.4507783707231283 sec
Compilation time: 5.76 seconds
Prefill latency: 2.3050069212913513 sec
Decode latency: 3.4493603371083736 sec
Prefill latency: 2.3039540331810713 sec
Decode latency: 3.4494039118289948 sec
Prefill latency: 2.30538122728467 sec
Decode latency: 3.449523586779833 sec
Prefill latency: 2.303113365545869 sec
Decode latency: 3.4501246362924576 sec
Prefill latency: 2.3050222489982843 sec
Decode latency: 3.4492777790874243 sec
Time for inference 1: 5.76 sec total, 2846.85 tokens/sec
Decode latency: 3.45 sec
Prefill latency: 2.31 sec
Bandwidth achieved: 42730.76 GB/s
FLOPS achieved: 213.65 TF/s

Prefill latency: 2.3081663586199284 sec
Decode latency: 3.388052536174655 sec
Time for inference 2: 5.70 sec total, 2875.89 tokens/sec
Decode latency: 3.39 sec
Prefill latency: 2.31 sec
Bandwidth achieved: 43166.71 GB/s
FLOPS achieved: 215.83 TF/s

Prefill latency: 2.309295691549778 sec
Decode latency: 3.4498648159205914 sec
Time for inference 3: 5.76 sec total, 2844.39 tokens/sec
Decode latency: 3.45 sec
Prefill latency: 2.31 sec
Bandwidth achieved: 42693.81 GB/s
FLOPS achieved: 213.47 TF/s

Prefill latency: 2.307509006932378 sec
Decode latency: 3.449678396806121 sec
Time for inference 4: 5.76 sec total, 2845.40 tokens/sec
Decode latency: 3.45 sec
Prefill latency: 2.31 sec
Bandwidth achieved: 42709.00 GB/s
FLOPS achieved: 213.55 TF/s

Prefill latency: 2.3058370295912027 sec
Decode latency: 3.449549611657858 sec
Time for inference 5: 5.76 sec total, 2846.27 tokens/sec
Decode latency: 3.45 sec
Prefill latency: 2.31 sec
Bandwidth achieved: 42722.04 GB/s
FLOPS achieved: 213.61 TF/s

Prefill latency: 2.3062253315001726 sec
Decode latency: 3.3986456729471684 sec
Time for inference 6: 5.71 sec total, 2871.49 tokens/sec
Decode latency: 3.40 sec
Prefill latency: 2.31 sec
Bandwidth achieved: 43100.56 GB/s
FLOPS achieved: 215.50 TF/s

Prefill latency: 2.3085946701467037 sec
Decode latency: 3.449630431830883 sec
Time for inference 7: 5.76 sec total, 2844.91 tokens/sec
Decode latency: 3.45 sec
Prefill latency: 2.31 sec
Bandwidth achieved: 42701.69 GB/s
FLOPS achieved: 213.51 TF/s

Prefill latency: 2.3112897630780935 sec
Decode latency: 3.4494784772396088 sec
Time for inference 8: 5.76 sec total, 2843.62 tokens/sec
Decode latency: 3.45 sec
Prefill latency: 2.31 sec
Bandwidth achieved: 42682.25 GB/s
FLOPS achieved: 213.41 TF/s

Prefill latency: 2.311361389234662 sec
Decode latency: 3.4499409589916468 sec
Time for inference 9: 5.76 sec total, 2843.39 tokens/sec
Decode latency: 3.45 sec
Prefill latency: 2.31 sec
Bandwidth achieved: 42678.85 GB/s
FLOPS achieved: 213.39 TF/s

Prefill latency: 2.309251584112644 sec
Decode latency: 3.450846379622817 sec
Time for inference 10: 5.76 sec total, 2843.93 tokens/sec
Decode latency: 3.45 sec
Prefill latency: 2.31 sec
Bandwidth achieved: 42686.89 GB/s
FLOPS achieved: 213.43 TF/s

==========
Batch Size: 64
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 3.4385 sec
Average prefill latency: 2.3083 sec
Average tokens/sec: 2850.61
Memory used: 72.58 GB
Done. we are killing the process
[rank0]:[W1113 12:53:25.960367634 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
