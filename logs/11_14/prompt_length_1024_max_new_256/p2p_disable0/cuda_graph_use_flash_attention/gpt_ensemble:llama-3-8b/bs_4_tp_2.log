W1113 11:42:53.354000 1354250 site-packages/torch/distributed/run.py:793] 
W1113 11:42:53.354000 1354250 site-packages/torch/distributed/run.py:793] *****************************************
W1113 11:42:53.354000 1354250 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1113 11:42:53.354000 1354250 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=3072, bias=False)
        (wo): Linear(in_features=2048, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=14336, bias=False)
        (w2): Linear(in_features=7168, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 2.04 seconds
CUDA_GRAPH are activate
Prefill latency: 0.07758510857820511 sec
Decode latency: 1.8075749762356281 sec
Compilation time: 1.90 seconds
Compilation time: 1.89 seconds
Prefill latency: 0.07658910751342773 sec
Decode latency: 1.8057455848902464 sec
Prefill latency: 0.07673845998942852 sec
Decode latency: 1.8054688721895218 sec
Prefill latency: 0.07690677605569363 sec
Decode latency: 1.8055979926139116 sec
Prefill latency: 0.07665906473994255 sec
Decode latency: 1.8058647271245718 sec
Prefill latency: 0.0769354049116373 sec
Decode latency: 1.8059250991791487 sec
Time for inference 1: 1.88 sec total, 543.64 tokens/sec
Decode latency: 1.81 sec
Prefill latency: 0.08 sec
Bandwidth achieved: 4365.71 GB/s
FLOPS achieved: 21.83 TF/s

Prefill latency: 0.0768237691372633 sec
Decode latency: 1.8066351879388094 sec
Time for inference 2: 1.88 sec total, 543.47 tokens/sec
Decode latency: 1.81 sec
Prefill latency: 0.08 sec
Bandwidth achieved: 4364.37 GB/s
FLOPS achieved: 21.82 TF/s

Prefill latency: 0.07666781730949879 sec
Decode latency: 1.8058862499892712 sec
Time for inference 3: 1.88 sec total, 543.76 tokens/sec
Decode latency: 1.81 sec
Prefill latency: 0.08 sec
Bandwidth achieved: 4366.64 GB/s
FLOPS achieved: 21.83 TF/s

Prefill latency: 0.07683924399316311 sec
Decode latency: 1.8055583275854588 sec
Time for inference 4: 1.88 sec total, 543.81 tokens/sec
Decode latency: 1.81 sec
Prefill latency: 0.08 sec
Bandwidth achieved: 4367.04 GB/s
FLOPS achieved: 21.84 TF/s

Prefill latency: 0.07669973000884056 sec
Decode latency: 1.8056365475058556 sec
Time for inference 5: 1.88 sec total, 543.80 tokens/sec
Decode latency: 1.81 sec
Prefill latency: 0.08 sec
Bandwidth achieved: 4367.03 GB/s
FLOPS achieved: 21.84 TF/s

Prefill latency: 0.07703864015638828 sec
Decode latency: 1.8060671761631966 sec
Time for inference 6: 1.88 sec total, 543.56 tokens/sec
Decode latency: 1.81 sec
Prefill latency: 0.08 sec
Bandwidth achieved: 4365.08 GB/s
FLOPS achieved: 21.83 TF/s

Prefill latency: 0.07686329632997513 sec
Decode latency: 1.8070710767060518 sec
Time for inference 7: 1.88 sec total, 543.29 tokens/sec
Decode latency: 1.81 sec
Prefill latency: 0.08 sec
Bandwidth achieved: 4362.91 GB/s
FLOPS achieved: 21.81 TF/s

Prefill latency: 0.07683777436614037 sec
Decode latency: 1.805890692397952 sec
Time for inference 8: 1.88 sec total, 543.66 tokens/sec
Decode latency: 1.81 sec
Prefill latency: 0.08 sec
Bandwidth achieved: 4365.90 GB/s
FLOPS achieved: 21.83 TF/s

Prefill latency: 0.07668722979724407 sec
Decode latency: 1.8058966062963009 sec
Time for inference 9: 1.88 sec total, 543.70 tokens/sec
Decode latency: 1.81 sec
Prefill latency: 0.08 sec
Bandwidth achieved: 4366.16 GB/s
FLOPS achieved: 21.83 TF/s

Prefill latency: 0.0771243404597044 sec
Decode latency: 1.805770056322217 sec
Time for inference 10: 1.88 sec total, 543.63 tokens/sec
Decode latency: 1.81 sec
Prefill latency: 0.08 sec
Bandwidth achieved: 4365.65 GB/s
FLOPS achieved: 21.83 TF/s

==========
Batch Size: 4
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 1.8060 sec
Average prefill latency: 0.0769 sec
Average tokens/sec: 543.63
Memory used: 12.92 GB
Done. we are killing the process
[rank0]:[W1113 11:43:32.448103059 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
