W1113 12:04:22.606000 1437071 site-packages/torch/distributed/run.py:793] 
W1113 12:04:22.606000 1437071 site-packages/torch/distributed/run.py:793] *****************************************
W1113 12:04:22.606000 1437071 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1113 12:04:22.606000 1437071 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTDense(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x DenseTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=1536, bias=False)
        (wo): Linear(in_features=1024, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.48 seconds
CUDA_GRAPH are activate
Prefill latency: 0.21007437631487846 sec
Decode latency: 2.0669736489653587 sec
Compilation time: 2.27 seconds
Compilation time: 2.27 seconds
Compilation time: 2.28 seconds
Compilation time: 2.28 seconds
Prefill latency: 0.20414301753044128 sec
Decode latency: 2.0654793810099363 sec
Prefill latency: 0.2040264755487442 sec
Decode latency: 2.0649606846272945 sec
Prefill latency: 0.2042884100228548 sec
Decode latency: 2.065436067059636 sec
Prefill latency: 0.20441177673637867 sec
Decode latency: 2.0655150804668665 sec
Prefill latency: 0.20426030457019806 sec
Decode latency: 2.0655396692454815 sec
Time for inference 1: 2.27 sec total, 1803.93 tokens/sec
Decode latency: 2.07 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 8191.41 GB/s
FLOPS achieved: 40.96 TF/s

Prefill latency: 0.20444331131875515 sec
Decode latency: 2.065221033990383 sec
Time for inference 2: 2.27 sec total, 1803.97 tokens/sec
Decode latency: 2.07 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 8191.57 GB/s
FLOPS achieved: 40.96 TF/s

Prefill latency: 0.20451337844133377 sec
Decode latency: 2.0653933715075254 sec
Time for inference 3: 2.27 sec total, 1803.64 tokens/sec
Decode latency: 2.07 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 8190.11 GB/s
FLOPS achieved: 40.95 TF/s

Prefill latency: 0.20440625958144665 sec
Decode latency: 2.0661251172423363 sec
Time for inference 4: 2.27 sec total, 1803.41 tokens/sec
Decode latency: 2.07 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 8189.03 GB/s
FLOPS achieved: 40.95 TF/s

Prefill latency: 0.2048229929059744 sec
Decode latency: 2.0648731030523777 sec
Time for inference 5: 2.27 sec total, 1803.99 tokens/sec
Decode latency: 2.06 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 8191.69 GB/s
FLOPS achieved: 40.96 TF/s

Prefill latency: 0.20529087632894516 sec
Decode latency: 2.0651557203382254 sec
Time for inference 6: 2.27 sec total, 1803.45 tokens/sec
Decode latency: 2.07 sec
Prefill latency: 0.21 sec
Bandwidth achieved: 8189.24 GB/s
FLOPS achieved: 40.95 TF/s

Prefill latency: 0.20448931865394115 sec
Decode latency: 2.064839480444789 sec
Time for inference 7: 2.27 sec total, 1804.38 tokens/sec
Decode latency: 2.06 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 8193.44 GB/s
FLOPS achieved: 40.97 TF/s

Prefill latency: 0.2045285329222679 sec
Decode latency: 2.0645563043653965 sec
Time for inference 8: 2.27 sec total, 1804.59 tokens/sec
Decode latency: 2.06 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 8194.40 GB/s
FLOPS achieved: 40.97 TF/s

Prefill latency: 0.20463975705206394 sec
Decode latency: 2.0657772179692984 sec
Time for inference 9: 2.27 sec total, 1803.36 tokens/sec
Decode latency: 2.07 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 8188.80 GB/s
FLOPS achieved: 40.94 TF/s

Prefill latency: 0.20473778061568737 sec
Decode latency: 2.065025955438614 sec
Time for inference 10: 2.27 sec total, 1803.86 tokens/sec
Decode latency: 2.07 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 8191.07 GB/s
FLOPS achieved: 40.96 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 2.0653 sec
Average prefill latency: 0.2046 sec
Average tokens/sec: 1803.86
Memory used: 23.55 GB
Done. we are killing the process
[rank0]:[W1113 12:05:13.443733454 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
