flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTDense(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x DenseTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
        (wo): Linear(in_features=4096, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=28672, bias=False)
        (w2): Linear(in_features=14336, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.05 seconds
CUDA_GRAPH are activate
Prefill latency: 0.5235258620232344 sec
Decode latency: 2.5925728883594275 sec
Compilation time: 3.12 seconds
Prefill latency: 0.5244712494313717 sec
Decode latency: 2.5923371091485023 sec
Prefill latency: 0.5283109080046415 sec
Decode latency: 2.590614505112171 sec
Prefill latency: 0.5258698035031557 sec
Decode latency: 2.589162042364478 sec
Prefill latency: 0.5280771180987358 sec
Decode latency: 2.5897804740816355 sec
Prefill latency: 0.5259722452610731 sec
Decode latency: 2.590127255767584 sec
Time for inference 1: 3.12 sec total, 1314.18 tokens/sec
Decode latency: 2.59 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 19725.59 GB/s
FLOPS achieved: 98.63 TF/s

Prefill latency: 0.5261546280235052 sec
Decode latency: 2.5913949012756348 sec
Time for inference 2: 3.12 sec total, 1313.57 tokens/sec
Decode latency: 2.59 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 19716.52 GB/s
FLOPS achieved: 98.58 TF/s

Prefill latency: 0.5262479074299335 sec
Decode latency: 2.591040948405862 sec
Time for inference 3: 3.12 sec total, 1313.69 tokens/sec
Decode latency: 2.59 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 19718.25 GB/s
FLOPS achieved: 98.59 TF/s

Prefill latency: 0.5262945871800184 sec
Decode latency: 2.589527867734432 sec
Time for inference 4: 3.12 sec total, 1314.32 tokens/sec
Decode latency: 2.59 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 19727.69 GB/s
FLOPS achieved: 98.64 TF/s

Prefill latency: 0.5269819162786007 sec
Decode latency: 2.5895101241767406 sec
Time for inference 5: 3.12 sec total, 1314.05 tokens/sec
Decode latency: 2.59 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 19723.70 GB/s
FLOPS achieved: 98.62 TF/s

Prefill latency: 0.527095939964056 sec
Decode latency: 2.5901956371963024 sec
Time for inference 6: 3.12 sec total, 1313.67 tokens/sec
Decode latency: 2.59 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 19718.00 GB/s
FLOPS achieved: 98.59 TF/s

Prefill latency: 0.5267726797610521 sec
Decode latency: 2.5902652591466904 sec
Time for inference 7: 3.12 sec total, 1313.68 tokens/sec
Decode latency: 2.59 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 19718.18 GB/s
FLOPS achieved: 98.59 TF/s

Prefill latency: 0.5263531655073166 sec
Decode latency: 2.5910672526806593 sec
Time for inference 8: 3.12 sec total, 1313.55 tokens/sec
Decode latency: 2.59 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 19716.13 GB/s
FLOPS achieved: 98.58 TF/s

Prefill latency: 0.5283694118261337 sec
Decode latency: 2.589342290535569 sec
Time for inference 9: 3.12 sec total, 1313.46 tokens/sec
Decode latency: 2.59 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 19714.90 GB/s
FLOPS achieved: 98.57 TF/s

Prefill latency: 0.5269624218344688 sec
Decode latency: 2.589825363829732 sec
Time for inference 10: 3.12 sec total, 1313.87 tokens/sec
Decode latency: 2.59 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 19721.05 GB/s
FLOPS achieved: 98.61 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 2.5902 sec
Average prefill latency: 0.5267 sec
Average tokens/sec: 1313.80
Memory used: 35.48 GB
Done. we are killing the process
[rank0]:[W1113 12:04:05.130828498 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
