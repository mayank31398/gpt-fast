W1113 12:00:32.699000 1422244 site-packages/torch/distributed/run.py:793] 
W1113 12:00:32.699000 1422244 site-packages/torch/distributed/run.py:793] *****************************************
W1113 12:00:32.699000 1422244 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1113 12:00:32.699000 1422244 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTDense(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x DenseTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=3072, bias=False)
        (wo): Linear(in_features=2048, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=14336, bias=False)
        (w2): Linear(in_features=7168, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.10 seconds
CUDA_GRAPH are activate
Prefill latency: 0.261332580819726 sec
Decode latency: 2.0718368217349052 sec
Compilation time: 2.32 secondsCompilation time: 2.33 seconds

Prefill latency: 0.22548921033740044 sec
Decode latency: 2.0715165082365274 sec
Prefill latency: 0.22561180777847767 sec
Decode latency: 2.070554992184043 sec
Prefill latency: 0.2257254160940647 sec
Decode latency: 2.0721346363425255 sec
Prefill latency: 0.22586882300674915 sec
Decode latency: 2.0708286948502064 sec
Prefill latency: 0.2254765834659338 sec
Decode latency: 2.0704659447073936 sec
Time for inference 1: 2.30 sec total, 445.87 tokens/sec
Decode latency: 2.07 sec
Prefill latency: 0.23 sec
Bandwidth achieved: 3580.54 GB/s
FLOPS achieved: 17.90 TF/s

Prefill latency: 0.22552071698009968 sec
Decode latency: 2.071852944791317 sec
Time for inference 2: 2.30 sec total, 445.60 tokens/sec
Decode latency: 2.07 sec
Prefill latency: 0.23 sec
Bandwidth achieved: 3578.38 GB/s
FLOPS achieved: 17.89 TF/s

Prefill latency: 0.22528317011892796 sec
Decode latency: 2.0710964389145374 sec
Time for inference 3: 2.30 sec total, 445.79 tokens/sec
Decode latency: 2.07 sec
Prefill latency: 0.23 sec
Bandwidth achieved: 3579.95 GB/s
FLOPS achieved: 17.90 TF/s

Prefill latency: 0.2257616464048624 sec
Decode latency: 2.0705603007227182 sec
Time for inference 4: 2.30 sec total, 445.80 tokens/sec
Decode latency: 2.07 sec
Prefill latency: 0.23 sec
Bandwidth achieved: 3580.02 GB/s
FLOPS achieved: 17.90 TF/s

Prefill latency: 0.22570460848510265 sec
Decode latency: 2.07004339620471 sec
Time for inference 5: 2.30 sec total, 445.91 tokens/sec
Decode latency: 2.07 sec
Prefill latency: 0.23 sec
Bandwidth achieved: 3580.92 GB/s
FLOPS achieved: 17.90 TF/s

Prefill latency: 0.22546694427728653 sec
Decode latency: 2.069982711225748 sec
Time for inference 6: 2.30 sec total, 445.98 tokens/sec
Decode latency: 2.07 sec
Prefill latency: 0.23 sec
Bandwidth achieved: 3581.48 GB/s
FLOPS achieved: 17.91 TF/s

Prefill latency: 0.22525058314204216 sec
Decode latency: 2.0690390169620514 sec
Time for inference 7: 2.29 sec total, 446.22 tokens/sec
Decode latency: 2.07 sec
Prefill latency: 0.23 sec
Bandwidth achieved: 3583.39 GB/s
FLOPS achieved: 17.92 TF/s

Prefill latency: 0.2261920776218176 sec
Decode latency: 2.0701692923903465 sec
Time for inference 8: 2.30 sec total, 445.81 tokens/sec
Decode latency: 2.07 sec
Prefill latency: 0.23 sec
Bandwidth achieved: 3580.06 GB/s
FLOPS achieved: 17.90 TF/s

Prefill latency: 0.22569610737264156 sec
Decode latency: 2.0720114950090647 sec
Time for inference 9: 2.30 sec total, 445.55 tokens/sec
Decode latency: 2.07 sec
Prefill latency: 0.23 sec
Bandwidth achieved: 3578.02 GB/s
FLOPS achieved: 17.89 TF/s

Prefill latency: 0.22555508837103844 sec
Decode latency: 2.071234906092286 sec
Time for inference 10: 2.30 sec total, 445.72 tokens/sec
Decode latency: 2.07 sec
Prefill latency: 0.23 sec
Bandwidth achieved: 3579.33 GB/s
FLOPS achieved: 17.90 TF/s

==========
Batch Size: 4
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 2.0706 sec
Average prefill latency: 0.2256 sec
Average tokens/sec: 445.82
Memory used: 14.71 GB
Done. we are killing the process
[rank0]:[W1113 12:01:17.263762002 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
