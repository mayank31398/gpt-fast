W1113 12:10:32.240000 1453915 site-packages/torch/distributed/run.py:793] 
W1113 12:10:32.240000 1453915 site-packages/torch/distributed/run.py:793] *****************************************
W1113 12:10:32.240000 1453915 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1113 12:10:32.240000 1453915 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTLadder(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x LadderTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=1536, bias=False)
        (wo): Linear(in_features=1024, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 0.58 seconds
CUDA_GRAPH are activate
Prefill latency: 0.19176064059138298 sec
Decode latency: 1.5779747441411018 sec
Compilation time: 1.78 seconds
Compilation time: 1.78 secondsCompilation time: 1.77 seconds

Compilation time: 1.77 seconds
Prefill latency: 0.1876876838505268 sec
Decode latency: 1.5772608574479818 sec
Prefill latency: 0.18739861994981766 sec
Decode latency: 1.5764347147196531 sec
Prefill latency: 0.1883022841066122 sec
Decode latency: 1.5773590747267008 sec
Prefill latency: 0.1883314549922943 sec
Decode latency: 1.5766839683055878 sec
Prefill latency: 0.18771913647651672 sec
Decode latency: 1.5776149481534958 sec
Time for inference 1: 1.77 sec total, 2319.27 tokens/sec
Decode latency: 1.58 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 10531.50 GB/s
FLOPS achieved: 52.66 TF/s

Prefill latency: 0.18837677128612995 sec
Decode latency: 1.5756307486444712 sec
Time for inference 2: 1.76 sec total, 2320.89 tokens/sec
Decode latency: 1.58 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 10538.87 GB/s
FLOPS achieved: 52.69 TF/s

Prefill latency: 0.18812944367527962 sec
Decode latency: 1.5761826653033495 sec
Time for inference 3: 1.77 sec total, 2320.65 tokens/sec
Decode latency: 1.58 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 10537.76 GB/s
FLOPS achieved: 52.69 TF/s

Prefill latency: 0.18811911344528198 sec
Decode latency: 1.5765846390277147 sec
Time for inference 4: 1.77 sec total, 2320.08 tokens/sec
Decode latency: 1.58 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 10535.19 GB/s
FLOPS achieved: 52.68 TF/s

Prefill latency: 0.18753126077353954 sec
Decode latency: 1.5771775543689728 sec
Time for inference 5: 1.77 sec total, 2320.14 tokens/sec
Decode latency: 1.58 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 10535.46 GB/s
FLOPS achieved: 52.68 TF/s

Prefill latency: 0.18759119883179665 sec
Decode latency: 1.5785867664963007 sec
Time for inference 6: 1.77 sec total, 2318.21 tokens/sec
Decode latency: 1.58 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 10526.70 GB/s
FLOPS achieved: 52.63 TF/s

Prefill latency: 0.1888342034071684 sec
Decode latency: 1.5755919851362705 sec
Time for inference 7: 1.77 sec total, 2320.36 tokens/sec
Decode latency: 1.58 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 10536.47 GB/s
FLOPS achieved: 52.68 TF/s

Prefill latency: 0.18798145093023777 sec
Decode latency: 1.576265163719654 sec
Time for inference 8: 1.77 sec total, 2320.66 tokens/sec
Decode latency: 1.58 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 10537.81 GB/s
FLOPS achieved: 52.69 TF/s

Prefill latency: 0.18859469704329967 sec
Decode latency: 1.5765223186463118 sec
Time for inference 9: 1.77 sec total, 2319.75 tokens/sec
Decode latency: 1.58 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 10533.67 GB/s
FLOPS achieved: 52.67 TF/s

Prefill latency: 0.18791812472045422 sec
Decode latency: 1.5760233644396067 sec
Time for inference 10: 1.76 sec total, 2321.22 tokens/sec
Decode latency: 1.58 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 10540.35 GB/s
FLOPS achieved: 52.70 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 1.5766 sec
Average prefill latency: 0.1881 sec
Average tokens/sec: 2320.12
Memory used: 23.45 GB
Done. we are killing the process
[rank0]:[W1113 12:11:16.437750692 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
