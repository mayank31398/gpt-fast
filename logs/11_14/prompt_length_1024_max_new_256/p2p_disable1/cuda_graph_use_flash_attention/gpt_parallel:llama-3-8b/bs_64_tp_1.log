flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTParallel(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x ParallelTransformerBlock(
      semi_compiled = False
      (attention): FuseAttentionMLP(
        (wqkv1): Linear(in_features=4096, out_features=34816, bias=False)
        (wo): Linear(in_features=4096, out_features=4096, bias=False)
        (w2): Linear(in_features=14336, out_features=4096, bias=False)
      )
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.52 seconds
CUDA_GRAPH are activate
Prefill latency: 2.1670456137508154 sec
Decode latency: 3.391610100865364 sec
Compilation time: 5.56 seconds
Prefill latency: 2.158658228814602 sec
Decode latency: 3.3909695483744144 sec
Prefill latency: 2.1654153745621443 sec
Decode latency: 3.3908732160925865 sec
Prefill latency: 2.173461765050888 sec
Decode latency: 3.391992464661598 sec
Prefill latency: 2.1678456906229258 sec
Decode latency: 3.391316495835781 sec
Prefill latency: 2.172638328745961 sec
Decode latency: 3.391085498034954 sec
Time for inference 1: 5.56 sec total, 2944.35 tokens/sec
Decode latency: 3.39 sec
Prefill latency: 2.17 sec
Bandwidth achieved: 44193.49 GB/s
FLOPS achieved: 220.97 TF/s

Prefill latency: 2.1739972606301308 sec
Decode latency: 3.391063991934061 sec
Time for inference 2: 5.57 sec total, 2943.70 tokens/sec
Decode latency: 3.39 sec
Prefill latency: 2.17 sec
Bandwidth achieved: 44183.79 GB/s
FLOPS achieved: 220.92 TF/s

Prefill latency: 2.1743632070720196 sec
Decode latency: 3.386986695230007 sec
Time for inference 3: 5.56 sec total, 2945.63 tokens/sec
Decode latency: 3.39 sec
Prefill latency: 2.17 sec
Bandwidth achieved: 44212.73 GB/s
FLOPS achieved: 221.06 TF/s

Prefill latency: 2.176240472123027 sec
Decode latency: 3.3911992013454437 sec
Time for inference 4: 5.57 sec total, 2942.45 tokens/sec
Decode latency: 3.39 sec
Prefill latency: 2.18 sec
Bandwidth achieved: 44164.89 GB/s
FLOPS achieved: 220.82 TF/s

Prefill latency: 2.168690897524357 sec
Decode latency: 3.3910248689353466 sec
Time for inference 5: 5.56 sec total, 2946.49 tokens/sec
Decode latency: 3.39 sec
Prefill latency: 2.17 sec
Bandwidth achieved: 44225.67 GB/s
FLOPS achieved: 221.13 TF/s

Prefill latency: 2.1713686659932137 sec
Decode latency: 3.391084374859929 sec
Time for inference 6: 5.56 sec total, 2945.01 tokens/sec
Decode latency: 3.39 sec
Prefill latency: 2.17 sec
Bandwidth achieved: 44203.45 GB/s
FLOPS achieved: 221.02 TF/s

Prefill latency: 2.1759919729083776 sec
Decode latency: 3.390732765197754 sec
Time for inference 7: 5.57 sec total, 2942.75 tokens/sec
Decode latency: 3.39 sec
Prefill latency: 2.18 sec
Bandwidth achieved: 44169.47 GB/s
FLOPS achieved: 220.85 TF/s

Prefill latency: 2.173590099439025 sec
Decode latency: 3.39154464378953 sec
Time for inference 8: 5.57 sec total, 2943.66 tokens/sec
Decode latency: 3.39 sec
Prefill latency: 2.17 sec
Bandwidth achieved: 44183.14 GB/s
FLOPS achieved: 220.92 TF/s

Prefill latency: 2.1710763014853 sec
Decode latency: 3.3596563301980495 sec
Time for inference 9: 5.53 sec total, 2961.88 tokens/sec
Decode latency: 3.36 sec
Prefill latency: 2.17 sec
Bandwidth achieved: 44456.60 GB/s
FLOPS achieved: 222.28 TF/s

Prefill latency: 2.1741700619459152 sec
Decode latency: 3.3926369920372963 sec
Time for inference 10: 5.57 sec total, 2942.73 tokens/sec
Decode latency: 3.39 sec
Prefill latency: 2.17 sec
Bandwidth achieved: 44169.12 GB/s
FLOPS achieved: 220.85 TF/s

==========
Batch Size: 64
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 3.3877 sec
Average prefill latency: 2.1732 sec
Average tokens/sec: 2945.87
Memory used: 74.46 GB
Done. we are killing the process
[rank0]:[W1113 13:09:41.699364640 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
