flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTParallel(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x ParallelTransformerBlock(
      semi_compiled = False
      (attention): FuseAttentionMLP(
        (wqkv1): Linear(in_features=4096, out_features=34816, bias=False)
        (wo): Linear(in_features=4096, out_features=4096, bias=False)
        (w2): Linear(in_features=14336, out_features=4096, bias=False)
      )
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.12 seconds
CUDA_GRAPH are activate
Prefill latency: 0.12935595214366913 sec
Decode latency: 2.3527876995503902 sec
Compilation time: 2.58 seconds
Prefill latency: 0.12948053516447544 sec
Decode latency: 2.3519332222640514 sec
Prefill latency: 0.1298471186310053 sec
Decode latency: 2.352463884279132 sec
Prefill latency: 0.12858041562139988 sec
Decode latency: 2.351888319477439 sec
Prefill latency: 0.1294251885265112 sec
Decode latency: 2.3520418722182512 sec
Prefill latency: 0.12818754836916924 sec
Decode latency: 2.352054264396429 sec
Time for inference 1: 2.48 sec total, 412.76 tokens/sec
Decode latency: 2.35 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6195.39 GB/s
FLOPS achieved: 30.98 TF/s

Prefill latency: 0.13065537624061108 sec
Decode latency: 2.352460725232959 sec
Time for inference 2: 2.48 sec total, 412.28 tokens/sec
Decode latency: 2.35 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6188.20 GB/s
FLOPS achieved: 30.94 TF/s

Prefill latency: 0.13049869798123837 sec
Decode latency: 2.35213241353631 sec
Time for inference 3: 2.48 sec total, 412.37 tokens/sec
Decode latency: 2.35 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6189.47 GB/s
FLOPS achieved: 30.95 TF/s

Prefill latency: 0.1293351948261261 sec
Decode latency: 2.351489359512925 sec
Time for inference 4: 2.48 sec total, 412.68 tokens/sec
Decode latency: 2.35 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6194.08 GB/s
FLOPS achieved: 30.97 TF/s

Prefill latency: 0.1303469743579626 sec
Decode latency: 2.3520950321108103 sec
Time for inference 5: 2.48 sec total, 412.41 tokens/sec
Decode latency: 2.35 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6190.10 GB/s
FLOPS achieved: 30.95 TF/s

Prefill latency: 0.12990242801606655 sec
Decode latency: 2.352414460852742 sec
Time for inference 6: 2.48 sec total, 412.43 tokens/sec
Decode latency: 2.35 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6190.38 GB/s
FLOPS achieved: 30.95 TF/s

Prefill latency: 0.12933492846786976 sec
Decode latency: 2.352756045758724 sec
Time for inference 7: 2.48 sec total, 412.46 tokens/sec
Decode latency: 2.35 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6190.86 GB/s
FLOPS achieved: 30.95 TF/s

Prefill latency: 0.12977891974151134 sec
Decode latency: 2.352158458903432 sec
Time for inference 8: 2.48 sec total, 412.49 tokens/sec
Decode latency: 2.35 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6191.27 GB/s
FLOPS achieved: 30.96 TF/s

Prefill latency: 0.13171454146504402 sec
Decode latency: 2.3519412260502577 sec
Time for inference 9: 2.48 sec total, 412.19 tokens/sec
Decode latency: 2.35 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6186.79 GB/s
FLOPS achieved: 30.93 TF/s

Prefill latency: 0.1292790323495865 sec
Decode latency: 2.3530014157295227 sec
Time for inference 10: 2.48 sec total, 412.39 tokens/sec
Decode latency: 2.35 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6189.87 GB/s
FLOPS achieved: 30.95 TF/s

==========
Batch Size: 4
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 2.3523 sec
Average prefill latency: 0.1299 sec
Average tokens/sec: 412.45
Memory used: 19.85 GB
Done. we are killing the process
[rank0]:[W1113 12:24:32.416136027 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
