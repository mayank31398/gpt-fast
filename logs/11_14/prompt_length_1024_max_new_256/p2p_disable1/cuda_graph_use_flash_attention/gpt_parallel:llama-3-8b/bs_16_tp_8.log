W1113 12:28:51.547000 1518388 site-packages/torch/distributed/run.py:793] 
W1113 12:28:51.547000 1518388 site-packages/torch/distributed/run.py:793] *****************************************
W1113 12:28:51.547000 1518388 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1113 12:28:51.547000 1518388 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTParallel(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x ParallelTransformerBlock(
      semi_compiled = False
      (attention): FuseAttentionMLP(
        (wqkv1): Linear(in_features=4096, out_features=4352, bias=False)
        (wo): Linear(in_features=512, out_features=4096, bias=False)
        (w2): Linear(in_features=1792, out_features=4096, bias=False)
      )
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.57 seconds
CUDA_GRAPH are activate
Prefill latency: 0.13492068834602833 sec
Decode latency: 1.7844794634729624 sec
Compilation time: 1.92 seconds
Compilation time: 1.94 seconds
Compilation time: 1.92 seconds
Compilation time: 1.92 seconds
Compilation time: 1.91 seconds
Compilation time: 1.92 secondsCompilation time: 1.95 seconds

Compilation time: 1.91 seconds
Prefill latency: 0.1268683820962906 sec
Decode latency: 1.7729231603443623 sec
Prefill latency: 0.1267684679478407 sec
Decode latency: 1.7648010402917862 sec
Prefill latency: 0.12405074201524258 sec
Decode latency: 1.8275810964405537 sec
Prefill latency: 0.12350917793810368 sec
Decode latency: 1.774908160790801 sec
Prefill latency: 0.12473279051482677 sec
Decode latency: 1.7832988742738962 sec
Time for inference 1: 1.91 sec total, 2145.34 tokens/sec
Decode latency: 1.78 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 5997.89 GB/s
FLOPS achieved: 29.99 TF/s

Prefill latency: 0.12347191944718361 sec
Decode latency: 1.7821840066462755 sec
Time for inference 2: 1.91 sec total, 2148.15 tokens/sec
Decode latency: 1.78 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 6005.74 GB/s
FLOPS achieved: 30.03 TF/s

Prefill latency: 0.12354964762926102 sec
Decode latency: 1.7891224157065153 sec
Time for inference 3: 1.91 sec total, 2140.62 tokens/sec
Decode latency: 1.79 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 5984.70 GB/s
FLOPS achieved: 29.92 TF/s

Prefill latency: 0.12356438674032688 sec
Decode latency: 1.7995038591325283 sec
Time for inference 4: 1.92 sec total, 2129.04 tokens/sec
Decode latency: 1.80 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 5952.32 GB/s
FLOPS achieved: 29.76 TF/s

Prefill latency: 0.12353027798235416 sec
Decode latency: 1.8253840412944555 sec
Time for inference 5: 1.95 sec total, 2100.56 tokens/sec
Decode latency: 1.83 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 5872.70 GB/s
FLOPS achieved: 29.36 TF/s

Prefill latency: 0.12435238994657993 sec
Decode latency: 1.7839818932116032 sec
Time for inference 6: 1.91 sec total, 2145.45 tokens/sec
Decode latency: 1.78 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 5998.20 GB/s
FLOPS achieved: 29.99 TF/s

Prefill latency: 0.1236353050917387 sec
Decode latency: 1.7713007144629955 sec
Time for inference 7: 1.90 sec total, 2160.35 tokens/sec
Decode latency: 1.77 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 6039.84 GB/s
FLOPS achieved: 30.20 TF/s

Prefill latency: 0.12382069788873196 sec
Decode latency: 1.786741005256772 sec
Time for inference 8: 1.91 sec total, 2142.84 tokens/sec
Decode latency: 1.79 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 5990.89 GB/s
FLOPS achieved: 29.95 TF/s

Prefill latency: 0.12472512386739254 sec
Decode latency: 1.7768244836479425 sec
Time for inference 9: 1.90 sec total, 2153.22 tokens/sec
Decode latency: 1.78 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 6019.93 GB/s
FLOPS achieved: 30.10 TF/s

Prefill latency: 0.12600867077708244 sec
Decode latency: 1.8307048883289099 sec
Time for inference 10: 1.96 sec total, 2092.49 tokens/sec
Decode latency: 1.83 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 5850.13 GB/s
FLOPS achieved: 29.25 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 1.7929 sec
Average prefill latency: 0.1241 sec
Average tokens/sec: 2135.81
Memory used: 16.06 GB
Done. we are killing the process
[rank0]:[W1113 12:29:43.829164275 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
