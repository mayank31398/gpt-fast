W1113 12:28:03.742000 1514899 site-packages/torch/distributed/run.py:793] 
W1113 12:28:03.742000 1514899 site-packages/torch/distributed/run.py:793] *****************************************
W1113 12:28:03.742000 1514899 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1113 12:28:03.742000 1514899 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTParallel(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x ParallelTransformerBlock(
      semi_compiled = False
      (attention): FuseAttentionMLP(
        (wqkv1): Linear(in_features=4096, out_features=8704, bias=False)
        (wo): Linear(in_features=1024, out_features=4096, bias=False)
        (w2): Linear(in_features=3584, out_features=4096, bias=False)
      )
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.36 seconds
CUDA_GRAPH are activate
Prefill latency: 0.2010780293494463 sec
Decode latency: 1.674425220116973 sec
Compilation time: 1.85 seconds
Compilation time: 1.88 secondsCompilation time: 1.88 seconds

Compilation time: 1.88 seconds
Prefill latency: 0.17653639614582062 sec
Decode latency: 1.673939736559987 sec
Prefill latency: 0.17678521573543549 sec
Decode latency: 1.6738400273025036 sec
Prefill latency: 0.17663557641208172 sec
Decode latency: 1.6740622110664845 sec
Prefill latency: 0.1765911765396595 sec
Decode latency: 1.674867456778884 sec
Prefill latency: 0.17706025950610638 sec
Decode latency: 1.6737144403159618 sec
Time for inference 1: 1.85 sec total, 2212.30 tokens/sec
Decode latency: 1.67 sec
Prefill latency: 0.18 sec
Bandwidth achieved: 10045.17 GB/s
FLOPS achieved: 50.23 TF/s

Prefill latency: 0.17655064910650253 sec
Decode latency: 1.674678297713399 sec
Time for inference 2: 1.85 sec total, 2211.48 tokens/sec
Decode latency: 1.67 sec
Prefill latency: 0.18 sec
Bandwidth achieved: 10041.48 GB/s
FLOPS achieved: 50.21 TF/s

Prefill latency: 0.17681579664349556 sec
Decode latency: 1.6737608965486288 sec
Time for inference 3: 1.85 sec total, 2212.27 tokens/sec
Decode latency: 1.67 sec
Prefill latency: 0.18 sec
Bandwidth achieved: 10045.05 GB/s
FLOPS achieved: 50.23 TF/s

Prefill latency: 0.176877211779356 sec
Decode latency: 1.673660546541214 sec
Time for inference 4: 1.85 sec total, 2212.40 tokens/sec
Decode latency: 1.67 sec
Prefill latency: 0.18 sec
Bandwidth achieved: 10045.64 GB/s
FLOPS achieved: 50.23 TF/s

Prefill latency: 0.17676829919219017 sec
Decode latency: 1.673954887315631 sec
Time for inference 5: 1.85 sec total, 2212.19 tokens/sec
Decode latency: 1.67 sec
Prefill latency: 0.18 sec
Bandwidth achieved: 10044.67 GB/s
FLOPS achieved: 50.22 TF/s

Prefill latency: 0.17647729814052582 sec
Decode latency: 1.6738304775208235 sec
Time for inference 6: 1.85 sec total, 2212.76 tokens/sec
Decode latency: 1.67 sec
Prefill latency: 0.18 sec
Bandwidth achieved: 10047.25 GB/s
FLOPS achieved: 50.24 TF/s

Prefill latency: 0.1766213085502386 sec
Decode latency: 1.6737323813140392 sec
Time for inference 7: 1.85 sec total, 2212.75 tokens/sec
Decode latency: 1.67 sec
Prefill latency: 0.18 sec
Bandwidth achieved: 10047.21 GB/s
FLOPS achieved: 50.24 TF/s

Prefill latency: 0.17671903036534786 sec
Decode latency: 1.6734557524323463 sec
Time for inference 8: 1.85 sec total, 2212.98 tokens/sec
Decode latency: 1.67 sec
Prefill latency: 0.18 sec
Bandwidth achieved: 10048.28 GB/s
FLOPS achieved: 50.24 TF/s

Prefill latency: 0.17665481194853783 sec
Decode latency: 1.6755342539399862 sec
Time for inference 9: 1.85 sec total, 2210.60 tokens/sec
Decode latency: 1.68 sec
Prefill latency: 0.18 sec
Bandwidth achieved: 10037.46 GB/s
FLOPS achieved: 50.19 TF/s

Prefill latency: 0.1767805591225624 sec
Decode latency: 1.6736276093870401 sec
Time for inference 10: 1.85 sec total, 2212.74 tokens/sec
Decode latency: 1.67 sec
Prefill latency: 0.18 sec
Bandwidth achieved: 10047.16 GB/s
FLOPS achieved: 50.24 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 1.6740 sec
Average prefill latency: 0.1767 sec
Average tokens/sec: 2212.25
Memory used: 19.21 GB
Done. we are killing the process
[rank0]:[W1113 12:28:47.744249648 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
