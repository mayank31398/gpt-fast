flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTParallel(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x ParallelTransformerBlock(
      semi_compiled = False
      (attention): FuseAttentionMLP(
        (wqkv1): Linear(in_features=4096, out_features=34816, bias=False)
        (wo): Linear(in_features=4096, out_features=4096, bias=False)
        (w2): Linear(in_features=14336, out_features=4096, bias=False)
      )
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.13 seconds
CUDA_GRAPH are activate
Prefill latency: 0.5172943640500307 sec
Decode latency: 2.5639459900557995 sec
Compilation time: 3.08 seconds
Prefill latency: 0.5185535307973623 sec
Decode latency: 2.5630387235432863 sec
Prefill latency: 0.5150042325258255 sec
Decode latency: 2.563163598999381 sec
Prefill latency: 0.518138075247407 sec
Decode latency: 2.563248671591282 sec
Prefill latency: 0.5170742683112621 sec
Decode latency: 2.5628765150904655 sec
Prefill latency: 0.5189422890543938 sec
Decode latency: 2.563385171815753 sec
Time for inference 1: 3.08 sec total, 1328.55 tokens/sec
Decode latency: 2.56 sec
Prefill latency: 0.52 sec
Bandwidth achieved: 19940.96 GB/s
FLOPS achieved: 99.70 TF/s

Prefill latency: 0.5194271113723516 sec
Decode latency: 2.5631868597120047 sec
Time for inference 2: 3.08 sec total, 1328.44 tokens/sec
Decode latency: 2.56 sec
Prefill latency: 0.52 sec
Bandwidth achieved: 19939.36 GB/s
FLOPS achieved: 99.70 TF/s

Prefill latency: 0.5188626646995544 sec
Decode latency: 2.5630854703485966 sec
Time for inference 3: 3.08 sec total, 1328.71 tokens/sec
Decode latency: 2.56 sec
Prefill latency: 0.52 sec
Bandwidth achieved: 19943.38 GB/s
FLOPS achieved: 99.72 TF/s

Prefill latency: 0.5168473403900862 sec
Decode latency: 2.562233906239271 sec
Time for inference 4: 3.08 sec total, 1329.90 tokens/sec
Decode latency: 2.56 sec
Prefill latency: 0.52 sec
Bandwidth achieved: 19961.26 GB/s
FLOPS achieved: 99.81 TF/s

Prefill latency: 0.5172936003655195 sec
Decode latency: 2.562711503356695 sec
Time for inference 5: 3.08 sec total, 1329.54 tokens/sec
Decode latency: 2.56 sec
Prefill latency: 0.52 sec
Bandwidth achieved: 19955.86 GB/s
FLOPS achieved: 99.78 TF/s

Prefill latency: 0.5179373156279325 sec
Decode latency: 2.5630717612802982 sec
Time for inference 6: 3.08 sec total, 1329.09 tokens/sec
Decode latency: 2.56 sec
Prefill latency: 0.52 sec
Bandwidth achieved: 19949.10 GB/s
FLOPS achieved: 99.75 TF/s

Prefill latency: 0.5190249010920525 sec
Decode latency: 2.5629320964217186 sec
Time for inference 7: 3.08 sec total, 1328.72 tokens/sec
Decode latency: 2.56 sec
Prefill latency: 0.52 sec
Bandwidth achieved: 19943.53 GB/s
FLOPS achieved: 99.72 TF/s

Prefill latency: 0.520278237760067 sec
Decode latency: 2.5632878951728344 sec
Time for inference 8: 3.08 sec total, 1328.02 tokens/sec
Decode latency: 2.56 sec
Prefill latency: 0.52 sec
Bandwidth achieved: 19933.08 GB/s
FLOPS achieved: 99.67 TF/s

Prefill latency: 0.5203484054654837 sec
Decode latency: 2.5636661928147078 sec
Time for inference 9: 3.08 sec total, 1327.81 tokens/sec
Decode latency: 2.56 sec
Prefill latency: 0.52 sec
Bandwidth achieved: 19929.82 GB/s
FLOPS achieved: 99.65 TF/s

Prefill latency: 0.5188394729048014 sec
Decode latency: 2.5633866619318724 sec
Time for inference 10: 3.08 sec total, 1328.61 tokens/sec
Decode latency: 2.56 sec
Prefill latency: 0.52 sec
Bandwidth achieved: 19941.87 GB/s
FLOPS achieved: 99.71 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 2.5631 sec
Average prefill latency: 0.5188 sec
Average tokens/sec: 1328.74
Memory used: 30.77 GB
Done. we are killing the process
[rank0]:[W1113 12:27:46.109937378 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
