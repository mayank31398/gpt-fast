W1113 12:13:02.215000 1456589 site-packages/torch/distributed/run.py:793] 
W1113 12:13:02.215000 1456589 site-packages/torch/distributed/run.py:793] *****************************************
W1113 12:13:02.215000 1456589 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1113 12:13:02.215000 1456589 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=3072, bias=False)
        (wo): Linear(in_features=2048, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=14336, bias=False)
        (w2): Linear(in_features=7168, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 0.98 seconds
CUDA_GRAPH are activate
Prefill latency: 0.15346966311335564 sec
Decode latency: 1.8701470475643873 sec
Compilation time: 2.16 seconds
Compilation time: 2.14 seconds
Prefill latency: 0.14741365984082222 sec
Decode latency: 1.8673443533480167 sec
Prefill latency: 0.14733713120222092 sec
Decode latency: 1.8682525679469109 sec
Prefill latency: 0.14766577631235123 sec
Decode latency: 1.8676246087998152 sec
Prefill latency: 0.1478368639945984 sec
Decode latency: 1.868072560057044 sec
Prefill latency: 0.1476213075220585 sec
Decode latency: 1.867246337234974 sec
Time for inference 1: 2.02 sec total, 508.06 tokens/sec
Decode latency: 1.87 sec
Prefill latency: 0.15 sec
Bandwidth achieved: 4080.00 GB/s
FLOPS achieved: 20.40 TF/s

Prefill latency: 0.14718881621956825 sec
Decode latency: 1.8689411040395498 sec
Time for inference 2: 2.02 sec total, 507.67 tokens/sec
Decode latency: 1.87 sec
Prefill latency: 0.15 sec
Bandwidth achieved: 4076.89 GB/s
FLOPS achieved: 20.38 TF/s

Prefill latency: 0.14760334976017475 sec
Decode latency: 1.8675686530768871 sec
Time for inference 3: 2.02 sec total, 507.93 tokens/sec
Decode latency: 1.87 sec
Prefill latency: 0.15 sec
Bandwidth achieved: 4078.91 GB/s
FLOPS achieved: 20.39 TF/s

Prefill latency: 0.14806637540459633 sec
Decode latency: 1.867878733202815 sec
Time for inference 4: 2.02 sec total, 507.76 tokens/sec
Decode latency: 1.87 sec
Prefill latency: 0.15 sec
Bandwidth achieved: 4077.55 GB/s
FLOPS achieved: 20.39 TF/s

Prefill latency: 0.14763209968805313 sec
Decode latency: 1.8678300436586142 sec
Time for inference 5: 2.02 sec total, 507.84 tokens/sec
Decode latency: 1.87 sec
Prefill latency: 0.15 sec
Bandwidth achieved: 4078.23 GB/s
FLOPS achieved: 20.39 TF/s

Prefill latency: 0.14746983908116817 sec
Decode latency: 1.868051027879119 sec
Time for inference 6: 2.02 sec total, 507.87 tokens/sec
Decode latency: 1.87 sec
Prefill latency: 0.15 sec
Bandwidth achieved: 4078.46 GB/s
FLOPS achieved: 20.39 TF/s

Prefill latency: 0.14761727675795555 sec
Decode latency: 1.8671545125544071 sec
Time for inference 7: 2.02 sec total, 508.07 tokens/sec
Decode latency: 1.87 sec
Prefill latency: 0.15 sec
Bandwidth achieved: 4080.09 GB/s
FLOPS achieved: 20.40 TF/s

Prefill latency: 0.1478727962821722 sec
Decode latency: 1.8678040318191051 sec
Time for inference 8: 2.02 sec total, 507.86 tokens/sec
Decode latency: 1.87 sec
Prefill latency: 0.15 sec
Bandwidth achieved: 4078.38 GB/s
FLOPS achieved: 20.39 TF/s

Prefill latency: 0.14761155657470226 sec
Decode latency: 1.867580795660615 sec
Time for inference 9: 2.02 sec total, 507.94 tokens/sec
Decode latency: 1.87 sec
Prefill latency: 0.15 sec
Bandwidth achieved: 4078.99 GB/s
FLOPS achieved: 20.39 TF/s

Prefill latency: 0.14731615595519543 sec
Decode latency: 1.8680554125458002 sec
Time for inference 10: 2.02 sec total, 507.93 tokens/sec
Decode latency: 1.87 sec
Prefill latency: 0.15 sec
Bandwidth achieved: 4078.93 GB/s
FLOPS achieved: 20.39 TF/s

==========
Batch Size: 4
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 1.8678 sec
Average prefill latency: 0.1476 sec
Average tokens/sec: 507.89
Memory used: 12.83 GB
Done. we are killing the process
[rank0]:[W1113 12:13:42.572497282 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
