W1113 12:16:33.202000 1469610 site-packages/torch/distributed/run.py:793] 
W1113 12:16:33.202000 1469610 site-packages/torch/distributed/run.py:793] *****************************************
W1113 12:16:33.202000 1469610 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1113 12:16:33.202000 1469610 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=1536, bias=False)
        (wo): Linear(in_features=1024, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.42 seconds
CUDA_GRAPH are activate
Prefill latency: 0.20197254791855812 sec
Decode latency: 1.783553458750248 sec
Compilation time: 1.98 secondsCompilation time: 1.97 seconds

Compilation time: 1.97 seconds
Compilation time: 1.99 seconds
Prefill latency: 0.1897884514182806 sec
Decode latency: 1.7548486087471247 sec
Prefill latency: 0.1900492887943983 sec
Decode latency: 1.7546426355838776 sec
Prefill latency: 0.19000512920320034 sec
Decode latency: 1.7550865486264229 sec
Prefill latency: 0.1894952692091465 sec
Decode latency: 1.7544846404343843 sec
Prefill latency: 0.18997860699892044 sec
Decode latency: 1.755300523713231 sec
Time for inference 1: 1.95 sec total, 2104.71 tokens/sec
Decode latency: 1.76 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 9557.20 GB/s
FLOPS achieved: 47.79 TF/s

Prefill latency: 0.18985782004892826 sec
Decode latency: 1.7556622568517923 sec
Time for inference 2: 1.95 sec total, 2104.52 tokens/sec
Decode latency: 1.76 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 9556.34 GB/s
FLOPS achieved: 47.78 TF/s

Prefill latency: 0.18998616375029087 sec
Decode latency: 1.7551323845982552 sec
Time for inference 3: 1.95 sec total, 2105.00 tokens/sec
Decode latency: 1.76 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 9558.53 GB/s
FLOPS achieved: 47.79 TF/s

Prefill latency: 0.1896562445908785 sec
Decode latency: 1.7546407785266638 sec
Time for inference 4: 1.95 sec total, 2105.80 tokens/sec
Decode latency: 1.75 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 9562.14 GB/s
FLOPS achieved: 47.81 TF/s

Prefill latency: 0.1900724396109581 sec
Decode latency: 1.7551857326179743 sec
Time for inference 5: 1.95 sec total, 2104.86 tokens/sec
Decode latency: 1.76 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 9557.90 GB/s
FLOPS achieved: 47.79 TF/s

Prefill latency: 0.18983044475317 sec
Decode latency: 1.7553074806928635 sec
Time for inference 6: 1.95 sec total, 2104.96 tokens/sec
Decode latency: 1.76 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 9558.35 GB/s
FLOPS achieved: 47.79 TF/s

Prefill latency: 0.18952670320868492 sec
Decode latency: 1.7548692896962166 sec
Time for inference 7: 1.95 sec total, 2105.84 tokens/sec
Decode latency: 1.75 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 9562.36 GB/s
FLOPS achieved: 47.81 TF/s

Prefill latency: 0.1896187886595726 sec
Decode latency: 1.7557546459138393 sec
Time for inference 8: 1.95 sec total, 2104.86 tokens/sec
Decode latency: 1.76 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 9557.91 GB/s
FLOPS achieved: 47.79 TF/s

Prefill latency: 0.19035335816442966 sec
Decode latency: 1.7547544315457344 sec
Time for inference 9: 1.95 sec total, 2105.05 tokens/sec
Decode latency: 1.75 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 9558.74 GB/s
FLOPS achieved: 47.79 TF/s

Prefill latency: 0.1895588394254446 sec
Decode latency: 1.755499105900526 sec
Time for inference 10: 1.95 sec total, 2105.00 tokens/sec
Decode latency: 1.76 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 9558.54 GB/s
FLOPS achieved: 47.79 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 1.7552 sec
Average prefill latency: 0.1898 sec
Average tokens/sec: 2105.06
Memory used: 19.01 GB
Done. we are killing the process
[rank0]:[W1113 12:17:18.693417153 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
