flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
        (wo): Linear(in_features=4096, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=28672, bias=False)
        (w2): Linear(in_features=14336, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.04 seconds
CUDA_GRAPH are activate
Prefill latency: 0.13178294897079468 sec
Decode latency: 2.3507472816854715 sec
Compilation time: 2.49 seconds
Prefill latency: 0.13125755079090595 sec
Decode latency: 2.348012177273631 sec
Prefill latency: 0.13195541314780712 sec
Decode latency: 2.3479286432266235 sec
Prefill latency: 0.13264594785869122 sec
Decode latency: 2.349721448495984 sec
Prefill latency: 0.12992331385612488 sec
Decode latency: 2.3481294345110655 sec
Prefill latency: 0.1313456892967224 sec
Decode latency: 2.3489225301891565 sec
Time for inference 1: 2.48 sec total, 412.75 tokens/sec
Decode latency: 2.35 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6195.36 GB/s
FLOPS achieved: 30.98 TF/s

Prefill latency: 0.13276277482509613 sec
Decode latency: 2.347483716905117 sec
Time for inference 2: 2.48 sec total, 412.76 tokens/sec
Decode latency: 2.35 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6195.44 GB/s
FLOPS achieved: 30.98 TF/s

Prefill latency: 0.13131986930966377 sec
Decode latency: 2.3473349139094353 sec
Time for inference 3: 2.48 sec total, 413.02 tokens/sec
Decode latency: 2.35 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6199.39 GB/s
FLOPS achieved: 31.00 TF/s

Prefill latency: 0.1319236047565937 sec
Decode latency: 2.348904959857464 sec
Time for inference 4: 2.48 sec total, 412.65 tokens/sec
Decode latency: 2.35 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6193.78 GB/s
FLOPS achieved: 30.97 TF/s

Prefill latency: 0.13229306787252426 sec
Decode latency: 2.3483386673033237 sec
Time for inference 5: 2.48 sec total, 412.69 tokens/sec
Decode latency: 2.35 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6194.43 GB/s
FLOPS achieved: 30.97 TF/s

Prefill latency: 0.13071150332689285 sec
Decode latency: 2.3472933154553175 sec
Time for inference 6: 2.48 sec total, 413.14 tokens/sec
Decode latency: 2.35 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6201.20 GB/s
FLOPS achieved: 31.01 TF/s

Prefill latency: 0.13133849203586578 sec
Decode latency: 2.348583724349737 sec
Time for inference 7: 2.48 sec total, 412.82 tokens/sec
Decode latency: 2.35 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6196.36 GB/s
FLOPS achieved: 30.98 TF/s

Prefill latency: 0.13081539422273636 sec
Decode latency: 2.3471907805651426 sec
Time for inference 8: 2.48 sec total, 413.15 tokens/sec
Decode latency: 2.35 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6201.25 GB/s
FLOPS achieved: 31.01 TF/s

Prefill latency: 0.1301863957196474 sec
Decode latency: 2.3463935796171427 sec
Time for inference 9: 2.48 sec total, 413.34 tokens/sec
Decode latency: 2.35 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6204.22 GB/s
FLOPS achieved: 31.02 TF/s

Prefill latency: 0.13136282935738564 sec
Decode latency: 2.3479394670575857 sec
Time for inference 10: 2.48 sec total, 412.90 tokens/sec
Decode latency: 2.35 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6197.63 GB/s
FLOPS achieved: 30.99 TF/s

==========
Batch Size: 4
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 2.3478 sec
Average prefill latency: 0.1314 sec
Average tokens/sec: 412.92
Memory used: 19.73 GB
Done. we are killing the process
[rank0]:[W1113 12:12:59.248870170 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
