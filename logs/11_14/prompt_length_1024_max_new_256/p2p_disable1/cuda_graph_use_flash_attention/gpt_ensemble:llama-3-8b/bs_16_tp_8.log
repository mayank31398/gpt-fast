W1113 12:17:22.268000 1473108 site-packages/torch/distributed/run.py:793] 
W1113 12:17:22.268000 1473108 site-packages/torch/distributed/run.py:793] *****************************************
W1113 12:17:22.268000 1473108 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1113 12:17:22.268000 1473108 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=768, bias=False)
        (wo): Linear(in_features=512, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=3584, bias=False)
        (w2): Linear(in_features=1792, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.38 seconds
CUDA_GRAPH are activate
Prefill latency: 0.17385012842714787 sec
Decode latency: 1.8684864677488804 sec
Compilation time: 2.02 seconds
Compilation time: 2.00 seconds
Compilation time: 2.05 secondsCompilation time: 2.02 seconds
Compilation time: 2.04 seconds
Compilation time: 2.03 seconds

Compilation time: 2.03 seconds
Compilation time: 2.04 seconds
Prefill latency: 0.12987378425896168 sec
Decode latency: 1.9340355042368174 sec
Prefill latency: 0.12976440973579884 sec
Decode latency: 1.8989117741584778 sec
Prefill latency: 0.12913577444851398 sec
Decode latency: 1.8972894009202719 sec
Prefill latency: 0.12936453521251678 sec
Decode latency: 1.8683144580572844 sec
Prefill latency: 0.12966502830386162 sec
Decode latency: 1.865023074671626 sec
Time for inference 1: 2.00 sec total, 2052.33 tokens/sec
Decode latency: 1.87 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 5738.38 GB/s
FLOPS achieved: 28.69 TF/s

Prefill latency: 0.13081311993300915 sec
Decode latency: 1.8684514537453651 sec
Time for inference 2: 2.00 sec total, 2047.81 tokens/sec
Decode latency: 1.87 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 5725.76 GB/s
FLOPS achieved: 28.63 TF/s

Prefill latency: 0.1292569823563099 sec
Decode latency: 1.8926451746374369 sec
Time for inference 3: 2.02 sec total, 2024.94 tokens/sec
Decode latency: 1.89 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 5661.80 GB/s
FLOPS achieved: 28.31 TF/s

Prefill latency: 0.13046492263674736 sec
Decode latency: 1.8722143359482288 sec
Time for inference 4: 2.00 sec total, 2044.23 tokens/sec
Decode latency: 1.87 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 5715.73 GB/s
FLOPS achieved: 28.58 TF/s

Prefill latency: 0.129998492076993 sec
Decode latency: 1.8664669580757618 sec
Time for inference 5: 2.00 sec total, 2050.76 tokens/sec
Decode latency: 1.87 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 5734.00 GB/s
FLOPS achieved: 28.67 TF/s

Prefill latency: 0.12978395260870457 sec
Decode latency: 1.8696449249982834 sec
Time for inference 6: 2.00 sec total, 2047.72 tokens/sec
Decode latency: 1.87 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 5725.50 GB/s
FLOPS achieved: 28.63 TF/s

Prefill latency: 0.13076423481106758 sec
Decode latency: 1.873019339516759 sec
Time for inference 7: 2.00 sec total, 2043.29 tokens/sec
Decode latency: 1.87 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 5713.11 GB/s
FLOPS achieved: 28.57 TF/s

Prefill latency: 0.12954391911625862 sec
Decode latency: 1.88433681987226 sec
Time for inference 8: 2.01 sec total, 2033.01 tokens/sec
Decode latency: 1.88 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 5684.36 GB/s
FLOPS achieved: 28.42 TF/s

Prefill latency: 0.13937710784375668 sec
Decode latency: 1.8660160675644875 sec
Time for inference 9: 2.01 sec total, 2041.62 tokens/sec
Decode latency: 1.87 sec
Prefill latency: 0.14 sec
Bandwidth achieved: 5708.44 GB/s
FLOPS achieved: 28.54 TF/s

Prefill latency: 0.1295251715928316 sec
Decode latency: 1.8663145434111357 sec
Time for inference 10: 2.00 sec total, 2051.47 tokens/sec
Decode latency: 1.87 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 5735.98 GB/s
FLOPS achieved: 28.68 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 1.8724 sec
Average prefill latency: 0.1309 sec
Average tokens/sec: 2043.72
Memory used: 15.73 GB
Done. we are killing the process
[rank0]:[W1113 12:18:15.399828909 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
