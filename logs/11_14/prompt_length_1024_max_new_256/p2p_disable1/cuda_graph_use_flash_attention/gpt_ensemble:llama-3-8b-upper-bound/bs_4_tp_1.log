flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
        (wo): Linear(in_features=4096, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=28672, bias=False)
        (w2): Linear(in_features=14336, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.18 seconds
CUDA_GRAPH are activate
Prefill latency: 0.1299966610968113 sec
Decode latency: 2.326652070507407 sec
Compilation time: 2.46 seconds
Prefill latency: 0.129975788295269 sec
Decode latency: 2.3263824209570885 sec
Prefill latency: 0.1300788875669241 sec
Decode latency: 2.327902091667056 sec
Prefill latency: 0.13052606582641602 sec
Decode latency: 2.3061897791922092 sec
Prefill latency: 0.12997721321880817 sec
Decode latency: 2.3259702511131763 sec
Prefill latency: 0.13232515193521976 sec
Decode latency: 2.3252900019288063 sec
Time for inference 1: 2.46 sec total, 416.55 tokens/sec
Decode latency: 2.33 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6252.41 GB/s
FLOPS achieved: 31.26 TF/s

Prefill latency: 0.13044150359928608 sec
Decode latency: 2.324976332485676 sec
Time for inference 2: 2.46 sec total, 416.93 tokens/sec
Decode latency: 2.32 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6258.10 GB/s
FLOPS achieved: 31.29 TF/s

Prefill latency: 0.13033820129930973 sec
Decode latency: 2.3197320625185966 sec
Time for inference 3: 2.45 sec total, 417.84 tokens/sec
Decode latency: 2.32 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6271.79 GB/s
FLOPS achieved: 31.36 TF/s

Prefill latency: 0.13001963682472706 sec
Decode latency: 2.3250214345753193 sec
Time for inference 4: 2.46 sec total, 417.01 tokens/sec
Decode latency: 2.33 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6259.20 GB/s
FLOPS achieved: 31.30 TF/s

Prefill latency: 0.1302283313125372 sec
Decode latency: 2.3255692925304174 sec
Time for inference 5: 2.46 sec total, 416.88 tokens/sec
Decode latency: 2.33 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6257.27 GB/s
FLOPS achieved: 31.29 TF/s

Prefill latency: 0.1310454159975052 sec
Decode latency: 2.3246542308479548 sec
Time for inference 6: 2.46 sec total, 416.88 tokens/sec
Decode latency: 2.32 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6257.31 GB/s
FLOPS achieved: 31.29 TF/s

Prefill latency: 0.12974895164370537 sec
Decode latency: 2.3268299605697393 sec
Time for inference 7: 2.46 sec total, 416.72 tokens/sec
Decode latency: 2.33 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6254.87 GB/s
FLOPS achieved: 31.27 TF/s

Prefill latency: 0.13013630732893944 sec
Decode latency: 2.3250788412988186 sec
Time for inference 8: 2.46 sec total, 416.97 tokens/sec
Decode latency: 2.33 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6258.62 GB/s
FLOPS achieved: 31.29 TF/s

Prefill latency: 0.12743630819022655 sec
Decode latency: 2.325562749058008 sec
Time for inference 9: 2.45 sec total, 417.34 tokens/sec
Decode latency: 2.33 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6264.17 GB/s
FLOPS achieved: 31.32 TF/s

Prefill latency: 0.12973175942897797 sec
Decode latency: 2.2521297484636307 sec
Time for inference 10: 2.38 sec total, 429.81 tokens/sec
Decode latency: 2.25 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 6451.32 GB/s
FLOPS achieved: 32.26 TF/s

==========
Batch Size: 4
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 2.3175 sec
Average prefill latency: 0.1301 sec
Average tokens/sec: 418.29
Memory used: 18.79 GB
[rank0]:[W1113 12:19:04.657623336 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Done. we are killing the process
[rank0]:[W1113 12:19:06.838837237 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
