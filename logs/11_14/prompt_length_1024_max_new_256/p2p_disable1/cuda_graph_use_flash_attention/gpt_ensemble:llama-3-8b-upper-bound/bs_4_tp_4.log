W1113 12:19:47.844000 1482573 site-packages/torch/distributed/run.py:793] 
W1113 12:19:47.844000 1482573 site-packages/torch/distributed/run.py:793] *****************************************
W1113 12:19:47.844000 1482573 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1113 12:19:47.844000 1482573 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=1536, bias=False)
        (wo): Linear(in_features=1024, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.43 seconds
CUDA_GRAPH are activate
Prefill latency: 0.041405679658055305 sec
Decode latency: 1.3100469633936882 sec
Compilation time: 1.35 seconds
Prefill latency: 0.04126504808664322 sec
Decode latency: 1.3087286408990622 sec
Compilation time: 1.35 seconds
Prefill latency: 0.04127907194197178 sec
Compilation time: 1.35 seconds
Compilation time: 1.35 seconds
Decode latency: 1.3091838639229536 sec
Prefill latency: 0.041322121396660805 sec
Decode latency: 1.3088563438504934 sec
Prefill latency: 0.04127926379442215 sec
Decode latency: 1.3093178272247314 sec
Prefill latency: 0.04117743484675884 sec
Decode latency: 1.309159466996789 sec
Time for inference 1: 1.35 sec total, 758.02 tokens/sec
Decode latency: 1.31 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 3442.08 GB/s
FLOPS achieved: 17.21 TF/s

Prefill latency: 0.04123596474528313 sec
Decode latency: 1.225723059847951 sec
Time for inference 2: 1.27 sec total, 807.90 tokens/sec
Decode latency: 1.23 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 3668.59 GB/s
FLOPS achieved: 18.34 TF/s

Prefill latency: 0.04133208282291889 sec
Decode latency: 1.3092392459511757 sec
Time for inference 3: 1.35 sec total, 757.87 tokens/sec
Decode latency: 1.31 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 3441.39 GB/s
FLOPS achieved: 17.21 TF/s

Prefill latency: 0.04120836593210697 sec
Decode latency: 1.3091425318270922 sec
Time for inference 4: 1.35 sec total, 758.01 tokens/sec
Decode latency: 1.31 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 3442.02 GB/s
FLOPS achieved: 17.21 TF/s

Prefill latency: 0.041316695511341095 sec
Decode latency: 1.3093473874032497 sec
Time for inference 5: 1.35 sec total, 757.85 tokens/sec
Decode latency: 1.31 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 3441.29 GB/s
FLOPS achieved: 17.21 TF/s

Prefill latency: 0.041244130581617355 sec
Decode latency: 1.3088328000158072 sec
Time for inference 6: 1.35 sec total, 758.14 tokens/sec
Decode latency: 1.31 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 3442.59 GB/s
FLOPS achieved: 17.21 TF/s

Prefill latency: 0.04141774773597717 sec
Decode latency: 1.3100120536983013 sec
Time for inference 7: 1.35 sec total, 757.34 tokens/sec
Decode latency: 1.31 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 3439.00 GB/s
FLOPS achieved: 17.19 TF/s

Prefill latency: 0.04124470613896847 sec
Decode latency: 1.3084566500037909 sec
Time for inference 8: 1.35 sec total, 758.35 tokens/sec
Decode latency: 1.31 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 3443.56 GB/s
FLOPS achieved: 17.22 TF/s

Prefill latency: 0.04132885858416557 sec
Decode latency: 1.3093959614634514 sec
Time for inference 9: 1.35 sec total, 757.81 tokens/sec
Decode latency: 1.31 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 3441.12 GB/s
FLOPS achieved: 17.21 TF/s

Prefill latency: 0.04127985052764416 sec
Decode latency: 1.3086000885814428 sec
Time for inference 10: 1.35 sec total, 758.29 tokens/sec
Decode latency: 1.31 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 3443.30 GB/s
FLOPS achieved: 17.22 TF/s

==========
Batch Size: 4
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 1.3008 sec
Average prefill latency: 0.0413 sec
Average tokens/sec: 762.96
Memory used: 7.29 GB
[rank0]:[W1113 12:20:16.534845861 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank1]:[W1113 12:20:18.714580542 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank3]:[W1113 12:20:18.013489280 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank2]:[W1113 12:20:18.043920645 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Done. we are killing the process
[rank0]:[W1113 12:20:22.902992917 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
