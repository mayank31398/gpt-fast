W1113 12:23:01.159000 1495740 site-packages/torch/distributed/run.py:793] 
W1113 12:23:01.159000 1495740 site-packages/torch/distributed/run.py:793] *****************************************
W1113 12:23:01.159000 1495740 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1113 12:23:01.159000 1495740 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=768, bias=False)
        (wo): Linear(in_features=512, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=3584, bias=False)
        (w2): Linear(in_features=1792, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.27 seconds
CUDA_GRAPH are activate
Prefill latency: 0.09852796420454979 sec
Decode latency: 1.169925831258297 sec
Compilation time: 1.27 seconds
Compilation time: 1.32 seconds
Prefill latency: 0.09928693249821663 sec
Compilation time: 1.30 seconds
Decode latency: 1.2154758386313915 sec
Compilation time: 1.31 seconds
Compilation time: 1.31 seconds
Prefill latency: 0.0990478117018938 sec
Compilation time: 1.33 seconds
Compilation time: 1.31 seconds
Compilation time: 1.27 seconds
Decode latency: 1.2151859533041716 sec
Prefill latency: 0.0986271295696497 sec
Decode latency: 1.2149982247501612 sec
Prefill latency: 0.09870197996497154 sec
Decode latency: 1.2147081885486841 sec
Prefill latency: 0.09824427962303162 sec
Decode latency: 1.2037705350667238 sec
Time for inference 1: 1.30 sec total, 3144.36 tokens/sec
Decode latency: 1.20 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 8791.74 GB/s
FLOPS achieved: 43.96 TF/s

Prefill latency: 0.09965628013014793 sec
Decode latency: 1.2166319750249386 sec
Time for inference 2: 1.32 sec total, 3110.30 tokens/sec
Decode latency: 1.22 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 8696.52 GB/s
FLOPS achieved: 43.48 TF/s

Prefill latency: 0.09928111173212528 sec
Decode latency: 1.215055262669921 sec
Time for inference 3: 1.31 sec total, 3115.12 tokens/sec
Decode latency: 1.22 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 8709.99 GB/s
FLOPS achieved: 43.55 TF/s

Prefill latency: 0.09975256770849228 sec
Decode latency: 1.2153642121702433 sec
Time for inference 4: 1.32 sec total, 3113.32 tokens/sec
Decode latency: 1.22 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 8704.95 GB/s
FLOPS achieved: 43.52 TF/s

Prefill latency: 0.09903782419860363 sec
Decode latency: 1.205152552574873 sec
Time for inference 5: 1.30 sec total, 3139.40 tokens/sec
Decode latency: 1.21 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 8777.89 GB/s
FLOPS achieved: 43.89 TF/s

Prefill latency: 0.09888765215873718 sec
Decode latency: 1.2148832883685827 sec
Time for inference 6: 1.31 sec total, 3116.02 tokens/sec
Decode latency: 1.21 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 8712.50 GB/s
FLOPS achieved: 43.56 TF/s

Prefill latency: 0.09945750422775745 sec
Decode latency: 1.2016869205981493 sec
Time for inference 7: 1.30 sec total, 3146.27 tokens/sec
Decode latency: 1.20 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 8797.09 GB/s
FLOPS achieved: 43.99 TF/s

Prefill latency: 0.09897752292454243 sec
Decode latency: 1.12151332013309 sec
Time for inference 8: 1.22 sec total, 3354.44 tokens/sec
Decode latency: 1.12 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 9379.15 GB/s
FLOPS achieved: 46.90 TF/s

Prefill latency: 0.09951002523303032 sec
Decode latency: 1.2158161159604788 sec
Time for inference 9: 1.32 sec total, 3112.81 tokens/sec
Decode latency: 1.22 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 8703.52 GB/s
FLOPS achieved: 43.52 TF/s

Prefill latency: 0.09863535314798355 sec
[rank2]:[W1113 12:23:30.059996626 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Decode latency: 1.2162970714271069 sec
Time for inference 10: 1.32 sec total, 3113.55 tokens/sec
Decode latency: 1.22 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 8705.60 GB/s
FLOPS achieved: 43.53 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 1.2026 sec
Average prefill latency: 0.0991 sec
Average tokens/sec: 3146.56
Memory used: 9.47 GB
[rank0]:[W1113 12:23:30.251093966 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank3]:[W1113 12:23:31.284402053 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank4]:[W1113 12:23:31.552847014 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank7]:[W1113 12:23:31.651782107 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank1]:[W1113 12:23:32.753110784 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank5]:[W1113 12:23:32.769783950 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank6]:[W1113 12:23:32.875500801 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Done. we are killing the process
[rank0]:[W1113 12:23:41.443948393 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
