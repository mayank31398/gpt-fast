W1113 13:06:53.316000 1559722 site-packages/torch/distributed/run.py:793] 
W1113 13:06:53.316000 1559722 site-packages/torch/distributed/run.py:793] *****************************************
W1113 13:06:53.316000 1559722 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1113 13:06:53.316000 1559722 site-packages/torch/distributed/run.py:793] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=3072, bias=False)
        (wo): Linear(in_features=2048, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=14336, bias=False)
        (w2): Linear(in_features=7168, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.14 seconds
CUDA_GRAPH are activate
Prefill latency: 1.2169504165649414 sec
Compilation time: 3.45 seconds
Decode latency: 2.2719096057116985 sec
Compilation time: 3.49 seconds
Prefill latency: 1.2099970243871212 sec
Decode latency: 2.2706666607409716 sec
Prefill latency: 1.2105455994606018 sec
Decode latency: 2.271729689091444 sec
Prefill latency: 1.2077589891850948 sec
Decode latency: 2.271214537322521 sec
Prefill latency: 1.2080292329192162 sec
Decode latency: 2.2716133277863264 sec
Prefill latency: 1.2079275771975517 sec
Decode latency: 2.271205674856901 sec
Time for inference 1: 3.48 sec total, 4708.20 tokens/sec
Decode latency: 2.27 sec
Prefill latency: 1.21 sec
Bandwidth achieved: 37809.36 GB/s
FLOPS achieved: 189.05 TF/s

Prefill latency: 1.2066234182566404 sec
Decode latency: 2.221489420160651 sec
Time for inference 2: 3.43 sec total, 4778.27 tokens/sec
Decode latency: 2.22 sec
Prefill latency: 1.21 sec
Bandwidth achieved: 38372.03 GB/s
FLOPS achieved: 191.86 TF/s

Prefill latency: 1.2147499322891235 sec
Decode latency: 2.2703367359936237 sec
Time for inference 3: 3.49 sec total, 4700.17 tokens/sec
Decode latency: 2.27 sec
Prefill latency: 1.21 sec
Bandwidth achieved: 37744.84 GB/s
FLOPS achieved: 188.72 TF/s

Prefill latency: 1.2132789269089699 sec
Decode latency: 2.2343490328639746 sec
Time for inference 4: 3.45 sec total, 4751.29 tokens/sec
Decode latency: 2.23 sec
Prefill latency: 1.21 sec
Bandwidth achieved: 38155.33 GB/s
FLOPS achieved: 190.78 TF/s

Prefill latency: 1.2099507506936789 sec
Decode latency: 2.2720861192792654 sec
Time for inference 5: 3.48 sec total, 4704.50 tokens/sec
Decode latency: 2.27 sec
Prefill latency: 1.21 sec
Bandwidth achieved: 37779.64 GB/s
FLOPS achieved: 188.90 TF/s

Prefill latency: 1.2101739943027496 sec
Decode latency: 2.2711837124079466 sec
Time for inference 6: 3.48 sec total, 4705.45 tokens/sec
Decode latency: 2.27 sec
Prefill latency: 1.21 sec
Bandwidth achieved: 37787.27 GB/s
FLOPS achieved: 188.94 TF/s

Prefill latency: 1.2056306302547455 sec
Decode latency: 2.2717556450515985 sec
Time for inference 7: 3.48 sec total, 4710.83 tokens/sec
Decode latency: 2.27 sec
Prefill latency: 1.21 sec
Bandwidth achieved: 37830.46 GB/s
FLOPS achieved: 189.15 TF/s

Prefill latency: 1.2054689601063728 sec
Decode latency: 2.2721831891685724 sec
Time for inference 8: 3.48 sec total, 4710.45 tokens/sec
Decode latency: 2.27 sec
Prefill latency: 1.21 sec
Bandwidth achieved: 37827.39 GB/s
FLOPS achieved: 189.14 TF/s

Prefill latency: 1.2100477665662766 sec
Decode latency: 2.2711815126240253 sec
Time for inference 9: 3.48 sec total, 4705.55 tokens/sec
Decode latency: 2.27 sec
Prefill latency: 1.21 sec
Bandwidth achieved: 37788.02 GB/s
FLOPS achieved: 188.94 TF/s

Prefill latency: 1.2086581364274025 sec
[rank1]:[W1113 13:07:56.472551993 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Decode latency: 2.271405801177025 sec
Time for inference 10: 3.48 sec total, 4706.93 tokens/sec
Decode latency: 2.27 sec
Prefill latency: 1.21 sec
Bandwidth achieved: 37799.15 GB/s
FLOPS achieved: 189.00 TF/s

==========
Batch Size: 64
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 2.2627 sec
Average prefill latency: 1.2093 sec
Average tokens/sec: 4718.16
Memory used: 38.87 GB
[rank0]:[W1113 13:07:58.880330113 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Done. we are killing the process
[rank0]:[W1113 13:07:59.358926585 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
