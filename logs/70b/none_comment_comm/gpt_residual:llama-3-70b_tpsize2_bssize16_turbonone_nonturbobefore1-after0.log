[2024-09-13 13:03:49,458] torch.distributed.run: [WARNING] 
[2024-09-13 13:03:49,458] torch.distributed.run: [WARNING] *****************************************
[2024-09-13 13:03:49,458] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-09-13 13:03:49,458] torch.distributed.run: [WARNING] *****************************************
our tp world size is 2
our tp world size is 2
our tp world size is 2
our tp world size is 2
our tp world size is 2
our tp world size is 2
our tp world size is 2
our tp world size is 2
our tp world size is 2Using device=cuda

Loading model ...our tp world size is 
2
our tp world size is our tp world size is 2
2
our tp world size is our tp world size is 2
2
our tp world size is our tp world size is 2
2
our tp world size is our tp world size is 2
2
our tp world size is our tp world size is 2
2
our tp world size is our tp world size is 2
2
our tp world size is our tp world size is 2
2
our tp world size isour tp world size is 2 
2
our tp world size isour tp world size is  22

our tp world size is 2our tp world size is
 2our tp world size is 
2our tp world size is 
our tp world size is2
 our tp world size is 2
2our tp world size is 
2
our tp world size isour tp world size is  22

our tp world size is our tp world size is 2
2
our tp world size is our tp world size is 2
2
our tp world size is our tp world size is2
 2
our tp world size is our tp world size is 2
2
our tp world size is our tp world size is 2
2
our tp world size is our tp world size is 2
2our tp world size is 
2
our tp world size is our tp world size is 2
2
our tp world size is our tp world size is 2
2
our tp world size isour tp world size is  22

our tp world size is our tp world size is 2
2
our tp world size isour tp world size is  22

our tp world size is our tp world size is 2
2
our tp world size isour tp world size is  22

our tp world size is our tp world size is 2
2
our tp world size isour tp world size is 2 2

our tp world size is our tp world size is 22

our tp world size isour tp world size is  22

our tp world size isour tp world size is  22

our tp world size is our tp world size is 22

our tp world size is our tp world size is2
 2our tp world size is 
2
our tp world size is our tp world size is 2
2
our tp world size is our tp world size is 2
2
our tp world size is our tp world size is 2
2
our tp world size is our tp world size is 2
2
our tp world size is our tp world size is 2
2
our tp world size is our tp world size is 22

our tp world size is our tp world size is 2
2
our tp world size isour tp world size is  22

our tp world size isour tp world size is  22

our tp world size isour tp world size is  22

our tp world size isour tp world size is  22

our tp world size isour tp world size is  22

our tp world size isour tp world size is  22

our tp world size is our tp world size is 2
2
our tp world size is our tp world size is 2
2
our tp world size is our tp world size is 2
2
our tp world size is our tp world size is 2
2
our tp world size is our tp world size is 2
2our tp world size is 
2
our tp world size is our tp world size is 2
2
our tp world size is our tp world size is 2
2our tp world size is 
2
our tp world size is our tp world size is 2
2
our tp world size is our tp world size is 22

our tp world size isour tp world size is  22

our tp world size is our tp world size is 2
2
our tp world size is our tp world size is 2
2
our tp world size is our tp world size is 2
2
our tp world size is our tp world size is 2
2
our tp world size is our tp world size is 2
2
our tp world size is our tp world size is2
 2our tp world size is 
our tp world size is2
 our tp world size is 2
2
our tp world size is our tp world size is 2
2
our tp world size is our tp world size is 2
2
our tp world size is our tp world size is 2
2
our tp world size is 2our tp world size is 
2
our tp world size is our tp world size is 2
2
our tp world size is applying tp to block 0
2
applying tp to block 0
our tp world size is applying tp to block 0
2
applying tp to block 0
our tp world size isapplying tp to block 0
 2applying tp to block 0

applying tp to block 0
our tp world size is applying tp to block 02

our tp world size isapplying tp to block 0
 applying tp to block 0
2applying tp to block 0

applying tp to block 0our tp world size is 
2
applying tp to block 0
our tp world size is applying tp to block 0
2
applying tp to block 0
our tp world size is applying tp to block 0
2
applying tp to block 0
Applying tensor parallel to model ...
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0applying tp to block 0

applying tp to block 0applying tp to block 0

applying tp to block 0applying tp to block 0

applying tp to block 0applying tp to block 0

applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0applying tp to block 0

applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0applying tp to block 0

applying tp to block 0
applying tp to block 0applying tp to block 0

applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0applying tp to block 0

applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0applying tp to block 0

applying tp to block 0
applying tp to block 0applying tp to block 0

applying tp to block 0we finish operating the TP!

applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
we finish operating the TP!
GPTResidual(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x TurboTransformerBlock(
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=5120, bias=False)
        (wo): Linear(in_features=4096, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=28672, bias=False)
        (w2): Linear(in_features=14336, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
we comment comm is True
models all reduce stream is None
Time to load model: 1.06 seconds
the shape of input is torch.Size([16, 1024])
Traceback (most recent call last):
  File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 381, in <module>
    main(
  File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 307, in main
    y, prefill_latency = generate(
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 147, in generate
    next_token = prefill(model, prompt.view(batch_size, -1), input_pos, **sampling_kwargs).clone()
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 489, in _fn
    return fn(*args, **kwargs)
  File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 78, in prefill
    def prefill(model: torch.nn.Module, x: torch.Tensor, input_pos: torch.Tensor, **sampling_kwargs) -> torch.Tensor:
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 489, in _fn
    return fn(*args, **kwargs)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/external_utils.py", line 17, in inner
    return fn(*args, **kwargs)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 901, in forward
    return compiled_fn(full_args)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py", line 81, in g
    return f(*args)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 94, in runtime_wrapper
    all_outs = call_func_at_runtime_with_args(
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py", line 105, in call_func_at_runtime_with_args
    out = normalize_as_list(f(args))
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 118, in rng_functionalization_wrapper
    return compiled_fw(args)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 864, in __call__
    return self.get_current_callable()(inputs)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 665, in run
    return compiled_fn(new_inputs)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/cudagraph_trees.py", line 380, in deferred_cudagraphify
    fn, out = cudagraphify(model, inputs, new_static_input_idxs, *args, **kwargs)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/cudagraph_trees.py", line 408, in cudagraphify
    return manager.add_function(
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/cudagraph_trees.py", line 1941, in add_function
    return fn, fn(inputs)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/cudagraph_trees.py", line 1755, in run
    out = self._run(new_inputs, function_id)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/cudagraph_trees.py", line 1796, in _run
    return self.run_eager(new_inputs, function_id)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/cudagraph_trees.py", line 1911, in run_eager
    return node.run(new_inputs)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/cudagraph_trees.py", line 611, in run
    out = self.wrapped_function.model(new_inputs)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 892, in _run_from_cache
    return compiled_graph.compiled_artifact(inputs)
  File "/tmp/torchinductor_charlie/u5/cu5xcmpnmbzkuvtriao77e4fsrqnxqvmc3svs2rv3djjphw22cem.py", line 2169, in call
    buf52 = empty((16, 32, s0, 2048), device='cuda', dtype=torch.bfloat16)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/utils/_device.py", line 77, in __torch_function__
    return func(*args, **kwargs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 1 has a total capacity of 79.33 GiB of which 269.44 MiB is free. Including non-PyTorch memory, this process has 79.06 GiB memory in use. Of the allocated memory 77.74 GiB is allocated by PyTorch, and 415.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 381, in <module>
    main(
  File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 307, in main
    y, prefill_latency = generate(
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 147, in generate
    next_token = prefill(model, prompt.view(batch_size, -1), input_pos, **sampling_kwargs).clone()
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 489, in _fn
    return fn(*args, **kwargs)
  File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 78, in prefill
    def prefill(model: torch.nn.Module, x: torch.Tensor, input_pos: torch.Tensor, **sampling_kwargs) -> torch.Tensor:
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 489, in _fn
    return fn(*args, **kwargs)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/external_utils.py", line 17, in inner
    return fn(*args, **kwargs)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 901, in forward
    return compiled_fn(full_args)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py", line 81, in g
    return f(*args)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 94, in runtime_wrapper
    all_outs = call_func_at_runtime_with_args(
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py", line 105, in call_func_at_runtime_with_args
    out = normalize_as_list(f(args))
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 118, in rng_functionalization_wrapper
    return compiled_fw(args)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 864, in __call__
    return self.get_current_callable()(inputs)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 665, in run
    return compiled_fn(new_inputs)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/cudagraph_trees.py", line 380, in deferred_cudagraphify
    fn, out = cudagraphify(model, inputs, new_static_input_idxs, *args, **kwargs)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/cudagraph_trees.py", line 408, in cudagraphify
    return manager.add_function(
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/cudagraph_trees.py", line 1941, in add_function
    return fn, fn(inputs)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/cudagraph_trees.py", line 1755, in run
    out = self._run(new_inputs, function_id)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/cudagraph_trees.py", line 1796, in _run
    return self.run_eager(new_inputs, function_id)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/cudagraph_trees.py", line 1911, in run_eager
    return node.run(new_inputs)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/cudagraph_trees.py", line 611, in run
    out = self.wrapped_function.model(new_inputs)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 892, in _run_from_cache
    return compiled_graph.compiled_artifact(inputs)
  File "/tmp/torchinductor_charlie/gr/cgrsr3rtq4zhcwtfvrpcftm2svfbhbp2shnmdwkujogi7aj36pdy.py", line 2168, in call
    buf30 = empty((16, 32, s0, 2048), device='cuda', dtype=torch.bfloat16)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/utils/_device.py", line 77, in __torch_function__
    return func(*args, **kwargs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 79.33 GiB of which 1.75 GiB is free. Process 1856039 has 520.00 MiB memory in use. Including non-PyTorch memory, this process has 77.06 GiB memory in use. Of the allocated memory 75.74 GiB is allocated by PyTorch, and 415.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2024-09-13 13:13:20,269] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 455933) of binary: /home/charlie/anaconda3/envs/gpt-fast/bin/python
Traceback (most recent call last):
  File "/home/charlie/anaconda3/envs/gpt-fast/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.2.2', 'console_scripts', 'torchrun')())
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
benchmark.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-09-13_13:13:20
  host      : mk-xii-20.cloud.together.ai
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 455934)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-09-13_13:13:20
  host      : mk-xii-20.cloud.together.ai
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 455933)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
