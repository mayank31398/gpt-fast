[2024-09-13 11:56:47,767] torch.distributed.run: [WARNING] 
[2024-09-13 11:56:47,767] torch.distributed.run: [WARNING] *****************************************
[2024-09-13 11:56:47,767] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-09-13 11:56:47,767] torch.distributed.run: [WARNING] *****************************************
our tp world size is 2
our tp world size is 2
our tp world size is 2
our tp world size is 2
our tp world size is 2
our tp world size is 2
our tp world size is 2
our tp world size is 2
our tp world size is 2
our tp world size is 2
our tp world size is 2
our tp world size is 2
our tp world size is 2
our tp world size is 2Using device=cuda

Loading model ...our tp world size is 
2
our tp world size is our tp world size is 2
2
our tp world size is our tp world size is 2
2
our tp world size is our tp world size is 2
2
our tp world size is our tp world size is 2
2
our tp world size is our tp world size is 2
2
our tp world size is our tp world size is 2
2
our tp world size is our tp world size is 2
2
our tp world size is our tp world size is 2
2
our tp world size is our tp world size is 2
2
our tp world size is our tp world size is 2
2
our tp world size is our tp world size is 2
2
our tp world size is 2our tp world size is 
2
our tp world size is our tp world size is 22

our tp world size isour tp world size is  22

our tp world size is 2our tp world size is 
2our tp world size is 
2
our tp world size is our tp world size is 2
2
our tp world size is 2our tp world size is
 2
our tp world size is our tp world size is 2
2
our tp world size isour tp world size is 2 2
our tp world size is
 2our tp world size is 
our tp world size is2
 our tp world size is 2
2
our tp world size is our tp world size is 22

our tp world size isour tp world size is  22

our tp world size isour tp world size is  22

our tp world size is our tp world size is 2
2
our tp world size is our tp world size is 2
2our tp world size is 
our tp world size is2
 2our tp world size is 
our tp world size is2
 2our tp world size is 
our tp world size is2
 2our tp world size is 
our tp world size is2
 2our tp world size is 
our tp world size is2
 2our tp world size is 
our tp world size is2
 2our tp world size is
 our tp world size is 2
2
our tp world size is our tp world size is 22

our tp world size isour tp world size is  22

our tp world size is our tp world size is 2
2
our tp world size is our tp world size is 2
2
our tp world size is our tp world size is 2
2
our tp world size isour tp world size is  22

our tp world size isour tp world size is  22

our tp world size is our tp world size is 22

our tp world size is 2our tp world size is 
2
our tp world size is our tp world size is 22

our tp world size is our tp world size is2
 2our tp world size is 
2
our tp world size is our tp world size is 2
2
our tp world size is our tp world size is 2
2
our tp world size is our tp world size is 2
2
our tp world size is our tp world size is 2
2
our tp world size is our tp world size is 2
2
our tp world size is our tp world size is 2
2
our tp world size is our tp world size is 2
2
our tp world size is our tp world size is 2
2
our tp world size is our tp world size is2
 2our tp world size is 
2
our tp world size is our tp world size is 2
2
our tp world size is our tp world size is 2
2
our tp world size is our tp world size is 2
2
our tp world size is our tp world size is 2
2
our tp world size is our tp world size is 2
2
our tp world size is our tp world size is 2
2
our tp world size is our tp world size is 2
2
our tp world size is our tp world size is 2
2
our tp world size is our tp world size is 2
2
our tp world size is our tp world size is 2
2
applying tp to block 0
our tp world size is applying tp to block 0
2
applying tp to block 0
our tp world size is applying tp to block 0
2
applying tp to block 0
our tp world size is applying tp to block 0
2
applying tp to block 0
our tp world size is applying tp to block 0
2
applying tp to block 0
our tp world size is applying tp to block 0
2
applying tp to block 0
our tp world size is applying tp to block 0
2
applying tp to block 0our tp world size is 
2
applying tp to block 0
our tp world size is applying tp to block 0
2
applying tp to block 0
our tp world size isapplying tp to block 0
 applying tp to block 0
2
applying tp to block 0
our tp world size is applying tp to block 0
2
applying tp to block 0
our tp world size is applying tp to block 0
2
applying tp to block 0our tp world size is 
2
applying tp to block 0
our tp world size is applying tp to block 0
2
applying tp to block 0
our tp world size isapplying tp to block 0 2
applying tp to block 0

our tp world size is applying tp to block 0
2
applying tp to block 0
our tp world size isapplying tp to block 0
 applying tp to block 0
2applying tp to block 0

applying tp to block 0Applying tensor parallel to model ...

applying tp to block 0
applying tp to block 0applying tp to block 0

applying tp to block 0applying tp to block 0

applying tp to block 0applying tp to block 0

applying tp to block 0applying tp to block 0

applying tp to block 0applying tp to block 0

applying tp to block 0applying tp to block 0

applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0applying tp to block 0

applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0applying tp to block 0

applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0applying tp to block 0

applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0applying tp to block 0

applying tp to block 0
applying tp to block 0
applying tp to block 0
we finish operating the TP!
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
applying tp to block 0
we finish operating the TP!
GPTResidual(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x TurboTransformerBlock(
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=5120, bias=False)
        (wo): Linear(in_features=4096, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=28672, bias=False)
        (w2): Linear(in_features=14336, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
we comment comm is True
models all reduce stream is None
Time to load model: 1.08 seconds
the shape of input is torch.Size([4, 1024])
Traceback (most recent call last):
  File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 381, in <module>
    main(
  File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 307, in main
    y, prefill_latency = generate(
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 155, in generate
    generated_tokens, _ = decode_n_tokens(model, next_token.view(batch_size, -1), input_pos, max_new_tokens - 1, callback=callback, **sampling_kwargs)
  File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 95, in decode_n_tokens
    next_token, next_prob = decode_one_token(
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 489, in _fn
    return fn(*args, **kwargs)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 655, in catch_errors
    return callback(frame, cache_entry, hooks, frame_state)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 383, in _convert_frame_assert
    compiled_product = _compile(
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 646, in _compile
    guarded_code = compile_inner(code, one_graph, hooks, transform)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 244, in time_wrapper
    r = func(*args, **kwargs)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 562, in compile_inner
    out_code = transform_code_object(code, transform)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py", line 1033, in transform_code_object
    transformations(instructions, code_options)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 151, in _fn
    return fn(*args, **kwargs)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 527, in transform
    tracer.run()
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 2128, in run
    super().run()
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 818, in run
    and self.step()
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 781, in step
    getattr(self, inst.opname)(inst)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 2243, in RETURN_VALUE
    self.output.compile_subgraph(
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 945, in compile_subgraph
    self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 1087, in compile_and_call_fx_graph
    compiled_fn = self.call_user_compiler(gm)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 244, in time_wrapper
    r = func(*args, **kwargs)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 1159, in call_user_compiler
    raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 1140, in call_user_compiler
    compiled_fn = compiler_fn(gm, self.example_inputs())
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/repro/after_dynamo.py", line 117, in debug_wrapper
    compiled_gm = compiler_fn(gm, example_inputs)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/repro/after_dynamo.py", line 117, in debug_wrapper
    compiled_gm = compiler_fn(gm, example_inputs)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/__init__.py", line 1668, in __call__
    return compile_fx(model_, inputs_, config_patches=self.config)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 952, in compile_fx
    return compile_fx(
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1168, in compile_fx
    return aot_autograd(
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/backends/common.py", line 55, in compiler_fn
    cg = aot_module_simplified(gm, example_inputs, **kwargs)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 887, in aot_module_simplified
    compiled_fn = create_aot_dispatcher_function(
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 244, in time_wrapper
    r = func(*args, **kwargs)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 600, in create_aot_dispatcher_function
    compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 425, in aot_wrapper_dedupe
    return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 630, in aot_wrapper_synthetic_base
    return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 97, in aot_dispatch_base
    compiled_fw = compiler(fw_module, updated_flat_args)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/utils.py", line 244, in time_wrapper
    r = func(*args, **kwargs)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1043, in fw_compiler_base
    joint_graph_passes(model)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/fx_passes/joint_graph.py", line 274, in joint_graph_passes
    count += patterns.apply(graph.graph)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/pattern_matcher.py", line 1147, in apply
    if is_match(m) and entry.extra_check(m):
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/pattern_matcher.py", line 938, in check_fn
    if specific_pattern_match and extra_check(specific_pattern_match):
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/fx_passes/pad_mm.py", line 327, in should_pad_mm
    return should_pad_common(mat1, mat2) and should_pad_bench(
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/fx_passes/pad_mm.py", line 292, in should_pad_bench
    pad_time = do_bench(
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/utils.py", line 167, in do_bench
    return triton_do_bench(*args, **kwargs)[0]
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/triton/testing.py", line 109, in do_bench
    cache = torch.empty(int(256e6 // 4), dtype=torch.int, device='cuda')
torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:
OutOfMemoryError: CUDA out of memory. Tried to allocate 246.00 MiB. GPU 0 has a total capacity of 79.33 GiB of which 167.75 MiB is free. Process 1856039 has 520.00 MiB memory in use. Including non-PyTorch memory, this process has 78.64 GiB memory in use. Of the allocated memory 72.87 GiB is allocated by PyTorch, and 4.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

[2024-09-13 12:14:39,151] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 418632 closing signal SIGTERM
[2024-09-13 12:14:39,418] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 418631) of binary: /home/charlie/anaconda3/envs/gpt-fast/bin/python
Traceback (most recent call last):
  File "/home/charlie/anaconda3/envs/gpt-fast/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.2.2', 'console_scripts', 'torchrun')())
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
benchmark.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-09-13_12:14:39
  host      : mk-xii-20.cloud.together.ai
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 418631)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
