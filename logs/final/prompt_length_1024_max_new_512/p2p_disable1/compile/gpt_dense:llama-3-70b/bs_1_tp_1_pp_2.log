W1202 12:40:05.431000 1786174 site-packages/torch/distributed/run.py:793] 
W1202 12:40:05.431000 1786174 site-packages/torch/distributed/run.py:793] *****************************************
W1202 12:40:05.431000 1786174 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1202 12:40:05.431000 1786174 site-packages/torch/distributed/run.py:793] *****************************************
DeviceMesh('cuda', [[0], [1]], mesh_dim_names=('pp', 'tp'))
DeviceMesh('cuda', [[0], [1]], mesh_dim_names=('pp', 'tp'))
Using device=cuda
Loading model ...
GPTDense(
  (layers): ModuleList(
    (0-39): 40 x DenseTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=10240, bias=False)
        (wo): Linear(in_features=8192, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=57344, bias=False)
        (w2): Linear(in_features=28672, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Time to load model: 1.15 seconds
Compiling prefill with dynamic=False
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Prefill latency: 33.85186370275915 sec
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 44.68164055701345 secCompilation time: 78.65 seconds

Compilation time: 78.54 seconds
Prefill latency: 0.3737141489982605 sec
Decode latency: 25.11339903390035 sec
Prefill latency: 0.3726617251522839 sec
Decode latency: 25.113493506796658 sec
Prefill latency: 0.3722914569079876 sec
Decode latency: 25.113598498050123 sec
Prefill latency: 0.37251156428828835 sec
Decode latency: 25.113442310597748 sec
Prefill latency: 0.37435276759788394 sec
Decode latency: 25.1149733052589 sec
Time for inference 1: 25.49 sec total, 20.08 tokens/sec
Decode latency: 25.11 sec
Prefill latency: 0.37 sec
Bandwidth achieved: 1417.03 GB/s
FLOPS achieved: 4.25 TF/s

Prefill latency: 0.3751691719517112 sec
Decode latency: 25.11597349308431 sec
Time for inference 2: 25.49 sec total, 20.08 tokens/sec
Decode latency: 25.12 sec
Prefill latency: 0.38 sec
Bandwidth achieved: 1416.94 GB/s
FLOPS achieved: 4.25 TF/s

Prefill latency: 0.37263015611097217 sec
Decode latency: 25.11679168511182 sec
Time for inference 3: 25.49 sec total, 20.08 tokens/sec
Decode latency: 25.12 sec
Prefill latency: 0.37 sec
Bandwidth achieved: 1417.04 GB/s
FLOPS achieved: 4.25 TF/s

Prefill latency: 0.36915391869843006 sec
Decode latency: 25.11414926778525 sec
Time for inference 4: 25.49 sec total, 20.09 tokens/sec
Decode latency: 25.11 sec
Prefill latency: 0.37 sec
Bandwidth achieved: 1417.38 GB/s
FLOPS achieved: 4.25 TF/s

Prefill latency: 0.37225052434951067 sec
Decode latency: 25.11492264876142 sec
Time for inference 5: 25.49 sec total, 20.09 tokens/sec
Decode latency: 25.11 sec
Prefill latency: 0.37 sec
Bandwidth achieved: 1417.17 GB/s
FLOPS achieved: 4.25 TF/s

Prefill latency: 0.37277053808793426 sec
Decode latency: 25.11458957195282 sec
Time for inference 6: 25.49 sec total, 20.09 tokens/sec
Decode latency: 25.11 sec
Prefill latency: 0.37 sec
Bandwidth achieved: 1417.14 GB/s
FLOPS achieved: 4.25 TF/s

Prefill latency: 0.3724158788099885 sec
Decode latency: 25.113481035921723 sec
Time for inference 7: 25.49 sec total, 20.09 tokens/sec
Decode latency: 25.11 sec
Prefill latency: 0.37 sec
Bandwidth achieved: 1417.22 GB/s
FLOPS achieved: 4.25 TF/s

Prefill latency: 0.37326740100979805 sec
Decode latency: 25.115359541028738 sec
Time for inference 8: 25.49 sec total, 20.08 tokens/sec
Decode latency: 25.12 sec
Prefill latency: 0.37 sec
Bandwidth achieved: 1417.06 GB/s
FLOPS achieved: 4.25 TF/s

Prefill latency: 0.3739546616561711 sec
Decode latency: 25.114020970650017 sec
Time for inference 9: 25.49 sec total, 20.09 tokens/sec
Decode latency: 25.11 sec
Prefill latency: 0.37 sec
Bandwidth achieved: 1417.11 GB/s
FLOPS achieved: 4.25 TF/s

Prefill latency: 0.3740686038509011 sec
Decode latency: 25.11387665802613 sec
Time for inference 10: 25.49 sec total, 20.09 tokens/sec
Decode latency: 25.11 sec
Prefill latency: 0.37 sec
Bandwidth achieved: 1417.11 GB/s
FLOPS achieved: 4.25 TF/s

==========
Batch Size: 1
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 25.1148 sec
Average prefill latency: 0.3730 sec
Average tokens/sec: 20.09
Memory used: 72.08 GB
Done. we are killing the process
[rank0]:[W1202 12:47:25.650486917 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank1]:[W1202 12:47:26.323580425 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
