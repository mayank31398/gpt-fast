W1202 10:00:45.734000 1481719 site-packages/torch/distributed/run.py:793] 
W1202 10:00:45.734000 1481719 site-packages/torch/distributed/run.py:793] *****************************************
W1202 10:00:45.734000 1481719 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1202 10:00:45.734000 1481719 site-packages/torch/distributed/run.py:793] *****************************************
DeviceMesh('cuda', [[0, 1], [2, 3]], mesh_dim_names=('pp', 'tp'))
DeviceMesh('cuda', [[0, 1], [2, 3]], mesh_dim_names=('pp', 'tp'))
DeviceMesh('cuda', [[0, 1], [2, 3]], mesh_dim_names=('pp', 'tp'))
DeviceMesh('cuda', [[0, 1], [2, 3]], mesh_dim_names=('pp', 'tp'))
Using device=cuda
Loading model ...
GPTDense(
  (layers): ModuleList(
    (0-39): 40 x DenseTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=5120, bias=False)
        (wo): Linear(in_features=4096, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=28672, bias=False)
        (w2): Linear(in_features=14336, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Time to load model: 1.18 seconds
Compiling prefill with dynamic=False
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
Prefill latency: 166.43842991697602 sec
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 146.3284198360052 secCompilation time: 312.95 seconds

Compilation time: 312.95 seconds
Compilation time: 312.77 seconds
Compilation time: 312.76 seconds
Prefill latency: 5.646832398022525 sec
Decode latency: 23.120758561999537 sec
Prefill latency: 5.675758991972543 sec
Decode latency: 23.122490947018377 sec
Prefill latency: 5.673863739008084 sec
Decode latency: 23.121494362014346 sec
Prefill latency: 5.65655027597677 sec
Decode latency: 23.122990197036415 sec
Prefill latency: 5.684075228055008 sec
Decode latency: 23.125690361019224 sec
Time for inference 1: 28.81 sec total, 284.32 tokens/sec
Decode latency: 23.13 sec
Prefill latency: 5.68 sec
Bandwidth achieved: 10328.87 GB/s
FLOPS achieved: 30.99 TF/s

Prefill latency: 5.672762301983312 sec
Decode latency: 23.123994452995248 sec
Time for inference 2: 28.80 sec total, 284.45 tokens/sec
Decode latency: 23.12 sec
Prefill latency: 5.67 sec
Bandwidth achieved: 10333.50 GB/s
FLOPS achieved: 31.00 TF/s

Prefill latency: 5.644720371928997 sec
Decode latency: 23.122508452041075 sec
Time for inference 3: 28.77 sec total, 284.74 tokens/sec
Decode latency: 23.12 sec
Prefill latency: 5.64 sec
Bandwidth achieved: 10344.13 GB/s
FLOPS achieved: 31.03 TF/s

Prefill latency: 5.672923901001923 sec
Decode latency: 23.12357840896584 sec
Time for inference 4: 28.80 sec total, 284.45 tokens/sec
Decode latency: 23.12 sec
Prefill latency: 5.67 sec
Bandwidth achieved: 10333.62 GB/s
FLOPS achieved: 31.00 TF/s

Prefill latency: 5.675055760890245 sec
Decode latency: 23.12145471700933 sec
Time for inference 5: 28.80 sec total, 284.45 tokens/sec
Decode latency: 23.12 sec
Prefill latency: 5.68 sec
Bandwidth achieved: 10333.63 GB/s
FLOPS achieved: 31.00 TF/s

Prefill latency: 5.656457079923712 sec
Decode latency: 23.121847734088078 sec
Time for inference 6: 28.78 sec total, 284.63 tokens/sec
Decode latency: 23.12 sec
Prefill latency: 5.66 sec
Bandwidth achieved: 10340.20 GB/s
FLOPS achieved: 31.02 TF/s

Prefill latency: 5.675106744049117 sec
Decode latency: 23.12743983697146 sec
Time for inference 7: 28.81 sec total, 284.39 tokens/sec
Decode latency: 23.13 sec
Prefill latency: 5.68 sec
Bandwidth achieved: 10331.43 GB/s
FLOPS achieved: 30.99 TF/s

Prefill latency: 5.679534898954444 sec
Decode latency: 23.126373596023768 sec
Time for inference 8: 28.81 sec total, 284.36 tokens/sec
Decode latency: 23.13 sec
Prefill latency: 5.68 sec
Bandwidth achieved: 10330.26 GB/s
FLOPS achieved: 30.99 TF/s

Prefill latency: 5.6420092260232195 sec
Decode latency: 23.122621900052764 sec
Time for inference 9: 28.77 sec total, 284.77 tokens/sec
Decode latency: 23.12 sec
Prefill latency: 5.64 sec
Bandwidth achieved: 10345.07 GB/s
FLOPS achieved: 31.04 TF/s

Prefill latency: 5.674553029006347 sec
Decode latency: 23.120711761992425 sec
Time for inference 10: 28.80 sec total, 284.46 tokens/sec
Decode latency: 23.12 sec
Prefill latency: 5.67 sec
Bandwidth achieved: 10334.05 GB/s
FLOPS achieved: 31.00 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 23.1236 sec
Average prefill latency: 5.6677 sec
Average tokens/sec: 284.50
Memory used: 52.75 GB
Done. we are killing the process
[rank0]:[W1202 10:12:48.009721424 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank1]:[W1202 10:12:49.242766140 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank2]:[W1202 10:12:49.561405827 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
