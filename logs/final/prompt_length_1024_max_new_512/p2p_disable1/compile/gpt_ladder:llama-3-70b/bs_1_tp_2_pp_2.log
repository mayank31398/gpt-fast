W1202 11:12:59.537000 1530574 site-packages/torch/distributed/run.py:793] 
W1202 11:12:59.537000 1530574 site-packages/torch/distributed/run.py:793] *****************************************
W1202 11:12:59.537000 1530574 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1202 11:12:59.537000 1530574 site-packages/torch/distributed/run.py:793] *****************************************
DeviceMesh('cuda', [[0, 1], [2, 3]], mesh_dim_names=('pp', 'tp'))
DeviceMesh('cuda', [[0, 1], [2, 3]], mesh_dim_names=('pp', 'tp'))
Using device=cuda
Loading model ...
DeviceMesh('cuda', [[0, 1], [2, 3]], mesh_dim_names=('pp', 'tp'))
DeviceMesh('cuda', [[0, 1], [2, 3]], mesh_dim_names=('pp', 'tp'))
GPTLadder(
  (layers): ModuleList(
    (0-39): 40 x LadderTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=5120, bias=False)
        (wo): Linear(in_features=4096, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=28672, bias=False)
        (w2): Linear(in_features=14336, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 0.10 seconds
Compiling prefill with dynamic=False
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
Prefill latency: 83.15148224798031 sec
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 86.19238943199161 sec
Compilation time: 169.34 seconds
Compilation time: 169.33 seconds
Compilation time: 169.32 seconds
Compilation time: 169.35 seconds
Prefill latency: 0.22930672101210803 sec
Decode latency: 13.454257488017902 sec
Prefill latency: 0.22417816903907806 sec
Decode latency: 13.453986863023601 sec
Prefill latency: 0.22348850790876895 sec
Decode latency: 13.453326902003027 sec
Prefill latency: 0.22503968502860516 sec
Decode latency: 13.455067744944245 sec
Prefill latency: 0.22471111209597439 sec
Decode latency: 13.455749548971653 sec
Time for inference 1: 13.68 sec total, 37.42 tokens/sec
Decode latency: 13.46 sec
Prefill latency: 0.22 sec
Bandwidth achieved: 1359.37 GB/s
FLOPS achieved: 4.08 TF/s

Prefill latency: 0.22389006707817316 sec
Decode latency: 13.453821223927662 sec
Time for inference 2: 13.68 sec total, 37.43 tokens/sec
Decode latency: 13.45 sec
Prefill latency: 0.22 sec
Bandwidth achieved: 1359.64 GB/s
FLOPS achieved: 4.08 TF/s

Prefill latency: 0.22442263190168887 sec
Decode latency: 13.454794902005233 sec
Time for inference 3: 13.68 sec total, 37.42 tokens/sec
Decode latency: 13.45 sec
Prefill latency: 0.22 sec
Bandwidth achieved: 1359.49 GB/s
FLOPS achieved: 4.08 TF/s

Prefill latency: 0.22513139701914042 sec
Decode latency: 13.453152128029615 sec
Time for inference 4: 13.68 sec total, 37.43 tokens/sec
Decode latency: 13.45 sec
Prefill latency: 0.23 sec
Bandwidth achieved: 1359.59 GB/s
FLOPS achieved: 4.08 TF/s

Prefill latency: 0.22863588808104396 sec
Decode latency: 13.455327539006248 sec
Time for inference 5: 13.69 sec total, 37.41 tokens/sec
Decode latency: 13.46 sec
Prefill latency: 0.23 sec
Bandwidth achieved: 1359.05 GB/s
FLOPS achieved: 4.08 TF/s

Prefill latency: 0.22542213008273393 sec
Decode latency: 13.453506866004318 sec
Time for inference 6: 13.68 sec total, 37.42 tokens/sec
Decode latency: 13.45 sec
Prefill latency: 0.23 sec
Bandwidth achieved: 1359.55 GB/s
FLOPS achieved: 4.08 TF/s

Prefill latency: 0.22653504996560514 sec
Decode latency: 13.453733274014667 sec
Time for inference 7: 13.68 sec total, 37.42 tokens/sec
Decode latency: 13.45 sec
Prefill latency: 0.23 sec
Bandwidth achieved: 1359.40 GB/s
FLOPS achieved: 4.08 TF/s

Prefill latency: 0.22693670901935548 sec
Decode latency: 13.453060684958473 sec
Time for inference 8: 13.68 sec total, 37.42 tokens/sec
Decode latency: 13.45 sec
Prefill latency: 0.23 sec
Bandwidth achieved: 1359.42 GB/s
FLOPS achieved: 4.08 TF/s

Prefill latency: 0.22644128906540573 sec
Decode latency: 13.454740906017832 sec
Time for inference 9: 13.68 sec total, 37.42 tokens/sec
Decode latency: 13.45 sec
Prefill latency: 0.23 sec
Bandwidth achieved: 1359.27 GB/s
FLOPS achieved: 4.08 TF/s

Prefill latency: 0.2247526590945199 sec
Decode latency: 13.454306888976134 sec
Time for inference 10: 13.68 sec total, 37.42 tokens/sec
Decode latency: 13.45 sec
Prefill latency: 0.22 sec
Bandwidth achieved: 1359.50 GB/s
FLOPS achieved: 4.08 TF/s

==========
Batch Size: 1
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 13.4542 sec
Average prefill latency: 0.2257 sec
Average tokens/sec: 37.42
Memory used: 37.65 GB
Done. we are killing the process
[rank0]:[W1202 11:19:07.198608152 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank1]:[W1202 11:19:07.213841346 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank2]:[W1202 11:19:07.916866532 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
