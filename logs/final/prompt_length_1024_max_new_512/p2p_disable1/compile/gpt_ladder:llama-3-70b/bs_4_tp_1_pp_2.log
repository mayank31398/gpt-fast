W1202 13:25:34.681000 1795954 site-packages/torch/distributed/run.py:793] 
W1202 13:25:34.681000 1795954 site-packages/torch/distributed/run.py:793] *****************************************
W1202 13:25:34.681000 1795954 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1202 13:25:34.681000 1795954 site-packages/torch/distributed/run.py:793] *****************************************
DeviceMesh('cuda', [[0], [1]], mesh_dim_names=('pp', 'tp'))
DeviceMesh('cuda', [[0], [1]], mesh_dim_names=('pp', 'tp'))
Using device=cuda
Loading model ...
GPTLadder(
  (layers): ModuleList(
    (0-39): 40 x LadderTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=10240, bias=False)
        (wo): Linear(in_features=8192, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=57344, bias=False)
        (w2): Linear(in_features=28672, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 0.14 seconds
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Compiling prefill with dynamic=False
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
Prefill latency: 74.70150278881192 sec
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 90.28497237712145 sec
Compilation time: 165.00 seconds
Compilation time: 164.99 seconds
Prefill latency: 1.4276856710202992 sec
Decode latency: 28.04280713899061 sec
Prefill latency: 1.4319102591834962 sec
Decode latency: 28.042183958925307 sec
Prefill latency: 1.4333561668172479 sec
Decode latency: 28.041493088938296 sec
Prefill latency: 1.4315178338438272 sec
Decode latency: 28.040688081178814 sec
Prefill latency: 1.427589388564229 sec
Decode latency: 28.041584502905607 sec
Time for inference 1: 29.47 sec total, 69.49 tokens/sec
Decode latency: 28.04 sec
Prefill latency: 1.43 sec
Bandwidth achieved: 4902.78 GB/s
FLOPS achieved: 14.71 TF/s

Prefill latency: 1.4310429119504988 sec
Decode latency: 28.04138026293367 sec
Time for inference 2: 29.48 sec total, 69.48 tokens/sec
Decode latency: 28.04 sec
Prefill latency: 1.43 sec
Bandwidth achieved: 4902.26 GB/s
FLOPS achieved: 14.71 TF/s

Prefill latency: 1.427252443972975 sec
Decode latency: 28.042230729945004 sec
Time for inference 3: 29.47 sec total, 69.49 tokens/sec
Decode latency: 28.04 sec
Prefill latency: 1.43 sec
Bandwidth achieved: 4902.75 GB/s
FLOPS achieved: 14.71 TF/s

Prefill latency: 1.4339676019735634 sec
Decode latency: 28.04090910498053 sec
Time for inference 4: 29.48 sec total, 69.48 tokens/sec
Decode latency: 28.04 sec
Prefill latency: 1.43 sec
Bandwidth achieved: 4901.85 GB/s
FLOPS achieved: 14.71 TF/s

Prefill latency: 1.4340020059607923 sec
Decode latency: 28.04114804416895 sec
Time for inference 5: 29.48 sec total, 69.48 tokens/sec
Decode latency: 28.04 sec
Prefill latency: 1.43 sec
Bandwidth achieved: 4901.81 GB/s
FLOPS achieved: 14.71 TF/s

Prefill latency: 1.4317219802178442 sec
Decode latency: 28.041809703223407 sec
Time for inference 6: 29.48 sec total, 69.48 tokens/sec
Decode latency: 28.04 sec
Prefill latency: 1.43 sec
Bandwidth achieved: 4902.06 GB/s
FLOPS achieved: 14.71 TF/s

Prefill latency: 1.4271780690178275 sec
Decode latency: 28.041119553148746 sec
Time for inference 7: 29.47 sec total, 69.49 tokens/sec
Decode latency: 28.04 sec
Prefill latency: 1.43 sec
Bandwidth achieved: 4902.95 GB/s
FLOPS achieved: 14.71 TF/s

Prefill latency: 1.4321401817724109 sec
Decode latency: 28.041235838085413 sec
Time for inference 8: 29.48 sec total, 69.48 tokens/sec
Decode latency: 28.04 sec
Prefill latency: 1.43 sec
Bandwidth achieved: 4902.10 GB/s
FLOPS achieved: 14.71 TF/s

Prefill latency: 1.4267255067825317 sec
Decode latency: 28.041512259747833 sec
Time for inference 9: 29.47 sec total, 69.49 tokens/sec
Decode latency: 28.04 sec
Prefill latency: 1.43 sec
Bandwidth achieved: 4902.96 GB/s
FLOPS achieved: 14.71 TF/s

Prefill latency: 1.4302137061022222 sec
Decode latency: 28.04098181426525 sec
Time for inference 10: 29.47 sec total, 69.49 tokens/sec
Decode latency: 28.04 sec
Prefill latency: 1.43 sec
Bandwidth achieved: 4902.46 GB/s
FLOPS achieved: 14.71 TF/s

==========
Batch Size: 4
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 28.0414 sec
Average prefill latency: 1.4302 sec
Average tokens/sec: 69.48
Memory used: 75.62 GB
Done. we are killing the process
[rank0]:[W1202 13:35:18.078500987 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank1]:[W1202 13:35:18.766495822 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
