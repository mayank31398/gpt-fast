W1202 11:29:39.040000 1543415 site-packages/torch/distributed/run.py:793] 
W1202 11:29:39.040000 1543415 site-packages/torch/distributed/run.py:793] *****************************************
W1202 11:29:39.040000 1543415 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1202 11:29:39.040000 1543415 site-packages/torch/distributed/run.py:793] *****************************************
DeviceMesh('cuda', [[0, 1], [2, 3]], mesh_dim_names=('pp', 'tp'))
DeviceMesh('cuda', [[0, 1], [2, 3]], mesh_dim_names=('pp', 'tp'))
DeviceMesh('cuda', [[0, 1], [2, 3]], mesh_dim_names=('pp', 'tp'))
Using device=cuda
Loading model ...
DeviceMesh('cuda', [[0, 1], [2, 3]], mesh_dim_names=('pp', 'tp'))
GPTLadder(
  (layers): ModuleList(
    (0-39): 40 x LadderTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=5120, bias=False)
        (wo): Linear(in_features=4096, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=28672, bias=False)
        (w2): Linear(in_features=14336, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 0.12 seconds
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Compiling prefill with dynamic=False
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
Prefill latency: 92.48213187407237 sec
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 84.06979628896806 sec
Compilation time: 176.56 seconds
Compilation time: 176.55 seconds
Compilation time: 176.56 seconds
Compilation time: 176.58 seconds
Prefill latency: 0.8196815630653873 sec
Decode latency: 15.447080691927113 sec
Prefill latency: 0.8212287069763988 sec
Decode latency: 15.446238357922994 sec
Prefill latency: 0.8238231149734929 sec
Decode latency: 15.446614465909079 sec
Prefill latency: 0.8203255670377985 sec
Decode latency: 15.449171804008074 sec
Prefill latency: 0.8184318729909137 sec
Decode latency: 15.449608769034967 sec
Time for inference 1: 16.27 sec total, 125.87 tokens/sec
Decode latency: 15.45 sec
Prefill latency: 0.82 sec
Bandwidth achieved: 4572.61 GB/s
FLOPS achieved: 13.72 TF/s

Prefill latency: 0.8205827049678192 sec
Decode latency: 15.44679671805352 sec
Time for inference 2: 16.27 sec total, 125.88 tokens/sec
Decode latency: 15.45 sec
Prefill latency: 0.82 sec
Bandwidth achieved: 4572.91 GB/s
FLOPS achieved: 13.72 TF/s

Prefill latency: 0.8217299310490489 sec
Decode latency: 15.44804490194656 sec
Time for inference 3: 16.27 sec total, 125.86 tokens/sec
Decode latency: 15.45 sec
Prefill latency: 0.82 sec
Bandwidth achieved: 4572.19 GB/s
FLOPS achieved: 13.72 TF/s

Prefill latency: 0.8218650389462709 sec
Decode latency: 15.446998632978648 sec
Time for inference 4: 16.27 sec total, 125.87 tokens/sec
Decode latency: 15.45 sec
Prefill latency: 0.82 sec
Bandwidth achieved: 4572.46 GB/s
FLOPS achieved: 13.72 TF/s

Prefill latency: 0.8247646109666675 sec
Decode latency: 15.44755000504665 sec
Time for inference 5: 16.27 sec total, 125.84 tokens/sec
Decode latency: 15.45 sec
Prefill latency: 0.82 sec
Bandwidth achieved: 4571.48 GB/s
FLOPS achieved: 13.71 TF/s

Prefill latency: 0.8241897150874138 sec
Decode latency: 15.452286153915338 sec
Time for inference 6: 16.28 sec total, 125.81 tokens/sec
Decode latency: 15.45 sec
Prefill latency: 0.82 sec
Bandwidth achieved: 4570.31 GB/s
FLOPS achieved: 13.71 TF/s

Prefill latency: 0.821073955972679 sec
Decode latency: 15.447907156078145 sec
Time for inference 7: 16.27 sec total, 125.87 tokens/sec
Decode latency: 15.45 sec
Prefill latency: 0.82 sec
Bandwidth achieved: 4572.47 GB/s
FLOPS achieved: 13.72 TF/s

Prefill latency: 0.8190398559672758 sec
Decode latency: 15.448830627952702 sec
Time for inference 8: 16.27 sec total, 125.87 tokens/sec
Decode latency: 15.45 sec
Prefill latency: 0.82 sec
Bandwidth achieved: 4572.76 GB/s
FLOPS achieved: 13.72 TF/s

Prefill latency: 0.823009992018342 sec
Decode latency: 15.446587827987969 sec
Time for inference 9: 16.27 sec total, 125.86 tokens/sec
Decode latency: 15.45 sec
Prefill latency: 0.82 sec
Bandwidth achieved: 4572.27 GB/s
FLOPS achieved: 13.72 TF/s

Prefill latency: 0.8172759949229658 sec
Decode latency: 15.447869205963798 sec
Time for inference 10: 16.27 sec total, 125.90 tokens/sec
Decode latency: 15.45 sec
Prefill latency: 0.82 sec
Bandwidth achieved: 4573.56 GB/s
FLOPS achieved: 13.72 TF/s

==========
Batch Size: 4
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 15.4482 sec
Average prefill latency: 0.8212 sec
Average tokens/sec: 125.86
Memory used: 40.81 GB
Done. we are killing the process
[rank0]:[W1202 11:36:30.260531982 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank1]:[W1202 11:36:30.285939921 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank2]:[W1202 11:36:31.139829776 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
