W1202 11:51:47.006000 1558739 site-packages/torch/distributed/run.py:793] 
W1202 11:51:47.006000 1558739 site-packages/torch/distributed/run.py:793] *****************************************
W1202 11:51:47.006000 1558739 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1202 11:51:47.006000 1558739 site-packages/torch/distributed/run.py:793] *****************************************
DeviceMesh('cuda', [[0, 1], [2, 3]], mesh_dim_names=('pp', 'tp'))
DeviceMesh('cuda', [[0, 1], [2, 3]], mesh_dim_names=('pp', 'tp'))
Using device=cuda
Loading model ...
DeviceMesh('cuda', [[0, 1], [2, 3]], mesh_dim_names=('pp', 'tp'))
DeviceMesh('cuda', [[0, 1], [2, 3]], mesh_dim_names=('pp', 'tp'))
GPTLadder(
  (layers): ModuleList(
    (0-39): 40 x LadderTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=5120, bias=False)
        (wo): Linear(in_features=4096, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=28672, bias=False)
        (w2): Linear(in_features=14336, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 0.10 seconds
Compiling prefill with dynamic=False
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
Prefill latency: 99.70534474996384 sec
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 113.46176696999464 sec
Compilation time: 213.14 seconds
Compilation time: 213.13 seconds
Compilation time: 213.17 seconds
Compilation time: 213.12 seconds
Prefill latency: 3.265742801944725 sec
Decode latency: 18.942975701997057 sec
Prefill latency: 3.2435997870052233 sec
Decode latency: 18.944980648928322 sec
Prefill latency: 3.2545875719515607 sec
Decode latency: 18.94547181308735 sec
Prefill latency: 3.2445420490112156 sec
Decode latency: 18.93975135497749 sec
Prefill latency: 3.2503772779600695 sec
Decode latency: 18.9457104310859 sec
Time for inference 1: 22.20 sec total, 369.03 tokens/sec
Decode latency: 18.95 sec
Prefill latency: 3.25 sec
Bandwidth achieved: 13406.34 GB/s
FLOPS achieved: 40.22 TF/s

Prefill latency: 3.2624677980784327 sec
Decode latency: 18.94096994108986 sec
Time for inference 2: 22.21 sec total, 368.91 tokens/sec
Decode latency: 18.94 sec
Prefill latency: 3.26 sec
Bandwidth achieved: 13401.97 GB/s
FLOPS achieved: 40.21 TF/s

Prefill latency: 3.2613447159528732 sec
Decode latency: 18.93905933597125 sec
Time for inference 3: 22.20 sec total, 368.96 tokens/sec
Decode latency: 18.94 sec
Prefill latency: 3.26 sec
Bandwidth achieved: 13403.64 GB/s
FLOPS achieved: 40.21 TF/s

Prefill latency: 3.2527096590492874 sec
Decode latency: 18.941898021032102 sec
Time for inference 4: 22.20 sec total, 369.06 tokens/sec
Decode latency: 18.94 sec
Prefill latency: 3.25 sec
Bandwidth achieved: 13407.22 GB/s
FLOPS achieved: 40.22 TF/s

Prefill latency: 3.2556960340589285 sec
Decode latency: 18.94566555507481 sec
Time for inference 5: 22.20 sec total, 368.95 tokens/sec
Decode latency: 18.95 sec
Prefill latency: 3.26 sec
Bandwidth achieved: 13403.11 GB/s
FLOPS achieved: 40.21 TF/s

Prefill latency: 3.2532890089787543 sec
Decode latency: 18.943918280070648 sec
Time for inference 6: 22.20 sec total, 369.02 tokens/sec
Decode latency: 18.94 sec
Prefill latency: 3.25 sec
Bandwidth achieved: 13405.71 GB/s
FLOPS achieved: 40.22 TF/s

Prefill latency: 3.2646061530103907 sec
Decode latency: 18.94686008291319 sec
Time for inference 7: 22.21 sec total, 368.78 tokens/sec
Decode latency: 18.95 sec
Prefill latency: 3.26 sec
Bandwidth achieved: 13396.99 GB/s
FLOPS achieved: 40.19 TF/s

Prefill latency: 3.261215426027775 sec
Decode latency: 18.94571005704347 sec
Time for inference 8: 22.21 sec total, 368.85 tokens/sec
Decode latency: 18.95 sec
Prefill latency: 3.26 sec
Bandwidth achieved: 13399.75 GB/s
FLOPS achieved: 40.20 TF/s

Prefill latency: 3.252534109982662 sec
Decode latency: 18.946721028070897 sec
Time for inference 9: 22.20 sec total, 368.98 tokens/sec
Decode latency: 18.95 sec
Prefill latency: 3.25 sec
Bandwidth achieved: 13404.46 GB/s
FLOPS achieved: 40.21 TF/s

Prefill latency: 3.2585183050250635 sec
Decode latency: 18.944401771994308 sec
Time for inference 10: 22.21 sec total, 368.92 tokens/sec
Decode latency: 18.94 sec
Prefill latency: 3.26 sec
Bandwidth achieved: 13402.22 GB/s
FLOPS achieved: 40.21 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 18.9441 sec
Average prefill latency: 3.2573 sec
Average tokens/sec: 368.95
Memory used: 52.95 GB
Done. we are killing the process
[rank1]:[W1202 12:00:38.106071227 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank0]:[W1202 12:00:38.199585440 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank2]:[W1202 12:00:38.822870168 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
