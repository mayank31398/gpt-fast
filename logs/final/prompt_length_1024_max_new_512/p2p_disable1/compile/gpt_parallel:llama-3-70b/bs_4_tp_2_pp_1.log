W1202 14:35:01.274000 1809052 site-packages/torch/distributed/run.py:793] 
W1202 14:35:01.274000 1809052 site-packages/torch/distributed/run.py:793] *****************************************
W1202 14:35:01.274000 1809052 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1202 14:35:01.274000 1809052 site-packages/torch/distributed/run.py:793] *****************************************
DeviceMesh('cuda', [[0, 1]], mesh_dim_names=('pp', 'tp'))
Using device=cuda
Loading model ...
DeviceMesh('cuda', [[0, 1]], mesh_dim_names=('pp', 'tp'))
GPTParallel(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x ParallelTransformerBlock(
      semi_compiled = False
      (attention): FuseAttentionMLP(
        (wqkv1): Linear(in_features=8192, out_features=33792, bias=False)
        (wo): Linear(in_features=4096, out_features=8192, bias=False)
        (w2): Linear(in_features=14336, out_features=8192, bias=False)
      )
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 1.36 seconds
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Compiling prefill with dynamic=False
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Prefill latency: 30.672994369640946 sec
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 42.79278475185856 sec
Compilation time: 73.51 seconds
Compilation time: 73.47 seconds
Prefill latency: 1.0670751188881695 sec
Decode latency: 16.419734156690538 sec
Prefill latency: 1.0663455929607153 sec
Decode latency: 16.419872637838125 sec
Prefill latency: 1.0663479957729578 sec
Decode latency: 16.417940641287714 sec
Prefill latency: 1.0659976759925485 sec
Decode latency: 16.418658366892487 sec
Prefill latency: 1.0653125047683716 sec
Decode latency: 16.42017658567056 sec
Time for inference 1: 17.49 sec total, 117.11 tokens/sec
Decode latency: 16.42 sec
Prefill latency: 1.07 sec
Bandwidth achieved: 8262.40 GB/s
FLOPS achieved: 24.79 TF/s

Prefill latency: 1.0658242600038648 sec
Decode latency: 16.419431902933866 sec
Time for inference 2: 17.49 sec total, 117.11 tokens/sec
Decode latency: 16.42 sec
Prefill latency: 1.07 sec
Bandwidth achieved: 8262.62 GB/s
FLOPS achieved: 24.79 TF/s

Prefill latency: 1.0653520980849862 sec
Decode latency: 16.42105640284717 sec
Time for inference 3: 17.49 sec total, 117.10 tokens/sec
Decode latency: 16.42 sec
Prefill latency: 1.07 sec
Bandwidth achieved: 8261.97 GB/s
FLOPS achieved: 24.79 TF/s

Prefill latency: 1.065729564987123 sec
Decode latency: 16.419231333769858 sec
Time for inference 4: 17.49 sec total, 117.11 tokens/sec
Decode latency: 16.42 sec
Prefill latency: 1.07 sec
Bandwidth achieved: 8262.61 GB/s
FLOPS achieved: 24.79 TF/s

Prefill latency: 1.065610040910542 sec
Decode latency: 16.419543963856995 sec
Time for inference 5: 17.49 sec total, 117.11 tokens/sec
Decode latency: 16.42 sec
Prefill latency: 1.07 sec
Bandwidth achieved: 8262.56 GB/s
FLOPS achieved: 24.79 TF/s

Prefill latency: 1.0663047009147704 sec
Decode latency: 16.420453743077815 sec
Time for inference 6: 17.49 sec total, 117.10 tokens/sec
Decode latency: 16.42 sec
Prefill latency: 1.07 sec
Bandwidth achieved: 8261.78 GB/s
FLOPS achieved: 24.79 TF/s

Prefill latency: 1.0643797852098942 sec
Decode latency: 16.419047535397112 sec
Time for inference 7: 17.49 sec total, 117.12 tokens/sec
Decode latency: 16.42 sec
Prefill latency: 1.06 sec
Bandwidth achieved: 8263.49 GB/s
FLOPS achieved: 24.79 TF/s

Prefill latency: 1.0667245821096003 sec
Decode latency: 16.4203074676916 sec
Time for inference 8: 17.49 sec total, 117.10 tokens/sec
Decode latency: 16.42 sec
Prefill latency: 1.07 sec
Bandwidth achieved: 8261.58 GB/s
FLOPS achieved: 24.78 TF/s

Prefill latency: 1.0650078663602471 sec
Decode latency: 16.421324649825692 sec
Time for inference 9: 17.49 sec total, 117.10 tokens/sec
Decode latency: 16.42 sec
Prefill latency: 1.07 sec
Bandwidth achieved: 8261.97 GB/s
FLOPS achieved: 24.79 TF/s

Prefill latency: 1.0652323649264872 sec
Decode latency: 16.418936843052506 sec
Time for inference 10: 17.49 sec total, 117.12 tokens/sec
Decode latency: 16.42 sec
Prefill latency: 1.07 sec
Bandwidth achieved: 8263.04 GB/s
FLOPS achieved: 24.79 TF/s

==========
Batch Size: 4
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 16.4200 sec
Average prefill latency: 1.0655 sec
Average tokens/sec: 117.11
Memory used: 77.25 GB
Done. we are killing the process
[rank1]:[W1202 14:40:25.104704460 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank0]:[W1202 14:40:25.193236400 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
