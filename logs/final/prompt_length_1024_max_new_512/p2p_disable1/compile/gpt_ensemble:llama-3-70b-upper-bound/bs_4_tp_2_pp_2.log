W1202 13:14:07.393000 1606936 site-packages/torch/distributed/run.py:793] 
W1202 13:14:07.393000 1606936 site-packages/torch/distributed/run.py:793] *****************************************
W1202 13:14:07.393000 1606936 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1202 13:14:07.393000 1606936 site-packages/torch/distributed/run.py:793] *****************************************
DeviceMesh('cuda', [[0, 1], [2, 3]], mesh_dim_names=('pp', 'tp'))
DeviceMesh('cuda', [[0, 1], [2, 3]], mesh_dim_names=('pp', 'tp'))
DeviceMesh('cuda', [[0, 1], [2, 3]], mesh_dim_names=('pp', 'tp'))
Using device=cuda
Loading model ...
DeviceMesh('cuda', [[0, 1], [2, 3]], mesh_dim_names=('pp', 'tp'))
GPTEnsemble(
  (layers): ModuleList(
    (0-39): 40 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=5120, bias=False)
        (wo): Linear(in_features=4096, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=28672, bias=False)
        (w2): Linear(in_features=14336, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Time to load model: 1.16 seconds
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Compiling prefill with dynamic=False
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
Prefill latency: 75.10568060306832 sec
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 83.42489479598589 secCompilation time: 158.54 seconds

Compilation time: 158.69 seconds
Compilation time: 158.53 seconds
Compilation time: 158.63 seconds
Prefill latency: 0.7118884579977021 sec
Decode latency: 15.069932988961227 sec
Prefill latency: 0.7054152399068698 sec
Decode latency: 15.074975739000365 sec
Prefill latency: 0.7037668960401788 sec
Decode latency: 15.07538678101264 sec
Prefill latency: 0.7092852339847013 sec
Decode latency: 15.067767358967103 sec
Prefill latency: 0.7072352790273726 sec
Decode latency: 15.074165026075207 sec
Time for inference 1: 15.78 sec total, 129.76 tokens/sec
Decode latency: 15.07 sec
Prefill latency: 0.71 sec
Bandwidth achieved: 4713.83 GB/s
FLOPS achieved: 14.14 TF/s

Prefill latency: 0.704686963930726 sec
Decode latency: 15.074545944924466 sec
Time for inference 2: 15.78 sec total, 129.77 tokens/sec
Decode latency: 15.07 sec
Prefill latency: 0.70 sec
Bandwidth achieved: 4714.44 GB/s
FLOPS achieved: 14.14 TF/s

Prefill latency: 0.7054550630273297 sec
Decode latency: 15.0752091340255 sec
Time for inference 3: 15.78 sec total, 129.76 tokens/sec
Decode latency: 15.08 sec
Prefill latency: 0.71 sec
Bandwidth achieved: 4714.00 GB/s
FLOPS achieved: 14.14 TF/s

Prefill latency: 0.704074126901105 sec
Decode latency: 15.07396444992628 sec
Time for inference 4: 15.78 sec total, 129.78 tokens/sec
Decode latency: 15.07 sec
Prefill latency: 0.70 sec
Bandwidth achieved: 4714.85 GB/s
FLOPS achieved: 14.14 TF/s

Prefill latency: 0.7053300399566069 sec
Decode latency: 15.075157446903177 sec
Time for inference 5: 15.78 sec total, 129.76 tokens/sec
Decode latency: 15.08 sec
Prefill latency: 0.71 sec
Bandwidth achieved: 4714.09 GB/s
FLOPS achieved: 14.14 TF/s

Prefill latency: 0.7062873750692233 sec
Decode latency: 15.077891984023154 sec
Time for inference 6: 15.79 sec total, 129.73 tokens/sec
Decode latency: 15.08 sec
Prefill latency: 0.71 sec
Bandwidth achieved: 4713.00 GB/s
FLOPS achieved: 14.14 TF/s

Prefill latency: 0.7074237059568986 sec
Decode latency: 15.071743017993867 sec
Time for inference 7: 15.78 sec total, 129.77 tokens/sec
Decode latency: 15.07 sec
Prefill latency: 0.71 sec
Bandwidth achieved: 4714.48 GB/s
FLOPS achieved: 14.14 TF/s

Prefill latency: 0.7048518060473725 sec
Decode latency: 15.071834529051557 sec
Time for inference 8: 15.78 sec total, 129.79 tokens/sec
Decode latency: 15.07 sec
Prefill latency: 0.70 sec
Bandwidth achieved: 4715.18 GB/s
FLOPS achieved: 14.15 TF/s

Prefill latency: 0.7065003409516066 sec
Decode latency: 15.07774287299253 sec
Time for inference 9: 15.79 sec total, 129.73 tokens/sec
Decode latency: 15.08 sec
Prefill latency: 0.71 sec
Bandwidth achieved: 4712.98 GB/s
FLOPS achieved: 14.14 TF/s

Prefill latency: 0.7046047450276092 sec
Decode latency: 15.074504776042886 sec
Time for inference 10: 15.78 sec total, 129.77 tokens/sec
Decode latency: 15.07 sec
Prefill latency: 0.70 sec
Bandwidth achieved: 4714.46 GB/s
FLOPS achieved: 14.14 TF/s

==========
Batch Size: 4
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 15.0747 sec
Average prefill latency: 0.7056 sec
Average tokens/sec: 129.77
Memory used: 40.68 GB
Done. we are killing the process
[rank0]:[W1202 13:20:34.485062696 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank1]:[W1202 13:20:34.561617000 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank2]:[W1202 13:20:35.383179240 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
