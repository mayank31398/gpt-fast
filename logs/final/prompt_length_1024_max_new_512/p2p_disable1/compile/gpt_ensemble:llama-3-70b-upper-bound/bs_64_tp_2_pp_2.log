W1201 23:55:09.831000 1353417 site-packages/torch/distributed/run.py:793] 
W1201 23:55:09.831000 1353417 site-packages/torch/distributed/run.py:793] *****************************************
W1201 23:55:09.831000 1353417 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1201 23:55:09.831000 1353417 site-packages/torch/distributed/run.py:793] *****************************************
DeviceMesh('cuda', [[0, 1], [2, 3]], mesh_dim_names=('pp', 'tp'))
DeviceMesh('cuda', [[0, 1], [2, 3]], mesh_dim_names=('pp', 'tp'))
DeviceMesh('cuda', [[0, 1], [2, 3]], mesh_dim_names=('pp', 'tp'))
Using device=cuda
Loading model ...
DeviceMesh('cuda', [[0, 1], [2, 3]], mesh_dim_names=('pp', 'tp'))
GPTEnsemble(
  (layers): ModuleList(
    (0-39): 40 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=5120, bias=False)
        (wo): Linear(in_features=4096, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=28672, bias=False)
        (w2): Linear(in_features=14336, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 0.93 seconds
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Compiling prefill with dynamic=False
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Prefill latency: 46.426332511007786 sec
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 47.44625986798201 secCompilation time: 93.86 secondsCompilation time: 93.90 seconds


Compilation time: 93.73 seconds
Compilation time: 93.88 seconds
Prefill latency: 12.708781323046423 sec
Decode latency: 30.106310986098833 sec
Prefill latency: 11.996730476035737 sec
Decode latency: 30.10671216098126 sec
Prefill latency: 11.987674655974843 sec
Decode latency: 30.107147309929132 sec
Prefill latency: 12.062315595918335 sec
Decode latency: 30.110385638079606 sec
Prefill latency: 12.00984575599432 sec
Decode latency: 30.10648386203684 sec
Time for inference 1: 42.12 sec total, 777.99 tokens/sec
Decode latency: 30.11 sec
Prefill latency: 12.01 sec
Bandwidth achieved: 28262.95 GB/s
FLOPS achieved: 84.79 TF/s

Prefill latency: 12.043553273077123 sec
Decode latency: 30.107727746013552 sec
Time for inference 2: 42.15 sec total, 777.34 tokens/sec
Decode latency: 30.11 sec
Prefill latency: 12.04 sec
Bandwidth achieved: 28239.32 GB/s
FLOPS achieved: 84.72 TF/s

Prefill latency: 11.989116857992485 sec
Decode latency: 30.10940720909275 sec
Time for inference 3: 42.10 sec total, 778.31 tokens/sec
Decode latency: 30.11 sec
Prefill latency: 11.99 sec
Bandwidth achieved: 28274.71 GB/s
FLOPS achieved: 84.82 TF/s

Prefill latency: 12.03309908695519 sec
Decode latency: 30.10636196809355 sec
Time for inference 4: 42.14 sec total, 777.56 tokens/sec
Decode latency: 30.11 sec
Prefill latency: 12.03 sec
Bandwidth achieved: 28247.44 GB/s
FLOPS achieved: 84.74 TF/s

Prefill latency: 12.028700624010526 sec
Decode latency: 30.10851799394004 sec
Time for inference 5: 42.14 sec total, 777.60 tokens/sec
Decode latency: 30.11 sec
Prefill latency: 12.03 sec
Bandwidth achieved: 28248.64 GB/s
FLOPS achieved: 84.75 TF/s

Prefill latency: 12.01170594303403 sec
Decode latency: 30.10785248398315 sec
Time for inference 6: 42.12 sec total, 777.93 tokens/sec
Decode latency: 30.11 sec
Prefill latency: 12.01 sec
Bandwidth achieved: 28260.71 GB/s
FLOPS achieved: 84.78 TF/s

Prefill latency: 12.008536670007743 sec
Decode latency: 30.114333268953487 sec
Time for inference 7: 42.13 sec total, 777.86 tokens/sec
Decode latency: 30.11 sec
Prefill latency: 12.01 sec
Bandwidth achieved: 28258.27 GB/s
FLOPS achieved: 84.77 TF/s

Prefill latency: 12.010793577996083 sec
Decode latency: 30.105914138024673 sec
Time for inference 8: 42.12 sec total, 777.98 tokens/sec
Decode latency: 30.11 sec
Prefill latency: 12.01 sec
Bandwidth achieved: 28262.65 GB/s
FLOPS achieved: 84.79 TF/s

Prefill latency: 12.032377415103838 sec
Decode latency: 30.109343084972352 sec
Time for inference 9: 42.14 sec total, 777.52 tokens/sec
Decode latency: 30.11 sec
Prefill latency: 12.03 sec
Bandwidth achieved: 28245.82 GB/s
FLOPS achieved: 84.74 TF/s

Prefill latency: 12.00543706503231 sec
Decode latency: 30.108222323004156 sec
Time for inference 10: 42.12 sec total, 778.03 tokens/sec
Decode latency: 30.11 sec
Prefill latency: 12.01 sec
Bandwidth achieved: 28264.51 GB/s
FLOPS achieved: 84.79 TF/s

==========
Batch Size: 64
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 30.1084 sec
Average prefill latency: 12.0173 sec
Average tokens/sec: 777.81
Memory used: 82.99 GB
Done. we are killing the process
[rank0]:[W1202 00:06:40.185226916 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank1]:[W1202 00:06:40.278548742 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank2]:[W1202 00:06:40.689930955 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
