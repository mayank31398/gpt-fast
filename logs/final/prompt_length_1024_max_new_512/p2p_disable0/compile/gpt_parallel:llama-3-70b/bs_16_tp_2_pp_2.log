W1201 22:20:03.201000 1258232 site-packages/torch/distributed/run.py:793] 
W1201 22:20:03.201000 1258232 site-packages/torch/distributed/run.py:793] *****************************************
W1201 22:20:03.201000 1258232 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1201 22:20:03.201000 1258232 site-packages/torch/distributed/run.py:793] *****************************************
DeviceMesh('cuda', [[0, 1], [2, 3]], mesh_dim_names=('pp', 'tp'))
DeviceMesh('cuda', [[0, 1], [2, 3]], mesh_dim_names=('pp', 'tp'))
DeviceMesh('cuda', [[0, 1], [2, 3]], mesh_dim_names=('pp', 'tp'))
DeviceMesh('cuda', [[0, 1], [2, 3]], mesh_dim_names=('pp', 'tp'))
Using device=cuda
Loading model ...
GPTParallel(
  (layers): ModuleList(
    (0-39): 40 x ParallelTransformerBlock(
      semi_compiled = False
      (attention): FuseAttentionMLP(
        (wqkv1): Linear(in_features=8192, out_features=33792, bias=False)
        (wo): Linear(in_features=4096, out_features=8192, bias=False)
        (w2): Linear(in_features=14336, out_features=8192, bias=False)
      )
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 0.96 seconds
Compiling prefill with dynamic=False
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
Prefill latency: 160.60691844299436 sec
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 113.05648402695078 secCompilation time: 273.64 seconds
Compilation time: 273.61 seconds

Compilation time: 273.67 seconds
Compilation time: 273.60 seconds
Prefill latency: 3.075486625893973 sec
Decode latency: 18.83992604294326 sec
Prefill latency: 3.0822024339577183 sec
Decode latency: 18.84417752409354 sec
Prefill latency: 3.072753806016408 sec
Decode latency: 18.84091172902845 sec
Prefill latency: 3.070168885984458 sec
Decode latency: 18.843294702935964 sec
Prefill latency: 3.076752586988732 sec
Decode latency: 18.839419689960778 sec
Time for inference 1: 21.92 sec total, 373.74 tokens/sec
Decode latency: 18.84 sec
Prefill latency: 3.08 sec
Bandwidth achieved: 13577.13 GB/s
FLOPS achieved: 40.73 TF/s

Prefill latency: 3.0792975450167432 sec
Decode latency: 18.84129597595893 sec
Time for inference 2: 21.92 sec total, 373.67 tokens/sec
Decode latency: 18.84 sec
Prefill latency: 3.08 sec
Bandwidth achieved: 13574.57 GB/s
FLOPS achieved: 40.72 TF/s

Prefill latency: 3.076826842967421 sec
Decode latency: 18.841272480902262 sec
Time for inference 3: 21.92 sec total, 373.71 tokens/sec
Decode latency: 18.84 sec
Prefill latency: 3.08 sec
Bandwidth achieved: 13576.09 GB/s
FLOPS achieved: 40.73 TF/s

Prefill latency: 3.0794164390536025 sec
Decode latency: 18.841442849952728 sec
Time for inference 4: 21.92 sec total, 373.66 tokens/sec
Decode latency: 18.84 sec
Prefill latency: 3.08 sec
Bandwidth achieved: 13574.32 GB/s
FLOPS achieved: 40.72 TF/s

Prefill latency: 3.0777077020611614 sec
Decode latency: 18.84287077607587 sec
Time for inference 5: 21.92 sec total, 373.67 tokens/sec
Decode latency: 18.84 sec
Prefill latency: 3.08 sec
Bandwidth achieved: 13574.50 GB/s
FLOPS achieved: 40.72 TF/s

Prefill latency: 3.0748141629155725 sec
Decode latency: 18.8473545210436 sec
Time for inference 6: 21.92 sec total, 373.64 tokens/sec
Decode latency: 18.85 sec
Prefill latency: 3.07 sec
Bandwidth achieved: 13573.40 GB/s
FLOPS achieved: 40.72 TF/s

Prefill latency: 3.070671801106073 sec
Decode latency: 18.844259427045472 sec
Time for inference 7: 21.92 sec total, 373.76 tokens/sec
Decode latency: 18.84 sec
Prefill latency: 3.07 sec
Bandwidth achieved: 13577.95 GB/s
FLOPS achieved: 40.73 TF/s

Prefill latency: 3.073609145008959 sec
Decode latency: 18.84514745802153 sec
Time for inference 8: 21.92 sec total, 373.70 tokens/sec
Decode latency: 18.85 sec
Prefill latency: 3.07 sec
Bandwidth achieved: 13575.62 GB/s
FLOPS achieved: 40.73 TF/s

Prefill latency: 3.077717875945382 sec
Decode latency: 18.843716721981764 sec
Time for inference 9: 21.92 sec total, 373.66 tokens/sec
Decode latency: 18.84 sec
Prefill latency: 3.08 sec
Bandwidth achieved: 13574.12 GB/s
FLOPS achieved: 40.72 TF/s

Prefill latency: 3.0794681669212878 sec
Decode latency: 18.84376592002809 sec
Time for inference 10: 21.93 sec total, 373.63 tokens/sec
Decode latency: 18.84 sec
Prefill latency: 3.08 sec
Bandwidth achieved: 13572.92 GB/s
FLOPS achieved: 40.72 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 18.8431 sec
Average prefill latency: 3.0766 sec
Average tokens/sec: 373.69
Memory used: 52.57 GB
Done. we are killing the process
[rank1]:[W1201 22:29:50.985259991 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank0]:[W1201 22:29:51.189388657 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank2]:[W1201 22:29:51.002568934 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
