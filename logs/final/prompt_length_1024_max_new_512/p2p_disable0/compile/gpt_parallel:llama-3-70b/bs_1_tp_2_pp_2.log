W1201 21:40:39.186000 1176906 site-packages/torch/distributed/run.py:793] 
W1201 21:40:39.186000 1176906 site-packages/torch/distributed/run.py:793] *****************************************
W1201 21:40:39.186000 1176906 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1201 21:40:39.186000 1176906 site-packages/torch/distributed/run.py:793] *****************************************
DeviceMesh('cuda', [[0, 1], [2, 3]], mesh_dim_names=('pp', 'tp'))
DeviceMesh('cuda', [[0, 1], [2, 3]], mesh_dim_names=('pp', 'tp'))
DeviceMesh('cuda', [[0, 1], [2, 3]], mesh_dim_names=('pp', 'tp'))
Using device=cuda
Loading model ...
DeviceMesh('cuda', [[0, 1], [2, 3]], mesh_dim_names=('pp', 'tp'))
GPTParallel(
  (layers): ModuleList(
    (0-39): 40 x ParallelTransformerBlock(
      semi_compiled = False
      (attention): FuseAttentionMLP(
        (wqkv1): Linear(in_features=8192, out_features=33792, bias=False)
        (wo): Linear(in_features=4096, out_features=8192, bias=False)
        (w2): Linear(in_features=14336, out_features=8192, bias=False)
      )
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 0.96 seconds
Compiling prefill with dynamic=False
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
Prefill latency: 114.83302391204052 sec
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 131.63127770402934 secCompilation time: 246.44 seconds

Compilation time: 246.42 seconds
Compilation time: 246.40 seconds
Compilation time: 246.47 seconds
Prefill latency: 0.19584068993572146 sec
Decode latency: 13.607609572005458 sec
Prefill latency: 0.19251333305146545 sec
Decode latency: 13.609460389940068 sec
Prefill latency: 0.19202035595662892 sec
Decode latency: 13.608452687971294 sec
Prefill latency: 0.19265035702846944 sec
Decode latency: 13.608321464969777 sec
Prefill latency: 0.1935363650554791 sec
Decode latency: 13.607883439050056 sec
Time for inference 1: 13.80 sec total, 37.09 tokens/sec
Decode latency: 13.61 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 1347.35 GB/s
FLOPS achieved: 4.04 TF/s

Prefill latency: 0.19365365291014314 sec
Decode latency: 13.609534739051014 sec
Time for inference 2: 13.81 sec total, 37.09 tokens/sec
Decode latency: 13.61 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 1347.22 GB/s
FLOPS achieved: 4.04 TF/s

Prefill latency: 0.1938345559174195 sec
Decode latency: 13.607952894992195 sec
Time for inference 3: 13.80 sec total, 37.09 tokens/sec
Decode latency: 13.61 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 1347.37 GB/s
FLOPS achieved: 4.04 TF/s

Prefill latency: 0.19253185100387782 sec
Decode latency: 13.608174031949602 sec
Time for inference 4: 13.80 sec total, 37.09 tokens/sec
Decode latency: 13.61 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 1347.45 GB/s
FLOPS achieved: 4.04 TF/s

Prefill latency: 0.1922747310018167 sec
Decode latency: 13.606913528987207 sec
Time for inference 5: 13.80 sec total, 37.10 tokens/sec
Decode latency: 13.61 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 1347.61 GB/s
FLOPS achieved: 4.04 TF/s

Prefill latency: 0.19478293601423502 sec
Decode latency: 13.608636052929796 sec
Time for inference 6: 13.81 sec total, 37.08 tokens/sec
Decode latency: 13.61 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 1347.20 GB/s
FLOPS achieved: 4.04 TF/s

Prefill latency: 0.19416525098495185 sec
Decode latency: 13.608554160920903 sec
Time for inference 7: 13.81 sec total, 37.09 tokens/sec
Decode latency: 13.61 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 1347.27 GB/s
FLOPS achieved: 4.04 TF/s

Prefill latency: 0.19270014797803015 sec
Decode latency: 13.607252195011824 sec
Time for inference 8: 13.80 sec total, 37.09 tokens/sec
Decode latency: 13.61 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 1347.55 GB/s
FLOPS achieved: 4.04 TF/s

Prefill latency: 0.1933526920620352 sec
Decode latency: 13.60648155095987 sec
Time for inference 9: 13.80 sec total, 37.09 tokens/sec
Decode latency: 13.61 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 1347.53 GB/s
FLOPS achieved: 4.04 TF/s

Prefill latency: 0.1939470050856471 sec
Decode latency: 13.606701277079992 sec
Time for inference 10: 13.80 sec total, 37.09 tokens/sec
Decode latency: 13.61 sec
Prefill latency: 0.19 sec
Bandwidth achieved: 1347.47 GB/s
FLOPS achieved: 4.04 TF/s

==========
Batch Size: 1
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 13.6078 sec
Average prefill latency: 0.1935 sec
Average tokens/sec: 37.09
Memory used: 37.63 GB
Done. we are killing the process
[rank0]:[W1201 21:48:06.320946123 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank1]:[W1201 21:48:06.326854937 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank2]:[W1201 21:48:07.228980976 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
