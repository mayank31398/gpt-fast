W1201 10:52:18.472000 422986 site-packages/torch/distributed/run.py:793] 
W1201 10:52:18.472000 422986 site-packages/torch/distributed/run.py:793] *****************************************
W1201 10:52:18.472000 422986 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1201 10:52:18.472000 422986 site-packages/torch/distributed/run.py:793] *****************************************
DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('pp', 'tp'))
DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('pp', 'tp'))
DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('pp', 'tp'))
DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('pp', 'tp'))
DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('pp', 'tp'))
DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('pp', 'tp'))DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('pp', 'tp'))

Using device=cuda
Loading model ...
DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('pp', 'tp'))
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
GPTEnsemble(
  (layers): ModuleList(
    (0-39): 40 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=2560, bias=False)
        (wo): Linear(in_features=2048, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=14336, bias=False)
        (w2): Linear(in_features=7168, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Time to load model: 1.28 seconds
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Compiling prefill with dynamic=False
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/skip-residual/gpt-fast/tmpjgm8racs/main.c:422:1: fatal error: error writing to /tmp/cc9kSUJh.s: No space left on device
  422 | }
      | ^
compilation terminated.
/home/charlie/skip-residual/gpt-fast/tmplnam8dmm/main.c:422:1: fatal error: error writing to /tmp/ccmtA1x6.s: No space left on device
  422 | }
      | ^
compilation terminated.
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 708, in <module>
[rank0]:     main(
[rank0]:   File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 622, in main
[rank0]:     y, decode_latency, prefill_latency = generate(
[rank0]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 312, in generate
[rank0]:     prefill_intermediate = prefill(model, prompt.view(batch_size, -1), input_pos, **sampling_kwargs)
[rank0]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 556, in _fn
[rank0]:     return fn(*args, **kwargs)
[rank0]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 1395, in __call__
[rank0]:     return self._torchdynamo_orig_callable(
[rank0]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 545, in __call__
[rank0]:     return _compile(
[rank0]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 977, in _compile
[rank0]:     guarded_code = compile_inner(code, one_graph, hooks, transform)
[rank0]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 706, in compile_inner
[rank0]:     return _compile_inner(code, one_graph, hooks, transform)
[rank0]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_utils_internal.py", line 95, in wrapper_function
[rank0]:     return function(*args, **kwargs)
[rank0]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 741, in _compile_inner
[rank0]:     out_code = transform_code_object(code, transform)
[rank0]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py", line 1348, in transform_code_object
[rank0]:     transformations(instructions, code_options)
[rank0]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 229, in _fn
[rank0]:     return fn(*args, **kwargs)
[rank0]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 658, in transform
[rank0]:     tracer.run()
[rank0]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 2912, in run
[rank0]:     super().run()
[rank0]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 1120, in run
[rank0]:     while self.step():
[rank0]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 1032, in step
[rank0]:     self.dispatch_table[inst.opcode](self, inst)
[rank0]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 3103, in RETURN_VALUE
[rank0]:     self._return(inst)
[rank0]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 3088, in _return
[rank0]:     self.output.compile_subgraph(
[rank0]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 1077, in compile_subgraph
[rank0]:     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)
[rank0]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 1346, in compile_and_call_fx_graph
[rank0]:     compiled_fn = self.call_user_compiler(gm)
[rank0]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 1396, in call_user_compiler
[rank0]:     return self._call_user_compiler(gm)
[rank0]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 1445, in _call_user_compiler
[rank0]:     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(
[rank0]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 1426, in _call_user_compiler
[rank0]:     compiled_fn = compiler_fn(gm, self.example_inputs())
[rank0]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/repro/after_dynamo.py", line 130, in __call__
[rank0]:     compiled_gm = compiler_fn(gm, example_inputs)
[rank0]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/repro/after_dynamo.py", line 130, in __call__
[rank0]:     compiled_gm = compiler_fn(gm, example_inputs)
[rank0]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/__init__.py", line 2301, in __call__
[rank0]:     return compile_fx(model_, inputs_, config_patches=self.config)
[rank0]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1708, in compile_fx
[rank0]:     return aot_autograd(
[rank0]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/backends/common.py", line 72, in __call__
[rank0]:     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)
[rank0]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1103, in aot_module_simplified
[rank0]:     compiled_fn = dispatch_and_compile()
[rank0]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1079, in dispatch_and_compile
[rank0]:     compiled_fn, _ = create_aot_dispatcher_function(
[rank0]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 527, in create_aot_dispatcher_function
[rank0]:     return _create_aot_dispatcher_function(
[rank0]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 778, in _create_aot_dispatcher_function
[rank0]:     compiled_fn, fw_metadata = compiler_fn(
[rank0]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 197, in aot_dispatch_base
[rank0]:     compiled_fw = compiler(fw_module, updated_flat_args)
[rank0]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1525, in fw_compiler_base
[rank0]:     return _fw_compiler_base(model, example_inputs, is_inference)
[rank0]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1594, in _fw_compiler_base
[rank0]:     return inner_compile(
[rank0]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 587, in compile_fx_inner
[rank0]:     return wrap_compiler_debug(_compile_fx_inner, compiler_name="inductor")(
[rank0]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/repro/after_aot.py", line 102, in debug_wrapper
[rank0]:     inner_compiled_fn = compiler_fn(gm, example_inputs)
[rank0]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 744, in _compile_fx_inner
[rank0]:     compiled_graph = FxGraphCache.load(
[rank0]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 1515, in load
[rank0]:     compiled_graph = compile_fx_fn(
[rank0]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 651, in codegen_and_compile
[rank0]:     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)
[rank0]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 962, in fx_codegen_and_compile
[rank0]:     compiled_fn = graph.compile_to_fn()
[rank0]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/graph.py", line 2034, in compile_to_fn
[rank0]:     return self.compile_to_module().call
[rank0]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/graph.py", line 1954, in compile_to_module
[rank0]:     return self._compile_to_module()
[rank0]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/graph.py", line 1988, in _compile_to_module
[rank0]:     mod = PyCodeCache.load_by_key_path(
[rank0]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 3058, in load_by_key_path
[rank0]:     mod = _reload_python_module(key, path)
[rank0]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/runtime/compile_tasks.py", line 45, in _reload_python_module
[rank0]:     exec(code, mod.__dict__, mod.__dict__)
[rank0]:   File "/home/charlie/skip-residual/gpt-fast/torchinductor_charlie/g5/cg5z5r2tucpj7xb5rurpbo43tdmesttkeqtd5xe3gvqk6kx2z44i.py", line 1310, in <module>
[rank0]:     async_compile.wait(globals())
[rank0]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/async_compile.py", line 319, in wait
[rank0]:     scope[key] = result.result()
[rank0]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 3535, in result
[rank0]:     result = self.future.result()
[rank0]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/concurrent/futures/_base.py", line 458, in result
[rank0]:     return self.__get_result()
[rank0]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
[rank0]:     raise self._exception
[rank0]: torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:
[rank0]: SubprocException: An exception occurred in a subprocess:

[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_worker/subproc_pool.py", line 280, in do_job
[rank0]:     result = job()
[rank0]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/runtime/compile_tasks.py", line 68, in _worker_compile_triton
[rank0]:     load_kernel().precompile(warm_cache_only=True)
[rank0]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/runtime/compile_tasks.py", line 13, in _reload_triton_kernel_in_subproc
[rank0]:     return _module_to_triton_kernel(reload_module(), kernel_name)
[rank0]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/runtime/compile_tasks.py", line 29, in _reload_python_module_in_subproc
[rank0]:     return codecache.PyCodeCache.load_by_key_path(key, path)
[rank0]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 3058, in load_by_key_path
[rank0]:     mod = _reload_python_module(key, path)
[rank0]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/runtime/compile_tasks.py", line 45, in _reload_python_module
[rank0]:     exec(code, mod.__dict__, mod.__dict__)
[rank0]:   File "/home/charlie/skip-residual/gpt-fast/torchinductor_charlie/bf/cbf6zt7fqft2rxogpon5un5mzm6renvzt6d74j7yhnmn7xr7boje.py", line 9, in <module>
[rank0]:     triton_helpers.set_driver_to_gpu()
[rank0]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/runtime/triton_helpers.py", line 57, in set_driver_to_gpu
[rank0]:     driver.set_active(backend.driver())
[rank0]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/triton/backends/nvidia/driver.py", line 371, in __init__
[rank0]:     self.utils = CudaUtils()  # TODO: make static
[rank0]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/triton/backends/nvidia/driver.py", line 80, in __init__
[rank0]:     mod = compile_module_from_src(Path(os.path.join(dirname, "driver.c")).read_text(), "cuda_utils")
[rank0]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/triton/backends/nvidia/driver.py", line 57, in compile_module_from_src
[rank0]:     so = _build(name, src_path, tmpdir, library_dirs(), include_dir, libraries)
[rank0]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/triton/runtime/build.py", line 48, in _build
[rank0]:     ret = subprocess.check_call(cc_cmd)
[rank0]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/subprocess.py", line 369, in check_call
[rank0]:     raise CalledProcessError(retcode, cmd)
[rank0]: subprocess.CalledProcessError: Command '['/usr/bin/gcc', '/home/charlie/skip-residual/gpt-fast/tmpjgm8racs/main.c', '-O3', '-shared', '-fPIC', '-o', '/home/charlie/skip-residual/gpt-fast/tmpjgm8racs/cuda_utils.cpython-310-x86_64-linux-gnu.so', '-lcuda', '-L/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/triton/backends/nvidia/lib', '-L/lib/x86_64-linux-gnu', '-I/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/triton/backends/nvidia/include', '-I/home/charlie/skip-residual/gpt-fast/tmpjgm8racs', '-I/home/charlie/anaconda3/envs/gpt-fast/include/python3.10']' returned non-zero exit status 1.


[rank0]: Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


[rank0]: You can suppress this exception and fall back to eager by setting:
[rank0]:     import torch._dynamo
[rank0]:     torch._dynamo.config.suppress_errors = True

/home/charlie/skip-residual/gpt-fast/tmphrl46m94/main.c:422:1: fatal error: error writing to /tmp/ccKKYsVw.s: No space left on device
  422 | }
      | ^
compilation terminated.
/home/charlie/skip-residual/gpt-fast/tmpf8dkv1cw/main.c:422:1: fatal error: error writing to /tmp/cczp3Dui.s: No space left on device
  422 | }
      | ^
compilation terminated.
/home/charlie/skip-residual/gpt-fast/tmp49zpggyr/main.c:422:1: fatal error: error writing to /tmp/ccdIK2qW.s: No space left on device
  422 | }
      | ^
compilation terminated.
/home/charlie/skip-residual/gpt-fast/tmpbj2hhq7g/main.c:422:1: fatal error: error writing to /tmp/ccaFS8S5.s: No space left on device
  422 | }
      | ^
compilation terminated.
/home/charlie/skip-residual/gpt-fast/tmplr4ufibz/main.c:422:1: fatal error: error writing to /tmp/cc74FVQb.s: No space left on device
  422 | }
      | ^
compilation terminated.
/home/charlie/skip-residual/gpt-fast/tmp2yybsfae/main.c:422:1: fatal error: error writing to /tmp/ccGrDDMX.s: No space left on device
  422 | }
      | ^
compilation terminated.
/home/charlie/skip-residual/gpt-fast/tmp4dx0mxif/main.c:422:1: fatal error: error writing to /tmp/ccfUvjj6.s: No space left on device
  422 | }
      | ^
compilation terminated.
/home/charlie/skip-residual/gpt-fast/tmp6_d253ii/main.c:422:1: fatal error: error writing to /tmp/cc8NWJaq.s: No space left on device
  422 | }
      | ^
compilation terminated.
/home/charlie/skip-residual/gpt-fast/tmpom9k7uja/main.c:422:1: fatal error: error writing to /tmp/ccaobDB4.s: No space left on device
  422 | }
      | ^
compilation terminated.
/home/charlie/skip-residual/gpt-fast/tmpalfq3xbe/main.c:422:1: fatal error: error writing to /tmp/ccMxPrw7.s: No space left on device
  422 | }
      | ^
compilation terminated.
/home/charlie/skip-residual/gpt-fast/tmpb1rvsfka/main.c:422:1: fatal error: error writing to /tmp/ccUVjdZf.s: No space left on device
  422 | }
      | ^
compilation terminated.
/home/charlie/skip-residual/gpt-fast/tmpnfxd2hyp/main.c:422:1: fatal error: error writing to /tmp/cckGBSpA.s: No space left on device
  422 | }
      | ^
compilation terminated.
/home/charlie/skip-residual/gpt-fast/tmp37unmy86/main.c:422:1: fatal error: error writing to /tmp/ccKL2BtL.s: No space left on device
  422 | }
      | ^
compilation terminated.
/home/charlie/skip-residual/gpt-fast/tmpca_gar3c/main.c:422:1: fatal error: error writing to /tmp/ccBaxwi6.s: No space left on device
  422 | }
      | ^
compilation terminated.
/home/charlie/skip-residual/gpt-fast/tmp_4_1awc4/main.c:422:1: fatal error: error writing to /tmp/cce0vulP.s: No space left on device
  422 | }
      | ^
compilation terminated.
/home/charlie/skip-residual/gpt-fast/tmpz786ukdr/main.c:422:1: fatal error: error writing to /tmp/cci73R4f.s: No space left on device
  422 | }
      | ^
compilation terminated.
/home/charlie/skip-residual/gpt-fast/tmp4p8q2yd3/main.c:422:1: fatal error: error writing to /tmp/ccLWaU7V.s: No space left on device
  422 | }
      | ^
compilation terminated.
/home/charlie/skip-residual/gpt-fast/tmpzuhjgpws/main.c:422:1: fatal error: error writing to /tmp/ccTpo7t3.s: No space left on device
  422 | }
      | ^
compilation terminated.
/home/charlie/skip-residual/gpt-fast/tmpxqus2mbm/main.c:422:1: fatal error: error writing to /tmp/ccn8fVPQ.s: No space left on device
  422 | }
      | ^
compilation terminated.
[rank3]: Traceback (most recent call last):
[rank3]:   File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 708, in <module>
[rank3]:     main(
[rank3]:   File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 622, in main
[rank3]:     y, decode_latency, prefill_latency = generate(
[rank3]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank3]:     return func(*args, **kwargs)
[rank3]:   File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 312, in generate
[rank3]:     prefill_intermediate = prefill(model, prompt.view(batch_size, -1), input_pos, **sampling_kwargs)
[rank3]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 556, in _fn
[rank3]:     return fn(*args, **kwargs)
[rank3]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 1395, in __call__
[rank3]:     return self._torchdynamo_orig_callable(
[rank3]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 545, in __call__
[rank3]:     return _compile(
[rank3]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 977, in _compile
[rank3]:     guarded_code = compile_inner(code, one_graph, hooks, transform)
[rank3]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 706, in compile_inner
[rank3]:     return _compile_inner(code, one_graph, hooks, transform)
[rank3]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_utils_internal.py", line 95, in wrapper_function
[rank3]:     return function(*args, **kwargs)
[rank3]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 741, in _compile_inner
[rank3]:     out_code = transform_code_object(code, transform)
[rank3]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py", line 1348, in transform_code_object
[rank3]:     transformations(instructions, code_options)
[rank3]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 229, in _fn
[rank3]:     return fn(*args, **kwargs)
[rank3]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 658, in transform
[rank3]:     tracer.run()
[rank3]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 2912, in run
[rank3]:     super().run()
[rank3]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 1120, in run
[rank3]:     while self.step():
[rank3]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 1032, in step
[rank3]:     self.dispatch_table[inst.opcode](self, inst)
[rank3]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 3103, in RETURN_VALUE
[rank3]:     self._return(inst)
[rank3]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 3088, in _return
[rank3]:     self.output.compile_subgraph(
[rank3]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 1077, in compile_subgraph
[rank3]:     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)
[rank3]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 1346, in compile_and_call_fx_graph
[rank3]:     compiled_fn = self.call_user_compiler(gm)
[rank3]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 1396, in call_user_compiler
[rank3]:     return self._call_user_compiler(gm)
[rank3]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 1445, in _call_user_compiler
[rank3]:     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(
[rank3]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 1426, in _call_user_compiler
[rank3]:     compiled_fn = compiler_fn(gm, self.example_inputs())
[rank3]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/repro/after_dynamo.py", line 130, in __call__
[rank3]:     compiled_gm = compiler_fn(gm, example_inputs)
[rank3]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/repro/after_dynamo.py", line 130, in __call__
[rank3]:     compiled_gm = compiler_fn(gm, example_inputs)
[rank3]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/__init__.py", line 2301, in __call__
[rank3]:     return compile_fx(model_, inputs_, config_patches=self.config)
[rank3]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1708, in compile_fx
[rank3]:     return aot_autograd(
[rank3]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/backends/common.py", line 72, in __call__
[rank3]:     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)
[rank3]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1103, in aot_module_simplified
[rank3]:     compiled_fn = dispatch_and_compile()
[rank3]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1079, in dispatch_and_compile
[rank3]:     compiled_fn, _ = create_aot_dispatcher_function(
[rank3]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 527, in create_aot_dispatcher_function
[rank3]:     return _create_aot_dispatcher_function(
[rank3]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 778, in _create_aot_dispatcher_function
[rank3]:     compiled_fn, fw_metadata = compiler_fn(
[rank3]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 197, in aot_dispatch_base
[rank3]:     compiled_fw = compiler(fw_module, updated_flat_args)
[rank3]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1525, in fw_compiler_base
[rank3]:     return _fw_compiler_base(model, example_inputs, is_inference)
[rank3]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1594, in _fw_compiler_base
[rank3]:     return inner_compile(
[rank3]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 587, in compile_fx_inner
[rank3]:     return wrap_compiler_debug(_compile_fx_inner, compiler_name="inductor")(
[rank3]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/repro/after_aot.py", line 102, in debug_wrapper
[rank3]:     inner_compiled_fn = compiler_fn(gm, example_inputs)
[rank3]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 744, in _compile_fx_inner
[rank3]:     compiled_graph = FxGraphCache.load(
[rank3]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 1515, in load
[rank3]:     compiled_graph = compile_fx_fn(
[rank3]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 651, in codegen_and_compile
[rank3]:     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)
[rank3]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 962, in fx_codegen_and_compile
[rank3]:     compiled_fn = graph.compile_to_fn()
[rank3]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/graph.py", line 2034, in compile_to_fn
[rank3]:     return self.compile_to_module().call
[rank3]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/graph.py", line 1954, in compile_to_module
[rank3]:     return self._compile_to_module()
[rank3]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/graph.py", line 1988, in _compile_to_module
[rank3]:     mod = PyCodeCache.load_by_key_path(
[rank3]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 3058, in load_by_key_path
[rank3]:     mod = _reload_python_module(key, path)
[rank3]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/runtime/compile_tasks.py", line 45, in _reload_python_module
[rank3]:     exec(code, mod.__dict__, mod.__dict__)
[rank3]:   File "/home/charlie/skip-residual/gpt-fast/torchinductor_charlie/ih/cih3qgn6tft6rs3w6ldd36uxmuuq3badtlz7jkqs4qiuodncw3xd.py", line 1310, in <module>
[rank3]:     async_compile.wait(globals())
[rank3]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/async_compile.py", line 319, in wait
[rank3]:     scope[key] = result.result()
[rank3]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 3535, in result
[rank3]:     result = self.future.result()
[rank3]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/concurrent/futures/_base.py", line 458, in result
[rank3]:     return self.__get_result()
[rank3]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
[rank3]:     raise self._exception
[rank3]: torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:
[rank3]: SubprocException: An exception occurred in a subprocess:

[rank3]: Traceback (most recent call last):
[rank3]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_worker/subproc_pool.py", line 280, in do_job
[rank3]:     result = job()
[rank3]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/runtime/compile_tasks.py", line 68, in _worker_compile_triton
[rank3]:     load_kernel().precompile(warm_cache_only=True)
[rank3]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/runtime/compile_tasks.py", line 13, in _reload_triton_kernel_in_subproc
[rank3]:     return _module_to_triton_kernel(reload_module(), kernel_name)
[rank3]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/runtime/compile_tasks.py", line 29, in _reload_python_module_in_subproc
[rank3]:     return codecache.PyCodeCache.load_by_key_path(key, path)
[rank3]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 3058, in load_by_key_path
[rank3]:     mod = _reload_python_module(key, path)
[rank3]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/runtime/compile_tasks.py", line 45, in _reload_python_module
[rank3]:     exec(code, mod.__dict__, mod.__dict__)
[rank3]:   File "/home/charlie/skip-residual/gpt-fast/torchinductor_charlie/ra/cra2kmxtmqxllyub67ynzrwpcj4ltywvfx3v33iduwmbxd55obhq.py", line 9, in <module>
[rank3]:     triton_helpers.set_driver_to_gpu()
[rank3]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/runtime/triton_helpers.py", line 57, in set_driver_to_gpu
[rank3]:     driver.set_active(backend.driver())
[rank3]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/triton/backends/nvidia/driver.py", line 371, in __init__
[rank3]:     self.utils = CudaUtils()  # TODO: make static
[rank3]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/triton/backends/nvidia/driver.py", line 80, in __init__
[rank3]:     mod = compile_module_from_src(Path(os.path.join(dirname, "driver.c")).read_text(), "cuda_utils")
[rank3]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/triton/backends/nvidia/driver.py", line 57, in compile_module_from_src
[rank3]:     so = _build(name, src_path, tmpdir, library_dirs(), include_dir, libraries)
[rank3]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/triton/runtime/build.py", line 48, in _build
[rank3]:     ret = subprocess.check_call(cc_cmd)
[rank3]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/subprocess.py", line 369, in check_call
[rank3]:     raise CalledProcessError(retcode, cmd)
[rank3]: subprocess.CalledProcessError: Command '['/usr/bin/gcc', '/home/charlie/skip-residual/gpt-fast/tmp_4_1awc4/main.c', '-O3', '-shared', '-fPIC', '-o', '/home/charlie/skip-residual/gpt-fast/tmp_4_1awc4/cuda_utils.cpython-310-x86_64-linux-gnu.so', '-lcuda', '-L/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/triton/backends/nvidia/lib', '-L/lib/x86_64-linux-gnu', '-I/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/triton/backends/nvidia/include', '-I/home/charlie/skip-residual/gpt-fast/tmp_4_1awc4', '-I/home/charlie/anaconda3/envs/gpt-fast/include/python3.10']' returned non-zero exit status 1.


[rank3]: Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


[rank3]: You can suppress this exception and fall back to eager by setting:
[rank3]:     import torch._dynamo
[rank3]:     torch._dynamo.config.suppress_errors = True

/home/charlie/skip-residual/gpt-fast/tmpejn5xab9/main.c:422:1: fatal error: error writing to /tmp/ccSPdJ49.s: No space left on device
  422 | }
      | ^
/home/charlie/skip-residual/gpt-fast/tmpsg7c7qfy/main.c:422:1: fatal error: error writing to /tmp/ccjmyAYo.s: No space left on device
  422 | }
      | ^
compilation terminated.
compilation terminated.
[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 708, in <module>
[rank1]:     main(
[rank1]:   File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 622, in main
[rank1]:     y, decode_latency, prefill_latency = generate(
[rank1]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank1]:     return func(*args, **kwargs)
[rank1]:   File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 312, in generate
[rank1]:     prefill_intermediate = prefill(model, prompt.view(batch_size, -1), input_pos, **sampling_kwargs)
[rank1]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 556, in _fn
[rank1]:     return fn(*args, **kwargs)
[rank1]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 1395, in __call__
[rank1]:     return self._torchdynamo_orig_callable(
[rank1]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 545, in __call__
[rank1]:     return _compile(
[rank1]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 977, in _compile
[rank1]:     guarded_code = compile_inner(code, one_graph, hooks, transform)
[rank1]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 706, in compile_inner
[rank1]:     return _compile_inner(code, one_graph, hooks, transform)
[rank1]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_utils_internal.py", line 95, in wrapper_function
[rank1]:     return function(*args, **kwargs)
[rank1]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 741, in _compile_inner
[rank1]:     out_code = transform_code_object(code, transform)
[rank1]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py", line 1348, in transform_code_object
[rank1]:     transformations(instructions, code_options)
[rank1]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 229, in _fn
[rank1]:     return fn(*args, **kwargs)
[rank1]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 658, in transform
[rank1]:     tracer.run()
[rank1]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 2912, in run
[rank1]:     super().run()
[rank1]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 1120, in run
[rank1]:     while self.step():
[rank1]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 1032, in step
[rank1]:     self.dispatch_table[inst.opcode](self, inst)
[rank1]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 3103, in RETURN_VALUE
[rank1]:     self._return(inst)
[rank1]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 3088, in _return
[rank1]:     self.output.compile_subgraph(
[rank1]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 1077, in compile_subgraph
[rank1]:     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)
[rank1]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 1346, in compile_and_call_fx_graph
[rank1]:     compiled_fn = self.call_user_compiler(gm)
[rank1]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 1396, in call_user_compiler
[rank1]:     return self._call_user_compiler(gm)
[rank1]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 1445, in _call_user_compiler
[rank1]:     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(
[rank1]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 1426, in _call_user_compiler
[rank1]:     compiled_fn = compiler_fn(gm, self.example_inputs())
[rank1]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/repro/after_dynamo.py", line 130, in __call__
[rank1]:     compiled_gm = compiler_fn(gm, example_inputs)
[rank1]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/repro/after_dynamo.py", line 130, in __call__
[rank1]:     compiled_gm = compiler_fn(gm, example_inputs)
[rank1]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/__init__.py", line 2301, in __call__
[rank1]:     return compile_fx(model_, inputs_, config_patches=self.config)
[rank1]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1708, in compile_fx
[rank1]:     return aot_autograd(
[rank1]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/backends/common.py", line 72, in __call__
[rank1]:     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)
[rank1]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1103, in aot_module_simplified
[rank1]:     compiled_fn = dispatch_and_compile()
[rank1]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1079, in dispatch_and_compile
[rank1]:     compiled_fn, _ = create_aot_dispatcher_function(
[rank1]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 527, in create_aot_dispatcher_function
[rank1]:     return _create_aot_dispatcher_function(
[rank1]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 778, in _create_aot_dispatcher_function
[rank1]:     compiled_fn, fw_metadata = compiler_fn(
[rank1]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 197, in aot_dispatch_base
[rank1]:     compiled_fw = compiler(fw_module, updated_flat_args)
[rank1]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1525, in fw_compiler_base
[rank1]:     return _fw_compiler_base(model, example_inputs, is_inference)
[rank1]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1594, in _fw_compiler_base
[rank1]:     return inner_compile(
[rank1]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 587, in compile_fx_inner
[rank1]:     return wrap_compiler_debug(_compile_fx_inner, compiler_name="inductor")(
[rank1]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/repro/after_aot.py", line 102, in debug_wrapper
[rank1]:     inner_compiled_fn = compiler_fn(gm, example_inputs)
[rank1]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 744, in _compile_fx_inner
[rank1]:     compiled_graph = FxGraphCache.load(
[rank1]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 1515, in load
[rank1]:     compiled_graph = compile_fx_fn(
[rank1]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 651, in codegen_and_compile
[rank1]:     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)
[rank1]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 962, in fx_codegen_and_compile
[rank1]:     compiled_fn = graph.compile_to_fn()
[rank1]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/graph.py", line 2034, in compile_to_fn
[rank1]:     return self.compile_to_module().call
[rank1]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/graph.py", line 1954, in compile_to_module
[rank1]:     return self._compile_to_module()
[rank1]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/graph.py", line 1988, in _compile_to_module
[rank1]:     mod = PyCodeCache.load_by_key_path(
[rank1]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 3058, in load_by_key_path
[rank1]:     mod = _reload_python_module(key, path)
[rank1]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/runtime/compile_tasks.py", line 45, in _reload_python_module
[rank1]:     exec(code, mod.__dict__, mod.__dict__)
[rank1]:   File "/home/charlie/skip-residual/gpt-fast/torchinductor_charlie/x6/cx6qjge7f3qbek4qhxego5ukditir5ytu2g436t4zi4coxfvprd3.py", line 1310, in <module>
[rank1]:     async_compile.wait(globals())
[rank1]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/async_compile.py", line 319, in wait
[rank1]:     scope[key] = result.result()
[rank1]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 3535, in result
[rank1]:     result = self.future.result()
[rank1]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/concurrent/futures/_base.py", line 458, in result
[rank1]:     return self.__get_result()
[rank1]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
[rank1]:     raise self._exception
[rank1]: torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:
[rank1]: SubprocException: An exception occurred in a subprocess:

[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_worker/subproc_pool.py", line 280, in do_job
[rank1]:     result = job()
[rank1]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/runtime/compile_tasks.py", line 68, in _worker_compile_triton
[rank1]:     load_kernel().precompile(warm_cache_only=True)
[rank1]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/runtime/compile_tasks.py", line 13, in _reload_triton_kernel_in_subproc
[rank1]:     return _module_to_triton_kernel(reload_module(), kernel_name)
[rank1]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/runtime/compile_tasks.py", line 29, in _reload_python_module_in_subproc
[rank1]:     return codecache.PyCodeCache.load_by_key_path(key, path)
[rank1]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 3058, in load_by_key_path
[rank1]:     mod = _reload_python_module(key, path)
[rank1]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/runtime/compile_tasks.py", line 45, in _reload_python_module
[rank1]:     exec(code, mod.__dict__, mod.__dict__)
[rank1]:   File "/home/charlie/skip-residual/gpt-fast/torchinductor_charlie/pf/cpfufnyiu7xs7ydtwci5musv2tugkbvywb3gifynwxkzpbwynmxl.py", line 9, in <module>
[rank1]:     triton_helpers.set_driver_to_gpu()
[rank1]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/runtime/triton_helpers.py", line 57, in set_driver_to_gpu
[rank1]:     driver.set_active(backend.driver())
[rank1]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/triton/backends/nvidia/driver.py", line 371, in __init__
[rank1]:     self.utils = CudaUtils()  # TODO: make static
[rank1]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/triton/backends/nvidia/driver.py", line 80, in __init__
[rank1]:     mod = compile_module_from_src(Path(os.path.join(dirname, "driver.c")).read_text(), "cuda_utils")
[rank1]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/triton/backends/nvidia/driver.py", line 57, in compile_module_from_src
[rank1]:     so = _build(name, src_path, tmpdir, library_dirs(), include_dir, libraries)
[rank1]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/triton/runtime/build.py", line 48, in _build
[rank1]:     ret = subprocess.check_call(cc_cmd)
[rank1]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/subprocess.py", line 369, in check_call
[rank1]:     raise CalledProcessError(retcode, cmd)
[rank1]: subprocess.CalledProcessError: Command '['/usr/bin/gcc', '/home/charlie/skip-residual/gpt-fast/tmpzuhjgpws/main.c', '-O3', '-shared', '-fPIC', '-o', '/home/charlie/skip-residual/gpt-fast/tmpzuhjgpws/cuda_utils.cpython-310-x86_64-linux-gnu.so', '-lcuda', '-L/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/triton/backends/nvidia/lib', '-L/lib/x86_64-linux-gnu', '-I/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/triton/backends/nvidia/include', '-I/home/charlie/skip-residual/gpt-fast/tmpzuhjgpws', '-I/home/charlie/anaconda3/envs/gpt-fast/include/python3.10']' returned non-zero exit status 1.


[rank1]: Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


[rank1]: You can suppress this exception and fall back to eager by setting:
[rank1]:     import torch._dynamo
[rank1]:     torch._dynamo.config.suppress_errors = True

/home/charlie/skip-residual/gpt-fast/tmpwtw4cpdy/main.c:422:1: fatal error: error writing to /tmp/cc0mvgUh.s: No space left on device
  422 | }
      | ^
compilation terminated.
/home/charlie/skip-residual/gpt-fast/tmp3tmg5fv5/main.c:422:1: fatal error: error writing to /tmp/cc2QYQNH.s: No space left on device
  422 | }
      | ^
compilation terminated.
/home/charlie/skip-residual/gpt-fast/tmp_f_uqdp8/main.c:422:1: fatal error: error writing to /tmp/cc61M5Pp.s: No space left on device
  422 | }
      | ^
compilation terminated.
/home/charlie/skip-residual/gpt-fast/tmpm8xs5fdp/main.c:422:1: fatal error: error writing to /tmp/ccoAMv6O.s: No space left on device
  422 | }
      | ^
compilation terminated.
/home/charlie/skip-residual/gpt-fast/tmphwz6rvp3/main.c:422:1: fatal error: error writing to /tmp/ccrGV5Wm.s: No space left on device
  422 | }
      | ^
compilation terminated.
/home/charlie/skip-residual/gpt-fast/tmp2r1g9bq8/main.c:422:1: fatal error: error writing to /tmp/ccdgGxFK.s: No space left on device
  422 | }
      | ^
compilation terminated.
/home/charlie/skip-residual/gpt-fast/tmp488q_r22/main.c:422:1: fatal error: error writing to /tmp/ccDOJ5o6.s: No space left on device
  422 | }
      | ^
compilation terminated.
/home/charlie/skip-residual/gpt-fast/tmpgrohb3mw/main.c:422:1: fatal error: error writing to /tmp/ccOqKyPX.s: No space left on device
  422 | }
      | ^
compilation terminated.
/home/charlie/skip-residual/gpt-fast/tmpq8tlncf8/main.c:422:1: fatal error: error writing to /tmp/ccLhOMD7.s: No space left on device
  422 | }
      | ^
compilation terminated.
[rank2]: Traceback (most recent call last):
[rank2]:   File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 708, in <module>
[rank2]:     main(
[rank2]:   File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 622, in main
[rank2]:     y, decode_latency, prefill_latency = generate(
[rank2]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank2]:     return func(*args, **kwargs)
[rank2]:   File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 312, in generate
[rank2]:     prefill_intermediate = prefill(model, prompt.view(batch_size, -1), input_pos, **sampling_kwargs)
[rank2]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 556, in _fn
[rank2]:     return fn(*args, **kwargs)
[rank2]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 1395, in __call__
[rank2]:     return self._torchdynamo_orig_callable(
[rank2]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 545, in __call__
[rank2]:     return _compile(
[rank2]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 977, in _compile
[rank2]:     guarded_code = compile_inner(code, one_graph, hooks, transform)
[rank2]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 706, in compile_inner
[rank2]:     return _compile_inner(code, one_graph, hooks, transform)
[rank2]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_utils_internal.py", line 95, in wrapper_function
[rank2]:     return function(*args, **kwargs)
[rank2]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 741, in _compile_inner
[rank2]:     out_code = transform_code_object(code, transform)
[rank2]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py", line 1348, in transform_code_object
[rank2]:     transformations(instructions, code_options)
[rank2]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 229, in _fn
[rank2]:     return fn(*args, **kwargs)
[rank2]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 658, in transform
[rank2]:     tracer.run()
[rank2]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 2912, in run
[rank2]:     super().run()
[rank2]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 1120, in run
[rank2]:     while self.step():
[rank2]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 1032, in step
[rank2]:     self.dispatch_table[inst.opcode](self, inst)
[rank2]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 3103, in RETURN_VALUE
[rank2]:     self._return(inst)
[rank2]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 3088, in _return
[rank2]:     self.output.compile_subgraph(
[rank2]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 1077, in compile_subgraph
[rank2]:     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)
[rank2]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 1346, in compile_and_call_fx_graph
[rank2]:     compiled_fn = self.call_user_compiler(gm)
[rank2]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 1396, in call_user_compiler
[rank2]:     return self._call_user_compiler(gm)
[rank2]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 1445, in _call_user_compiler
[rank2]:     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(
[rank2]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 1426, in _call_user_compiler
[rank2]:     compiled_fn = compiler_fn(gm, self.example_inputs())
[rank2]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/repro/after_dynamo.py", line 130, in __call__
[rank2]:     compiled_gm = compiler_fn(gm, example_inputs)
[rank2]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/repro/after_dynamo.py", line 130, in __call__
[rank2]:     compiled_gm = compiler_fn(gm, example_inputs)
[rank2]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/__init__.py", line 2301, in __call__
[rank2]:     return compile_fx(model_, inputs_, config_patches=self.config)
[rank2]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1708, in compile_fx
[rank2]:     return aot_autograd(
[rank2]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/backends/common.py", line 72, in __call__
[rank2]:     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)
[rank2]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1103, in aot_module_simplified
[rank2]:     compiled_fn = dispatch_and_compile()
[rank2]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1079, in dispatch_and_compile
[rank2]:     compiled_fn, _ = create_aot_dispatcher_function(
[rank2]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 527, in create_aot_dispatcher_function
[rank2]:     return _create_aot_dispatcher_function(
[rank2]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 778, in _create_aot_dispatcher_function
[rank2]:     compiled_fn, fw_metadata = compiler_fn(
[rank2]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 197, in aot_dispatch_base
[rank2]:     compiled_fw = compiler(fw_module, updated_flat_args)
[rank2]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1525, in fw_compiler_base
[rank2]:     return _fw_compiler_base(model, example_inputs, is_inference)
[rank2]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1594, in _fw_compiler_base
[rank2]:     return inner_compile(
[rank2]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 587, in compile_fx_inner
[rank2]:     return wrap_compiler_debug(_compile_fx_inner, compiler_name="inductor")(
[rank2]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_dynamo/repro/after_aot.py", line 102, in debug_wrapper
[rank2]:     inner_compiled_fn = compiler_fn(gm, example_inputs)
[rank2]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 744, in _compile_fx_inner
[rank2]:     compiled_graph = FxGraphCache.load(
[rank2]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 1515, in load
[rank2]:     compiled_graph = compile_fx_fn(
[rank2]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 651, in codegen_and_compile
[rank2]:     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)
[rank2]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 962, in fx_codegen_and_compile
[rank2]:     compiled_fn = graph.compile_to_fn()
[rank2]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/graph.py", line 2034, in compile_to_fn
[rank2]:     return self.compile_to_module().call
[rank2]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/graph.py", line 1954, in compile_to_module
[rank2]:     return self._compile_to_module()
[rank2]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/graph.py", line 1988, in _compile_to_module
[rank2]:     mod = PyCodeCache.load_by_key_path(
[rank2]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 3058, in load_by_key_path
[rank2]:     mod = _reload_python_module(key, path)
[rank2]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/runtime/compile_tasks.py", line 45, in _reload_python_module
[rank2]:     exec(code, mod.__dict__, mod.__dict__)
[rank2]:   File "/home/charlie/skip-residual/gpt-fast/torchinductor_charlie/dk/cdktqh4shn2iz765trdqwxsuhkhonfm25xtcj4e77ny235lenf6u.py", line 1310, in <module>
[rank2]:     async_compile.wait(globals())
[rank2]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/async_compile.py", line 319, in wait
[rank2]:     scope[key] = result.result()
[rank2]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 3535, in result
[rank2]:     result = self.future.result()
[rank2]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/concurrent/futures/_base.py", line 451, in result
[rank2]:     return self.__get_result()
[rank2]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
[rank2]:     raise self._exception
[rank2]: torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:
[rank2]: SubprocException: An exception occurred in a subprocess:

[rank2]: Traceback (most recent call last):
[rank2]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_worker/subproc_pool.py", line 280, in do_job
[rank2]:     result = job()
[rank2]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/runtime/compile_tasks.py", line 68, in _worker_compile_triton
[rank2]:     load_kernel().precompile(warm_cache_only=True)
[rank2]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/runtime/compile_tasks.py", line 13, in _reload_triton_kernel_in_subproc
[rank2]:     return _module_to_triton_kernel(reload_module(), kernel_name)
[rank2]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/runtime/compile_tasks.py", line 29, in _reload_python_module_in_subproc
[rank2]:     return codecache.PyCodeCache.load_by_key_path(key, path)
[rank2]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 3058, in load_by_key_path
[rank2]:     mod = _reload_python_module(key, path)
[rank2]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/runtime/compile_tasks.py", line 45, in _reload_python_module
[rank2]:     exec(code, mod.__dict__, mod.__dict__)
[rank2]:   File "/home/charlie/skip-residual/gpt-fast/torchinductor_charlie/ry/cry2v3jp2mfocbuvl63otpypqfcdvwucjckojc5d2v6h4bpmk66c.py", line 9, in <module>
[rank2]:     triton_helpers.set_driver_to_gpu()
[rank2]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/runtime/triton_helpers.py", line 57, in set_driver_to_gpu
[rank2]:     driver.set_active(backend.driver())
[rank2]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/triton/backends/nvidia/driver.py", line 371, in __init__
[rank2]:     self.utils = CudaUtils()  # TODO: make static
[rank2]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/triton/backends/nvidia/driver.py", line 80, in __init__
[rank2]:     mod = compile_module_from_src(Path(os.path.join(dirname, "driver.c")).read_text(), "cuda_utils")
[rank2]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/triton/backends/nvidia/driver.py", line 57, in compile_module_from_src
[rank2]:     so = _build(name, src_path, tmpdir, library_dirs(), include_dir, libraries)
[rank2]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/triton/runtime/build.py", line 48, in _build
[rank2]:     ret = subprocess.check_call(cc_cmd)
[rank2]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/subprocess.py", line 369, in check_call
[rank2]:     raise CalledProcessError(retcode, cmd)
[rank2]: subprocess.CalledProcessError: Command '['/usr/bin/gcc', '/home/charlie/skip-residual/gpt-fast/tmp3tmg5fv5/main.c', '-O3', '-shared', '-fPIC', '-o', '/home/charlie/skip-residual/gpt-fast/tmp3tmg5fv5/cuda_utils.cpython-310-x86_64-linux-gnu.so', '-lcuda', '-L/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/triton/backends/nvidia/lib', '-L/lib/x86_64-linux-gnu', '-I/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/triton/backends/nvidia/include', '-I/home/charlie/skip-residual/gpt-fast/tmp3tmg5fv5', '-I/home/charlie/anaconda3/envs/gpt-fast/include/python3.10']' returned non-zero exit status 1.


[rank2]: Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


[rank2]: You can suppress this exception and fall back to eager by setting:
[rank2]:     import torch._dynamo
[rank2]:     torch._dynamo.config.suppress_errors = True

/home/charlie/skip-residual/gpt-fast/tmpkev_cxwb/main.c:422:1: fatal error: error writing to /tmp/ccnCsAu3.s: No space left on device
  422 | }
      | ^
compilation terminated.
/home/charlie/skip-residual/gpt-fast/tmpjhebhrgj/main.c:422:1: fatal error: error writing to /tmp/ccl2lodG.s: No space left on device
  422 | }
      | ^
compilation terminated.
/home/charlie/skip-residual/gpt-fast/tmpx0rhsmw8/main.c:422:1: fatal error: error writing to /tmp/cczEKpJG.s: No space left on device
  422 | }
      | ^
compilation terminated.
/home/charlie/skip-residual/gpt-fast/tmp3q8242vr/main.c:422:1: fatal error: error writing to /tmp/cctFWa4z.s: No space left on device
  422 | }
      | ^
compilation terminated.
/home/charlie/skip-residual/gpt-fast/tmpvow647ey/main.c:422:1: fatal error: error writing to /tmp/cctKWEeq.s: No space left on device
  422 | }
      | ^
compilation terminated.
/home/charlie/skip-residual/gpt-fast/tmpjyt9y1s7/main.c:422:1: fatal error: error writing to /tmp/ccnBpe78.s: No space left on device
  422 | }
      | ^
compilation terminated.
/home/charlie/skip-residual/gpt-fast/tmpmlu6lbp8/main.c:422:1: fatal error: error writing to /tmp/cckQb8B8.s: No space left on device
  422 | }
      | ^
compilation terminated.
/home/charlie/skip-residual/gpt-fast/tmp8uwpjnaw/main.c:422:1: fatal error: error writing to /tmp/ccCIMF01.s: No space left on device
  422 | }
      | ^
compilation terminated.
/home/charlie/skip-residual/gpt-fast/tmp50vyn0vf/main.c:422:1: fatal error: error writing to /tmp/ccOOHgJs.s: No space left on device
  422 | }
      | ^
compilation terminated.
/home/charlie/skip-residual/gpt-fast/tmp6n6n30go/main.c:422:1: fatal error: error writing to /tmp/cckkJjwi.s: No space left on device
  422 | }
      | ^
compilation terminated.
/home/charlie/skip-residual/gpt-fast/tmpify2rcvw/main.c:422:1: fatal error: error writing to /tmp/cc1R8R4f.s: No space left on device
  422 | }
      | ^
compilation terminated.
/home/charlie/skip-residual/gpt-fast/tmp8hsb51qk/main.c:422:1: fatal error: error writing to /tmp/ccbARFxE.s: No space left on device
  422 | }
      | ^
compilation terminated.
/home/charlie/skip-residual/gpt-fast/tmpwqt_6h3_/main.c:422:1: fatal error: error writing to /tmp/ccwXzNjw.s: No space left on device
  422 | }
      | ^
compilation terminated.
/home/charlie/skip-residual/gpt-fast/tmp1hc730ot/main.c:422:1: fatal error: error writing to /tmp/cc8eyPTg.s: No space left on device
  422 | }
      | ^
compilation terminated.
/home/charlie/skip-residual/gpt-fast/tmpo8zwijt4/main.c:422:1: fatal error: error writing to /tmp/cc5vJPeM.s: No space left on device
  422 | }
      | ^
compilation terminated.
/home/charlie/skip-residual/gpt-fast/tmp4z80m6ag/main.c:422:1: fatal error: error writing to /tmp/ccNUYGaG.s: No space left on device
  422 | }
      | ^
compilation terminated.
/home/charlie/skip-residual/gpt-fast/tmprurptgd2/main.c:422:1: fatal error: error writing to /tmp/ccTJUaK7.s: No space left on device
  422 | }
      | ^
compilation terminated.
/home/charlie/skip-residual/gpt-fast/tmpzgxi0_q2/main.c:422:1: fatal error: error writing to /tmp/cccPq4Vo.s: No space left on device
  422 | }
      | ^
compilation terminated.
/home/charlie/skip-residual/gpt-fast/tmp37_rxke2/main.c:422:1: fatal error: error writing to /tmp/cczKakCx.s: No space left on device
  422 | }
      | ^
compilation terminated.
/home/charlie/skip-residual/gpt-fast/tmpnesr18tj/main.c:422:1: fatal error: error writing to /tmp/ccjjy1A1.s: No space left on device
  422 | }
      | ^
compilation terminated.
/home/charlie/skip-residual/gpt-fast/tmpa8ipzh2o/main.c:422:1: fatal error: error writing to /tmp/ccfcHkff.s: No space left on device
  422 | }
      | ^
compilation terminated.
/home/charlie/skip-residual/gpt-fast/tmphort0t_s/main.c:422:1: fatal error: error writing to /tmp/ccJa5hUR.s: No space left on device
  422 | }
      | ^
compilation terminated.
/home/charlie/skip-residual/gpt-fast/tmpdwgy96z3/main.c:422:1: fatal error: error writing to /tmp/ccVEA8WZ.s: No space left on device
  422 | }
      | ^
compilation terminated.
/home/charlie/skip-residual/gpt-fast/tmpamzacop8/main.c:422:1: fatal error: error writing to /tmp/ccNtditu.s: No space left on device
  422 | }
      | ^
compilation terminated.
/home/charlie/skip-residual/gpt-fast/tmpa70v7h2y/main.c:422:1: fatal error: error writing to /tmp/ccl3rNzM.s: No space left on device
  422 | }
      | ^
compilation terminated.
/home/charlie/skip-residual/gpt-fast/tmpiik8hqn6/main.c:422:1: fatal error: error writing to /tmp/ccornvJf.s: No space left on device
  422 | }
      | ^
compilation terminated.
/home/charlie/skip-residual/gpt-fast/tmp9hxlb7tl/main.c:422:1: fatal error: error writing to /tmp/ccexwu8s.s: No space left on device
  422 | }
      | ^
compilation terminated.
/home/charlie/skip-residual/gpt-fast/tmpgxamwb2a/main.c:422:1: fatal error: error writing to /tmp/ccG8rG3K.s: No space left on device
  422 | }
      | ^
compilation terminated.
/home/charlie/skip-residual/gpt-fast/tmpcp6slrl0/main.c:422:1: fatal error: error writing to /tmp/cc1ewiOJ.s: No space left on device
  422 | }
      | ^
compilation terminated.
/home/charlie/skip-residual/gpt-fast/tmpm0sqy89i/main.c:422:1: fatal error: error writing to /tmp/ccQHRC5x.s: No space left on device
  422 | }
      | ^
compilation terminated.
/home/charlie/skip-residual/gpt-fast/tmpygwg0871/main.c:422:1: fatal error: error writing to /tmp/cc8rjV02.s: No space left on device
  422 | }
      | ^
compilation terminated.
/home/charlie/skip-residual/gpt-fast/tmpd_zpow38/main.c:422:1: fatal error: error writing to /tmp/cc5cFgHQ.s: No space left on device
  422 | }
      | ^
compilation terminated.
[rank0]:[W1201 10:52:57.659689104 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank2]:[W1201 10:52:58.368751595 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank3]:[W1201 10:52:58.802153747 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank1]:[W1201 10:52:58.978177896 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W1201 10:52:59.371000 422986 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 423058 closing signal SIGTERM
W1201 10:52:59.374000 422986 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 423059 closing signal SIGTERM
W1201 10:52:59.377000 422986 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 423060 closing signal SIGTERM
W1201 10:52:59.381000 422986 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 423061 closing signal SIGTERM
W1201 10:52:59.384000 422986 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 423062 closing signal SIGTERM
W1201 10:52:59.386000 422986 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 423063 closing signal SIGTERM
W1201 10:52:59.390000 422986 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 423064 closing signal SIGTERM
E1201 10:52:59.943000 422986 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 423057) of binary: /home/charlie/anaconda3/envs/gpt-fast/bin/python
Traceback (most recent call last):
  File "/home/charlie/anaconda3/envs/gpt-fast/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
benchmark.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-12-01_10:52:59
  host      : mk-xii-22.cloud.together.ai
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 423057)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
