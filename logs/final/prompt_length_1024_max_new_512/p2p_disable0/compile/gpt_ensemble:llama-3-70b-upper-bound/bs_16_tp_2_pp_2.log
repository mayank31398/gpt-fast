W1201 20:41:14.142000 1089782 site-packages/torch/distributed/run.py:793] 
W1201 20:41:14.142000 1089782 site-packages/torch/distributed/run.py:793] *****************************************
W1201 20:41:14.142000 1089782 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1201 20:41:14.142000 1089782 site-packages/torch/distributed/run.py:793] *****************************************
DeviceMesh('cuda', [[0, 1], [2, 3]], mesh_dim_names=('pp', 'tp'))
DeviceMesh('cuda', [[0, 1], [2, 3]], mesh_dim_names=('pp', 'tp'))
Using device=cuda
Loading model ...
DeviceMesh('cuda', [[0, 1], [2, 3]], mesh_dim_names=('pp', 'tp'))
DeviceMesh('cuda', [[0, 1], [2, 3]], mesh_dim_names=('pp', 'tp'))
GPTEnsemble(
  (layers): ModuleList(
    (0-39): 40 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=5120, bias=False)
        (wo): Linear(in_features=4096, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=28672, bias=False)
        (w2): Linear(in_features=14336, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 1.09 seconds
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Compiling prefill with dynamic=False
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
Prefill latency: 174.17074554995634 sec
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 131.41148813196924 secCompilation time: 305.62 seconds
Compilation time: 305.61 seconds

Compilation time: 305.66 seconds
Compilation time: 305.59 seconds
Prefill latency: 3.008589399047196 sec
Decode latency: 18.177690605982207 sec
Prefill latency: 3.0202951120445505 sec
Decode latency: 18.1718298610067 sec
Prefill latency: 3.0162464278982952 sec
Decode latency: 18.173004879965447 sec
Prefill latency: 3.0237936850171536 sec
Decode latency: 18.1809052110184 sec
Prefill latency: 3.0198377890046686 sec
Decode latency: 18.17426219501067 sec
Time for inference 1: 21.20 sec total, 386.47 tokens/sec
Decode latency: 18.17 sec
Prefill latency: 3.02 sec
Bandwidth achieved: 14039.75 GB/s
FLOPS achieved: 42.12 TF/s

Prefill latency: 3.0203783320030198 sec
Decode latency: 18.174261350999586 sec
Time for inference 2: 21.20 sec total, 386.47 tokens/sec
Decode latency: 18.17 sec
Prefill latency: 3.02 sec
Bandwidth achieved: 14039.58 GB/s
FLOPS achieved: 42.12 TF/s

Prefill latency: 3.0264370939694345 sec
Decode latency: 18.175500074052252 sec
Time for inference 3: 21.20 sec total, 386.33 tokens/sec
Decode latency: 18.18 sec
Prefill latency: 3.03 sec
Bandwidth achieved: 14034.66 GB/s
FLOPS achieved: 42.10 TF/s

Prefill latency: 3.0198407099815086 sec
Decode latency: 18.176033126888797 sec
Time for inference 4: 21.20 sec total, 386.44 tokens/sec
Decode latency: 18.18 sec
Prefill latency: 3.02 sec
Bandwidth achieved: 14038.73 GB/s
FLOPS achieved: 42.12 TF/s

Prefill latency: 3.0172340020071715 sec
Decode latency: 18.17425809998531 sec
Time for inference 5: 21.19 sec total, 386.52 tokens/sec
Decode latency: 18.17 sec
Prefill latency: 3.02 sec
Bandwidth achieved: 14041.55 GB/s
FLOPS achieved: 42.12 TF/s

Prefill latency: 3.0191274440148845 sec
Decode latency: 18.173230255022645 sec
Time for inference 6: 21.20 sec total, 386.51 tokens/sec
Decode latency: 18.17 sec
Prefill latency: 3.02 sec
Bandwidth achieved: 14041.06 GB/s
FLOPS achieved: 42.12 TF/s

Prefill latency: 3.0201021920656785 sec
Decode latency: 18.173304334981367 sec
Time for inference 7: 21.20 sec total, 386.49 tokens/sec
Decode latency: 18.17 sec
Prefill latency: 3.02 sec
Bandwidth achieved: 14040.36 GB/s
FLOPS achieved: 42.12 TF/s

Prefill latency: 3.0175612770253792 sec
Decode latency: 18.177245147060603 sec
Time for inference 8: 21.20 sec total, 386.46 tokens/sec
Decode latency: 18.18 sec
Prefill latency: 3.02 sec
Bandwidth achieved: 14039.35 GB/s
FLOPS achieved: 42.12 TF/s

Prefill latency: 3.016720140934922 sec
Decode latency: 18.173719573998824 sec
Time for inference 9: 21.19 sec total, 386.53 tokens/sec
Decode latency: 18.17 sec
Prefill latency: 3.02 sec
Bandwidth achieved: 14041.99 GB/s
FLOPS achieved: 42.13 TF/s

Prefill latency: 3.0160332329105586 sec
Decode latency: 18.176706639002077 sec
Time for inference 10: 21.20 sec total, 386.49 tokens/sec
Decode latency: 18.18 sec
Prefill latency: 3.02 sec
Bandwidth achieved: 14040.64 GB/s
FLOPS achieved: 42.12 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 18.1749 sec
Average prefill latency: 3.0193 sec
Average tokens/sec: 386.47
Memory used: 52.75 GB
Done. we are killing the process
[rank0]:[W1201 20:51:23.868745462 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank1]:[W1201 20:51:24.075411234 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank2]:[W1201 20:51:24.459035520 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
