W1201 21:04:47.809000 1132269 site-packages/torch/distributed/run.py:793] 
W1201 21:04:47.809000 1132269 site-packages/torch/distributed/run.py:793] *****************************************
W1201 21:04:47.809000 1132269 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1201 21:04:47.809000 1132269 site-packages/torch/distributed/run.py:793] *****************************************
DeviceMesh('cuda', [[0, 1], [2, 3]], mesh_dim_names=('pp', 'tp'))
DeviceMesh('cuda', [[0, 1], [2, 3]], mesh_dim_names=('pp', 'tp'))
Using device=cuda
Loading model ...
DeviceMesh('cuda', [[0, 1], [2, 3]], mesh_dim_names=('pp', 'tp'))
DeviceMesh('cuda', [[0, 1], [2, 3]], mesh_dim_names=('pp', 'tp'))
GPTEnsemble(
  (layers): ModuleList(
    (0-39): 40 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=5120, bias=False)
        (wo): Linear(in_features=4096, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=28672, bias=False)
        (w2): Linear(in_features=14336, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 1.08 seconds
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Compiling prefill with dynamic=False
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
Prefill latency: 251.86537748307455 sec
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 146.4981433399953 sec
Compilation time: 398.44 seconds
Compilation time: 398.36 seconds
Compilation time: 398.37 seconds
Compilation time: 398.40 seconds
Prefill latency: 12.62159143993631 sec
Decode latency: 30.15339822100941 sec
Prefill latency: 12.00875657692086 sec
Decode latency: 30.151206162991002 sec
Prefill latency: 11.99248685198836 sec
Decode latency: 30.150377096026205 sec
Prefill latency: 12.034445849945769 sec
Decode latency: 30.151659065973945 sec
Prefill latency: 11.964670401997864 sec
Decode latency: 30.152344783069566 sec
Time for inference 1: 42.12 sec total, 777.97 tokens/sec
Decode latency: 30.15 sec
Prefill latency: 11.96 sec
Bandwidth achieved: 28262.37 GB/s
FLOPS achieved: 84.79 TF/s

Prefill latency: 11.989769023959525 sec
Decode latency: 30.15338008897379 sec
Time for inference 2: 42.15 sec total, 777.49 tokens/sec
Decode latency: 30.15 sec
Prefill latency: 11.99 sec
Bandwidth achieved: 28244.91 GB/s
FLOPS achieved: 84.73 TF/s

Prefill latency: 11.949739102972671 sec
Decode latency: 30.150868291966617 sec
Time for inference 3: 42.10 sec total, 778.28 tokens/sec
Decode latency: 30.15 sec
Prefill latency: 11.95 sec
Bandwidth achieved: 28273.50 GB/s
FLOPS achieved: 84.82 TF/s

Prefill latency: 11.95332615391817 sec
Decode latency: 30.152006794000044 sec
Time for inference 4: 42.11 sec total, 778.19 tokens/sec
Decode latency: 30.15 sec
Prefill latency: 11.95 sec
Bandwidth achieved: 28270.27 GB/s
FLOPS achieved: 84.81 TF/s

Prefill latency: 11.986415541963652 sec
Decode latency: 30.152327956049703 sec
Time for inference 5: 42.14 sec total, 777.57 tokens/sec
Decode latency: 30.15 sec
Prefill latency: 11.99 sec
Bandwidth achieved: 28247.82 GB/s
FLOPS achieved: 84.74 TF/s

Prefill latency: 11.995377726037987 sec
Decode latency: 30.152597398031503 sec
Time for inference 6: 42.15 sec total, 777.40 tokens/sec
Decode latency: 30.15 sec
Prefill latency: 12.00 sec
Bandwidth achieved: 28241.67 GB/s
FLOPS achieved: 84.73 TF/s

Prefill latency: 11.997620427981019 sec
Decode latency: 30.149144256021827 sec
Time for inference 7: 42.15 sec total, 777.43 tokens/sec
Decode latency: 30.15 sec
Prefill latency: 12.00 sec
Bandwidth achieved: 28242.56 GB/s
FLOPS achieved: 84.73 TF/s

Prefill latency: 11.99124192504678 sec
Decode latency: 30.15148690599017 sec
Time for inference 8: 42.15 sec total, 777.50 tokens/sec
Decode latency: 30.15 sec
Prefill latency: 11.99 sec
Bandwidth achieved: 28245.32 GB/s
FLOPS achieved: 84.74 TF/s

Prefill latency: 11.96240860701073 sec
Decode latency: 30.153918877011165 sec
Time for inference 9: 42.12 sec total, 777.98 tokens/sec
Decode latency: 30.15 sec
Prefill latency: 11.96 sec
Bandwidth achieved: 28262.78 GB/s
FLOPS achieved: 84.79 TF/s

Prefill latency: 11.981188645004295 sec
Decode latency: 30.15132851107046 sec
Time for inference 10: 42.14 sec total, 777.69 tokens/sec
Decode latency: 30.15 sec
Prefill latency: 11.98 sec
Bandwidth achieved: 28252.07 GB/s
FLOPS achieved: 84.76 TF/s

==========
Batch Size: 64
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 30.1519 sec
Average prefill latency: 11.9772 sec
Average tokens/sec: 777.75
Memory used: 82.37 GB
Done. we are killing the process
[rank1]:[W1201 21:21:24.379858543 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank0]:[W1201 21:21:24.517410917 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank2]:[W1201 21:21:25.099021521 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
