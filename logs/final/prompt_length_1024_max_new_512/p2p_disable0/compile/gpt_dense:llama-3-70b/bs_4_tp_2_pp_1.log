W1202 10:33:27.351000 1737983 site-packages/torch/distributed/run.py:793] 
W1202 10:33:27.351000 1737983 site-packages/torch/distributed/run.py:793] *****************************************
W1202 10:33:27.351000 1737983 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1202 10:33:27.351000 1737983 site-packages/torch/distributed/run.py:793] *****************************************
DeviceMesh('cuda', [[0, 1]], mesh_dim_names=('pp', 'tp'))
Using device=cuda
Loading model ...
DeviceMesh('cuda', [[0, 1]], mesh_dim_names=('pp', 'tp'))
GPTDense(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x DenseTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=5120, bias=False)
        (wo): Linear(in_features=4096, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=28672, bias=False)
        (w2): Linear(in_features=14336, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 1.38 seconds
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Compiling prefill with dynamic=False
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
Prefill latency: 81.74320237804204 sec
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 114.98754757922143 sec
Compilation time: 196.73 seconds
Compilation time: 196.78 seconds
Prefill latency: 0.7847581002861261 sec
Decode latency: 16.24135229503736 sec
Prefill latency: 0.7854733718559146 sec
Decode latency: 16.240807850379497 sec
Prefill latency: 0.7833373239263892 sec
Decode latency: 16.241502634249628 sec
Prefill latency: 0.7878128979355097 sec
Decode latency: 16.242117141373456 sec
Prefill latency: 0.7906002085655928 sec
Decode latency: 16.24183928128332 sec
Time for inference 1: 17.03 sec total, 120.23 tokens/sec
Decode latency: 16.24 sec
Prefill latency: 0.79 sec
Bandwidth achieved: 8482.62 GB/s
FLOPS achieved: 25.45 TF/s

Prefill latency: 0.7919451207853854 sec
Decode latency: 16.241191221866757 sec
Time for inference 2: 17.04 sec total, 120.22 tokens/sec
Decode latency: 16.24 sec
Prefill latency: 0.79 sec
Bandwidth achieved: 8482.25 GB/s
FLOPS achieved: 25.45 TF/s

Prefill latency: 0.7862603929825127 sec
Decode latency: 16.242308360990137 sec
Time for inference 3: 17.03 sec total, 120.25 tokens/sec
Decode latency: 16.24 sec
Prefill latency: 0.79 sec
Bandwidth achieved: 8484.57 GB/s
FLOPS achieved: 25.45 TF/s

Prefill latency: 0.7889662310481071 sec
Decode latency: 16.241560967173427 sec
Time for inference 4: 17.03 sec total, 120.24 tokens/sec
Decode latency: 16.24 sec
Prefill latency: 0.79 sec
Bandwidth achieved: 8483.49 GB/s
FLOPS achieved: 25.45 TF/s

Prefill latency: 0.7908936450257897 sec
Decode latency: 16.245584398973733 sec
Time for inference 5: 17.04 sec total, 120.20 tokens/sec
Decode latency: 16.25 sec
Prefill latency: 0.79 sec
Bandwidth achieved: 8480.60 GB/s
FLOPS achieved: 25.44 TF/s

Prefill latency: 0.7860321090556681 sec
Decode latency: 16.241762291640043 sec
Time for inference 6: 17.03 sec total, 120.26 tokens/sec
Decode latency: 16.24 sec
Prefill latency: 0.79 sec
Bandwidth achieved: 8484.91 GB/s
FLOPS achieved: 25.45 TF/s

Prefill latency: 0.7841508048586547 sec
Decode latency: 16.243158138822764 sec
Time for inference 7: 17.03 sec total, 120.26 tokens/sec
Decode latency: 16.24 sec
Prefill latency: 0.78 sec
Bandwidth achieved: 8485.14 GB/s
FLOPS achieved: 25.46 TF/s

Prefill latency: 0.7831512442789972 sec
Decode latency: 16.242329119239002 sec
Time for inference 8: 17.03 sec total, 120.27 tokens/sec
Decode latency: 16.24 sec
Prefill latency: 0.78 sec
Bandwidth achieved: 8485.68 GB/s
FLOPS achieved: 25.46 TF/s

Prefill latency: 0.7859195722267032 sec
Decode latency: 16.241929170209914 sec
Time for inference 9: 17.03 sec total, 120.26 tokens/sec
Decode latency: 16.24 sec
Prefill latency: 0.79 sec
Bandwidth achieved: 8484.84 GB/s
FLOPS achieved: 25.45 TF/s

Prefill latency: 0.7845466020517051 sec
Decode latency: 16.242446575313807 sec
Time for inference 10: 17.03 sec total, 120.26 tokens/sec
Decode latency: 16.24 sec
Prefill latency: 0.78 sec
Bandwidth achieved: 8485.12 GB/s
FLOPS achieved: 25.46 TF/s

==========
Batch Size: 4
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 16.2424 sec
Average prefill latency: 0.7872 sec
Average tokens/sec: 120.25
Memory used: 77.51 GB
Done. we are killing the process
[rank0]:[W1202 10:40:51.981284476 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank1]:[W1202 10:40:51.618864623 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
