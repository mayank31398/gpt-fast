W1201 10:08:29.350000 384728 site-packages/torch/distributed/run.py:793] 
W1201 10:08:29.350000 384728 site-packages/torch/distributed/run.py:793] *****************************************
W1201 10:08:29.350000 384728 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1201 10:08:29.350000 384728 site-packages/torch/distributed/run.py:793] *****************************************
DeviceMesh('cuda', [[0, 1], [2, 3]], mesh_dim_names=('pp', 'tp'))
DeviceMesh('cuda', [[0, 1], [2, 3]], mesh_dim_names=('pp', 'tp'))
Using device=cuda
Loading model ...
DeviceMesh('cuda', [[0, 1], [2, 3]], mesh_dim_names=('pp', 'tp'))
DeviceMesh('cuda', [[0, 1], [2, 3]], mesh_dim_names=('pp', 'tp'))
GPTLadder(
  (layers): ModuleList(
    (0-39): 40 x LadderTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=5120, bias=False)
        (wo): Linear(in_features=4096, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=28672, bias=False)
        (w2): Linear(in_features=14336, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 0.09 seconds
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Compiling prefill with dynamic=False
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
Prefill latency: 262.1497636283748 sec
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 122.02844423614442 sec
Compilation time: 384.12 seconds
Compilation time: 384.18 seconds
Compilation time: 384.18 seconds
Compilation time: 384.14 seconds
Prefill latency: 13.31370617216453 sec
Decode latency: 30.918865371961147 sec
Prefill latency: 12.910409783944488 sec
Decode latency: 30.915225225966424 sec
Prefill latency: 12.710871706716716 sec
Decode latency: 30.919647702015936 sec
Prefill latency: 12.710512548219413 sec
Decode latency: 30.915491551160812 sec
Prefill latency: 12.705637502949685 sec
Decode latency: 30.919425726868212 sec
Time for inference 1: 43.63 sec total, 751.08 tokens/sec
Decode latency: 30.92 sec
Prefill latency: 12.71 sec
Bandwidth achieved: 27285.35 GB/s
FLOPS achieved: 81.86 TF/s

Prefill latency: 12.678750165738165 sec
Decode latency: 30.92354184295982 sec
Time for inference 2: 43.61 sec total, 751.47 tokens/sec
Decode latency: 30.92 sec
Prefill latency: 12.68 sec
Bandwidth achieved: 27299.55 GB/s
FLOPS achieved: 81.90 TF/s

Prefill latency: 12.670400397852063 sec
Decode latency: 30.917729471810162 sec
Time for inference 3: 43.59 sec total, 751.71 tokens/sec
Decode latency: 30.92 sec
Prefill latency: 12.67 sec
Bandwidth achieved: 27308.41 GB/s
FLOPS achieved: 81.93 TF/s

Prefill latency: 12.69714956684038 sec
Decode latency: 30.91384065710008 sec
Time for inference 4: 43.61 sec total, 751.32 tokens/sec
Decode latency: 30.91 sec
Prefill latency: 12.70 sec
Bandwidth achieved: 27294.15 GB/s
FLOPS achieved: 81.88 TF/s

Prefill latency: 12.715565321035683 sec
Decode latency: 30.928028769791126 sec
Time for inference 5: 43.65 sec total, 750.76 tokens/sec
Decode latency: 30.93 sec
Prefill latency: 12.72 sec
Bandwidth achieved: 27273.64 GB/s
FLOPS achieved: 81.82 TF/s

Prefill latency: 12.704655137844384 sec
Decode latency: 30.91940190223977 sec
Time for inference 6: 43.63 sec total, 751.10 tokens/sec
Decode latency: 30.92 sec
Prefill latency: 12.70 sec
Bandwidth achieved: 27285.99 GB/s
FLOPS achieved: 81.86 TF/s

Prefill latency: 12.666319526731968 sec
Decode latency: 30.916281334124506 sec
Time for inference 7: 43.59 sec total, 751.80 tokens/sec
Decode latency: 30.92 sec
Prefill latency: 12.67 sec
Bandwidth achieved: 27311.67 GB/s
FLOPS achieved: 81.94 TF/s

Prefill latency: 12.716251269448549 sec
Decode latency: 30.91947640804574 sec
Time for inference 8: 43.64 sec total, 750.90 tokens/sec
Decode latency: 30.92 sec
Prefill latency: 12.72 sec
Bandwidth achieved: 27278.67 GB/s
FLOPS achieved: 81.84 TF/s

Prefill latency: 12.696963805239648 sec
Decode latency: 30.918340681120753 sec
Time for inference 9: 43.62 sec total, 751.25 tokens/sec
Decode latency: 30.92 sec
Prefill latency: 12.70 sec
Bandwidth achieved: 27291.44 GB/s
FLOPS achieved: 81.87 TF/s

Prefill latency: 12.723036055453122 sec
Decode latency: 30.91670860396698 sec
Time for inference 10: 43.64 sec total, 750.82 tokens/sec
Decode latency: 30.92 sec
Prefill latency: 12.72 sec
Bandwidth achieved: 27276.05 GB/s
FLOPS achieved: 81.83 TF/s

==========
Batch Size: 64
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 30.9193 sec
Average prefill latency: 12.6975 sec
Average tokens/sec: 751.22
Memory used: 81.69 GB
Done. we are killing the process
[rank1]:[W1201 10:25:11.464334549 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank0]:[W1201 10:25:11.022364367 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank2]:[W1201 10:25:12.541835994 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
