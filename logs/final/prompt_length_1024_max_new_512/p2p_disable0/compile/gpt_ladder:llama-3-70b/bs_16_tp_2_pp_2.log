W1201 09:46:15.252000 358730 site-packages/torch/distributed/run.py:793] 
W1201 09:46:15.252000 358730 site-packages/torch/distributed/run.py:793] *****************************************
W1201 09:46:15.252000 358730 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1201 09:46:15.252000 358730 site-packages/torch/distributed/run.py:793] *****************************************
DeviceMesh('cuda', [[0, 1], [2, 3]], mesh_dim_names=('pp', 'tp'))
DeviceMesh('cuda', [[0, 1], [2, 3]], mesh_dim_names=('pp', 'tp'))
DeviceMesh('cuda', [[0, 1], [2, 3]], mesh_dim_names=('pp', 'tp'))
DeviceMesh('cuda', [[0, 1], [2, 3]], mesh_dim_names=('pp', 'tp'))
Using device=cuda
Loading model ...
GPTLadder(
  (layers): ModuleList(
    (0-39): 40 x LadderTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=5120, bias=False)
        (wo): Linear(in_features=4096, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=28672, bias=False)
        (w2): Linear(in_features=14336, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 0.11 seconds
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Compiling prefill with dynamic=False
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
Prefill latency: 137.81988192768767 sec
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 105.49746522819623 sec
Compilation time: 243.37 seconds
Compilation time: 243.35 seconds
Compilation time: 243.32 seconds
Compilation time: 243.33 seconds
Prefill latency: 3.0692178369499743 sec
Decode latency: 18.742762656882405 sec
Prefill latency: 3.070999678224325 sec
Decode latency: 18.74615509668365 sec
Prefill latency: 3.074261677917093 sec
Decode latency: 18.748453427106142 sec
Prefill latency: 3.076771091669798 sec
Decode latency: 18.750113933812827 sec
Prefill latency: 3.074398668948561 sec
Decode latency: 18.746565781068057 sec
Time for inference 1: 21.82 sec total, 375.37 tokens/sec
Decode latency: 18.75 sec
Prefill latency: 3.07 sec
Bandwidth achieved: 13636.52 GB/s
FLOPS achieved: 40.91 TF/s

Prefill latency: 3.072378981858492 sec
Decode latency: 18.745050902944058 sec
Time for inference 2: 21.82 sec total, 375.43 tokens/sec
Decode latency: 18.75 sec
Prefill latency: 3.07 sec
Bandwidth achieved: 13638.71 GB/s
FLOPS achieved: 40.92 TF/s

Prefill latency: 3.0719305891543627 sec
Decode latency: 18.745429954957217 sec
Time for inference 3: 21.82 sec total, 375.43 tokens/sec
Decode latency: 18.75 sec
Prefill latency: 3.07 sec
Bandwidth achieved: 13638.80 GB/s
FLOPS achieved: 40.92 TF/s

Prefill latency: 3.075245523825288 sec
Decode latency: 18.744202747941017 sec
Time for inference 4: 21.82 sec total, 375.39 tokens/sec
Decode latency: 18.74 sec
Prefill latency: 3.08 sec
Bandwidth achieved: 13637.41 GB/s
FLOPS achieved: 40.91 TF/s

Prefill latency: 3.0782009488902986 sec
Decode latency: 18.747119220905006 sec
Time for inference 5: 21.83 sec total, 375.29 tokens/sec
Decode latency: 18.75 sec
Prefill latency: 3.08 sec
Bandwidth achieved: 13633.72 GB/s
FLOPS achieved: 40.90 TF/s

Prefill latency: 3.0688218250870705 sec
Decode latency: 18.746626836247742 sec
Time for inference 6: 21.82 sec total, 375.46 tokens/sec
Decode latency: 18.75 sec
Prefill latency: 3.07 sec
Bandwidth achieved: 13639.83 GB/s
FLOPS achieved: 40.92 TF/s

Prefill latency: 3.071344224270433 sec
Decode latency: 18.74362649396062 sec
Time for inference 7: 21.82 sec total, 375.46 tokens/sec
Decode latency: 18.74 sec
Prefill latency: 3.07 sec
Bandwidth achieved: 13639.80 GB/s
FLOPS achieved: 40.92 TF/s

Prefill latency: 3.081387782935053 sec
Decode latency: 18.74490417027846 sec
Time for inference 8: 21.83 sec total, 375.28 tokens/sec
Decode latency: 18.74 sec
Prefill latency: 3.08 sec
Bandwidth achieved: 13633.17 GB/s
FLOPS achieved: 40.90 TF/s

Prefill latency: 3.081931198015809 sec
Decode latency: 18.74913767213002 sec
Time for inference 9: 21.83 sec total, 375.20 tokens/sec
Decode latency: 18.75 sec
Prefill latency: 3.08 sec
Bandwidth achieved: 13630.18 GB/s
FLOPS achieved: 40.89 TF/s

Prefill latency: 3.07705936813727 sec
Decode latency: 18.747486940585077 sec
Time for inference 10: 21.83 sec total, 375.30 tokens/sec
Decode latency: 18.75 sec
Prefill latency: 3.08 sec
Bandwidth achieved: 13634.01 GB/s
FLOPS achieved: 40.90 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 18.7460 sec
Average prefill latency: 3.0753 sec
Average tokens/sec: 375.36
Memory used: 52.95 GB
Done. we are killing the process
[rank0]:[W1201 09:55:30.842135301 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank1]:[W1201 09:55:30.988584268 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank2]:[W1201 09:55:31.137503762 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
