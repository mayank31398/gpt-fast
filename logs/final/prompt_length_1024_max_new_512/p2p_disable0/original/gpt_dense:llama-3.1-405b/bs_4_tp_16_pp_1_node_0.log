W1202 17:26:03.811000 130409 site-packages/torch/distributed/run.py:793] 
W1202 17:26:03.811000 130409 site-packages/torch/distributed/run.py:793] *****************************************
W1202 17:26:03.811000 130409 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1202 17:26:03.811000 130409 site-packages/torch/distributed/run.py:793] *****************************************
DeviceMesh('cuda', [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]], mesh_dim_names=('pp', 'tp'))
Using device=cuda
Loading model ...
DeviceMesh('cuda', [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]], mesh_dim_names=('pp', 'tp'))
DeviceMesh('cuda', [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]], mesh_dim_names=('pp', 'tp'))
DeviceMesh('cuda', [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]], mesh_dim_names=('pp', 'tp'))
DeviceMesh('cuda', [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]], mesh_dim_names=('pp', 'tp'))
DeviceMesh('cuda', [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]], mesh_dim_names=('pp', 'tp'))
DeviceMesh('cuda', [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]], mesh_dim_names=('pp', 'tp'))
DeviceMesh('cuda', [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]], mesh_dim_names=('pp', 'tp'))
GPTDense(
  (tok_embeddings): Embedding(128256, 16384)
  (layers): ModuleList(
    (0-125): 126 x DenseTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=16384, out_features=1280, bias=False)
        (wo): Linear(in_features=1024, out_features=16384, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=16384, out_features=6656, bias=False)
        (w2): Linear(in_features=3328, out_features=16384, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=16384, out_features=128256, bias=False)
)
Time to load model: 1.52 seconds
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Prefill latency: 10.781484085000557 sec
Decode latency: 98.44518450200121 sec
Compilation time: 106.86 secondsCompilation time: 106.83 seconds
Compilation time: 106.85 seconds

Compilation time: 106.86 seconds
Compilation time: 106.81 seconds
Compilation time: 106.86 seconds
Compilation time: 109.23 seconds
Compilation time: 106.88 seconds
Prefill latency: 0.8367478880027193 sec
Decode latency: 98.58014113800164 sec
Prefill latency: 0.8207031979982276 sec
Decode latency: 98.62271187599981 sec
Prefill latency: 0.8232836669994867 sec
Decode latency: 98.15462844599824 sec
Prefill latency: 0.8220970319998742 sec
Decode latency: 98.09649853999872 sec
Prefill latency: 0.8212296649980999 sec
Decode latency: 97.97939636600131 sec
Time for inference 1: 98.80 sec total, 20.73 tokens/sec
Decode latency: 97.98 sec
Prefill latency: 0.82 sec
Bandwidth achieved: 1138.91 GB/s
FLOPS achieved: 3.42 TF/s

Prefill latency: 0.8222119319980266 sec
Decode latency: 98.08467993700106 sec
Time for inference 2: 98.91 sec total, 20.71 tokens/sec
Decode latency: 98.08 sec
Prefill latency: 0.82 sec
Bandwidth achieved: 1137.69 GB/s
FLOPS achieved: 3.41 TF/s

Prefill latency: 0.8224424380023265 sec
Decode latency: 97.75833598100144 sec
Time for inference 3: 98.58 sec total, 20.77 tokens/sec
Decode latency: 97.76 sec
Prefill latency: 0.82 sec
Bandwidth achieved: 1141.45 GB/s
FLOPS achieved: 3.42 TF/s

Prefill latency: 0.821439225001086 sec
Decode latency: 97.31770603900077 sec
Time for inference 4: 98.14 sec total, 20.87 tokens/sec
Decode latency: 97.32 sec
Prefill latency: 0.82 sec
Bandwidth achieved: 1146.58 GB/s
FLOPS achieved: 3.44 TF/s

Prefill latency: 0.8230551439992269 sec
Decode latency: 96.71046732099785 sec
Time for inference 5: 97.54 sec total, 21.00 tokens/sec
Decode latency: 96.71 sec
Prefill latency: 0.82 sec
Bandwidth achieved: 1153.71 GB/s
FLOPS achieved: 3.46 TF/s

Prefill latency: 0.8224763350008288 sec
Decode latency: 96.80419497299954 sec
Time for inference 6: 97.63 sec total, 20.98 tokens/sec
Decode latency: 96.80 sec
Prefill latency: 0.82 sec
Bandwidth achieved: 1152.61 GB/s
FLOPS achieved: 3.46 TF/s

Prefill latency: 0.8237363529988215 sec
Decode latency: 95.97373118399992 sec
Time for inference 7: 96.80 sec total, 21.16 tokens/sec
Decode latency: 95.97 sec
Prefill latency: 0.82 sec
Bandwidth achieved: 1162.48 GB/s
FLOPS achieved: 3.49 TF/s

Prefill latency: 0.8217368220030039 sec
Decode latency: 96.8398427050015 sec
Time for inference 8: 97.66 sec total, 20.97 tokens/sec
Decode latency: 96.84 sec
Prefill latency: 0.82 sec
Bandwidth achieved: 1152.20 GB/s
FLOPS achieved: 3.46 TF/s

Prefill latency: 0.8197979619981197 sec
Decode latency: 98.90187803899971 sec
Time for inference 9: 99.72 sec total, 20.54 tokens/sec
Decode latency: 98.90 sec
Prefill latency: 0.82 sec
Bandwidth achieved: 1128.39 GB/s
FLOPS achieved: 3.39 TF/s

Prefill latency: 0.8201657239987981 sec
Decode latency: 97.04091813600098 sec
Time for inference 10: 97.86 sec total, 20.93 tokens/sec
Decode latency: 97.04 sec
Prefill latency: 0.82 sec
Bandwidth achieved: 1149.85 GB/s
FLOPS achieved: 3.45 TF/s

==========
Batch Size: 4
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 97.3411 sec
Average prefill latency: 0.8218 sec
Average tokens/sec: 20.86
Memory used: 68.42 GB
Done. we are killing the process
[rank6]:[W1202 17:50:57.601730577 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank2]:[W1202 17:50:57.629361273 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank4]:[W1202 17:50:57.644969488 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank7]:[W1202 17:50:57.647671207 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank0]:[W1202 17:50:57.670597085 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank1]:[W1202 17:50:57.687018258 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank3]:[W1202 17:50:57.732889452 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank5]:[W1202 17:50:57.738407041 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
