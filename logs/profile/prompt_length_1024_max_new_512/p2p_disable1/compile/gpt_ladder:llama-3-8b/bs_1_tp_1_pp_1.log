DeviceMesh('cuda', [[0]], mesh_dim_names=('pp', 'tp'))
Using device=cuda
Loading model ...
GPTLadder(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x LadderTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
        (wo): Linear(in_features=4096, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=28672, bias=False)
        (w2): Linear(in_features=14336, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 0.15 seconds
Compiling prefill with dynamic=False
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
Prefill latency: 39.77537511603441 sec
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 50.841564926085994 sec
Compilation time: 90.62 seconds
Prefill latency: 0.05282059404999018 sec
Decode latency: 3.234814477036707 sec
Prefill latency: 0.05161449604202062 sec
Decode latency: 3.235218561021611 sec
Prefill latency: 0.052070821984671056 sec
Decode latency: 3.2349057990359142 sec
Prefill latency: 0.05166620190721005 sec
Decode latency: 3.2348111639730632 sec
Prefill latency: 0.052443371969275177 sec
Decode latency: 3.235141303972341 sec
Time for inference 1: 3.29 sec total, 155.64 tokens/sec
Decode latency: 3.24 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 2336.06 GB/s
FLOPS achieved: 7.01 TF/s

Prefill latency: 0.05209772300440818 sec
Decode latency: 3.234686344047077 sec
Time for inference 2: 3.29 sec total, 155.67 tokens/sec
Decode latency: 3.23 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 2336.65 GB/s
FLOPS achieved: 7.01 TF/s

Prefill latency: 0.05225578206591308 sec
Decode latency: 3.2345721719320863 sec
Time for inference 3: 3.29 sec total, 155.68 tokens/sec
Decode latency: 3.23 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 2336.79 GB/s
FLOPS achieved: 7.01 TF/s

Prefill latency: 0.05170061998069286 sec
Decode latency: 3.2342217500554398 sec
Time for inference 4: 3.29 sec total, 155.73 tokens/sec
Decode latency: 3.23 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 2337.50 GB/s
FLOPS achieved: 7.01 TF/s

Prefill latency: 0.051785661024041474 sec
Decode latency: 3.2346092090010643 sec
Time for inference 5: 3.29 sec total, 155.71 tokens/sec
Decode latency: 3.23 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 2337.23 GB/s
FLOPS achieved: 7.01 TF/s

Prefill latency: 0.05173229402862489 sec
Decode latency: 3.2345419839257374 sec
Time for inference 6: 3.29 sec total, 155.72 tokens/sec
Decode latency: 3.23 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 2337.37 GB/s
FLOPS achieved: 7.01 TF/s

Prefill latency: 0.05153322592377663 sec
Decode latency: 3.2344723630230874 sec
Time for inference 7: 3.29 sec total, 155.72 tokens/sec
Decode latency: 3.23 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 2337.35 GB/s
FLOPS achieved: 7.01 TF/s

Prefill latency: 0.05207032093312591 sec
Decode latency: 3.2346810860326514 sec
Time for inference 8: 3.29 sec total, 155.69 tokens/sec
Decode latency: 3.23 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 2336.92 GB/s
FLOPS achieved: 7.01 TF/s

Prefill latency: 0.051666369079612195 sec
Decode latency: 3.234693148988299 sec
Time for inference 9: 3.29 sec total, 155.70 tokens/sec
Decode latency: 3.23 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 2336.99 GB/s
FLOPS achieved: 7.01 TF/s

Prefill latency: 0.05212198803201318 sec
Decode latency: 3.2350797390099615 sec
Time for inference 10: 3.29 sec total, 155.65 tokens/sec
Decode latency: 3.24 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 2336.30 GB/s
FLOPS achieved: 7.01 TF/s

==========
Batch Size: 1
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 3.2347 sec
Average prefill latency: 0.0519 sec
Average tokens/sec: 155.69
Memory used: 17.43 GB
Done. we are killing the process
[rank0]:[W1203 13:30:34.327607799 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
