W1203 14:05:49.214000 1945286 site-packages/torch/distributed/run.py:793] 
W1203 14:05:49.214000 1945286 site-packages/torch/distributed/run.py:793] *****************************************
W1203 14:05:49.214000 1945286 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1203 14:05:49.214000 1945286 site-packages/torch/distributed/run.py:793] *****************************************
DeviceMesh('cuda', [[0, 1, 2, 3]], mesh_dim_names=('pp', 'tp'))
Using device=cuda
Loading model ...
DeviceMesh('cuda', [[0, 1, 2, 3]], mesh_dim_names=('pp', 'tp'))
DeviceMesh('cuda', [[0, 1, 2, 3]], mesh_dim_names=('pp', 'tp'))
DeviceMesh('cuda', [[0, 1, 2, 3]], mesh_dim_names=('pp', 'tp'))
GPTLadder(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x LadderTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=1536, bias=False)
        (wo): Linear(in_features=1024, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 0.28 seconds
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Compiling prefill with dynamic=False
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
Prefill latency: 37.832324631977826 sec
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 45.98684002307709 sec
Compilation time: 83.88 seconds
Compilation time: 83.82 seconds
Compilation time: 83.88 seconds
Compilation time: 83.81 seconds
Prefill latency: 1.0436071259900928 sec
Decode latency: 3.9722416170407087 sec
Prefill latency: 1.0383941360050812 sec
Decode latency: 3.9697741609998047 sec
Prefill latency: 1.0413201180053875 sec
Decode latency: 3.973079393967055 sec
Prefill latency: 1.0412027209531516 sec
Decode latency: 3.9703844729810953 sec
Prefill latency: 1.040496475994587 sec
Decode latency: 3.9723839899525046 sec
Time for inference 1: 5.01 sec total, 6534.08 tokens/sec
Decode latency: 3.97 sec
Prefill latency: 1.04 sec
Bandwidth achieved: 29670.39 GB/s
FLOPS achieved: 89.01 TF/s

Prefill latency: 1.0428888739552349 sec
Decode latency: 3.9698152360506356 sec
Time for inference 2: 5.01 sec total, 6534.59 tokens/sec
Decode latency: 3.97 sec
Prefill latency: 1.04 sec
Bandwidth achieved: 29672.72 GB/s
FLOPS achieved: 89.02 TF/s

Prefill latency: 1.0453291019657627 sec
Decode latency: 3.970847751945257 sec
Time for inference 3: 5.02 sec total, 6529.29 tokens/sec
Decode latency: 3.97 sec
Prefill latency: 1.05 sec
Bandwidth achieved: 29648.62 GB/s
FLOPS achieved: 88.95 TF/s

Prefill latency: 1.0432107680244371 sec
Decode latency: 3.9695224640890956 sec
Time for inference 4: 5.01 sec total, 6534.01 tokens/sec
Decode latency: 3.97 sec
Prefill latency: 1.04 sec
Bandwidth achieved: 29670.07 GB/s
FLOPS achieved: 89.01 TF/s

Prefill latency: 1.0457730020862073 sec
Decode latency: 3.972666645073332 sec
Time for inference 5: 5.02 sec total, 6526.56 tokens/sec
Decode latency: 3.97 sec
Prefill latency: 1.05 sec
Bandwidth achieved: 29636.24 GB/s
FLOPS achieved: 88.91 TF/s

Prefill latency: 1.0474346430273727 sec
Decode latency: 3.9701017159968615 sec
Time for inference 6: 5.02 sec total, 6527.82 tokens/sec
Decode latency: 3.97 sec
Prefill latency: 1.05 sec
Bandwidth achieved: 29641.95 GB/s
FLOPS achieved: 88.93 TF/s

Prefill latency: 1.0461150989867747 sec
Decode latency: 3.96907700097654 sec
Time for inference 7: 5.02 sec total, 6530.77 tokens/sec
Decode latency: 3.97 sec
Prefill latency: 1.05 sec
Bandwidth achieved: 29655.36 GB/s
FLOPS achieved: 88.97 TF/s

Prefill latency: 1.0444298749789596 sec
Decode latency: 3.972182049998082 sec
Time for inference 8: 5.02 sec total, 6529.04 tokens/sec
Decode latency: 3.97 sec
Prefill latency: 1.04 sec
Bandwidth achieved: 29647.50 GB/s
FLOPS achieved: 88.94 TF/s

Prefill latency: 1.0418679970316589 sec
Decode latency: 3.9714439299423248 sec
Time for inference 9: 5.02 sec total, 6533.24 tokens/sec
Decode latency: 3.97 sec
Prefill latency: 1.04 sec
Bandwidth achieved: 29666.58 GB/s
FLOPS achieved: 89.00 TF/s

Prefill latency: 1.0459093239624053 sec
Decode latency: 3.9693049320485443 sec
Time for inference 10: 5.02 sec total, 6530.78 tokens/sec
Decode latency: 3.97 sec
Prefill latency: 1.05 sec
Bandwidth achieved: 29655.42 GB/s
FLOPS achieved: 88.97 TF/s

==========
Batch Size: 64
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 3.9707 sec
Average prefill latency: 1.0443 sec
Average tokens/sec: 6531.02
Memory used: 42.40 GB
Done. we are killing the process
[rank2]:[W1203 14:08:29.454547510 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank0]:[W1203 14:08:29.544832741 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank3]:[W1203 14:08:29.772663134 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank1]:[W1203 14:08:29.873368709 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
