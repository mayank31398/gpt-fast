W1203 14:21:59.542000 1962290 site-packages/torch/distributed/run.py:793] 
W1203 14:21:59.542000 1962290 site-packages/torch/distributed/run.py:793] *****************************************
W1203 14:21:59.542000 1962290 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1203 14:21:59.542000 1962290 site-packages/torch/distributed/run.py:793] *****************************************
DeviceMesh('cuda', [[0, 1]], mesh_dim_names=('pp', 'tp'))
Using device=cuda
Loading model ...
DeviceMesh('cuda', [[0, 1]], mesh_dim_names=('pp', 'tp'))
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=3072, bias=False)
        (wo): Linear(in_features=2048, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=14336, bias=False)
        (w2): Linear(in_features=7168, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.06 seconds
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Compiling prefill with dynamic=False
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
Prefill latency: 31.981656674994156 sec
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 40.353554693982005 sec
Compilation time: 72.36 seconds
Compilation time: 72.34 seconds
Prefill latency: 0.10734878294169903 sec
Decode latency: 2.524419759050943 sec
Prefill latency: 0.10576520103495568 sec
Decode latency: 2.5253107849275693 sec
Prefill latency: 0.10590893204789609 sec
Decode latency: 2.525520816911012 sec
Prefill latency: 0.10628454899415374 sec
Decode latency: 2.52633307792712 sec
Prefill latency: 0.10580516303889453 sec
Decode latency: 2.5268894049804658 sec
Time for inference 1: 2.63 sec total, 777.41 tokens/sec
Decode latency: 2.53 sec
Prefill latency: 0.11 sec
Bandwidth achieved: 6243.00 GB/s
FLOPS achieved: 18.73 TF/s

Prefill latency: 0.10559950501192361 sec
Decode latency: 2.527138751000166 sec
Time for inference 2: 2.63 sec total, 777.40 tokens/sec
Decode latency: 2.53 sec
Prefill latency: 0.11 sec
Bandwidth achieved: 6242.96 GB/s
FLOPS achieved: 18.73 TF/s

Prefill latency: 0.10518075304571539 sec
Decode latency: 2.5267957739997655 sec
Time for inference 3: 2.63 sec total, 777.67 tokens/sec
Decode latency: 2.53 sec
Prefill latency: 0.11 sec
Bandwidth achieved: 6245.07 GB/s
FLOPS achieved: 18.74 TF/s

Prefill latency: 0.10512146598193794 sec
Decode latency: 2.5265586930327117 sec
Time for inference 4: 2.63 sec total, 777.80 tokens/sec
Decode latency: 2.53 sec
Prefill latency: 0.11 sec
Bandwidth achieved: 6246.12 GB/s
FLOPS achieved: 18.74 TF/s

Prefill latency: 0.10571070807054639 sec
Decode latency: 2.5288012539967895 sec
Time for inference 5: 2.64 sec total, 776.95 tokens/sec
Decode latency: 2.53 sec
Prefill latency: 0.11 sec
Bandwidth achieved: 6239.34 GB/s
FLOPS achieved: 18.72 TF/s

Prefill latency: 0.10505638201721013 sec
Decode latency: 2.524094098014757 sec
Time for inference 6: 2.63 sec total, 778.54 tokens/sec
Decode latency: 2.52 sec
Prefill latency: 0.11 sec
Bandwidth achieved: 6252.07 GB/s
FLOPS achieved: 18.76 TF/s

Prefill latency: 0.10541898990049958 sec
Decode latency: 2.5236451430246234 sec
Time for inference 7: 2.63 sec total, 778.59 tokens/sec
Decode latency: 2.52 sec
Prefill latency: 0.11 sec
Bandwidth achieved: 6252.48 GB/s
FLOPS achieved: 18.76 TF/s

Prefill latency: 0.10521729092579335 sec
Decode latency: 2.5244342170190066 sec
Time for inference 8: 2.63 sec total, 778.37 tokens/sec
Decode latency: 2.52 sec
Prefill latency: 0.11 sec
Bandwidth achieved: 6250.76 GB/s
FLOPS achieved: 18.75 TF/s

Prefill latency: 0.10586862196214497 sec
Decode latency: 2.523578242980875 sec
Time for inference 9: 2.63 sec total, 778.46 tokens/sec
Decode latency: 2.52 sec
Prefill latency: 0.11 sec
Bandwidth achieved: 6251.44 GB/s
FLOPS achieved: 18.75 TF/s

Prefill latency: 0.10596889595035464 sec
Decode latency: 2.524101887946017 sec
Time for inference 10: 2.63 sec total, 778.13 tokens/sec
Decode latency: 2.52 sec
Prefill latency: 0.11 sec
Bandwidth achieved: 6248.76 GB/s
FLOPS achieved: 18.75 TF/s

==========
Batch Size: 4
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 2.5256 sec
Average prefill latency: 0.1055 sec
Average tokens/sec: 777.93
Memory used: 12.36 GB
Done. we are killing the process
[rank0]:[W1203 14:23:54.993159923 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank1]:[W1203 14:23:54.005947424 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
