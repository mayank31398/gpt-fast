DeviceMesh('cuda', [[0]], mesh_dim_names=('pp', 'tp'))
Using device=cuda
Loading model ...
GPTParallel(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x ParallelTransformerBlock(
      semi_compiled = False
      (attention): FuseAttentionMLP(
        (wqkv1): Linear(in_features=4096, out_features=34816, bias=False)
        (wo): Linear(in_features=4096, out_features=4096, bias=False)
        (w2): Linear(in_features=14336, out_features=4096, bias=False)
      )
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.11 seconds
Compiling prefill with dynamic=False
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
Prefill latency: 50.88857054198161 sec
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 42.255864139064215 sec
Compilation time: 93.15 seconds
Prefill latency: 0.19804842805024236 sec
Decode latency: 3.9416426609968767 sec
Prefill latency: 0.1966564110480249 sec
Decode latency: 3.940512021072209 sec
Prefill latency: 0.19623956410214305 sec
Decode latency: 3.939995639026165 sec
Prefill latency: 0.19760330591816455 sec
Decode latency: 3.9405874450458214 sec
Prefill latency: 0.19747850601561368 sec
Decode latency: 3.94031662796624 sec
Time for inference 1: 4.14 sec total, 494.67 tokens/sec
Decode latency: 3.94 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 7424.82 GB/s
FLOPS achieved: 22.27 TF/s

Prefill latency: 0.19791441794950515 sec
Decode latency: 3.940414375043474 sec
Time for inference 2: 4.14 sec total, 494.61 tokens/sec
Decode latency: 3.94 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 7423.96 GB/s
FLOPS achieved: 22.27 TF/s

Prefill latency: 0.1962615130469203 sec
Decode latency: 3.940765317995101 sec
Time for inference 3: 4.14 sec total, 494.81 tokens/sec
Decode latency: 3.94 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 7426.89 GB/s
FLOPS achieved: 22.28 TF/s

Prefill latency: 0.19775153300724924 sec
Decode latency: 3.940571194048971 sec
Time for inference 4: 4.14 sec total, 494.65 tokens/sec
Decode latency: 3.94 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 7424.49 GB/s
FLOPS achieved: 22.27 TF/s

Prefill latency: 0.19684186403173953 sec
Decode latency: 3.9403821809682995 sec
Time for inference 5: 4.14 sec total, 494.76 tokens/sec
Decode latency: 3.94 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 7426.12 GB/s
FLOPS achieved: 22.28 TF/s

Prefill latency: 0.19848711392842233 sec
Decode latency: 3.9409884720807895 sec
Time for inference 6: 4.14 sec total, 494.52 tokens/sec
Decode latency: 3.94 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 7422.56 GB/s
FLOPS achieved: 22.27 TF/s

Prefill latency: 0.1976473400136456 sec
Decode latency: 3.9407707940554246 sec
Time for inference 7: 4.14 sec total, 494.65 tokens/sec
Decode latency: 3.94 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 7424.51 GB/s
FLOPS achieved: 22.27 TF/s

Prefill latency: 0.19794979703146964 sec
Decode latency: 3.940980019979179 sec
Time for inference 8: 4.14 sec total, 494.60 tokens/sec
Decode latency: 3.94 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 7423.78 GB/s
FLOPS achieved: 22.27 TF/s

Prefill latency: 0.19659296702593565 sec
Decode latency: 3.940244633005932 sec
Time for inference 9: 4.14 sec total, 494.86 tokens/sec
Decode latency: 3.94 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 7427.63 GB/s
FLOPS achieved: 22.28 TF/s

Prefill latency: 0.1963402150431648 sec
Decode latency: 3.9412280580727383 sec
Time for inference 10: 4.14 sec total, 494.78 tokens/sec
Decode latency: 3.94 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 7426.51 GB/s
FLOPS achieved: 22.28 TF/s

==========
Batch Size: 4
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 3.9407 sec
Average prefill latency: 0.1973 sec
Average tokens/sec: 494.69
Memory used: 20.43 GB
Done. we are killing the process
[rank0]:[W1203 15:00:16.774094391 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
