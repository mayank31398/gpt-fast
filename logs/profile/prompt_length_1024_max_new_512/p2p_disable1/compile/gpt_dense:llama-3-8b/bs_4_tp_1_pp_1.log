DeviceMesh('cuda', [[0]], mesh_dim_names=('pp', 'tp'))
Using device=cuda
Loading model ...
GPTDense(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x DenseTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
        (wo): Linear(in_features=4096, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=28672, bias=False)
        (w2): Linear(in_features=14336, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.09 seconds
Compiling prefill with dynamic=False
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
Prefill latency: 52.60895301902201 sec
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 52.6768125470262 sec
Compilation time: 105.29 seconds
Prefill latency: 0.198008731007576 sec
Decode latency: 3.976672074990347 sec
Prefill latency: 0.19806127296760678 sec
Decode latency: 3.9763496801024303 sec
Prefill latency: 0.198783662985079 sec
Decode latency: 3.9761747210286558 sec
Prefill latency: 0.1978823570534587 sec
Decode latency: 3.976609380915761 sec
Prefill latency: 0.19813560601323843 sec
Decode latency: 3.976048058946617 sec
Time for inference 1: 4.18 sec total, 490.43 tokens/sec
Decode latency: 3.98 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 7361.24 GB/s
FLOPS achieved: 22.08 TF/s

Prefill latency: 0.19802372599951923 sec
Decode latency: 3.977519436040893 sec
Time for inference 2: 4.18 sec total, 490.25 tokens/sec
Decode latency: 3.98 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 7358.53 GB/s
FLOPS achieved: 22.08 TF/s

Prefill latency: 0.1981291399570182 sec
Decode latency: 3.977186669013463 sec
Time for inference 3: 4.18 sec total, 490.29 tokens/sec
Decode latency: 3.98 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 7359.12 GB/s
FLOPS achieved: 22.08 TF/s

Prefill latency: 0.19815070100594312 sec
Decode latency: 3.9764788950560614 sec
Time for inference 4: 4.18 sec total, 490.31 tokens/sec
Decode latency: 3.98 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 7359.46 GB/s
FLOPS achieved: 22.08 TF/s

Prefill latency: 0.19871597504243255 sec
Decode latency: 3.9761309480527416 sec
Time for inference 5: 4.18 sec total, 490.33 tokens/sec
Decode latency: 3.98 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 7359.73 GB/s
FLOPS achieved: 22.08 TF/s

Prefill latency: 0.19893121102359146 sec
Decode latency: 3.9764068159274757 sec
Time for inference 6: 4.18 sec total, 490.28 tokens/sec
Decode latency: 3.98 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 7359.02 GB/s
FLOPS achieved: 22.08 TF/s

Prefill latency: 0.19658548792358488 sec
Decode latency: 3.9761555739678442 sec
Time for inference 7: 4.17 sec total, 490.59 tokens/sec
Decode latency: 3.98 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 7363.75 GB/s
FLOPS achieved: 22.09 TF/s

Prefill latency: 0.19779159489553422 sec
Decode latency: 3.9771050970302895 sec
Time for inference 8: 4.18 sec total, 490.36 tokens/sec
Decode latency: 3.98 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 7360.22 GB/s
FLOPS achieved: 22.08 TF/s

Prefill latency: 0.19876247900538146 sec
Decode latency: 3.976494795992039 sec
Time for inference 9: 4.18 sec total, 490.28 tokens/sec
Decode latency: 3.98 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 7359.10 GB/s
FLOPS achieved: 22.08 TF/s

Prefill latency: 0.19787206407636404 sec
Decode latency: 3.9757736179744825 sec
Time for inference 10: 4.18 sec total, 490.50 tokens/sec
Decode latency: 3.98 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 7362.33 GB/s
FLOPS achieved: 22.09 TF/s

==========
Batch Size: 4
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 3.9765 sec
Average prefill latency: 0.1981 sec
Average tokens/sec: 490.36
Memory used: 20.61 GB
Done. we are killing the process
[rank0]:[W1203 12:51:58.657542081 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
