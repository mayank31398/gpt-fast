W1203 12:44:00.603000 1858786 site-packages/torch/distributed/run.py:793] 
W1203 12:44:00.603000 1858786 site-packages/torch/distributed/run.py:793] *****************************************
W1203 12:44:00.603000 1858786 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1203 12:44:00.603000 1858786 site-packages/torch/distributed/run.py:793] *****************************************
DeviceMesh('cuda', [[0, 1, 2, 3]], mesh_dim_names=('pp', 'tp'))
Using device=cuda
Loading model ...
DeviceMesh('cuda', [[0, 1, 2, 3]], mesh_dim_names=('pp', 'tp'))
DeviceMesh('cuda', [[0, 1, 2, 3]], mesh_dim_names=('pp', 'tp'))
DeviceMesh('cuda', [[0, 1, 2, 3]], mesh_dim_names=('pp', 'tp'))
GPTDense(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x DenseTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=1536, bias=False)
        (wo): Linear(in_features=1024, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.26 seconds
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Compiling prefill with dynamic=False
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
Prefill latency: 47.548907465999946 sec
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 51.503963407012634 sec
Compilation time: 99.09 seconds
Compilation time: 99.09 seconds
Compilation time: 99.06 seconds
Compilation time: 99.03 seconds
Prefill latency: 0.029034891980700195 sec
Decode latency: 2.298435486969538 sec
Prefill latency: 0.02423564193304628 sec
Decode latency: 2.2992274139542133 sec
Prefill latency: 0.024767644936218858 sec
Decode latency: 2.298122876090929 sec
Prefill latency: 0.02448354405350983 sec
Decode latency: 2.3001894579501823 sec
Prefill latency: 0.02420802798587829 sec
Decode latency: 2.297364259022288 sec
Time for inference 1: 2.32 sec total, 220.37 tokens/sec
Decode latency: 2.30 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 1000.69 GB/s
FLOPS achieved: 3.00 TF/s

Prefill latency: 0.02412991097662598 sec
Decode latency: 2.2976405329536647 sec
Time for inference 2: 2.32 sec total, 220.35 tokens/sec
Decode latency: 2.30 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 1000.56 GB/s
FLOPS achieved: 3.00 TF/s

Prefill latency: 0.024031621986068785 sec
Decode latency: 2.299203463946469 sec
Time for inference 3: 2.33 sec total, 220.16 tokens/sec
Decode latency: 2.30 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 999.72 GB/s
FLOPS achieved: 3.00 TF/s

Prefill latency: 0.02453068597242236 sec
Decode latency: 2.2976555719505996 sec
Time for inference 4: 2.32 sec total, 220.30 tokens/sec
Decode latency: 2.30 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 1000.34 GB/s
FLOPS achieved: 3.00 TF/s

Prefill latency: 0.0256731960689649 sec
Decode latency: 2.298843375989236 sec
Time for inference 5: 2.33 sec total, 220.08 tokens/sec
Decode latency: 2.30 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 999.34 GB/s
FLOPS achieved: 3.00 TF/s

Prefill latency: 0.024898267001844943 sec
Decode latency: 2.297681241063401 sec
Time for inference 6: 2.32 sec total, 220.26 tokens/sec
Decode latency: 2.30 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 1000.19 GB/s
FLOPS achieved: 3.00 TF/s

Prefill latency: 0.02423076005652547 sec
Decode latency: 2.2975294389761984 sec
Time for inference 7: 2.32 sec total, 220.35 tokens/sec
Decode latency: 2.30 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 1000.60 GB/s
FLOPS achieved: 3.00 TF/s

Prefill latency: 0.02407416806090623 sec
Decode latency: 2.2979609169997275 sec
Time for inference 8: 2.32 sec total, 220.33 tokens/sec
Decode latency: 2.30 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 1000.47 GB/s
FLOPS achieved: 3.00 TF/s

Prefill latency: 0.024167774943634868 sec
Decode latency: 2.2980328319827095 sec
Time for inference 9: 2.32 sec total, 220.32 tokens/sec
Decode latency: 2.30 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 1000.46 GB/s
FLOPS achieved: 3.00 TF/s

Prefill latency: 0.024079401046037674 sec
Decode latency: 2.2976102830143645 sec
Time for inference 10: 2.32 sec total, 220.31 tokens/sec
Decode latency: 2.30 sec
Prefill latency: 0.02 sec
Bandwidth achieved: 1000.38 GB/s
FLOPS achieved: 3.00 TF/s

==========
Batch Size: 1
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 2.2980 sec
Average prefill latency: 0.0244 sec
Average tokens/sec: 220.28
Memory used: 6.76 GB
Done. we are killing the process
[rank2]:[W1203 12:46:19.200186202 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank3]:[W1203 12:46:19.323299386 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank0]:[W1203 12:46:19.565034110 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank1]:[W1203 12:46:19.629057069 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
