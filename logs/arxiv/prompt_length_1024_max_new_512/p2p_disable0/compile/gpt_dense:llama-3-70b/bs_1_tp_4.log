W1208 23:02:16.189000 517765 site-packages/torch/distributed/run.py:793] 
W1208 23:02:16.189000 517765 site-packages/torch/distributed/run.py:793] *****************************************
W1208 23:02:16.189000 517765 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1208 23:02:16.189000 517765 site-packages/torch/distributed/run.py:793] *****************************************
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4

rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
flash_kv_decode is set to False
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
our world size=4
Using device=cuda
Loading model ...
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4rank: 1, global_rank: 1, world_size: 4, global_world_size: 4

GPTDense(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x DenseTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=2560, bias=False)
        (wo): Linear(in_features=2048, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=14336, bias=False)
        (w2): Linear(in_features=7168, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 1.28 seconds
Compiling prefill with dynamic=False
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
Prefill latency: 105.0203109998256 sec
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 142.50572951324284 sec
Compilation time: 247.18 secondsCompilation time: 247.53 seconds

Compilation time: 247.26 seconds
Compilation time: 247.17 seconds
Prefill latency: 0.1281818449497223 sec
Decode latency: 8.53092329017818 sec
Prefill latency: 0.12290837988257408 sec
Decode latency: 8.531463986262679 sec
Prefill latency: 0.12308891117572784 sec
Decode latency: 8.53118746355176 sec
Prefill latency: 0.12300722859799862 sec
Decode latency: 8.53135391511023 sec
Prefill latency: 0.12335878796875477 sec
Decode latency: 8.531354367733002 sec
Time for inference 1: 8.66 sec total, 59.14 tokens/sec
Decode latency: 8.53 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 2148.55 GB/s
FLOPS achieved: 6.45 TF/s

Prefill latency: 0.12315465696156025 sec
Decode latency: 8.531351828947663 sec
Time for inference 2: 8.66 sec total, 59.14 tokens/sec
Decode latency: 8.53 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 2148.61 GB/s
FLOPS achieved: 6.45 TF/s

Prefill latency: 0.12332837469875813 sec
Decode latency: 8.531380681321025 sec
Time for inference 3: 8.66 sec total, 59.14 tokens/sec
Decode latency: 8.53 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 2148.55 GB/s
FLOPS achieved: 6.45 TF/s

Prefill latency: 0.12362333200871944 sec
Decode latency: 8.530248254537582 sec
Time for inference 4: 8.66 sec total, 59.15 tokens/sec
Decode latency: 8.53 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 2148.77 GB/s
FLOPS achieved: 6.45 TF/s

Prefill latency: 0.12343387305736542 sec
Decode latency: 8.53074967674911 sec
Time for inference 5: 8.66 sec total, 59.14 tokens/sec
Decode latency: 8.53 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 2148.70 GB/s
FLOPS achieved: 6.45 TF/s

Prefill latency: 0.12312472611665726 sec
Decode latency: 8.531013397499919 sec
Time for inference 6: 8.66 sec total, 59.14 tokens/sec
Decode latency: 8.53 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 2148.68 GB/s
FLOPS achieved: 6.45 TF/s

Prefill latency: 0.123419139534235 sec
Decode latency: 8.533226339146495 sec
Time for inference 7: 8.66 sec total, 59.13 tokens/sec
Decode latency: 8.53 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 2148.06 GB/s
FLOPS achieved: 6.44 TF/s

Prefill latency: 0.12341775372624397 sec
Decode latency: 8.532251339405775 sec
Time for inference 8: 8.66 sec total, 59.13 tokens/sec
Decode latency: 8.53 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 2148.34 GB/s
FLOPS achieved: 6.45 TF/s

Prefill latency: 0.1236407682299614 sec
Decode latency: 8.530375009402633 sec
Time for inference 9: 8.66 sec total, 59.15 tokens/sec
Decode latency: 8.53 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 2148.73 GB/s
FLOPS achieved: 6.45 TF/s

Prefill latency: 0.12355127930641174 sec
Decode latency: 8.533557320013642 sec
Time for inference 10: 8.66 sec total, 59.12 tokens/sec
Decode latency: 8.53 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 2147.95 GB/s
FLOPS achieved: 6.44 TF/s

==========
Batch Size: 1
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 8.5316 sec
Average prefill latency: 0.1234 sec
Average tokens/sec: 59.14
Memory used: 39.80 GB
Done. we are killing the process
[rank0]:[W1208 23:08:34.652205811 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
