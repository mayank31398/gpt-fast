W1209 10:07:32.775000 657727 site-packages/torch/distributed/run.py:793] 
W1209 10:07:32.775000 657727 site-packages/torch/distributed/run.py:793] *****************************************
W1209 10:07:32.775000 657727 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1209 10:07:32.775000 657727 site-packages/torch/distributed/run.py:793] *****************************************
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
flash_kv_decode is set to False
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
our world size=4
Using device=cuda
Loading model ...
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
GPTLadder(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x LadderTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=2560, bias=False)
        (wo): Linear(in_features=2048, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=14336, bias=False)
        (w2): Linear(in_features=7168, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 0.39 seconds
Compiling prefill with dynamic=False
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
Prefill latency: 94.01608755253255 sec
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 131.2434839019552 sec
Compilation time: 225.21 secondsCompilation time: 225.22 seconds
Compilation time: 225.17 seconds

Compilation time: 225.26 seconds
Prefill latency: 0.1221423065289855 sec
Decode latency: 7.567011464387178 sec
Prefill latency: 0.12027780991047621 sec
Decode latency: 7.5666661178693175 sec
Prefill latency: 0.12040426023304462 sec
Decode latency: 7.567520231939852 sec
Prefill latency: 0.12041776534169912 sec
Decode latency: 7.566794773563743 sec
Prefill latency: 0.12036098074167967 sec
Decode latency: 7.564981781877577 sec
Time for inference 1: 7.69 sec total, 66.60 tokens/sec
Decode latency: 7.56 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 2419.39 GB/s
FLOPS achieved: 7.26 TF/s

Prefill latency: 0.11982167884707451 sec
Decode latency: 7.564702060073614 sec
Time for inference 2: 7.69 sec total, 66.60 tokens/sec
Decode latency: 7.56 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 2419.67 GB/s
FLOPS achieved: 7.26 TF/s

Prefill latency: 0.11988002806901932 sec
Decode latency: 7.563935668207705 sec
Time for inference 3: 7.69 sec total, 66.61 tokens/sec
Decode latency: 7.56 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 2419.94 GB/s
FLOPS achieved: 7.26 TF/s

Prefill latency: 0.11985201109200716 sec
Decode latency: 7.564217105507851 sec
Time for inference 4: 7.69 sec total, 66.61 tokens/sec
Decode latency: 7.56 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 2419.78 GB/s
FLOPS achieved: 7.26 TF/s

Prefill latency: 0.11979881208389997 sec
Decode latency: 7.564341192133725 sec
Time for inference 5: 7.69 sec total, 66.61 tokens/sec
Decode latency: 7.56 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 2419.75 GB/s
FLOPS achieved: 7.26 TF/s

Prefill latency: 0.12045263964682817 sec
Decode latency: 7.564504072070122 sec
Time for inference 6: 7.69 sec total, 66.60 tokens/sec
Decode latency: 7.56 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 2419.53 GB/s
FLOPS achieved: 7.26 TF/s

Prefill latency: 0.12052785232663155 sec
Decode latency: 7.564167818985879 sec
Time for inference 7: 7.69 sec total, 66.60 tokens/sec
Decode latency: 7.56 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 2419.63 GB/s
FLOPS achieved: 7.26 TF/s

Prefill latency: 0.12027504947036505 sec
Decode latency: 7.564141024835408 sec
Time for inference 8: 7.69 sec total, 66.60 tokens/sec
Decode latency: 7.56 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 2419.66 GB/s
FLOPS achieved: 7.26 TF/s

Prefill latency: 0.12003878504037857 sec
Decode latency: 7.564603543840349 sec
Time for inference 9: 7.69 sec total, 66.60 tokens/sec
Decode latency: 7.56 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 2419.63 GB/s
FLOPS achieved: 7.26 TF/s

Prefill latency: 0.11987425200641155 sec
Decode latency: 7.564631227403879 sec
Time for inference 10: 7.69 sec total, 66.60 tokens/sec
Decode latency: 7.56 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 2419.70 GB/s
FLOPS achieved: 7.26 TF/s

==========
Batch Size: 1
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 7.5644 sec
Average prefill latency: 0.1201 sec
Average tokens/sec: 66.60
Memory used: 39.51 GB
Done. we are killing the process
[rank0]:[W1209 10:13:14.091796507 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
