W1210 17:15:08.313000 1123991 site-packages/torch/distributed/run.py:793] 
W1210 17:15:08.313000 1123991 site-packages/torch/distributed/run.py:793] *****************************************
W1210 17:15:08.313000 1123991 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1210 17:15:08.313000 1123991 site-packages/torch/distributed/run.py:793] *****************************************
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
flash_kv_decode is set to False
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
our world size=8
Using device=cuda
Loading model ...
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8rank: 7, global_rank: 7, world_size: 8, global_world_size: 8

rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
GPTParallel(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x ParallelTransformerBlock(
      semi_compiled = False
      (attention): FuseAttentionMLP(
        (wqkv1): Linear(in_features=4096, out_features=4352, bias=False)
        (wo): Linear(in_features=512, out_features=4096, bias=False)
        (w2): Linear(in_features=1792, out_features=4096, bias=False)
      )
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.10 seconds
Compiling prefill with dynamic=False
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Prefill latency: 62.781783163547516 sec
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 56.307877806946635 sec
Compilation time: 118.66 secondsCompilation time: 118.61 seconds

Compilation time: 119.09 seconds
Compilation time: 118.76 seconds
Compilation time: 118.78 seconds
Compilation time: 118.76 seconds
Compilation time: 118.62 seconds
Compilation time: 118.74 seconds
Prefill latency: 0.05349934846162796 sec
Decode latency: 2.411628583446145 sec
Prefill latency: 0.04583150893449783 sec
Decode latency: 2.4005153942853212 sec
Prefill latency: 0.046212922781705856 sec
Decode latency: 2.39925579726696 sec
Prefill latency: 0.046187338419258595 sec
Decode latency: 2.432444194331765 sec
Prefill latency: 0.04644951410591602 sec
Decode latency: 2.4326393892988563 sec
Time for inference 1: 2.48 sec total, 825.24 tokens/sec
Decode latency: 2.43 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 2307.19 GB/s
FLOPS achieved: 6.92 TF/s

Prefill latency: 0.04574444890022278 sec
Decode latency: 2.403729412704706 sec
Time for inference 2: 2.45 sec total, 835.45 tokens/sec
Decode latency: 2.40 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 2335.73 GB/s
FLOPS achieved: 7.01 TF/s

Prefill latency: 0.04631256777793169 sec
Decode latency: 2.4227667516097426 sec
Time for inference 3: 2.47 sec total, 828.79 tokens/sec
Decode latency: 2.42 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 2317.12 GB/s
FLOPS achieved: 6.95 TF/s

Prefill latency: 0.052741194143891335 sec
Decode latency: 2.4424506993964314 sec
Time for inference 4: 2.50 sec total, 820.20 tokens/sec
Decode latency: 2.44 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 2293.10 GB/s
FLOPS achieved: 6.88 TF/s

[mk-xii-21:1124065:0:1124065] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x5e0)
[rank2]:[E1210 17:18:32.417502185 ProcessGroupNCCL.cpp:627] [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=379, OpType=ALLREDUCE, NumelIn=16777216, NumelOut=16777216, Timeout(ms)=60000) ran for 60002 milliseconds before timing out.
[rank2]:[E1210 17:18:32.417737455 ProcessGroupNCCL.cpp:2007] [PG ID 0 PG GUID 0(default_pg) Rank 2]  failure detected by watchdog at work sequence id: 379 PG status: last enqueued work: 379, last completed work: 378
[rank2]:[E1210 17:18:32.417744103 ProcessGroupNCCL.cpp:665] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank4]:[E1210 17:18:32.469929207 ProcessGroupNCCL.cpp:627] [Rank 4] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=379, OpType=ALLREDUCE, NumelIn=16777216, NumelOut=16777216, Timeout(ms)=60000) ran for 60052 milliseconds before timing out.
[rank4]:[E1210 17:18:32.470148087 ProcessGroupNCCL.cpp:2007] [PG ID 0 PG GUID 0(default_pg) Rank 4]  failure detected by watchdog at work sequence id: 379 PG status: last enqueued work: 384, last completed work: 378
[rank4]:[E1210 17:18:32.470155104 ProcessGroupNCCL.cpp:665] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank5]:[E1210 17:18:32.475171277 ProcessGroupNCCL.cpp:627] [Rank 5] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=379, OpType=ALLREDUCE, NumelIn=16777216, NumelOut=16777216, Timeout(ms)=60000) ran for 60058 milliseconds before timing out.
[rank5]:[E1210 17:18:32.475464686 ProcessGroupNCCL.cpp:2007] [PG ID 0 PG GUID 0(default_pg) Rank 5]  failure detected by watchdog at work sequence id: 379 PG status: last enqueued work: 384, last completed work: 378
[rank5]:[E1210 17:18:32.475472563 ProcessGroupNCCL.cpp:665] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank6]:[E1210 17:18:32.484727863 ProcessGroupNCCL.cpp:627] [Rank 6] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=379, OpType=ALLREDUCE, NumelIn=16777216, NumelOut=16777216, Timeout(ms)=60000) ran for 60067 milliseconds before timing out.
[rank6]:[E1210 17:18:32.484868233 ProcessGroupNCCL.cpp:2007] [PG ID 0 PG GUID 0(default_pg) Rank 6]  failure detected by watchdog at work sequence id: 379 PG status: last enqueued work: 384, last completed work: 378
[rank6]:[E1210 17:18:32.484874295 ProcessGroupNCCL.cpp:665] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank7]:[E1210 17:18:32.490942374 ProcessGroupNCCL.cpp:627] [Rank 7] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=379, OpType=ALLREDUCE, NumelIn=16777216, NumelOut=16777216, Timeout(ms)=60000) ran for 60073 milliseconds before timing out.
[rank7]:[E1210 17:18:32.491402780 ProcessGroupNCCL.cpp:2007] [PG ID 0 PG GUID 0(default_pg) Rank 7]  failure detected by watchdog at work sequence id: 379 PG status: last enqueued work: 384, last completed work: 378
[rank7]:[E1210 17:18:32.491410713 ProcessGroupNCCL.cpp:665] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank0]:[E1210 17:18:32.515542229 ProcessGroupNCCL.cpp:627] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=379, OpType=ALLREDUCE, NumelIn=16777216, NumelOut=16777216, Timeout(ms)=60000) ran for 60100 milliseconds before timing out.
[rank1]:[E1210 17:18:32.515583534 ProcessGroupNCCL.cpp:627] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=379, OpType=ALLREDUCE, NumelIn=16777216, NumelOut=16777216, Timeout(ms)=60000) ran for 60099 milliseconds before timing out.
[rank0]:[E1210 17:18:32.515683394 ProcessGroupNCCL.cpp:2007] [PG ID 0 PG GUID 0(default_pg) Rank 0]  failure detected by watchdog at work sequence id: 379 PG status: last enqueued work: 379, last completed work: 378
[rank0]:[E1210 17:18:32.515689052 ProcessGroupNCCL.cpp:665] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank1]:[E1210 17:18:32.515721709 ProcessGroupNCCL.cpp:2007] [PG ID 0 PG GUID 0(default_pg) Rank 1]  failure detected by watchdog at work sequence id: 379 PG status: last enqueued work: 379, last completed work: 378
[rank1]:[E1210 17:18:32.515727454 ProcessGroupNCCL.cpp:665] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
==== backtrace (tid:1124065) ====
 0 0x0000000000042520 __sigaction()  ???:0
 1 0x0000000000049b8a ncclMemoryPoolAlloc<ncclProxyOp>()  /dvs/p4/build/sw/gpgpu/nccl/gitfusion/stable/src/include/utils.h:280
 2 0x0000000000049b8a addProxyOpIfNeeded()  /dvs/p4/build/sw/gpgpu/nccl/gitfusion/stable/src/enqueue.cc:180
 3 0x0000000000049b8a addProxyOpIfNeeded()  /dvs/p4/build/sw/gpgpu/nccl/gitfusion/stable/src/enqueue.cc:176
 4 0x000000000004c496 addCBDCollToPlan()  /dvs/p4/build/sw/gpgpu/nccl/gitfusion/stable/src/enqueue.cc:481
 5 0x000000000004f5bd ncclLaunchPrepare()  /dvs/p4/build/sw/gpgpu/nccl/gitfusion/stable/src/enqueue.cc:844
 6 0x000000000004f5bd ncclLaunchPrepare()  /dvs/p4/build/sw/gpgpu/nccl/gitfusion/stable/src/enqueue.cc:1260
 7 0x0000000000053d4b groupLaunch()  /dvs/p4/build/sw/gpgpu/nccl/gitfusion/stable/src/group.cc:129
 8 0x0000000000053d4b groupLaunch()  /dvs/p4/build/sw/gpgpu/nccl/gitfusion/stable/src/group.cc:339
 9 0x0000000000054f88 ncclGroupEndInternal()  /dvs/p4/build/sw/gpgpu/nccl/gitfusion/stable/src/group.cc:418
10 0x0000000000054f88 ncclGroupEndInternal()  /dvs/p4/build/sw/gpgpu/nccl/gitfusion/stable/src/group.cc:368
11 0x000000000004d74f ncclEnqueueCheck()  /dvs/p4/build/sw/gpgpu/nccl/gitfusion/stable/src/enqueue.cc:2032
12 0x00000000000452af ncclAllReduce()  /dvs/p4/build/sw/gpgpu/nccl/gitfusion/stable/src/collectives.cc:50
13 0x000000000125c922 c10d::ProcessGroupNCCL::collective<c10d::ProcessGroupNCCL::allreduce_impl(at::Tensor&, c10d::AllreduceOptions const&)::{lambda(at::Tensor&, at::Tensor&, ncclComm*, c10::cuda::CUDAStream&)#1}, c10d::ProcessGroupNCCL::collective<c10d::ProcessGroupNCCL::allreduce_impl(at::Tensor&, c10d::AllreduceOptions const&)::{lambda(at::Tensor&, at::Tensor&, ncclComm*, c10::cuda::CUDAStream&)#1}>(at::Tensor&, at::Tensor&, c10d::ProcessGroupNCCL::allreduce_impl(at::Tensor&, c10d::AllreduceOptions const&)::{lambda(at::Tensor&, at::Tensor&, ncclComm*, c10::cuda::CUDAStream&)#1}, c10d::OpType, char const*, bool, bool)::{lambda(c10::cuda::CUDAStream&, c10::intrusive_ptr<c10d::ProcessGroupNCCL::WorkNCCL, c10::detail::intrusive_target_default_null_type<c10d::ProcessGroupNCCL::WorkNCCL> >&)#1}, c10d::ProcessGroupNCCL::collective<c10d::ProcessGroupNCCL::allreduce_impl(at::Tensor&, c10d::AllreduceOptions const&)::{lambda(at::Tensor&, at::Tensor&, ncclComm*, c10::cuda::CUDAStream&)#1}>(at::Tensor&, at::Tensor&, c10d::ProcessGroupNCCL::allreduce_impl(at::Tensor&, c10d::AllreduceOptions const&)::{lambda(at::Tensor&, at::Tensor&, ncclComm*, c10::cuda::CUDAStream&)#1}, c10d::OpType, char const*, bool, bool)::{lambda(c10::cuda::CUDAStream&, c10::intrusive_ptr<c10d::ProcessGroupNCCL::WorkNCCL, c10::detail::intrusive_target_default_null_type<c10d::ProcessGroupNCCL::WorkNCCL> >&)#2}>()  ProcessGroupNCCL.cpp:0
14 0x000000000125d9fc c10d::ProcessGroupNCCL::allreduce_impl()  ???:0
15 0x000000000125e283 c10d::ProcessGroupNCCL::allreduce()  ???:0
16 0x000000000629407e c10d::ops::(anonymous namespace)::allreduce_CUDA()  Ops.cpp:0
17 0x000000000629eb54 c10::impl::call_functor_with_args_from_stack_<c10::impl::detail::WrapFunctionIntoRuntimeFunctor_<std::tuple<std::vector<at::Tensor, std::allocator<at::Tensor> >, c10::intrusive_ptr<c10d::Work, c10::detail::intrusive_target_default_null_type<c10d::Work> > > (*)(c10::ArrayRef<at::Tensor>, c10::intrusive_ptr<c10d::ProcessGroup, c10::detail::intrusive_target_default_null_type<c10d::ProcessGroup> > const&, c10::intrusive_ptr<c10d::ReduceOp, c10::detail::intrusive_target_default_null_type<c10d::ReduceOp> > const&, std::optional<at::Tensor> const&, long), std::tuple<std::vector<at::Tensor, std::allocator<at::Tensor> >, c10::intrusive_ptr<c10d::Work, c10::detail::intrusive_target_default_null_type<c10d::Work> > >, c10::guts::typelist::typelist<c10::ArrayRef<at::Tensor>, c10::intrusive_ptr<c10d::ProcessGroup, c10::detail::intrusive_target_default_null_type<c10d::ProcessGroup> > const&, c10::intrusive_ptr<c10d::ReduceOp, c10::detail::intrusive_target_default_null_type<c10d::ReduceOp> > const&, std::optional<at::Tensor> const&, long> >, false, 0ul, 1ul, 2ul, 3ul, 4ul, c10::ArrayRef<at::Tensor>, c10::intrusive_ptr<c10d::ProcessGroup, c10::detail::intrusive_target_default_null_type<c10d::ProcessGroup> > const&, c10::intrusive_ptr<c10d::ReduceOp, c10::detail::intrusive_target_default_null_type<c10d::ReduceOp> > const&, std::optional<at::Tensor> const&, long>()  :0
18 0x000000000629f8d9 c10::impl::make_boxed_from_unboxed_functor<c10::impl::detail::WrapFunctionIntoRuntimeFunctor_<std::tuple<std::vector<at::Tensor, std::allocator<at::Tensor> >, c10::intrusive_ptr<c10d::Work, c10::detail::intrusive_target_default_null_type<c10d::Work> > > (*)(c10::ArrayRef<at::Tensor>, c10::intrusive_ptr<c10d::ProcessGroup, c10::detail::intrusive_target_default_null_type<c10d::ProcessGroup> > const&, c10::intrusive_ptr<c10d::ReduceOp, c10::detail::intrusive_target_default_null_type<c10d::ReduceOp> > const&, std::optional<at::Tensor> const&, long), std::tuple<std::vector<at::Tensor, std::allocator<at::Tensor> >, c10::intrusive_ptr<c10d::Work, c10::detail::intrusive_target_default_null_type<c10d::Work> > >, c10::guts::typelist::typelist<c10::ArrayRef<at::Tensor>, c10::intrusive_ptr<c10d::ProcessGroup, c10::detail::intrusive_target_default_null_type<c10d::ProcessGroup> > const&, c10::intrusive_ptr<c10d::ReduceOp, c10::detail::intrusive_target_default_null_type<c10d::ReduceOp> > const&, std::optional<at::Tensor> const&, long> >, false>::call()  :0
19 0x00000000058a83bb c10::OperatorHandle::redispatchBoxed()  :0
20 0x00000000058a5c39 torch::autograd::basicAutogradNotImplementedFallbackImpl()  autograd_not_implemented_fallback.cpp:0
21 0x0000000001aa3808 c10::BoxedKernel::make_boxed_function<&(anonymous namespace)::autograd_fallback>()  VariableFallbackKernel.cpp:0
22 0x00000000062a7385 c10::impl::BoxedKernelWrapper<std::tuple<std::vector<at::Tensor, std::allocator<at::Tensor> >, c10::intrusive_ptr<c10d::Work, c10::detail::intrusive_target_default_null_type<c10d::Work> > > (c10::ArrayRef<at::Tensor>, c10::intrusive_ptr<c10d::ProcessGroup, c10::detail::intrusive_target_default_null_type<c10d::ProcessGroup> > const&, c10::intrusive_ptr<c10d::ReduceOp, c10::detail::intrusive_target_default_null_type<c10d::ReduceOp> > const&, std::optional<at::Tensor> const&, long), void>::call()  :0
23 0x00000000062b58cd c10d::ProcessGroup::allreduce()  :0
24 0x0000000006275480 (anonymous namespace)::all_reduce_()  Functional.cpp:0
25 0x0000000006282f17 c10::impl::call_functor_with_args_from_stack_<c10::impl::detail::WrapFunctionIntoRuntimeFunctor_<at::Tensor& (*)(at::Tensor&, std::string, std::string), at::Tensor&, c10::guts::typelist::typelist<at::Tensor&, std::string, std::string> >, false, 0ul, 1ul, 2ul, at::Tensor&, std::string, std::string>()  :0
26 0x0000000006283085 c10::impl::make_boxed_from_unboxed_functor<c10::impl::detail::WrapFunctionIntoRuntimeFunctor_<at::Tensor& (*)(at::Tensor&, std::string, std::string), at::Tensor&, c10::guts::typelist::typelist<at::Tensor&, std::string, std::string> >, false>::call()  :0
27 0x00000000058a83bb c10::OperatorHandle::redispatchBoxed()  :0
28 0x00000000058a5c39 torch::autograd::basicAutogradNotImplementedFallbackImpl()  autograd_not_implemented_fallback.cpp:0
29 0x0000000001aa3808 c10::BoxedKernel::make_boxed_function<&(anonymous namespace)::autograd_fallback>()  VariableFallbackKernel.cpp:0
30 0x0000000000cff908 c10::Dispatcher::callBoxed()  ???:0
31 0x0000000000a89c46 torch::jit::invokeOperatorFromPython()  ???:0
32 0x0000000000a89fa7 torch::jit::_get_operation_for_overload_or_packet()  ???:0
33 0x000000000097ada2 pybind11::cpp_function::initialize<torch::jit::initJITBindings(_object*)::{lambda(std::string const&, std::string const&)#214}::operator()(std::string const&, std::string const&) const::{lambda(pybind11::args const&, pybind11::kwargs const&)#1}, pybind11::object, pybind11::args const&, pybind11::kwargs const&>(torch::jit::initJITBindings(_object*)::{lambda(std::string const&, std::string const&)#214}::operator()(std::string const&, std::string const&) const::{lambda(pybind11::args const&, pybind11::kwargs const&)#1}&&, pybind11::object (*)(pybind11::args const&, pybind11::kwargs const&))::{lambda(pybind11::detail::function_call&)#3}::_FUN()  init.cpp:0
34 0x00000000004d9367 pybind11::cpp_function::dispatcher()  :0
35 0x00000000004fdc87 cfunction_call()  /usr/local/src/conda/python-3.10.14/Objects/methodobject.c:543
36 0x00000000004fdc87 _Py_CheckFunctionResult()  /usr/local/src/conda/python-3.10.14/Objects/call.c:39
37 0x00000000004fdc87 cfunction_call()  /usr/local/src/conda/python-3.10.14/Objects/methodobject.c:554
38 0x000000000050a659 _PyObject_Call()  /usr/local/src/conda/python-3.10.14/Objects/call.c:305
39 0x000000000050a659 _PyObject_Call()  /usr/local/src/conda/python-3.10.14/Objects/call.c:307
40 0x000000000050a659 PyObject_Call()  /usr/local/src/conda/python-3.10.14/Objects/call.c:317
41 0x00000000004f3b64 do_call_core()  /usr/local/src/conda/python-3.10.14/Python/ceval.c:5917
42 0x00000000004f3b64 _PyEval_EvalFrameDefault()  /usr/local/src/conda/python-3.10.14/Python/ceval.c:4277
43 0x00000000004f676d _PyEval_EvalFrame()  /usr/local/src/conda/python-3.10.14/Include/internal/pycore_ceval.h:46
44 0x00000000004f676d _PyEval_Vector()  /usr/local/src/conda/python-3.10.14/Python/ceval.c:5074
45 0x00000000004f676d _PyFunction_Vectorcall()  /usr/local/src/conda/python-3.10.14/Objects/call.c:342
46 0x00000000004f676d _PyObject_FastCallDictTstate()  /usr/local/src/conda/python-3.10.14/Objects/call.c:142
47 0x0000000000507f36 _PyObject_Call_Prepend()  /usr/local/src/conda/python-3.10.14/Objects/call.c:431
48 0x00000000005cf883 slot_tp_call()  /usr/local/src/conda/python-3.10.14/Objects/typeobject.c:7494
49 0x00000000005cf883 slot_tp_call()  /usr/local/src/conda/python-3.10.14/Objects/typeobject.c:7500
50 0x00000000004f741b _PyObject_MakeTpCall()  /usr/local/src/conda/python-3.10.14/Objects/call.c:215
51 0x00000000004f741b _PyObject_MakeTpCall()  /usr/local/src/conda/python-3.10.14/Objects/call.c:216
52 0x00000000004f34c6 _PyObject_VectorcallTstate()  /usr/local/src/conda/python-3.10.14/Include/cpython/abstract.h:112
53 0x00000000004f34c6 _PyObject_VectorcallTstate()  /usr/local/src/conda/python-3.10.14/Include/cpython/abstract.h:99
54 0x00000000004f34c6 PyObject_Vectorcall()  /usr/local/src/conda/python-3.10.14/Include/cpython/abstract.h:123
55 0x00000000004f34c6 call_function()  /usr/local/src/conda/python-3.10.14/Python/ceval.c:5893
56 0x00000000004f34c6 _PyEval_EvalFrameDefault()  /usr/local/src/conda/python-3.10.14/Python/ceval.c:4181
=================================
W1210 17:22:37.379000 1123991 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 1124062 closing signal SIGTERM
W1210 17:22:37.386000 1123991 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 1124063 closing signal SIGTERM
W1210 17:22:37.394000 1123991 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 1124064 closing signal SIGTERM
W1210 17:22:37.398000 1123991 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 1124066 closing signal SIGTERM
W1210 17:22:37.408000 1123991 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 1124067 closing signal SIGTERM
W1210 17:22:37.418000 1123991 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 1124068 closing signal SIGTERM
W1210 17:22:37.424000 1123991 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 1124069 closing signal SIGTERM
E1210 17:22:39.081000 1123991 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: -11) local_rank: 3 (pid: 1124065) of binary: /home/charlie/anaconda3/envs/gpt-fast/bin/python
Traceback (most recent call last):
  File "/home/charlie/anaconda3/envs/gpt-fast/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
=========================================================
benchmark.py FAILED
---------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
---------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-12-10_17:22:37
  host      : mk-xii-21.cloud.together.ai
  rank      : 3 (local_rank: 3)
  exitcode  : -11 (pid: 1124065)
  error_file: <N/A>
  traceback : Signal 11 (SIGSEGV) received by PID 1124065
=========================================================
