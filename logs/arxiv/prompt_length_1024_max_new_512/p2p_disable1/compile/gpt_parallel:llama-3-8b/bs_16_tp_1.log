rank: 0, global_rank: 0, world_size: 1, global_world_size: 1
rank: 0, global_rank: 0, world_size: 1, global_world_size: 1
rank: 0, global_rank: 0, world_size: 1, global_world_size: 1
rank: 0, global_rank: 0, world_size: 1, global_world_size: 1
rank: 0, global_rank: 0, world_size: 1, global_world_size: 1
flash_kv_decode is set to False
rank: 0, global_rank: 0, world_size: 1, global_world_size: 1
our world size=1
Using device=cuda
Loading model ...
GPTParallel(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x ParallelTransformerBlock(
      semi_compiled = False
      (attention): FuseAttentionMLP(
        (wqkv1): Linear(in_features=4096, out_features=34816, bias=False)
        (wo): Linear(in_features=4096, out_features=4096, bias=False)
        (w2): Linear(in_features=14336, out_features=4096, bias=False)
      )
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.02 seconds
Compiling prefill with dynamic=False
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
Prefill latency: 46.68638277892023 sec
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 50.45028582401574 sec
Compilation time: 97.14 seconds
Prefill latency: 0.776615765877068 sec
Decode latency: 5.270387995988131 sec
Prefill latency: 0.7790846107527614 sec
Decode latency: 5.270235643722117 sec
Prefill latency: 0.7801307206973433 sec
Decode latency: 5.272196946665645 sec
Prefill latency: 0.7816802076995373 sec
Decode latency: 5.2747510224580765 sec
Prefill latency: 0.7819123882800341 sec
Decode latency: 5.270116504281759 sec
Time for inference 1: 6.05 sec total, 1353.07 tokens/sec
Decode latency: 5.27 sec
Prefill latency: 0.78 sec
Bandwidth achieved: 20309.03 GB/s
FLOPS achieved: 60.93 TF/s

Prefill latency: 0.7832165863364935 sec
Decode latency: 5.272431532852352 sec
Time for inference 2: 6.06 sec total, 1352.32 tokens/sec
Decode latency: 5.27 sec
Prefill latency: 0.78 sec
Bandwidth achieved: 20297.71 GB/s
FLOPS achieved: 60.89 TF/s

Prefill latency: 0.7803660314530134 sec
Decode latency: 5.270573475398123 sec
Time for inference 3: 6.05 sec total, 1353.41 tokens/sec
Decode latency: 5.27 sec
Prefill latency: 0.78 sec
Bandwidth achieved: 20314.16 GB/s
FLOPS achieved: 60.94 TF/s

Prefill latency: 0.7815070161595941 sec
Decode latency: 5.273404116742313 sec
Time for inference 4: 6.06 sec total, 1352.54 tokens/sec
Decode latency: 5.27 sec
Prefill latency: 0.78 sec
Bandwidth achieved: 20301.04 GB/s
FLOPS achieved: 60.90 TF/s

Prefill latency: 0.780872899107635 sec
Decode latency: 5.269752719439566 sec
Time for inference 5: 6.05 sec total, 1353.54 tokens/sec
Decode latency: 5.27 sec
Prefill latency: 0.78 sec
Bandwidth achieved: 20316.06 GB/s
FLOPS achieved: 60.95 TF/s

Prefill latency: 0.7814609510824084 sec
Decode latency: 5.27133721113205 sec
Time for inference 6: 6.05 sec total, 1352.95 tokens/sec
Decode latency: 5.27 sec
Prefill latency: 0.78 sec
Bandwidth achieved: 20307.15 GB/s
FLOPS achieved: 60.92 TF/s

Prefill latency: 0.782228454016149 sec
Decode latency: 5.272396168671548 sec
Time for inference 7: 6.06 sec total, 1352.56 tokens/sec
Decode latency: 5.27 sec
Prefill latency: 0.78 sec
Bandwidth achieved: 20301.42 GB/s
FLOPS achieved: 60.90 TF/s

Prefill latency: 0.7799766166135669 sec
Decode latency: 5.272668738849461 sec
Time for inference 8: 6.05 sec total, 1353.00 tokens/sec
Decode latency: 5.27 sec
Prefill latency: 0.78 sec
Bandwidth achieved: 20307.94 GB/s
FLOPS achieved: 60.92 TF/s

Prefill latency: 0.7809121059253812 sec
Decode latency: 5.273868163116276 sec
Time for inference 9: 6.06 sec total, 1352.54 tokens/sec
Decode latency: 5.27 sec
Prefill latency: 0.78 sec
Bandwidth achieved: 20301.06 GB/s
FLOPS achieved: 60.90 TF/s

Prefill latency: 0.7820543069392443 sec
Decode latency: 5.273121659643948 sec
Time for inference 10: 6.06 sec total, 1352.50 tokens/sec
Decode latency: 5.27 sec
Prefill latency: 0.78 sec
Bandwidth achieved: 20300.53 GB/s
FLOPS achieved: 60.90 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 5.2720 sec
Average prefill latency: 0.7815 sec
Average tokens/sec: 1352.84
Memory used: 32.64 GB
Done. we are killing the process
[rank0]:[W1210 17:25:49.842631310 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
