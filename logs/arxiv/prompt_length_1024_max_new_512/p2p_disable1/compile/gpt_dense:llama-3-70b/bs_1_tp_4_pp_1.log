W1208 16:12:15.378000 383049 site-packages/torch/distributed/run.py:793] 
W1208 16:12:15.378000 383049 site-packages/torch/distributed/run.py:793] *****************************************
W1208 16:12:15.378000 383049 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1208 16:12:15.378000 383049 site-packages/torch/distributed/run.py:793] *****************************************
DeviceMesh('cuda', [[0, 1, 2, 3]], mesh_dim_names=('pp', 'tp'))
Using device=cuda
Loading model ...
DeviceMesh('cuda', [[0, 1, 2, 3]], mesh_dim_names=('pp', 'tp'))
DeviceMesh('cuda', [[0, 1, 2, 3]], mesh_dim_names=('pp', 'tp'))
DeviceMesh('cuda', [[0, 1, 2, 3]], mesh_dim_names=('pp', 'tp'))
GPTDense(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x DenseTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=2560, bias=False)
        (wo): Linear(in_features=2048, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=14336, bias=False)
        (w2): Linear(in_features=7168, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 1.49 seconds
Compiling prefill with dynamic=False
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
Prefill latency: 76.39446648396552 sec
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 103.86042272299528 sec
Compilation time: 180.11 seconds
Compilation time: 180.16 seconds
Compilation time: 180.26 seconds
Compilation time: 180.13 seconds
Prefill latency: 0.12940099835395813 sec
Decode latency: 9.388953080400825 sec
Prefill latency: 0.1258208230137825 sec
Decode latency: 9.38508173264563 sec
Prefill latency: 0.1256280429661274 sec
Decode latency: 9.386509504169226 sec
Prefill latency: 0.12548447586596012 sec
Decode latency: 9.384587792679667 sec
Prefill latency: 0.1286216899752617 sec
Decode latency: 9.386747241020203 sec
Time for inference 1: 9.52 sec total, 53.79 tokens/sec
Decode latency: 9.39 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 1954.32 GB/s
FLOPS achieved: 5.86 TF/s

Prefill latency: 0.12635526433587074 sec
Decode latency: 9.383532604202628 sec
Time for inference 2: 9.51 sec total, 53.82 tokens/sec
Decode latency: 9.38 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 1955.37 GB/s
FLOPS achieved: 5.87 TF/s

Prefill latency: 0.12551749497652054 sec
Decode latency: 9.38667113147676 sec
Time for inference 3: 9.52 sec total, 53.81 tokens/sec
Decode latency: 9.39 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 1954.87 GB/s
FLOPS achieved: 5.86 TF/s

Prefill latency: 0.12738222815096378 sec
Decode latency: 9.387253884226084 sec
Time for inference 4: 9.52 sec total, 53.80 tokens/sec
Decode latency: 9.39 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 1954.48 GB/s
FLOPS achieved: 5.86 TF/s

Prefill latency: 0.12606182880699635 sec
Decode latency: 9.38604854233563 sec
Time for inference 5: 9.51 sec total, 53.81 tokens/sec
Decode latency: 9.39 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 1954.93 GB/s
FLOPS achieved: 5.86 TF/s

Prefill latency: 0.1265369988977909 sec
Decode latency: 9.38511611521244 sec
Time for inference 6: 9.51 sec total, 53.81 tokens/sec
Decode latency: 9.39 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 1954.95 GB/s
FLOPS achieved: 5.86 TF/s

Prefill latency: 0.12581429071724415 sec
Decode latency: 9.388362126424909 sec
Time for inference 7: 9.52 sec total, 53.80 tokens/sec
Decode latency: 9.39 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 1954.46 GB/s
FLOPS achieved: 5.86 TF/s

Prefill latency: 0.12574485689401627 sec
Decode latency: 9.386357959359884 sec
Time for inference 8: 9.51 sec total, 53.81 tokens/sec
Decode latency: 9.39 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 1954.91 GB/s
FLOPS achieved: 5.86 TF/s

Prefill latency: 0.12561865709722042 sec
Decode latency: 9.385253895074129 sec
Time for inference 9: 9.51 sec total, 53.82 tokens/sec
Decode latency: 9.39 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 1955.19 GB/s
FLOPS achieved: 5.87 TF/s

Prefill latency: 0.12607051245868206 sec
Decode latency: 9.38483233936131 sec
Time for inference 10: 9.51 sec total, 53.82 tokens/sec
Decode latency: 9.38 sec
Prefill latency: 0.13 sec
Bandwidth achieved: 1955.17 GB/s
FLOPS achieved: 5.87 TF/s

==========
Batch Size: 1
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 9.3860 sec
Average prefill latency: 0.1264 sec
Average tokens/sec: 53.81
Memory used: 39.80 GB
Done. we are killing the process
[rank3]:[W1208 16:17:38.782674353 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank2]:[W1208 16:17:38.168252357 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank0]:[W1208 16:17:38.227496561 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank1]:[W1208 16:17:39.705363369 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
