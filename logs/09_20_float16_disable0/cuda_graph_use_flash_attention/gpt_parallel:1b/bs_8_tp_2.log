W1001 02:12:05.592000 23058552489792 torch/distributed/run.py:779] 
W1001 02:12:05.592000 23058552489792 torch/distributed/run.py:779] *****************************************
W1001 02:12:05.592000 23058552489792 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1001 02:12:05.592000 23058552489792 torch/distributed/run.py:779] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTParallel(
  (tok_embeddings): Embedding(49152, 1536)
  (layers): ModuleList(
    (0-39): 40 x ParallelTransformerBlock(
      semi_compiled = False
      (attention): FuseAttentionMLP(
        (wqkv1): Linear(in_features=1536, out_features=6400, bias=False)
        (wo): Linear(in_features=768, out_features=1536, bias=False)
        (w2): Linear(in_features=2048, out_features=1536, bias=False)
      )
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=1536, out_features=49152, bias=False)
)
Time to load model: 1.03 seconds
Prefill latency: 0.06567678099963814 sec
Decode latency: 2.382586265914142 sec
Compilation time: 2.45 seconds
Compilation time: 2.45 seconds
Prefill latency: 0.060923335957340896 sec
Decode latency: 2.383054383099079 sec
Prefill latency: 0.06101632001809776 sec
Decode latency: 2.3839601480867714 sec
Prefill latency: 0.06097771099302918 sec
Decode latency: 2.3814945879857987 sec
Prefill latency: 0.0610493419226259 sec
Decode latency: 2.3810382350347936 sec
Prefill latency: 0.06097818003036082 sec
Decode latency: 2.382261647027917 sec
Time for inference 1: 2.44 sec total, 837.91 tokens/sec
Decode latency: 2.38 sec
Prefill latency: 0.06 sec
Bandwidth achieved: 1075.53 GB/s
FLOPS achieved: 5.38 TF/s

Prefill latency: 0.061078414088115096 sec
Decode latency: 2.381658933009021 sec
Time for inference 2: 2.44 sec total, 838.13 tokens/sec
Decode latency: 2.38 sec
Prefill latency: 0.06 sec
Bandwidth achieved: 1075.80 GB/s
FLOPS achieved: 5.38 TF/s

Prefill latency: 0.060979404021054506 sec
Decode latency: 2.381560769979842 sec
Time for inference 3: 2.44 sec total, 838.13 tokens/sec
Decode latency: 2.38 sec
Prefill latency: 0.06 sec
Bandwidth achieved: 1075.80 GB/s
FLOPS achieved: 5.38 TF/s

Prefill latency: 0.06106641096994281 sec
Decode latency: 2.3835859280079603 sec
Time for inference 4: 2.45 sec total, 837.42 tokens/sec
Decode latency: 2.38 sec
Prefill latency: 0.06 sec
Bandwidth achieved: 1074.89 GB/s
FLOPS achieved: 5.37 TF/s

Prefill latency: 0.061014965060167015 sec
Decode latency: 2.3822539559332654 sec
Time for inference 5: 2.44 sec total, 837.92 tokens/sec
Decode latency: 2.38 sec
Prefill latency: 0.06 sec
Bandwidth achieved: 1075.54 GB/s
FLOPS achieved: 5.38 TF/s

Prefill latency: 0.06103514099959284 sec
Decode latency: 2.3829199459869415 sec
Time for inference 6: 2.44 sec total, 837.70 tokens/sec
Decode latency: 2.38 sec
Prefill latency: 0.06 sec
Bandwidth achieved: 1075.26 GB/s
FLOPS achieved: 5.38 TF/s

Prefill latency: 0.06097901298198849 sec
Decode latency: 2.3820787029108033 sec
Time for inference 7: 2.44 sec total, 838.01 tokens/sec
Decode latency: 2.38 sec
Prefill latency: 0.06 sec
Bandwidth achieved: 1075.66 GB/s
FLOPS achieved: 5.38 TF/s

Prefill latency: 0.06109548406675458 sec
Decode latency: 2.381421878002584 sec
Time for inference 8: 2.44 sec total, 838.23 tokens/sec
Decode latency: 2.38 sec
Prefill latency: 0.06 sec
Bandwidth achieved: 1075.94 GB/s
FLOPS achieved: 5.38 TF/s

Prefill latency: 0.06105428992304951 sec
Decode latency: 2.3818593590985984 sec
Time for inference 9: 2.44 sec total, 838.10 tokens/sec
Decode latency: 2.38 sec
Prefill latency: 0.06 sec
Bandwidth achieved: 1075.78 GB/s
FLOPS achieved: 5.38 TF/s

Prefill latency: 0.061101582017727196 sec
Decode latency: 2.3825082749826834 sec
Time for inference 10: 2.44 sec total, 837.83 tokens/sec
Decode latency: 2.38 sec
Prefill latency: 0.06 sec
Bandwidth achieved: 1075.42 GB/s
FLOPS achieved: 5.38 TF/s

==========
Batch Size: 8
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 2.3822 sec
Average prefill latency: 0.0610 sec
Average tokens/sec: 837.94
Memory used: 5.48 GB
Done. we are killing the process
