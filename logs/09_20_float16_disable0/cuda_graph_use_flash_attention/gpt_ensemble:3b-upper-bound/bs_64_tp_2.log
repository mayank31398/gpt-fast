W1001 02:03:03.650000 23151559575360 torch/distributed/run.py:779] 
W1001 02:03:03.650000 23151559575360 torch/distributed/run.py:779] *****************************************
W1001 02:03:03.650000 23151559575360 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1001 02:03:03.650000 23151559575360 torch/distributed/run.py:779] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(49152, 2304)
  (layers): ModuleList(
    (0-39): 40 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=2304, out_features=3456, bias=False)
        (wo): Linear(in_features=1152, out_features=2304, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=2304, out_features=9216, bias=False)
        (w2): Linear(in_features=4608, out_features=2304, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=2304, out_features=49152, bias=False)
)
Time to load model: 1.04 seconds
Prefill latency: 0.8264708460774273 sec
Compilation time: 8.07 seconds
Decode latency: 7.217997012892738 sec
Compilation time: 8.05 seconds
Prefill latency: 0.8213437019148842 sec
Decode latency: 7.215355082997121 sec
Prefill latency: 0.8233726429753006 sec
Decode latency: 7.217655677930452 sec
Prefill latency: 0.8221549849258736 sec
Decode latency: 7.219381989911199 sec
Prefill latency: 0.8204320339718834 sec
Decode latency: 7.220264118979685 sec
Prefill latency: 0.8316705139586702 sec
Decode latency: 7.220790505991317 sec
Time for inference 1: 8.05 sec total, 2034.41 tokens/sec
Decode latency: 7.22 sec
Prefill latency: 0.83 sec
Bandwidth achieved: 7373.23 GB/s
FLOPS achieved: 36.87 TF/s

Prefill latency: 0.824552835081704 sec
