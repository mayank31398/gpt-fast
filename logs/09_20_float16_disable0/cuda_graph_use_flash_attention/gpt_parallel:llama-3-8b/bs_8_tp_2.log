W0928 17:13:09.998000 22520563324736 torch/distributed/run.py:779] 
W0928 17:13:09.998000 22520563324736 torch/distributed/run.py:779] *****************************************
W0928 17:13:09.998000 22520563324736 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0928 17:13:09.998000 22520563324736 torch/distributed/run.py:779] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTParallel(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x ParallelTransformerBlock(
      semi_compiled = False
      (attention): FuseAttentionMLP(
        (wqkv1): Linear(in_features=4096, out_features=17408, bias=False)
        (wo): Linear(in_features=2048, out_features=4096, bias=False)
        (w2): Linear(in_features=7168, out_features=4096, bias=False)
      )
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 0.92 seconds
Prefill latency: 0.17268904112279415 sec
Decode latency: 3.106729251332581 sec
Compilation time: 3.28 seconds
Compilation time: 3.28 seconds
Prefill latency: 0.16824111249297857 sec
Decode latency: 3.106660076417029 sec
Prefill latency: 0.16725372802466154 sec
Decode latency: 3.1067801620811224 sec
Prefill latency: 0.16830186266452074 sec
Decode latency: 3.1083223670721054 sec
Prefill latency: 0.1669961716979742 sec
Decode latency: 3.107141826301813 sec
Prefill latency: 0.16865942813456059 sec
Decode latency: 3.1064905263483524 sec
Time for inference 1: 3.28 sec total, 625.20 tokens/sec
Decode latency: 3.11 sec
Prefill latency: 0.17 sec
Bandwidth achieved: 5020.51 GB/s
FLOPS achieved: 25.10 TF/s

Prefill latency: 0.1692166170105338 sec
Decode latency: 3.1071105282753706 sec
Time for inference 2: 3.28 sec total, 624.95 tokens/sec
Decode latency: 3.11 sec
Prefill latency: 0.17 sec
Bandwidth achieved: 5018.54 GB/s
FLOPS achieved: 25.09 TF/s

Prefill latency: 0.16849092487245798 sec
Decode latency: 3.1075202636420727 sec
Time for inference 3: 3.28 sec total, 625.01 tokens/sec
Decode latency: 3.11 sec
Prefill latency: 0.17 sec
Bandwidth achieved: 5019.02 GB/s
FLOPS achieved: 25.10 TF/s

Prefill latency: 0.1693141544237733 sec
Decode latency: 3.1071142395958304 sec
Time for inference 4: 3.28 sec total, 624.95 tokens/sec
Decode latency: 3.11 sec
Prefill latency: 0.17 sec
Bandwidth achieved: 5018.54 GB/s
FLOPS achieved: 25.09 TF/s

Prefill latency: 0.16846254654228687 sec
Decode latency: 3.1072616884484887 sec
Time for inference 5: 3.28 sec total, 625.09 tokens/sec
Decode latency: 3.11 sec
Prefill latency: 0.17 sec
Bandwidth achieved: 5019.65 GB/s
FLOPS achieved: 25.10 TF/s

Prefill latency: 0.16825186274945736 sec
Decode latency: 3.1061666421592236 sec
Time for inference 6: 3.28 sec total, 625.34 tokens/sec
Decode latency: 3.11 sec
Prefill latency: 0.17 sec
Bandwidth achieved: 5021.67 GB/s
FLOPS achieved: 25.11 TF/s

Prefill latency: 0.1683886693790555 sec
Decode latency: 3.105604005046189 sec
Time for inference 7: 3.27 sec total, 625.42 tokens/sec
Decode latency: 3.11 sec
Prefill latency: 0.17 sec
Bandwidth achieved: 5022.31 GB/s
FLOPS achieved: 25.11 TF/s

Prefill latency: 0.1689609782770276 sec
Decode latency: 3.105049369856715 sec
Time for inference 8: 3.27 sec total, 625.42 tokens/sec
Decode latency: 3.11 sec
Prefill latency: 0.17 sec
Bandwidth achieved: 5022.27 GB/s
FLOPS achieved: 25.11 TF/s

Prefill latency: 0.1688481578603387 sec
Decode latency: 3.1055894047021866 sec
Time for inference 9: 3.28 sec total, 625.34 tokens/sec
Decode latency: 3.11 sec
Prefill latency: 0.17 sec
Bandwidth achieved: 5021.61 GB/s
FLOPS achieved: 25.11 TF/s

Prefill latency: 0.16786125022917986 sec
Decode latency: 3.106186119839549 sec
Time for inference 10: 3.27 sec total, 625.41 tokens/sec
Decode latency: 3.11 sec
Prefill latency: 0.17 sec
Bandwidth achieved: 5022.24 GB/s
FLOPS achieved: 25.11 TF/s

==========
Batch Size: 8
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 3.1064 sec
Average prefill latency: 0.1686 sec
Average tokens/sec: 625.21
Memory used: 17.61 GB
Done. we are killing the process
