W1130 15:30:23.093000 3350201 site-packages/torch/distributed/run.py:793] 
W1130 15:30:23.093000 3350201 site-packages/torch/distributed/run.py:793] *****************************************
W1130 15:30:23.093000 3350201 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1130 15:30:23.093000 3350201 site-packages/torch/distributed/run.py:793] *****************************************
DeviceMesh('cuda', [[0], [1]], mesh_dim_names=('pp', 'tp'))
DeviceMesh('cuda', [[0], [1]], mesh_dim_names=('pp', 'tp'))
Using device=cuda
Loading model ...
GPTParallel(
  (layers): ModuleList(
    (0-15): 16 x ParallelTransformerBlock(
      semi_compiled = False
      (attention): FuseAttentionMLP(
        (wqkv1): Linear(in_features=4096, out_features=34816, bias=False)
        (wo): Linear(in_features=4096, out_features=4096, bias=False)
        (w2): Linear(in_features=14336, out_features=4096, bias=False)
      )
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 0.72 seconds
Compiling prefill with dynamic=False
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:222: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
Prefill latency: 59.29118165397085 sec
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Compilation time: 123.55 seconds
Decode latency: 64.31179708591662 sec
Compilation time: 123.61 seconds
Prefill latency: 0.06374792195856571 sec
Decode latency: 3.11142214294523 sec
Prefill latency: 0.048607705160975456 sec
Decode latency: 3.111329552019015 sec
Prefill latency: 0.04932685010135174 sec
Decode latency: 3.11083454801701 sec
Prefill latency: 0.04917829390615225 sec
Decode latency: 3.113093617837876 sec
Prefill latency: 0.0512739778496325 sec
Decode latency: 3.1108675349969417 sec
Time for inference 1: 3.16 sec total, 161.83 tokens/sec
Decode latency: 3.11 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 1299.50 GB/s
FLOPS achieved: 3.90 TF/s

Prefill latency: 0.04967354005202651 sec
Decode latency: 3.111043202923611 sec
Time for inference 2: 3.16 sec total, 161.90 tokens/sec
Decode latency: 3.11 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 1300.10 GB/s
FLOPS achieved: 3.90 TF/s

Prefill latency: 0.05141702899709344 sec
Decode latency: 3.111367305042222 sec
Time for inference 3: 3.16 sec total, 161.79 tokens/sec
Decode latency: 3.11 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 1299.23 GB/s
FLOPS achieved: 3.90 TF/s

Prefill latency: 0.04941485612653196 sec
Decode latency: 3.112047428963706 sec
Time for inference 4: 3.16 sec total, 161.86 tokens/sec
Decode latency: 3.11 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 1299.80 GB/s
FLOPS achieved: 3.90 TF/s

Prefill latency: 0.049554368015378714 sec
Decode latency: 3.111712040146813 sec
Time for inference 5: 3.16 sec total, 161.87 tokens/sec
Decode latency: 3.11 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 1299.85 GB/s
FLOPS achieved: 3.90 TF/s

Prefill latency: 0.049329994013532996 sec
Decode latency: 3.1119060209020972 sec
Time for inference 6: 3.16 sec total, 161.81 tokens/sec
Decode latency: 3.11 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 1299.37 GB/s
FLOPS achieved: 3.90 TF/s

Prefill latency: 0.04888542904518545 sec
Decode latency: 3.1115668199490756 sec
Time for inference 7: 3.16 sec total, 161.90 tokens/sec
Decode latency: 3.11 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 1300.11 GB/s
FLOPS achieved: 3.90 TF/s

Prefill latency: 0.05061698402278125 sec
Decode latency: 3.111450648168102 sec
Time for inference 8: 3.16 sec total, 161.83 tokens/sec
Decode latency: 3.11 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 1299.51 GB/s
FLOPS achieved: 3.90 TF/s

Prefill latency: 0.04914045496843755 sec
Decode latency: 3.111406843876466 sec
Time for inference 9: 3.16 sec total, 161.91 tokens/sec
Decode latency: 3.11 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 1300.16 GB/s
FLOPS achieved: 3.90 TF/s

Prefill latency: 0.04907111497595906 sec
Decode latency: 3.111080410890281 sec
Time for inference 10: 3.16 sec total, 161.93 tokens/sec
Decode latency: 3.11 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 1300.29 GB/s
FLOPS achieved: 3.90 TF/s

==========
Batch Size: 1
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 3.1114 sec
Average prefill latency: 0.0498 sec
Average tokens/sec: 161.86
Memory used: 9.37 GB
Done. we are killing the process
[rank0]:[W1130 15:33:15.695619348 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank1]:[W1130 15:33:16.149025986 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
