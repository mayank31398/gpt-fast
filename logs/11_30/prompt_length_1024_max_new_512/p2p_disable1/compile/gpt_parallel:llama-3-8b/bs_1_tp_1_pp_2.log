W1130 21:45:45.746000 3444886 site-packages/torch/distributed/run.py:793] 
W1130 21:45:45.746000 3444886 site-packages/torch/distributed/run.py:793] *****************************************
W1130 21:45:45.746000 3444886 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1130 21:45:45.746000 3444886 site-packages/torch/distributed/run.py:793] *****************************************
DeviceMesh('cuda', [[0], [1]], mesh_dim_names=('pp', 'tp'))
DeviceMesh('cuda', [[0], [1]], mesh_dim_names=('pp', 'tp'))
Using device=cuda
Loading model ...
GPTParallel(
  (layers): ModuleList(
    (0-15): 16 x ParallelTransformerBlock(
      semi_compiled = False
      (attention): FuseAttentionMLP(
        (wqkv1): Linear(in_features=4096, out_features=34816, bias=False)
        (wo): Linear(in_features=4096, out_features=4096, bias=False)
        (w2): Linear(in_features=14336, out_features=4096, bias=False)
      )
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 0.70 seconds
Compiling prefill with dynamic=False
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Prefill latency: 12.49822406587191 sec
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 8.691465154988691 secCompilation time: 21.12 seconds

Compilation time: 21.19 seconds
Prefill latency: 0.05678386799991131 sec
Decode latency: 3.117146949050948 sec
Prefill latency: 0.05136867891997099 sec
Decode latency: 3.116765539860353 sec
Prefill latency: 0.04935417999513447 sec
Decode latency: 3.115614489885047 sec
Prefill latency: 0.05006009293720126 sec
Decode latency: 3.1152919630985707 sec
Prefill latency: 0.05105398711748421 sec
Decode latency: 3.1151519850827754 sec
Time for inference 1: 3.17 sec total, 161.63 tokens/sec
Decode latency: 3.12 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 1297.95 GB/s
FLOPS achieved: 3.89 TF/s

Prefill latency: 0.049821130000054836 sec
Decode latency: 3.1153191870544106 sec
Time for inference 2: 3.17 sec total, 161.67 tokens/sec
Decode latency: 3.12 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 1298.23 GB/s
FLOPS achieved: 3.89 TF/s

Prefill latency: 0.04927416192367673 sec
Decode latency: 3.1153966579586267 sec
Time for inference 3: 3.17 sec total, 161.67 tokens/sec
Decode latency: 3.12 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 1298.27 GB/s
FLOPS achieved: 3.89 TF/s

Prefill latency: 0.049345023930072784 sec
Decode latency: 3.1150700820144266 sec
Time for inference 4: 3.17 sec total, 161.72 tokens/sec
Decode latency: 3.12 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 1298.64 GB/s
FLOPS achieved: 3.90 TF/s

Prefill latency: 0.049895279109478 sec
Decode latency: 3.1155273818876594 sec
Time for inference 5: 3.17 sec total, 161.68 tokens/sec
Decode latency: 3.12 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 1298.30 GB/s
FLOPS achieved: 3.89 TF/s

Prefill latency: 0.04992687003687024 sec
Decode latency: 3.1158903958275914 sec
Time for inference 6: 3.17 sec total, 161.65 tokens/sec
Decode latency: 3.12 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 1298.09 GB/s
FLOPS achieved: 3.89 TF/s

Prefill latency: 0.049889168003574014 sec
Decode latency: 3.1152396658435464 sec
Time for inference 7: 3.17 sec total, 161.68 tokens/sec
Decode latency: 3.12 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 1298.34 GB/s
FLOPS achieved: 3.90 TF/s

Prefill latency: 0.04989267303608358 sec
Decode latency: 3.1158192111179233 sec
Time for inference 8: 3.17 sec total, 161.66 tokens/sec
Decode latency: 3.12 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 1298.12 GB/s
FLOPS achieved: 3.89 TF/s

Prefill latency: 0.04988333093933761 sec
Decode latency: 3.115631617838517 sec
Time for inference 9: 3.17 sec total, 161.66 tokens/sec
Decode latency: 3.12 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 1298.18 GB/s
FLOPS achieved: 3.89 TF/s

Prefill latency: 0.04986805305816233 sec
Decode latency: 3.11576424096711 sec
Time for inference 10: 3.17 sec total, 161.66 tokens/sec
Decode latency: 3.12 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 1298.14 GB/s
FLOPS achieved: 3.89 TF/s

==========
Batch Size: 1
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 3.1155 sec
Average prefill latency: 0.0499 sec
Average tokens/sec: 161.67
Memory used: 9.02 GB
Done. we are killing the process
[rank0]:[W1130 21:46:55.616132695 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank1]:[W1130 21:46:56.191949223 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
