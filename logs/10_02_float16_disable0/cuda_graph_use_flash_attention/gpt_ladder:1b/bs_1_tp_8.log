W1001 20:03:36.111000 23193929254720 torch/distributed/run.py:779] 
W1001 20:03:36.111000 23193929254720 torch/distributed/run.py:779] *****************************************
W1001 20:03:36.111000 23193929254720 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1001 20:03:36.111000 23193929254720 torch/distributed/run.py:779] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTLadder(
  (tok_embeddings): Embedding(49152, 1536)
  (layers): ModuleList(
    (0-39): 40 x LadderTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=1536, out_features=576, bias=False)
        (wo): Linear(in_features=192, out_features=1536, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=1536, out_features=1024, bias=False)
        (w2): Linear(in_features=512, out_features=1536, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=1536, out_features=49152, bias=False)
)
Time to load model: 0.17 seconds
[rank7]:[W1001 20:03:48.698537193 CUDAGraph.cpp:150] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank3]:[W1001 20:03:48.699013774 CUDAGraph.cpp:150] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank1]:[W1001 20:03:48.704858515 CUDAGraph.cpp:150] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank0]:[W1001 20:03:48.717495111 CUDAGraph.cpp:150] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank6]:[W1001 20:03:48.732935198 CUDAGraph.cpp:150] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
Prefill latency: 0.02460698108188808 sec
Decode latency: 1.8278992359992117 sec
Compilation time: 1.86 seconds
Compilation time: 1.86 secondsCompilation time: 1.85 seconds

Compilation time: 1.86 seconds
Compilation time: 1.85 seconds
Compilation time: 1.87 seconds
Compilation time: 1.88 seconds
Compilation time: 1.86 seconds
Prefill latency: 0.016329800942912698 sec
Decode latency: 1.8399911959422752 sec
Prefill latency: 0.017248672083951533 sec
Decode latency: 1.8470834430772811 sec
Prefill latency: 0.016938050044700503 sec
Decode latency: 1.8471887989435345 sec
Prefill latency: 0.019988809013739228 sec
Decode latency: 1.850550269940868 sec
Prefill latency: 0.019965656916610897 sec
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 490, in <module>
[rank0]:     main(
[rank0]:   File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 401, in main
[rank0]:     y, decode_latency, prefill_latency = generate_using_cuda_graphs(
[rank0]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 208, in generate_using_cuda_graphs
[rank0]:     new_tokens.append(static_next_token_decode.clone())
[rank0]: RuntimeError: CUDA error: unspecified launch failure
[rank0]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[rank0]: For debugging consider passing CUDA_LAUNCH_BLOCKING=1
[rank0]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

W1001 20:03:59.672000 23193929254720 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2068659 closing signal SIGTERM
W1001 20:03:59.672000 23193929254720 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2068660 closing signal SIGTERM
W1001 20:03:59.673000 23193929254720 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2068661 closing signal SIGTERM
W1001 20:03:59.673000 23193929254720 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2068662 closing signal SIGTERM
W1001 20:03:59.674000 23193929254720 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2068663 closing signal SIGTERM
W1001 20:03:59.675000 23193929254720 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2068664 closing signal SIGTERM
W1001 20:03:59.675000 23193929254720 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2068665 closing signal SIGTERM
E1001 20:04:00.369000 23193929254720 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 0 (pid: 2068658) of binary: /home/charlie/anaconda3/envs/gpt-fast/bin/python
Traceback (most recent call last):
  File "/home/charlie/anaconda3/envs/gpt-fast/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
benchmark.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-10-01_20:03:59
  host      : mk-xii-02.cloud.together.ai
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 2068658)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
