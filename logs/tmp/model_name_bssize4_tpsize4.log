W0919 17:26:37.857000 23034996303680 torch/distributed/run.py:779] 
W0919 17:26:37.857000 23034996303680 torch/distributed/run.py:779] *****************************************
W0919 17:26:37.857000 23034996303680 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0919 17:26:37.857000 23034996303680 torch/distributed/run.py:779] *****************************************
flash_kv_decode is set to False
Using device=cuda
Loading model ...
GPTLadder(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x LadderTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=1536, bias=False)
        (wo): Linear(in_features=1024, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 0.12 seconds
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Compiling prefill with dynamic=False
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
[rank2]:W0919 17:27:07.005000 23184895285056 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] xindex is not in var_ranges, defaulting to unknown range.
[rank3]:W0919 17:27:07.165000 23119487874880 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] xindex is not in var_ranges, defaulting to unknown range.
[rank1]:W0919 17:27:07.242000 22701912311616 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] xindex is not in var_ranges, defaulting to unknown range.
[rank0]:W0919 17:27:07.293000 23118079194944 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] xindex is not in var_ranges, defaulting to unknown range.
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Prefill latency: 51.59909866563976 sec
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
[rank3]:W0919 17:27:58.143000 23119487874880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] xindex is not in var_ranges, defaulting to unknown range.
[rank2]:W0919 17:27:58.205000 23184895285056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] xindex is not in var_ranges, defaulting to unknown range.
[rank1]:W0919 17:27:58.917000 22701912311616 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] xindex is not in var_ranges, defaulting to unknown range.
[rank0]:W0919 17:27:59.005000 23118079194944 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] xindex is not in var_ranges, defaulting to unknown range.
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 48.818950736895204 sec
Compilation time: 100.49 seconds
Compilation time: 100.51 seconds
Compilation time: 100.57 seconds
Compilation time: 100.42 seconds
Prefill latency: 1.0670452807098627 sec
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 1.885278733447194 sec
Prefill latency: 0.05346406064927578 sec
Decode latency: 1.880771104246378 sec
Prefill latency: 0.05298839509487152 sec
Decode latency: 1.880968064069748 sec
Prefill latency: 0.052936259657144547 sec
Decode latency: 1.8817359656095505 sec
Prefill latency: 0.05275272950530052 sec
Decode latency: 1.880849463865161 sec
Time for inference 1: 1.94 sec total, 263.89 tokens/sec
Decode latency: 1.88 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 1198.31 GB/s
FLOPS achieved: 14.38 TF/s

Prefill latency: 0.05309479497373104 sec
Decode latency: 1.8794735167175531 sec
Time for inference 2: 1.94 sec total, 264.08 tokens/sec
Decode latency: 1.88 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 1199.16 GB/s
FLOPS achieved: 14.39 TF/s

Prefill latency: 0.05288552679121494 sec
Decode latency: 1.8808268029242754 sec
Time for inference 3: 1.94 sec total, 263.87 tokens/sec
Decode latency: 1.88 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 1198.18 GB/s
FLOPS achieved: 14.38 TF/s

Prefill latency: 0.053011590614914894 sec
Decode latency: 1.8801324870437384 sec
Time for inference 4: 1.94 sec total, 263.96 tokens/sec
Decode latency: 1.88 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 1198.63 GB/s
FLOPS achieved: 14.38 TF/s

Prefill latency: 0.05288570746779442 sec
Decode latency: 1.88105976767838 sec
Time for inference 5: 1.94 sec total, 263.97 tokens/sec
Decode latency: 1.88 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 1198.67 GB/s
FLOPS achieved: 14.38 TF/s

Prefill latency: 0.05286104045808315 sec
Decode latency: 1.8809575252234936 sec
Time for inference 6: 1.94 sec total, 263.84 tokens/sec
Decode latency: 1.88 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 1198.04 GB/s
FLOPS achieved: 14.38 TF/s

Prefill latency: 0.052800752222537994 sec
Decode latency: 1.8799913227558136 sec
Time for inference 7: 1.94 sec total, 264.04 tokens/sec
Decode latency: 1.88 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 1198.95 GB/s
FLOPS achieved: 14.39 TF/s

Prefill latency: 0.0530136413872242 sec
Decode latency: 1.8801411036401987 sec
Time for inference 8: 1.94 sec total, 264.02 tokens/sec
Decode latency: 1.88 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 1198.87 GB/s
FLOPS achieved: 14.39 TF/s

Prefill latency: 0.05318883992731571 sec
Decode latency: 1.8796211145818233 sec
Time for inference 9: 1.94 sec total, 264.01 tokens/sec
Decode latency: 1.88 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 1198.83 GB/s
FLOPS achieved: 14.39 TF/s

Prefill latency: 0.05286499671638012 sec
Decode latency: 1.8801952693611383 sec
Time for inference 10: 1.94 sec total, 264.05 tokens/sec
Decode latency: 1.88 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 1199.00 GB/s
FLOPS achieved: 14.39 TF/s

==========
Batch Size: 4
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 1.88 sec
Average prefill latency: 0.05 sec
Average tokens/sec: 263.97
Memory used: 8.52 GB
W0919 17:35:26.694000 23034996303680 torch/distributed/elastic/agent/server/api.py:688] Received Signals.SIGINT death signal, shutting down workers
W0919 17:35:26.696000 23034996303680 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1068071 closing signal SIGINT
W0919 17:35:26.698000 23034996303680 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1068072 closing signal SIGINT
W0919 17:35:26.698000 23034996303680 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1068073 closing signal SIGINT
W0919 17:35:26.699000 23034996303680 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1068074 closing signal SIGINT
W0919 17:35:26.897000 23034996303680 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1068071 closing signal SIGTERM
W0919 17:35:26.906000 23034996303680 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1068072 closing signal SIGTERM
W0919 17:35:26.919000 23034996303680 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1068073 closing signal SIGTERM
W0919 17:35:26.924000 23034996303680 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1068074 closing signal SIGTERM
Traceback (most recent call last):
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 680, in run
    result = self._invoke_run(role)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 835, in _invoke_run
    time.sleep(monitor_interval)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 79, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 1068003 got signal: 2

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 689, in run
    self._shutdown(e.sigval)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 347, in _shutdown
    self._pcontext.close(death_sig)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 544, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 868, in _close
    handler.proc.wait(time_to_wait)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/subprocess.py", line 1953, in _wait
    time.sleep(delay)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 79, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 1068003 got signal: 2

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/charlie/anaconda3/envs/gpt-fast/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 694, in run
    self._shutdown()
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 347, in _shutdown
    self._pcontext.close(death_sig)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 544, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 868, in _close
    handler.proc.wait(time_to_wait)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/subprocess.py", line 1953, in _wait
    time.sleep(delay)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 79, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 1068003 got signal: 2
