W1001 03:54:56.741000 22460228327232 torch/distributed/run.py:779] 
W1001 03:54:56.741000 22460228327232 torch/distributed/run.py:779] *****************************************
W1001 03:54:56.741000 22460228327232 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1001 03:54:56.741000 22460228327232 torch/distributed/run.py:779] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTParallel(
  (tok_embeddings): Embedding(49152, 2304)
  (layers): ModuleList(
    (0-39): 40 x ParallelTransformerBlock(
      semi_compiled = False
      (attention): FuseAttentionMLP(
        (wqkv1): Linear(in_features=2304, out_features=12672, bias=False)
        (wo): Linear(in_features=1152, out_features=2304, bias=False)
        (w2): Linear(in_features=4608, out_features=2304, bias=False)
      )
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=2304, out_features=49152, bias=False)
)
Time to load model: 1.12 seconds
Prefill latency: 0.2281708080554381 sec
Decode latency: 2.90817735705059 sec
Compilation time: 3.14 seconds
Compilation time: 3.13 seconds
Prefill latency: 0.21118787105660886 sec
Decode latency: 2.9097171020694077 sec
Prefill latency: 0.21106610202696174 sec
Decode latency: 2.909165443968959 sec
Prefill latency: 0.21172355290036649 sec
Decode latency: 2.9065774789778516 sec
Prefill latency: 0.21096497704274952 sec
Decode latency: 2.9073481530649588 sec
Prefill latency: 0.21125442604534328 sec
Decode latency: 2.90955457999371 sec
Time for inference 1: 3.12 sec total, 656.07 tokens/sec
Decode latency: 2.91 sec
Prefill latency: 0.21 sec
Bandwidth achieved: 2377.66 GB/s
FLOPS achieved: 11.89 TF/s

Prefill latency: 0.21097647002898157 sec
Decode latency: 2.9097743909806013 sec
Time for inference 2: 3.12 sec total, 656.04 tokens/sec
Decode latency: 2.91 sec
Prefill latency: 0.21 sec
Bandwidth achieved: 2377.53 GB/s
FLOPS achieved: 11.89 TF/s

Prefill latency: 0.21151438110973686 sec
Decode latency: 2.9079802790656686 sec
Time for inference 3: 3.12 sec total, 656.33 tokens/sec
Decode latency: 2.91 sec
Prefill latency: 0.21 sec
Bandwidth achieved: 2378.60 GB/s
FLOPS achieved: 11.89 TF/s

Prefill latency: 0.21104347193613648 sec
Decode latency: 2.9079834439326078 sec
Time for inference 4: 3.12 sec total, 656.44 tokens/sec
Decode latency: 2.91 sec
Prefill latency: 0.21 sec
Bandwidth achieved: 2378.98 GB/s
FLOPS achieved: 11.89 TF/s

Prefill latency: 0.21079591300804168 sec
Decode latency: 2.90834405599162 sec
Time for inference 5: 3.12 sec total, 656.42 tokens/sec
Decode latency: 2.91 sec
Prefill latency: 0.21 sec
Bandwidth achieved: 2378.91 GB/s
FLOPS achieved: 11.89 TF/s

Prefill latency: 0.2113879919052124 sec
Decode latency: 2.908628751989454 sec
Time for inference 6: 3.12 sec total, 656.24 tokens/sec
Decode latency: 2.91 sec
Prefill latency: 0.21 sec
Bandwidth achieved: 2378.26 GB/s
FLOPS achieved: 11.89 TF/s

Prefill latency: 0.21139716007746756 sec
Decode latency: 2.906995915924199 sec
Time for inference 7: 3.12 sec total, 656.60 tokens/sec
Decode latency: 2.91 sec
Prefill latency: 0.21 sec
Bandwidth achieved: 2379.56 GB/s
FLOPS achieved: 11.90 TF/s

Prefill latency: 0.21095158101525158 sec
Decode latency: 2.9014020789181814 sec
Time for inference 8: 3.11 sec total, 657.86 tokens/sec
Decode latency: 2.90 sec
Prefill latency: 0.21 sec
Bandwidth achieved: 2384.12 GB/s
FLOPS achieved: 11.92 TF/s

Prefill latency: 0.21047104999888688 sec
Decode latency: 2.9033332499675453 sec
Time for inference 9: 3.11 sec total, 657.56 tokens/sec
Decode latency: 2.90 sec
Prefill latency: 0.21 sec
Bandwidth achieved: 2383.06 GB/s
FLOPS achieved: 11.92 TF/s

Prefill latency: 0.21189272101037204 sec
Decode latency: 2.9037687169620767 sec
Time for inference 10: 3.12 sec total, 657.15 tokens/sec
Decode latency: 2.90 sec
Prefill latency: 0.21 sec
Bandwidth achieved: 2381.55 GB/s
FLOPS achieved: 11.91 TF/s

==========
Batch Size: 8
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 2.9068 sec
Average prefill latency: 0.2112 sec
Average tokens/sec: 656.67
Memory used: 9.14 GB
Done. we are killing the process
