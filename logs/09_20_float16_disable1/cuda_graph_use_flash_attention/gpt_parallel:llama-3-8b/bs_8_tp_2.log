W0928 19:09:32.939000 23382212454208 torch/distributed/run.py:779] 
W0928 19:09:32.939000 23382212454208 torch/distributed/run.py:779] *****************************************
W0928 19:09:32.939000 23382212454208 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0928 19:09:32.939000 23382212454208 torch/distributed/run.py:779] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTParallel(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x ParallelTransformerBlock(
      semi_compiled = False
      (attention): FuseAttentionMLP(
        (wqkv1): Linear(in_features=4096, out_features=17408, bias=False)
        (wo): Linear(in_features=2048, out_features=4096, bias=False)
        (w2): Linear(in_features=7168, out_features=4096, bias=False)
      )
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 0.87 seconds
Prefill latency: 0.3154435735195875 sec
Decode latency: 3.24788751360029 sec
Compilation time: 3.56 seconds
Compilation time: 3.56 seconds
Prefill latency: 0.29570482671260834 sec
Decode latency: 3.2430343190208077 sec
Prefill latency: 0.2950941529124975 sec
Decode latency: 3.2431229101493955 sec
Prefill latency: 0.29615844413638115 sec
Decode latency: 3.245078256353736 sec
Prefill latency: 0.2946483641862869 sec
Decode latency: 3.2422796608880162 sec
Prefill latency: 0.29590766225010157 sec
Decode latency: 3.243787406012416 sec
Time for inference 1: 3.54 sec total, 578.46 tokens/sec
Decode latency: 3.24 sec
Prefill latency: 0.30 sec
Bandwidth achieved: 4645.15 GB/s
FLOPS achieved: 23.23 TF/s

Prefill latency: 0.2952236868441105 sec
Decode latency: 3.242630018852651 sec
Time for inference 2: 3.54 sec total, 578.78 tokens/sec
Decode latency: 3.24 sec
Prefill latency: 0.30 sec
Bandwidth achieved: 4647.74 GB/s
FLOPS achieved: 23.24 TF/s

Prefill latency: 0.29542627092450857 sec
Decode latency: 3.243493511341512 sec
Time for inference 3: 3.54 sec total, 578.59 tokens/sec
Decode latency: 3.24 sec
Prefill latency: 0.30 sec
Bandwidth achieved: 4646.22 GB/s
FLOPS achieved: 23.23 TF/s

Prefill latency: 0.29548395797610283 sec
Decode latency: 3.245261006988585 sec
Time for inference 4: 3.54 sec total, 578.27 tokens/sec
Decode latency: 3.25 sec
Prefill latency: 0.30 sec
Bandwidth achieved: 4643.67 GB/s
FLOPS achieved: 23.22 TF/s

Prefill latency: 0.29561404418200254 sec
Decode latency: 3.2436487143859267 sec
Time for inference 5: 3.54 sec total, 578.53 tokens/sec
Decode latency: 3.24 sec
Prefill latency: 0.30 sec
Bandwidth achieved: 4645.72 GB/s
FLOPS achieved: 23.23 TF/s

Prefill latency: 0.29594862554222345 sec
Decode latency: 3.2432662211358547 sec
Time for inference 6: 3.54 sec total, 578.55 tokens/sec
Decode latency: 3.24 sec
Prefill latency: 0.30 sec
Bandwidth achieved: 4645.90 GB/s
FLOPS achieved: 23.23 TF/s

Prefill latency: 0.2961140302941203 sec
Decode latency: 3.2453193236142397 sec
Time for inference 7: 3.54 sec total, 578.19 tokens/sec
Decode latency: 3.25 sec
Prefill latency: 0.30 sec
Bandwidth achieved: 4643.01 GB/s
FLOPS achieved: 23.22 TF/s

Prefill latency: 0.2946062898263335 sec
Decode latency: 3.2421302907168865 sec
Time for inference 8: 3.54 sec total, 578.95 tokens/sec
Decode latency: 3.24 sec
Prefill latency: 0.29 sec
Bandwidth achieved: 4649.11 GB/s
FLOPS achieved: 23.25 TF/s

Prefill latency: 0.2953460356220603 sec
Decode latency: 3.2439840687438846 sec
Time for inference 9: 3.54 sec total, 578.53 tokens/sec
Decode latency: 3.24 sec
Prefill latency: 0.30 sec
Bandwidth achieved: 4645.71 GB/s
FLOPS achieved: 23.23 TF/s

Prefill latency: 0.2945680720731616 sec
Decode latency: 3.2441389532759786 sec
Time for inference 10: 3.54 sec total, 578.63 tokens/sec
Decode latency: 3.24 sec
Prefill latency: 0.29 sec
Bandwidth achieved: 4646.52 GB/s
FLOPS achieved: 23.23 TF/s

==========
Batch Size: 8
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 3.2438 sec
Average prefill latency: 0.2954 sec
Average tokens/sec: 578.55
Memory used: 17.61 GB
Done. we are killing the process
