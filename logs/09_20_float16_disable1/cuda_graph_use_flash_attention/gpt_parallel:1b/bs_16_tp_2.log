W0930 20:52:47.737000 23319867094848 torch/distributed/run.py:779] 
W0930 20:52:47.737000 23319867094848 torch/distributed/run.py:779] *****************************************
W0930 20:52:47.737000 23319867094848 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0930 20:52:47.737000 23319867094848 torch/distributed/run.py:779] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTParallel(
  (tok_embeddings): Embedding(49152, 1536)
  (layers): ModuleList(
    (0-39): 40 x ParallelTransformerBlock(
      semi_compiled = False
      (attention): FuseAttentionMLP(
        (wqkv1): Linear(in_features=1536, out_features=5376, bias=False)
        (wo): Linear(in_features=768, out_features=1536, bias=False)
        (w2): Linear(in_features=2048, out_features=1536, bias=False)
      )
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=1536, out_features=49152, bias=False)
)
Time to load model: 0.99 seconds
Prefill latency: 0.2540427050553262 sec
Decode latency: 3.1017877008998767 sec
Compilation time: 3.36 seconds
Compilation time: 3.36 seconds
Prefill latency: 0.234928859048523 sec
Decode latency: 3.101011453079991 sec
Prefill latency: 0.23473078303504735 sec
Decode latency: 3.099422386032529 sec
Prefill latency: 0.23523475602269173 sec
Decode latency: 3.097392563940957 sec
Prefill latency: 0.2349093590164557 sec
Decode latency: 3.098478290019557 sec
Prefill latency: 0.23493332706857473 sec
Decode latency: 3.0966236580861732 sec
Time for inference 1: 3.33 sec total, 1229.20 tokens/sec
Decode latency: 3.10 sec
Prefill latency: 0.23 sec
Bandwidth achieved: 1423.11 GB/s
FLOPS achieved: 7.12 TF/s

Prefill latency: 0.2342564300633967 sec
Decode latency: 3.0987973450683057 sec
Time for inference 2: 3.33 sec total, 1228.51 tokens/sec
Decode latency: 3.10 sec
Prefill latency: 0.23 sec
Bandwidth achieved: 1422.31 GB/s
FLOPS achieved: 7.11 TF/s

Prefill latency: 0.23409022903069854 sec
Decode latency: 3.0993914450518787 sec
Time for inference 3: 3.33 sec total, 1228.40 tokens/sec
Decode latency: 3.10 sec
Prefill latency: 0.23 sec
Bandwidth achieved: 1422.18 GB/s
FLOPS achieved: 7.11 TF/s

Prefill latency: 0.23462158197071403 sec
Decode latency: 3.096401529968716 sec
Time for inference 4: 3.33 sec total, 1229.35 tokens/sec
Decode latency: 3.10 sec
Prefill latency: 0.23 sec
Bandwidth achieved: 1423.29 GB/s
FLOPS achieved: 7.12 TF/s

Prefill latency: 0.2342094569467008 sec
Decode latency: 3.098600464989431 sec
Time for inference 5: 3.33 sec total, 1228.67 tokens/sec
Decode latency: 3.10 sec
Prefill latency: 0.23 sec
Bandwidth achieved: 1422.50 GB/s
FLOPS achieved: 7.11 TF/s

Prefill latency: 0.234741747030057 sec
Decode latency: 3.097533272113651 sec
Time for inference 6: 3.33 sec total, 1228.88 tokens/sec
Decode latency: 3.10 sec
Prefill latency: 0.23 sec
Bandwidth achieved: 1422.75 GB/s
FLOPS achieved: 7.11 TF/s

Prefill latency: 0.23447522905189544 sec
Decode latency: 3.097658448969014 sec
Time for inference 7: 3.33 sec total, 1228.95 tokens/sec
Decode latency: 3.10 sec
Prefill latency: 0.23 sec
Bandwidth achieved: 1422.82 GB/s
FLOPS achieved: 7.11 TF/s

Prefill latency: 0.23495289799757302 sec
Decode latency: 3.0993452729890123 sec
Time for inference 8: 3.33 sec total, 1228.19 tokens/sec
Decode latency: 3.10 sec
Prefill latency: 0.23 sec
Bandwidth achieved: 1421.94 GB/s
FLOPS achieved: 7.11 TF/s

Prefill latency: 0.23464339703787118 sec
Decode latency: 3.099153227987699 sec
Time for inference 9: 3.33 sec total, 1228.32 tokens/sec
Decode latency: 3.10 sec
Prefill latency: 0.23 sec
Bandwidth achieved: 1422.09 GB/s
FLOPS achieved: 7.11 TF/s

Prefill latency: 0.23505082807969302 sec
Decode latency: 3.095289097051136 sec
Time for inference 10: 3.33 sec total, 1229.57 tokens/sec
Decode latency: 3.10 sec
Prefill latency: 0.24 sec
Bandwidth achieved: 1423.54 GB/s
FLOPS achieved: 7.12 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 3.0979 sec
Average prefill latency: 0.2346 sec
Average tokens/sec: 1228.80
Memory used: 7.28 GB
Done. we are killing the process
