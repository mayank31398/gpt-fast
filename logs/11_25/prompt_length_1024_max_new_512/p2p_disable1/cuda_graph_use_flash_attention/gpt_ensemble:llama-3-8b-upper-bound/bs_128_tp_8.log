W1124 21:48:04.662000 419988 site-packages/torch/distributed/run.py:793] 
W1124 21:48:04.662000 419988 site-packages/torch/distributed/run.py:793] *****************************************
W1124 21:48:04.662000 419988 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1124 21:48:04.662000 419988 site-packages/torch/distributed/run.py:793] *****************************************
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
flash_kv_decode is set to True
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
our world size=8
Using device=cuda
Loading model ...
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=768, bias=False)
        (wo): Linear(in_features=512, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=3584, bias=False)
        (w2): Linear(in_features=1792, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 2.11 seconds
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
CUDA_GRAPH are activate
Prefill latency: 0.7951083537191153 sec
Decode latency: 3.3491861689835787 sec
Compilation time: 4.15 seconds
Compilation time: 4.20 seconds
Compilation time: 4.13 seconds
Prefill latency: 0.8015071712434292 sec
Compilation time: 4.15 seconds
Compilation time: 4.13 seconds
Compilation time: 4.11 seconds
Compilation time: 4.20 seconds
Compilation time: 4.14 seconds
Decode latency: 3.3517398461699486 sec
Prefill latency: 0.8030638843774796 sec
Decode latency: 3.352276476100087 sec
Prefill latency: 0.8041783403605223 sec
Decode latency: 3.3509867042303085 sec
Prefill latency: 0.8011646773666143 sec
Decode latency: 3.351782988756895 sec
Prefill latency: 0.8045533485710621 sec
Decode latency: 3.3523523304611444 sec
Time for inference 1: 4.16 sec total, 15762.54 tokens/sec
Decode latency: 3.35 sec
Prefill latency: 0.80 sec
Bandwidth achieved: 44072.64 GB/s
FLOPS achieved: 132.22 TF/s

Prefill latency: 0.7996211517602205 sec
Decode latency: 3.349831683561206 sec
Time for inference 2: 4.15 sec total, 15791.00 tokens/sec
Decode latency: 3.35 sec
Prefill latency: 0.80 sec
Bandwidth achieved: 44152.22 GB/s
FLOPS achieved: 132.46 TF/s

Prefill latency: 0.8006440419703722 sec
Decode latency: 3.3519800007343292 sec
Time for inference 3: 4.15 sec total, 15779.05 tokens/sec
Decode latency: 3.35 sec
Prefill latency: 0.80 sec
Bandwidth achieved: 44118.78 GB/s
FLOPS achieved: 132.36 TF/s

Prefill latency: 0.8036427199840546 sec
Decode latency: 3.35235807672143 sec
Time for inference 4: 4.16 sec total, 15765.77 tokens/sec
Decode latency: 3.35 sec
Prefill latency: 0.80 sec
Bandwidth achieved: 44081.66 GB/s
FLOPS achieved: 132.24 TF/s

Prefill latency: 0.8043598718941212 sec
Decode latency: 3.3508167527616024 sec
Time for inference 5: 4.16 sec total, 15768.72 tokens/sec
Decode latency: 3.35 sec
Prefill latency: 0.80 sec
Bandwidth achieved: 44089.91 GB/s
FLOPS achieved: 132.27 TF/s

Prefill latency: 0.8046269435435534 sec
Decode latency: 3.353302424773574 sec
Time for inference 6: 4.16 sec total, 15758.65 tokens/sec
Decode latency: 3.35 sec
Prefill latency: 0.80 sec
Bandwidth achieved: 44061.75 GB/s
FLOPS achieved: 132.19 TF/s

Prefill latency: 0.7997676357626915 sec
Decode latency: 3.349785787984729 sec
Time for inference 7: 4.15 sec total, 15790.77 tokens/sec
Decode latency: 3.35 sec
Prefill latency: 0.80 sec
Bandwidth achieved: 44151.57 GB/s
FLOPS achieved: 132.45 TF/s

Prefill latency: 0.8047604206949472 sec
Decode latency: 3.3527231626212597 sec
Time for inference 8: 4.16 sec total, 15760.48 tokens/sec
Decode latency: 3.35 sec
Prefill latency: 0.80 sec
Bandwidth achieved: 44066.87 GB/s
FLOPS achieved: 132.20 TF/s

Prefill latency: 0.803598390892148 sec
Decode latency: 3.3500436525791883 sec
Time for inference 9: 4.15 sec total, 15775.16 tokens/sec
Decode latency: 3.35 sec
Prefill latency: 0.80 sec
Bandwidth achieved: 44107.93 GB/s
FLOPS achieved: 132.32 TF/s

Prefill latency: 0.8055956345051527 sec
Decode latency: 3.3523657508194447 sec
Time for inference 10: 4.16 sec total, 15758.87 tokens/sec
Decode latency: 3.35 sec
Prefill latency: 0.81 sec
Bandwidth achieved: 44062.36 GB/s
FLOPS achieved: 132.19 TF/s

==========
Batch Size: 128
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 3.3516 sec
Average prefill latency: 0.8031 sec
Average tokens/sec: 15771.10
Memory used: 48.45 GB
[rank0]:[W1124 21:49:23.969483287 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank5]:[W1124 21:49:23.348882696 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank7]:[W1124 21:49:23.461059121 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank6]:[W1124 21:49:24.744934335 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank2]:[W1124 21:49:24.938162645 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank4]:[W1124 21:49:24.125434708 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank1]:[W1124 21:49:24.445925497 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank3]:[W1124 21:49:25.394508400 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Done. we are killing the process
[rank0]:[W1124 21:49:33.566116204 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
