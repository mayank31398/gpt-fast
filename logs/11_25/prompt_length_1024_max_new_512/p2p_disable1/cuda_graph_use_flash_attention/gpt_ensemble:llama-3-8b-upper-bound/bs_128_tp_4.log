W1124 21:46:15.800000 419241 site-packages/torch/distributed/run.py:793] 
W1124 21:46:15.800000 419241 site-packages/torch/distributed/run.py:793] *****************************************
W1124 21:46:15.800000 419241 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1124 21:46:15.800000 419241 site-packages/torch/distributed/run.py:793] *****************************************
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4

rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
flash_kv_decode is set to True
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
our world size=4
Using device=cuda
Loading model ...
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=1536, bias=False)
        (wo): Linear(in_features=1024, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.76 seconds
CUDA_GRAPH are activate
Prefill latency: 1.318518802523613 sec
Decode latency: 4.106573697179556 sec
Compilation time: 5.43 seconds
Compilation time: 5.52 seconds
Compilation time: 5.59 seconds
Compilation time: 5.61 seconds
Prefill latency: 1.3240474443882704 sec
Decode latency: 4.27582685276866 sec
Prefill latency: 1.326730940490961 sec
Decode latency: 4.27799086458981 sec
Prefill latency: 1.3225832860916853 sec
Decode latency: 4.27659166790545 sec
Prefill latency: 1.3230007085949183 sec
Decode latency: 4.131167722865939 sec
Prefill latency: 1.3283043317496777 sec
Decode latency: 4.279095705598593 sec
Time for inference 1: 5.61 sec total, 11685.30 tokens/sec
Decode latency: 4.28 sec
Prefill latency: 1.33 sec
Bandwidth achieved: 53061.38 GB/s
FLOPS achieved: 159.18 TF/s

Prefill latency: 1.3241673186421394 sec
Decode latency: 4.28123096935451 sec
Time for inference 2: 5.61 sec total, 11689.63 tokens/sec
Decode latency: 4.28 sec
Prefill latency: 1.32 sec
Bandwidth achieved: 53081.07 GB/s
FLOPS achieved: 159.24 TF/s

Prefill latency: 1.324875583872199 sec
Decode latency: 4.280115466564894 sec
Time for inference 3: 5.61 sec total, 11690.40 tokens/sec
Decode latency: 4.28 sec
Prefill latency: 1.32 sec
Bandwidth achieved: 53084.56 GB/s
FLOPS achieved: 159.25 TF/s

Prefill latency: 1.3279127776622772 sec
Decode latency: 4.264189230278134 sec
Time for inference 4: 5.59 sec total, 11717.45 tokens/sec
Decode latency: 4.26 sec
Prefill latency: 1.33 sec
Bandwidth achieved: 53207.38 GB/s
FLOPS achieved: 159.62 TF/s

Prefill latency: 1.3233600854873657 sec
Decode latency: 4.278804989531636 sec
Time for inference 5: 5.60 sec total, 11696.14 tokens/sec
Decode latency: 4.28 sec
Prefill latency: 1.32 sec
Bandwidth achieved: 53110.60 GB/s
FLOPS achieved: 159.33 TF/s

Prefill latency: 1.3229384627193213 sec
Decode latency: 4.155480304732919 sec
Time for inference 6: 5.48 sec total, 11960.22 tokens/sec
Decode latency: 4.16 sec
Prefill latency: 1.32 sec
Bandwidth achieved: 54309.75 GB/s
FLOPS achieved: 162.93 TF/s

Prefill latency: 1.3244603481143713 sec
Decode latency: 4.279568128287792 sec
Time for inference 7: 5.60 sec total, 11692.65 tokens/sec
Decode latency: 4.28 sec
Prefill latency: 1.32 sec
Bandwidth achieved: 53094.74 GB/s
FLOPS achieved: 159.28 TF/s

Prefill latency: 1.3268828094005585 sec
Decode latency: 4.280613217502832 sec
Time for inference 8: 5.61 sec total, 11685.13 tokens/sec
Decode latency: 4.28 sec
Prefill latency: 1.33 sec
Bandwidth achieved: 53060.60 GB/s
FLOPS achieved: 159.18 TF/s

Prefill latency: 1.3242663443088531 sec
Decode latency: 4.279919181019068 sec
Time for inference 9: 5.61 sec total, 11692.29 tokens/sec
Decode latency: 4.28 sec
Prefill latency: 1.32 sec
Bandwidth achieved: 53093.14 GB/s
FLOPS achieved: 159.28 TF/s

Prefill latency: 1.3249610103666782 sec
[rank1]:[W1124 21:47:54.507103218 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank3]:[W1124 21:47:55.054793830 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Decode latency: 4.224038852378726 sec
Time for inference 10: 5.55 sec total, 11808.26 tokens/sec
Decode latency: 4.22 sec
Prefill latency: 1.32 sec
Bandwidth achieved: 53619.74 GB/s
FLOPS achieved: 160.86 TF/s

==========
Batch Size: 128
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 4.2603 sec
Average prefill latency: 1.3252 sec
Average tokens/sec: 11731.75
Memory used: 56.56 GB
[rank0]:[W1124 21:47:55.244326644 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank2]:[W1124 21:47:56.815788042 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Done. we are killing the process
[rank0]:[W1124 21:47:59.204802799 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
