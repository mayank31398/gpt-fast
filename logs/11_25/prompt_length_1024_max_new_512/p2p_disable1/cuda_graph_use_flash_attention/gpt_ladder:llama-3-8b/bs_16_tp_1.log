rank: 0, global_rank: 0, world_size: 1, global_world_size: 1
rank: 0, global_rank: 0, world_size: 1, global_world_size: 1
rank: 0, global_rank: 0, world_size: 1, global_world_size: 1
rank: 0, global_rank: 0, world_size: 1, global_world_size: 1
rank: 0, global_rank: 0, world_size: 1, global_world_size: 1
flash_kv_decode is set to True
rank: 0, global_rank: 0, world_size: 1, global_world_size: 1
our world size=1
Using device=cuda
Loading model ...
GPTLadder(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x LadderTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
        (wo): Linear(in_features=4096, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=28672, bias=False)
        (w2): Linear(in_features=14336, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 0.30 seconds
CUDA_GRAPH are activate
Prefill latency: 0.5312434020452201 sec
Decode latency: 5.262150939088315 sec
Compilation time: 5.79 seconds
Prefill latency: 0.5320987980812788 sec
Decode latency: 5.257015906274319 sec
Prefill latency: 0.5299134068191051 sec
Decode latency: 5.2580813048407435 sec
Prefill latency: 0.5331593360751867 sec
Decode latency: 5.257189902011305 sec
Prefill latency: 0.53317213896662 sec
Decode latency: 5.258713365998119 sec
Prefill latency: 0.5316451950930059 sec
Decode latency: 5.257481337990612 sec
Time for inference 1: 5.79 sec total, 1414.80 tokens/sec
Decode latency: 5.26 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 21236.00 GB/s
FLOPS achieved: 63.71 TF/s

Prefill latency: 0.5325960027985275 sec
Decode latency: 5.257257931865752 sec
Time for inference 2: 5.79 sec total, 1414.63 tokens/sec
Decode latency: 5.26 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 21233.38 GB/s
FLOPS achieved: 63.70 TF/s

Prefill latency: 0.5327097317203879 sec
Decode latency: 5.257792606949806 sec
Time for inference 3: 5.79 sec total, 1414.40 tokens/sec
Decode latency: 5.26 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 21229.86 GB/s
FLOPS achieved: 63.69 TF/s

Prefill latency: 0.531279708724469 sec
Decode latency: 5.255356587935239 sec
Time for inference 4: 5.79 sec total, 1415.37 tokens/sec
Decode latency: 5.26 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 21244.54 GB/s
FLOPS achieved: 63.73 TF/s

Prefill latency: 0.53239352023229 sec
Decode latency: 5.258220147807151 sec
Time for inference 5: 5.79 sec total, 1414.41 tokens/sec
Decode latency: 5.26 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 21230.01 GB/s
FLOPS achieved: 63.69 TF/s

Prefill latency: 0.5303356098011136 sec
Decode latency: 5.257117378991097 sec
Time for inference 6: 5.79 sec total, 1415.20 tokens/sec
Decode latency: 5.26 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 21241.97 GB/s
FLOPS achieved: 63.73 TF/s

Prefill latency: 0.5319561241194606 sec
Decode latency: 5.258045944850892 sec
Time for inference 7: 5.79 sec total, 1414.59 tokens/sec
Decode latency: 5.26 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 21232.71 GB/s
FLOPS achieved: 63.70 TF/s

Prefill latency: 0.5310391583479941 sec
Decode latency: 5.256639082916081 sec
Time for inference 8: 5.79 sec total, 1415.10 tokens/sec
Decode latency: 5.26 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 21240.41 GB/s
FLOPS achieved: 63.72 TF/s

Prefill latency: 0.5324405529536307 sec
Decode latency: 5.257230292074382 sec
Time for inference 9: 5.79 sec total, 1414.64 tokens/sec
Decode latency: 5.26 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 21233.50 GB/s
FLOPS achieved: 63.70 TF/s

Prefill latency: 0.5308259990997612 sec
Decode latency: 5.2568659437820315 sec
Time for inference 10: 5.79 sec total, 1415.12 tokens/sec
Decode latency: 5.26 sec
Prefill latency: 0.53 sec
Bandwidth achieved: 21240.75 GB/s
FLOPS achieved: 63.72 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 5.2572 sec
Average prefill latency: 0.5317 sec
Average tokens/sec: 1414.83
Memory used: 35.95 GB
Done. we are killing the process
[rank0]:[W1201 16:14:12.386207305 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
