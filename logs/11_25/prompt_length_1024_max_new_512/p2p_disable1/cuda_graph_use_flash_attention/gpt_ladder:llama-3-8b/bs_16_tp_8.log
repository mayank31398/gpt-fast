W1201 16:16:42.422000 533056 site-packages/torch/distributed/run.py:793] 
W1201 16:16:42.422000 533056 site-packages/torch/distributed/run.py:793] *****************************************
W1201 16:16:42.422000 533056 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1201 16:16:42.422000 533056 site-packages/torch/distributed/run.py:793] *****************************************
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
flash_kv_decode is set to True
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
our world size=8
Using device=cuda
Loading model ...
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8rank: 7, global_rank: 7, world_size: 8, global_world_size: 8

rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
GPTLadder(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x LadderTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=768, bias=False)
        (wo): Linear(in_features=512, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=3584, bias=False)
        (w2): Linear(in_features=1792, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 0.43 seconds
CUDA_GRAPH are activate
Prefill latency: 0.11775849806144834 sec
Decode latency: 3.386332610156387 sec
Compilation time: 3.52 seconds
Compilation time: 3.52 seconds
Compilation time: 3.52 seconds
Compilation time: 3.51 seconds
Compilation time: 3.53 seconds
Compilation time: 3.51 seconds
Compilation time: 3.55 seconds
Compilation time: 3.53 seconds
Prefill latency: 0.11795469373464584 sec
Decode latency: 3.370861331000924 sec
Prefill latency: 0.11865164712071419 sec
Decode latency: 3.384419246111065 sec
Prefill latency: 0.1179827582091093 sec
Decode latency: 3.374298666138202 sec
Prefill latency: 0.11804019892588258 sec
Decode latency: 3.360200885217637 sec
Prefill latency: 0.12006374495103955 sec
Decode latency: 3.351926769129932 sec
Time for inference 1: 3.47 sec total, 2358.58 tokens/sec
Decode latency: 3.35 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 6594.67 GB/s
FLOPS achieved: 19.78 TF/s

Prefill latency: 0.11786509398370981 sec
Decode latency: 3.3803221732378006 sec
Time for inference 2: 3.50 sec total, 2340.57 tokens/sec
Decode latency: 3.38 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 6544.32 GB/s
FLOPS achieved: 19.63 TF/s

Prefill latency: 0.11794861499220133 sec
Decode latency: 3.3415063181892037 sec
Time for inference 3: 3.46 sec total, 2367.03 tokens/sec
Decode latency: 3.34 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 6618.30 GB/s
FLOPS achieved: 19.85 TF/s

Prefill latency: 0.11859844904392958 sec
Decode latency: 3.3598143910057843 sec
Time for inference 4: 3.48 sec total, 2354.12 tokens/sec
Decode latency: 3.36 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 6582.21 GB/s
FLOPS achieved: 19.75 TF/s

Prefill latency: 0.1186288152821362 sec
Decode latency: 3.3697994179092348 sec
Time for inference 5: 3.49 sec total, 2347.39 tokens/sec
Decode latency: 3.37 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 6563.39 GB/s
FLOPS achieved: 19.69 TF/s

Prefill latency: 0.11870686803013086 sec
Decode latency: 3.3471186831593513 sec
Time for inference 6: 3.47 sec total, 2362.76 tokens/sec
Decode latency: 3.35 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 6606.37 GB/s
FLOPS achieved: 19.82 TF/s

Prefill latency: 0.11841333797201514 sec
Decode latency: 3.37165263062343 sec
Time for inference 7: 3.49 sec total, 2346.08 tokens/sec
Decode latency: 3.37 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 6559.73 GB/s
FLOPS achieved: 19.68 TF/s

Prefill latency: 0.11879066191613674 sec
Decode latency: 3.383646896108985 sec
Time for inference 8: 3.50 sec total, 2337.88 tokens/sec
Decode latency: 3.38 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 6536.79 GB/s
FLOPS achieved: 19.61 TF/s

Prefill latency: 0.11949356412515044 sec
Decode latency: 3.368602486792952 sec
Time for inference 9: 3.49 sec total, 2347.70 tokens/sec
Decode latency: 3.37 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 6564.27 GB/s
FLOPS achieved: 19.69 TF/s

Prefill latency: 0.1186815220862627 sec
Decode latency: 3.3763113990426064 sec
Time for inference 10: 3.50 sec total, 2342.83 tokens/sec
Decode latency: 3.38 sec
Prefill latency: 0.12 sec
Bandwidth achieved: 6550.64 GB/s
FLOPS achieved: 19.65 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 3.3651 sec
Average prefill latency: 0.1187 sec
Average tokens/sec: 2350.49
Memory used: 19.63 GB
Done. we are killing the process
[rank0]:[W1201 16:17:49.602493321 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
