W1124 21:24:06.175000 397636 site-packages/torch/distributed/run.py:793] 
W1124 21:24:06.175000 397636 site-packages/torch/distributed/run.py:793] *****************************************
W1124 21:24:06.175000 397636 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1124 21:24:06.175000 397636 site-packages/torch/distributed/run.py:793] *****************************************
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
flash_kv_decode is set to True
rank: 0, global_rank: 0, world_size: 4, global_world_size: 4
our world size=4
Using device=cuda
Loading model ...
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 2, global_rank: 2, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 3, global_rank: 3, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
rank: 1, global_rank: 1, world_size: 4, global_world_size: 4
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=1536, bias=False)
        (wo): Linear(in_features=1024, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.61 seconds
CUDA_GRAPH are activate
Prefill latency: 1.318643195554614 sec
Compilation time: 5.47 seconds
Compilation time: 5.46 seconds
Compilation time: 5.51 seconds
Decode latency: 4.279761703684926 sec
Compilation time: 5.60 seconds
Prefill latency: 1.3210472147911787 sec
Decode latency: 4.280122593045235 sec
Prefill latency: 1.3203627299517393 sec
Decode latency: 4.281243788078427 sec
Prefill latency: 1.3212375435978174 sec
Decode latency: 4.280422382056713 sec
Prefill latency: 1.3213222306221724 sec
Decode latency: 4.280360808596015 sec
Prefill latency: 1.3287061527371407 sec
Decode latency: 4.279237147420645 sec
Time for inference 1: 5.61 sec total, 11684.23 tokens/sec
Decode latency: 4.28 sec
Prefill latency: 1.33 sec
Bandwidth achieved: 53056.51 GB/s
FLOPS achieved: 159.17 TF/s

Prefill latency: 1.3249716777354479 sec
Decode latency: 4.279928766191006 sec
Time for inference 2: 5.61 sec total, 11690.48 tokens/sec
Decode latency: 4.28 sec
Prefill latency: 1.32 sec
Bandwidth achieved: 53084.90 GB/s
FLOPS achieved: 159.25 TF/s

Prefill latency: 1.3274189289659262 sec
Decode latency: 4.28048531152308 sec
Time for inference 3: 5.61 sec total, 11684.46 tokens/sec
Decode latency: 4.28 sec
Prefill latency: 1.33 sec
Bandwidth achieved: 53057.56 GB/s
FLOPS achieved: 159.17 TF/s

Prefill latency: 1.3278597723692656 sec
Decode latency: 4.281047508120537 sec
Time for inference 4: 5.61 sec total, 11682.39 tokens/sec
Decode latency: 4.28 sec
Prefill latency: 1.33 sec
Bandwidth achieved: 53048.19 GB/s
FLOPS achieved: 159.14 TF/s

Prefill latency: 1.3242847975343466 sec
Decode latency: 4.281819108873606 sec
Time for inference 5: 5.61 sec total, 11688.38 tokens/sec
Decode latency: 4.28 sec
Prefill latency: 1.32 sec
Bandwidth achieved: 53075.38 GB/s
FLOPS achieved: 159.23 TF/s

Prefill latency: 1.3241632971912622 sec
Decode latency: 4.280926391482353 sec
Time for inference 6: 5.61 sec total, 11690.55 tokens/sec
Decode latency: 4.28 sec
Prefill latency: 1.32 sec
Bandwidth achieved: 53085.24 GB/s
FLOPS achieved: 159.26 TF/s

Prefill latency: 1.3250669203698635 sec
Decode latency: 4.281201761215925 sec
Time for inference 7: 5.61 sec total, 11687.43 tokens/sec
Decode latency: 4.28 sec
Prefill latency: 1.33 sec
Bandwidth achieved: 53071.07 GB/s
FLOPS achieved: 159.21 TF/s

Prefill latency: 1.3317834790796041 sec
Decode latency: 4.2819475289434195 sec
Time for inference 8: 5.61 sec total, 11672.26 tokens/sec
Decode latency: 4.28 sec
Prefill latency: 1.33 sec
Bandwidth achieved: 53002.19 GB/s
FLOPS achieved: 159.01 TF/s

Prefill latency: 1.3227600939571857 sec
Decode latency: 4.280678387731314 sec
Time for inference 9: 5.60 sec total, 11693.92 tokens/sec
Decode latency: 4.28 sec
Prefill latency: 1.32 sec
Bandwidth achieved: 53100.52 GB/s
FLOPS achieved: 159.30 TF/s

Prefill latency: 1.323850216343999 sec
[rank1]:[W1124 21:25:45.954379411 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank3]:[W1124 21:25:45.074704199 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank2]:[W1124 21:25:45.122231694 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Decode latency: 4.279641622677445 sec
Time for inference 10: 5.60 sec total, 11693.81 tokens/sec
Decode latency: 4.28 sec
Prefill latency: 1.32 sec
Bandwidth achieved: 53100.05 GB/s
FLOPS achieved: 159.30 TF/s

==========
Batch Size: 128
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 4.2807 sec
Average prefill latency: 1.3261 sec
Average tokens/sec: 11686.79
Memory used: 56.56 GB
[rank0]:[W1124 21:25:46.104436161 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Done. we are killing the process
[rank0]:[W1124 21:25:51.819479101 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
