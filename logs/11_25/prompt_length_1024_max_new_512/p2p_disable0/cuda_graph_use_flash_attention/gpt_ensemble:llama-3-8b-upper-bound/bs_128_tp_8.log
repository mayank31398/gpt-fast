W1124 21:25:55.023000 398391 site-packages/torch/distributed/run.py:793] 
W1124 21:25:55.023000 398391 site-packages/torch/distributed/run.py:793] *****************************************
W1124 21:25:55.023000 398391 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1124 21:25:55.023000 398391 site-packages/torch/distributed/run.py:793] *****************************************
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
flash_kv_decode is set to True
rank: 0, global_rank: 0, world_size: 8, global_world_size: 8
our world size=8
Using device=cuda
Loading model ...
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 6, global_rank: 6, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 5, global_rank: 5, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 1, global_rank: 1, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 3, global_rank: 3, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 4, global_rank: 4, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 7, global_rank: 7, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
rank: 2, global_rank: 2, world_size: 8, global_world_size: 8
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=768, bias=False)
        (wo): Linear(in_features=512, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=3584, bias=False)
        (w2): Linear(in_features=1792, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 2.34 seconds
CUDA_GRAPH are activate
Prefill latency: 0.7983919382095337 sec
Decode latency: 3.3514057751744986 sec
Compilation time: 4.16 seconds
Compilation time: 4.09 seconds
Compilation time: 4.14 seconds
Compilation time: 4.13 seconds
Compilation time: 4.13 seconds
Compilation time: 4.19 seconds
Compilation time: 4.13 seconds
Compilation time: 4.12 seconds
Prefill latency: 0.8044643271714449 sec
Decode latency: 3.3527441397309303 sec
Prefill latency: 0.8080944679677486 sec
Decode latency: 3.3532136399298906 sec
Prefill latency: 0.8027902934700251 sec
Decode latency: 3.353389797732234 sec
Prefill latency: 0.8079516086727381 sec
Decode latency: 3.3543037082999945 sec
Prefill latency: 0.8063619434833527 sec
Decode latency: 3.354449136182666 sec
Time for inference 1: 4.16 sec total, 15747.46 tokens/sec
Decode latency: 3.35 sec
Prefill latency: 0.81 sec
Bandwidth achieved: 44030.46 GB/s
FLOPS achieved: 132.09 TF/s

Prefill latency: 0.8020281475037336 sec
Decode latency: 3.3527778033167124 sec
Time for inference 2: 4.16 sec total, 15770.57 tokens/sec
Decode latency: 3.35 sec
Prefill latency: 0.80 sec
Bandwidth achieved: 44095.08 GB/s
FLOPS achieved: 132.29 TF/s

Prefill latency: 0.8089616354554892 sec
Decode latency: 3.3530178517103195 sec
Time for inference 3: 4.16 sec total, 15743.57 tokens/sec
Decode latency: 3.35 sec
Prefill latency: 0.81 sec
Bandwidth achieved: 44019.58 GB/s
FLOPS achieved: 132.06 TF/s

Prefill latency: 0.8077470324933529 sec
Decode latency: 3.359707735478878 sec
Time for inference 4: 4.17 sec total, 15722.88 tokens/sec
Decode latency: 3.36 sec
Prefill latency: 0.81 sec
Bandwidth achieved: 43961.74 GB/s
FLOPS achieved: 131.89 TF/s

Prefill latency: 0.8073099199682474 sec
Decode latency: 3.353846848011017 sec
Time for inference 5: 4.16 sec total, 15746.66 tokens/sec
Decode latency: 3.35 sec
Prefill latency: 0.81 sec
Bandwidth achieved: 44028.22 GB/s
FLOPS achieved: 132.08 TF/s

Prefill latency: 0.8065274748951197 sec
Decode latency: 3.3570228666067123 sec
Time for inference 6: 4.16 sec total, 15736.91 tokens/sec
Decode latency: 3.36 sec
Prefill latency: 0.81 sec
Bandwidth achieved: 44000.97 GB/s
FLOPS achieved: 132.00 TF/s

Prefill latency: 0.8082035034894943 sec
Decode latency: 3.353140102699399 sec
Time for inference 7: 4.16 sec total, 15745.25 tokens/sec
Decode latency: 3.35 sec
Prefill latency: 0.81 sec
Bandwidth achieved: 44024.28 GB/s
FLOPS achieved: 132.07 TF/s

Prefill latency: 0.8080405592918396 sec
Decode latency: 3.3535868003964424 sec
Time for inference 8: 4.16 sec total, 15744.61 tokens/sec
Decode latency: 3.35 sec
Prefill latency: 0.81 sec
Bandwidth achieved: 44022.51 GB/s
FLOPS achieved: 132.07 TF/s

Prefill latency: 0.8077402580529451 sec
Decode latency: 3.354650877416134 sec
Time for inference 9: 4.16 sec total, 15742.01 tokens/sec
Decode latency: 3.35 sec
Prefill latency: 0.81 sec
Bandwidth achieved: 44015.23 GB/s
FLOPS achieved: 132.05 TF/s

Prefill latency: 0.8082861173897982 sec
[rank2]:[W1124 21:27:14.836292196 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank6]:[W1124 21:27:14.892556337 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank5]:[W1124 21:27:14.090130129 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank4]:[W1124 21:27:14.152084327 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank1]:[W1124 21:27:14.168022025 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Decode latency: 3.355853658169508 sec
Time for inference 10: 4.16 sec total, 15735.30 tokens/sec
Decode latency: 3.36 sec
Prefill latency: 0.81 sec
Bandwidth achieved: 43996.47 GB/s
FLOPS achieved: 131.99 TF/s

==========
Batch Size: 128
Prompt Length: 1024
Generated tokens: 512
Average decode latency: 3.3548 sec
Average prefill latency: 0.8071 sec
Average tokens/sec: 15743.52
Memory used: 48.45 GB
[rank0]:[W1124 21:27:14.342675056 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank7]:[W1124 21:27:15.953839349 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank3]:[W1124 21:27:15.075082753 ProcessGroupNCCL.cpp:4393] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Done. we are killing the process
[rank0]:[W1124 21:27:25.499218011 ProcessGroupNCCL.cpp:1374] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
