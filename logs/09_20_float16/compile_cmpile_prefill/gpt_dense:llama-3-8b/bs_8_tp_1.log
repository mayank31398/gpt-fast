flash_kv_decode is set to False
Using device=cuda
Loading model ...
GPTDense(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x DenseTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
        (wo): Linear(in_features=4096, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=28672, bias=False)
        (w2): Linear(in_features=14336, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 0.97 seconds
Compiling prefill with dynamic=False
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
[rank0]:W0920 14:55:23.873000 23102861383488 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] xindex is not in var_ranges, defaulting to unknown range.
Prefill latency: 54.23643952421844 sec
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
[rank0]:W0920 14:56:17.851000 23102861383488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] xindex is not in var_ranges, defaulting to unknown range.
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 54.67878941260278 sec
Compilation time: 108.92 seconds
Prefill latency: 0.9995740167796612 sec
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 2.1277922932058573 sec
Prefill latency: 0.2540482096374035 sec
Decode latency: 2.1263749804347754 sec
Prefill latency: 0.25352359749376774 sec
Decode latency: 2.1272552274167538 sec
Prefill latency: 0.2551705986261368 sec
Decode latency: 2.1272395960986614 sec
Prefill latency: 0.2537357062101364 sec
Decode latency: 2.1273841224610806 sec
Time for inference 1: 2.38 sec total, 859.68 tokens/sec
Decode latency: 2.13 sec
Prefill latency: 0.25 sec
Bandwidth achieved: 12903.74 GB/s
FLOPS achieved: 64.52 TF/s

Prefill latency: 0.2525403890758753 sec
Decode latency: 2.1273808274418116 sec
Time for inference 2: 2.38 sec total, 860.17 tokens/sec
Decode latency: 2.13 sec
Prefill latency: 0.25 sec
Bandwidth achieved: 12911.04 GB/s
FLOPS achieved: 64.56 TF/s

Prefill latency: 0.25202438421547413 sec
Decode latency: 2.1275314893573523 sec
Time for inference 3: 2.38 sec total, 860.32 tokens/sec
Decode latency: 2.13 sec
Prefill latency: 0.25 sec
Bandwidth achieved: 12913.22 GB/s
FLOPS achieved: 64.57 TF/s

Prefill latency: 0.25148332864046097 sec
Decode latency: 2.127676099538803 sec
Time for inference 4: 2.38 sec total, 860.46 tokens/sec
Decode latency: 2.13 sec
Prefill latency: 0.25 sec
Bandwidth achieved: 12915.32 GB/s
FLOPS achieved: 64.58 TF/s

Prefill latency: 0.2540704719722271 sec
Decode latency: 2.1277380995452404 sec
Time for inference 5: 2.38 sec total, 859.51 tokens/sec
Decode latency: 2.13 sec
Prefill latency: 0.25 sec
Bandwidth achieved: 12901.07 GB/s
FLOPS achieved: 64.51 TF/s

Prefill latency: 0.2542458903044462 sec
Decode latency: 2.1282652262598276 sec
Time for inference 6: 2.38 sec total, 859.27 tokens/sec
Decode latency: 2.13 sec
Prefill latency: 0.25 sec
Bandwidth achieved: 12897.45 GB/s
FLOPS achieved: 64.49 TF/s

Prefill latency: 0.25143137015402317 sec
Decode latency: 2.127986002713442 sec
Time for inference 7: 2.38 sec total, 860.35 tokens/sec
Decode latency: 2.13 sec
Prefill latency: 0.25 sec
Bandwidth achieved: 12913.78 GB/s
FLOPS achieved: 64.57 TF/s

Prefill latency: 0.25137183628976345 sec
Decode latency: 2.1278864443302155 sec
Time for inference 8: 2.38 sec total, 860.36 tokens/sec
Decode latency: 2.13 sec
Prefill latency: 0.25 sec
Bandwidth achieved: 12913.88 GB/s
FLOPS achieved: 64.57 TF/s

Prefill latency: 0.24983680434525013 sec
Decode latency: 2.1279543675482273 sec
Time for inference 9: 2.38 sec total, 860.92 tokens/sec
Decode latency: 2.13 sec
Prefill latency: 0.25 sec
Bandwidth achieved: 12922.32 GB/s
FLOPS achieved: 64.61 TF/s

Prefill latency: 0.2558791246265173 sec
Decode latency: 2.127891894429922 sec
Time for inference 10: 2.38 sec total, 858.80 tokens/sec
Decode latency: 2.13 sec
Prefill latency: 0.26 sec
Bandwidth achieved: 12890.42 GB/s
FLOPS achieved: 64.45 TF/s

==========
Batch Size: 8
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 2.1278 sec
Average prefill latency: 0.2527 sec
Average tokens/sec: 859.98
Memory used: 23.29 GB
