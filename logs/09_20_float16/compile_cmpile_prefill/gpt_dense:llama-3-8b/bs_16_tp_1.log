flash_kv_decode is set to False
Using device=cuda
Loading model ...
GPTDense(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x DenseTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=6144, bias=False)
        (wo): Linear(in_features=4096, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=28672, bias=False)
        (w2): Linear(in_features=14336, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 0.99 seconds
Compiling prefill with dynamic=False
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Prefill latency: 19.211613080988172 sec
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 15.57689149401267 sec
Compilation time: 34.79 seconds
Prefill latency: 0.7761574250180274 sec
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 2.4967281030258164 sec
Prefill latency: 0.5158703640045132 sec
Decode latency: 2.497864513017703 sec
Prefill latency: 0.5155036729993299 sec
Decode latency: 2.498118368006544 sec
Prefill latency: 0.5169941530039068 sec
Decode latency: 2.4976340210123453 sec
Prefill latency: 0.5155911070178263 sec
Decode latency: 2.4978378560044803 sec
Time for inference 1: 3.01 sec total, 1358.78 tokens/sec
Decode latency: 2.50 sec
Prefill latency: 0.52 sec
Bandwidth achieved: 20395.05 GB/s
FLOPS achieved: 101.98 TF/s

Prefill latency: 0.5159936190175358 sec
Decode latency: 2.498283673019614 sec
Time for inference 2: 3.02 sec total, 1358.37 tokens/sec
Decode latency: 2.50 sec
Prefill latency: 0.52 sec
Bandwidth achieved: 20388.86 GB/s
FLOPS achieved: 101.94 TF/s

Prefill latency: 0.518004112993367 sec
Decode latency: 2.4970024889917113 sec
Time for inference 3: 3.02 sec total, 1358.06 tokens/sec
Decode latency: 2.50 sec
Prefill latency: 0.52 sec
Bandwidth achieved: 20384.26 GB/s
FLOPS achieved: 101.92 TF/s

Prefill latency: 0.5169661680120043 sec
Decode latency: 2.49758613697486 sec
Time for inference 4: 3.02 sec total, 1358.30 tokens/sec
Decode latency: 2.50 sec
Prefill latency: 0.52 sec
Bandwidth achieved: 20387.81 GB/s
FLOPS achieved: 101.94 TF/s

Prefill latency: 0.5142684130114503 sec
Decode latency: 2.4972421000129543 sec
Time for inference 5: 3.01 sec total, 1359.64 tokens/sec
Decode latency: 2.50 sec
Prefill latency: 0.51 sec
Bandwidth achieved: 20407.93 GB/s
FLOPS achieved: 102.04 TF/s

Prefill latency: 0.5161086870066356 sec
Decode latency: 2.4975301880040206 sec
Time for inference 6: 3.01 sec total, 1358.67 tokens/sec
Decode latency: 2.50 sec
Prefill latency: 0.52 sec
Bandwidth achieved: 20393.46 GB/s
FLOPS achieved: 101.97 TF/s

Prefill latency: 0.5153202190122101 sec
Decode latency: 2.497704267007066 sec
Time for inference 7: 3.01 sec total, 1359.00 tokens/sec
Decode latency: 2.50 sec
Prefill latency: 0.52 sec
Bandwidth achieved: 20398.31 GB/s
FLOPS achieved: 101.99 TF/s

Prefill latency: 0.5180249010154512 sec
Decode latency: 2.4974016879859846 sec
Time for inference 8: 3.02 sec total, 1357.79 tokens/sec
Decode latency: 2.50 sec
Prefill latency: 0.52 sec
Bandwidth achieved: 20380.17 GB/s
FLOPS achieved: 101.90 TF/s

Prefill latency: 0.5206443289935123 sec
Decode latency: 2.497588937985711 sec
Time for inference 9: 3.02 sec total, 1356.58 tokens/sec
Decode latency: 2.50 sec
Prefill latency: 0.52 sec
Bandwidth achieved: 20362.14 GB/s
FLOPS achieved: 101.81 TF/s

Prefill latency: 0.5201141670113429 sec
Decode latency: 2.498024136002641 sec
Time for inference 10: 3.02 sec total, 1356.66 tokens/sec
Decode latency: 2.50 sec
Prefill latency: 0.52 sec
Bandwidth achieved: 20363.30 GB/s
FLOPS achieved: 101.82 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 2.4976 sec
Average prefill latency: 0.5171 sec
Average tokens/sec: 1358.18
Memory used: 30.12 GB
