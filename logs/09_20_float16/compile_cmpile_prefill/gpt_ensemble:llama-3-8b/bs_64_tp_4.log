W0925 09:17:14.192000 22370020296512 torch/distributed/run.py:779] 
W0925 09:17:14.192000 22370020296512 torch/distributed/run.py:779] *****************************************
W0925 09:17:14.192000 22370020296512 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0925 09:17:14.192000 22370020296512 torch/distributed/run.py:779] *****************************************
flash_kv_decode is set to False
Using device=cuda
Loading model ...
[rank2]: Traceback (most recent call last):
[rank2]:   File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 518, in <module>
[rank2]:     main(
[rank2]:   File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 360, in main
[rank2]:     model = _load_model(model_name, device, precision)
[rank2]:   File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 240, in _load_model
[rank2]:     model = model.to_empty(device=device)
[rank2]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1037, in to_empty
[rank2]:     return self._apply(lambda t: torch.empty_like(t, device=device), recurse=recurse)
[rank2]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/nn/modules/module.py", line 780, in _apply
[rank2]:     module._apply(fn)
[rank2]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/nn/modules/module.py", line 805, in _apply
[rank2]:     param_applied = fn(param)
[rank2]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1037, in <lambda>
[rank2]:     return self._apply(lambda t: torch.empty_like(t, device=device), recurse=recurse)
[rank2]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_prims_common/wrappers.py", line 266, in _fn
[rank2]:     result = fn(*args, **kwargs)
[rank2]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_refs/__init__.py", line 4890, in empty_like
[rank2]:     return torch.empty_permuted(
[rank2]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 2 has a total capacity of 79.33 GiB of which 751.75 MiB is free. Process 618674 has 73.82 GiB memory in use. Including non-PyTorch memory, this process has 4.76 GiB memory in use. Of the allocated memory 4.23 GiB is allocated by PyTorch, and 13.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank3]: Traceback (most recent call last):
[rank3]:   File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 518, in <module>
[rank3]:     main(
[rank3]:   File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 360, in main
[rank3]:     model = _load_model(model_name, device, precision)
[rank3]:   File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 240, in _load_model
[rank3]:     model = model.to_empty(device=device)
[rank3]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1037, in to_empty
[rank3]:     return self._apply(lambda t: torch.empty_like(t, device=device), recurse=recurse)
[rank3]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/nn/modules/module.py", line 780, in _apply
[rank3]:     module._apply(fn)
[rank3]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/nn/modules/module.py", line 805, in _apply
[rank3]:     param_applied = fn(param)
[rank3]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1037, in <lambda>
[rank3]:     return self._apply(lambda t: torch.empty_like(t, device=device), recurse=recurse)
[rank3]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_prims_common/wrappers.py", line 266, in _fn
[rank3]:     result = fn(*args, **kwargs)
[rank3]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/_refs/__init__.py", line 4890, in empty_like
[rank3]:     return torch.empty_permuted(
[rank3]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 3 has a total capacity of 79.33 GiB of which 653.75 MiB is free. Process 618675 has 73.92 GiB memory in use. Including non-PyTorch memory, this process has 4.76 GiB memory in use. Of the allocated memory 4.23 GiB is allocated by PyTorch, and 13.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 518, in <module>
[rank1]:     main(
[rank1]:   File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 375, in main
[rank1]:     model.setup_caches(max_batch_size=batch_size, max_seq_length=max_seq_length)
[rank1]:   File "/home/charlie/skip-residual/gpt-fast/gpt_fast/gpt_ensemble_TP.py", line 122, in setup_caches
[rank1]:     b.attention.kv_cache = KVCache(max_batch_size, max_seq_length, self.config.n_local_heads // tp_world_size, head_dim, dtype)
[rank1]:   File "/home/charlie/skip-residual/gpt-fast/gpt_fast/utils.py", line 43, in __init__
[rank1]:     self.register_buffer('k_cache', torch.zeros(cache_shape, dtype=dtype))
[rank1]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/utils/_device.py", line 79, in __torch_function__
[rank1]:     return func(*args, **kwargs)
[rank1]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 40.00 MiB. GPU 1 has a total capacity of 79.33 GiB of which 9.75 MiB is free. Process 618673 has 72.39 GiB memory in use. Including non-PyTorch memory, this process has 6.92 GiB memory in use. Of the allocated memory 6.30 GiB is allocated by PyTorch, and 13.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=1536, bias=False)
        (wo): Linear(in_features=1024, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 0.90 seconds
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 518, in <module>
[rank0]:     main(
[rank0]:   File "/home/charlie/skip-residual/gpt-fast/benchmark.py", line 375, in main
[rank0]:     model.setup_caches(max_batch_size=batch_size, max_seq_length=max_seq_length)
[rank0]:   File "/home/charlie/skip-residual/gpt-fast/gpt_fast/gpt_ensemble_TP.py", line 122, in setup_caches
[rank0]:     b.attention.kv_cache = KVCache(max_batch_size, max_seq_length, self.config.n_local_heads // tp_world_size, head_dim, dtype)
[rank0]:   File "/home/charlie/skip-residual/gpt-fast/gpt_fast/utils.py", line 43, in __init__
[rank0]:     self.register_buffer('k_cache', torch.zeros(cache_shape, dtype=dtype))
[rank0]:   File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/utils/_device.py", line 79, in __torch_function__
[rank0]:     return func(*args, **kwargs)
[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 40.00 MiB. GPU 0 has a total capacity of 79.33 GiB of which 25.75 MiB is free. Process 618672 has 72.29 GiB memory in use. Including non-PyTorch memory, this process has 7.00 GiB memory in use. Of the allocated memory 6.38 GiB is allocated by PyTorch, and 13.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
W0925 09:17:18.335000 22370020296512 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 632411 closing signal SIGTERM
W0925 09:17:18.336000 22370020296512 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 632412 closing signal SIGTERM
W0925 09:17:18.336000 22370020296512 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 632414 closing signal SIGTERM
E0925 09:17:18.452000 22370020296512 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 2 (pid: 632413) of binary: /home/charlie/anaconda3/envs/gpt-fast/bin/python
Traceback (most recent call last):
  File "/home/charlie/anaconda3/envs/gpt-fast/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
benchmark.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-09-25_09:17:18
  host      : mk-xii-17.cloud.together.ai
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 632413)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
