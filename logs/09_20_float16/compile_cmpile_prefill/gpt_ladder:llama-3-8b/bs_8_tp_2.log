W0920 08:33:35.086000 23013152347968 torch/distributed/run.py:779] 
W0920 08:33:35.086000 23013152347968 torch/distributed/run.py:779] *****************************************
W0920 08:33:35.086000 23013152347968 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0920 08:33:35.086000 23013152347968 torch/distributed/run.py:779] *****************************************
flash_kv_decode is set to False
Using device=cuda
Loading model ...
GPTLadder(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x LadderTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=3072, bias=False)
        (wo): Linear(in_features=2048, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=14336, bias=False)
        (w2): Linear(in_features=7168, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 0.06 seconds
Compiling prefill with dynamic=False
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
[rank0]:W0920 08:34:05.945000 22635033675584 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] xindex is not in var_ranges, defaulting to unknown range.
[rank1]:W0920 08:34:06.024000 23092807014208 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] xindex is not in var_ranges, defaulting to unknown range.
Prefill latency: 52.417015157989226 sec
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
[rank0]:W0920 08:34:58.369000 22635033675584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] xindex is not in var_ranges, defaulting to unknown range.
[rank1]:W0920 08:34:59.036000 23092807014208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] xindex is not in var_ranges, defaulting to unknown range.
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 62.11265543100308 sec
Compilation time: 114.53 seconds
Compilation time: 114.53 seconds
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Prefill latency: 0.9660612949810456 sec
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 1.5384259760030545 sec
Prefill latency: 0.15211223298683763 sec
Decode latency: 1.537552018999122 sec
Prefill latency: 0.1517362230224535 sec
Decode latency: 1.537526517000515 sec
Prefill latency: 0.15075952600454912 sec
Decode latency: 1.5377687480067834 sec
Prefill latency: 0.1526574769814033 sec
Decode latency: 1.5381196050147992 sec
Time for inference 1: 1.69 sec total, 1210.58 tokens/sec
Decode latency: 1.54 sec
Prefill latency: 0.15 sec
Bandwidth achieved: 9721.56 GB/s
FLOPS achieved: 48.61 TF/s

Prefill latency: 0.1528600479941815 sec
Decode latency: 1.5386487809882965 sec
Time for inference 2: 1.69 sec total, 1210.06 tokens/sec
Decode latency: 1.54 sec
Prefill latency: 0.15 sec
Bandwidth achieved: 9717.43 GB/s
FLOPS achieved: 48.59 TF/s

Prefill latency: 0.15196567101520486 sec
Decode latency: 1.5387266670004465 sec
Time for inference 3: 1.69 sec total, 1210.66 tokens/sec
Decode latency: 1.54 sec
Prefill latency: 0.15 sec
Bandwidth achieved: 9722.23 GB/s
FLOPS achieved: 48.61 TF/s

Prefill latency: 0.1512281220057048 sec
Decode latency: 1.537916542001767 sec
Time for inference 4: 1.69 sec total, 1211.79 tokens/sec
Decode latency: 1.54 sec
Prefill latency: 0.15 sec
Bandwidth achieved: 9731.28 GB/s
FLOPS achieved: 48.66 TF/s

Prefill latency: 0.1520889750099741 sec
Decode latency: 1.538148503022967 sec
Time for inference 5: 1.69 sec total, 1210.99 tokens/sec
Decode latency: 1.54 sec
Prefill latency: 0.15 sec
Bandwidth achieved: 9724.86 GB/s
FLOPS achieved: 48.62 TF/s

Prefill latency: 0.15289793501142412 sec
Decode latency: 1.538613668992184 sec
Time for inference 6: 1.69 sec total, 1210.05 tokens/sec
Decode latency: 1.54 sec
Prefill latency: 0.15 sec
Bandwidth achieved: 9717.32 GB/s
FLOPS achieved: 48.59 TF/s

Prefill latency: 0.1516607099911198 sec
Decode latency: 1.5387680279964115 sec
Time for inference 7: 1.69 sec total, 1210.88 tokens/sec
Decode latency: 1.54 sec
Prefill latency: 0.15 sec
Bandwidth achieved: 9723.97 GB/s
FLOPS achieved: 48.62 TF/s

Prefill latency: 0.1521797910099849 sec
Decode latency: 1.5382736079918686 sec
Time for inference 8: 1.69 sec total, 1210.85 tokens/sec
Decode latency: 1.54 sec
Prefill latency: 0.15 sec
Bandwidth achieved: 9723.74 GB/s
FLOPS achieved: 48.62 TF/s

Prefill latency: 0.15142952802125365 sec
Decode latency: 1.53823623698554 sec
Time for inference 9: 1.69 sec total, 1211.42 tokens/sec
Decode latency: 1.54 sec
Prefill latency: 0.15 sec
Bandwidth achieved: 9728.32 GB/s
FLOPS achieved: 48.64 TF/s

Prefill latency: 0.15165646199602634 sec
Decode latency: 1.539087932993425 sec
Time for inference 10: 1.69 sec total, 1210.64 tokens/sec
Decode latency: 1.54 sec
Prefill latency: 0.15 sec
Bandwidth achieved: 9722.10 GB/s
FLOPS achieved: 48.61 TF/s

==========
Batch Size: 8
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 1.5385 sec
Average prefill latency: 0.1521 sec
Average tokens/sec: 1210.79
Memory used: 14.60 GB
