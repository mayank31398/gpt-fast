W0920 10:14:00.026000 22399945029440 torch/distributed/run.py:779] 
W0920 10:14:00.026000 22399945029440 torch/distributed/run.py:779] *****************************************
W0920 10:14:00.026000 22399945029440 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0920 10:14:00.026000 22399945029440 torch/distributed/run.py:779] *****************************************
flash_kv_decode is set to False
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=3072, bias=False)
        (wo): Linear(in_features=2048, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=14336, bias=False)
        (w2): Linear(in_features=7168, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 0.89 seconds
Compiling prefill with dynamic=False
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
[rank0]:W0920 10:14:29.980000 23190803425088 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] xindex is not in var_ranges, defaulting to unknown range.
[rank1]:W0920 10:14:30.404000 22736204343104 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] xindex is not in var_ranges, defaulting to unknown range.
Prefill latency: 32.32551554701058 sec
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
[rank0]:W0920 10:15:01.801000 23190803425088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] xindex is not in var_ranges, defaulting to unknown range.
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 31.969049188977806 sec
Compilation time: 64.30 seconds
Prefill latency: 0.9097381070023403 sec
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Decode latency: 1.5053604889835697 sec
Prefill latency: 0.27040479000424966 sec
Decode latency: 1.5051415580091998 sec
Prefill latency: 0.2730749390029814 sec
Decode latency: 1.5055874820100144 sec
Prefill latency: 0.2746294350072276 sec
Decode latency: 1.505406034993939 sec
Prefill latency: 0.27335474800202064 sec
Decode latency: 1.5067368970194366 sec
Time for inference 1: 1.78 sec total, 2299.60 tokens/sec
Decode latency: 1.51 sec
Prefill latency: 0.27 sec
Bandwidth achieved: 18466.99 GB/s
FLOPS achieved: 92.33 TF/s

Prefill latency: 0.2743141640094109 sec
Decode latency: 1.5054697720042896 sec
Time for inference 2: 1.78 sec total, 2300.07 tokens/sec
Decode latency: 1.51 sec
Prefill latency: 0.27 sec
Bandwidth achieved: 18470.77 GB/s
FLOPS achieved: 92.35 TF/s

Prefill latency: 0.2754616170132067 sec
Decode latency: 1.5055591459968127 sec
Time for inference 3: 1.78 sec total, 2298.57 tokens/sec
Decode latency: 1.51 sec
Prefill latency: 0.28 sec
Bandwidth achieved: 18458.71 GB/s
FLOPS achieved: 92.29 TF/s

Prefill latency: 0.2732645270007197 sec
Decode latency: 1.5057467510050628 sec
Time for inference 4: 1.78 sec total, 2301.21 tokens/sec
Decode latency: 1.51 sec
Prefill latency: 0.27 sec
Bandwidth achieved: 18479.92 GB/s
FLOPS achieved: 92.40 TF/s

Prefill latency: 0.27457009098725393 sec
Decode latency: 1.5063412850140594 sec
Time for inference 5: 1.78 sec total, 2298.08 tokens/sec
Decode latency: 1.51 sec
Prefill latency: 0.27 sec
Bandwidth achieved: 18454.79 GB/s
FLOPS achieved: 92.27 TF/s

Prefill latency: 0.27675835002446547 sec
Decode latency: 1.506039537023753 sec
Time for inference 6: 1.78 sec total, 2295.96 tokens/sec
Decode latency: 1.51 sec
Prefill latency: 0.28 sec
Bandwidth achieved: 18437.77 GB/s
FLOPS achieved: 92.19 TF/s

[rank1]:W0920 10:15:27.347000 22736204343104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] xindex is not in var_ranges, defaulting to unknown range.
Prefill latency: 0.276261285005603 sec
Decode latency: 1.5052747320150957 sec
Time for inference 7: 1.78 sec total, 2297.74 tokens/sec
Decode latency: 1.51 sec
Prefill latency: 0.28 sec
Bandwidth achieved: 18452.10 GB/s
FLOPS achieved: 92.26 TF/s

Prefill latency: 0.27451135200681165 sec
Decode latency: 1.5051817910280079 sec
Time for inference 8: 1.78 sec total, 2299.77 tokens/sec
Decode latency: 1.51 sec
Prefill latency: 0.27 sec
Bandwidth achieved: 18468.35 GB/s
FLOPS achieved: 92.34 TF/s

Prefill latency: 0.27586857898859307 sec
Decode latency: 1.5056860070035327 sec
Time for inference 9: 1.78 sec total, 2297.42 tokens/sec
Decode latency: 1.51 sec
Prefill latency: 0.28 sec
Bandwidth achieved: 18449.47 GB/s
FLOPS achieved: 92.25 TF/s

Prefill latency: 0.27523682499304414 sec
Decode latency: 1.5066483039990999 sec
Time for inference 10: 1.78 sec total, 2296.94 tokens/sec
Decode latency: 1.51 sec
Prefill latency: 0.28 sec
Bandwidth achieved: 18445.66 GB/s
FLOPS achieved: 92.23 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 1.5059 sec
Average prefill latency: 0.2750 sec
Average tokens/sec: 2298.54
Memory used: 19.47 GB
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Compilation time: 103.36 seconds
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
