W0920 11:19:17.749000 23048350107456 torch/distributed/run.py:779] 
W0920 11:19:17.749000 23048350107456 torch/distributed/run.py:779] *****************************************
W0920 11:19:17.749000 23048350107456 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0920 11:19:17.749000 23048350107456 torch/distributed/run.py:779] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=2560, bias=False)
        (wo): Linear(in_features=2048, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=14336, bias=False)
        (w2): Linear(in_features=7168, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 0.99 seconds
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Prefill latency: 7.202995514497161 sec
Decode latency: 14.88593964278698 sec
Compilation time: 22.07 seconds
Compilation time: 21.91 seconds
Compilation time: 22.02 seconds
Compilation time: 22.09 seconds
Prefill latency: 0.6557671464979649 sec
Decode latency: 14.867895955219865 sec
Prefill latency: 0.6549738869071007 sec
Decode latency: 15.349654234945774 sec
Prefill latency: 0.6572363432496786 sec
Decode latency: 14.997831052169204 sec
Prefill latency: 0.6565912049263716 sec
Decode latency: 14.927518891170621 sec
Prefill latency: 0.657727062702179 sec
Decode latency: 14.98604236729443 sec
Time for inference 1: 15.64 sec total, 130.91 tokens/sec
Decode latency: 14.99 sec
Prefill latency: 0.66 sec
Bandwidth achieved: 4755.79 GB/s
FLOPS achieved: 23.78 TF/s

Prefill latency: 0.6564677841961384 sec
Decode latency: 14.87163352407515 sec
Time for inference 2: 15.53 sec total, 131.88 tokens/sec
Decode latency: 14.87 sec
Prefill latency: 0.66 sec
Bandwidth achieved: 4791.21 GB/s
FLOPS achieved: 23.96 TF/s

Prefill latency: 0.6559651587158442 sec
Decode latency: 14.857163399457932 sec
Time for inference 3: 15.51 sec total, 132.01 tokens/sec
Decode latency: 14.86 sec
Prefill latency: 0.66 sec
Bandwidth achieved: 4795.85 GB/s
FLOPS achieved: 23.98 TF/s

Prefill latency: 0.6572933662682772 sec
Decode latency: 14.819240199401975 sec
Time for inference 4: 15.48 sec total, 132.32 tokens/sec
Decode latency: 14.82 sec
Prefill latency: 0.66 sec
Bandwidth achieved: 4807.19 GB/s
FLOPS achieved: 24.04 TF/s

Prefill latency: 0.6584968417882919 sec
Decode latency: 14.842513041570783 sec
Time for inference 5: 15.50 sec total, 132.11 tokens/sec
Decode latency: 14.84 sec
Prefill latency: 0.66 sec
Bandwidth achieved: 4799.60 GB/s
FLOPS achieved: 24.00 TF/s

Prefill latency: 0.6579610258340836 sec
Decode latency: 14.824632162228227 sec
Time for inference 6: 15.48 sec total, 132.27 tokens/sec
Decode latency: 14.82 sec
Prefill latency: 0.66 sec
Bandwidth achieved: 4805.29 GB/s
FLOPS achieved: 24.03 TF/s

Prefill latency: 0.6578621622174978 sec
Decode latency: 14.898881617933512 sec
Time for inference 7: 15.56 sec total, 131.64 tokens/sec
Decode latency: 14.90 sec
Prefill latency: 0.66 sec
Bandwidth achieved: 4782.39 GB/s
FLOPS achieved: 23.91 TF/s

Prefill latency: 0.6582601964473724 sec
Decode latency: 14.889363588765264 sec
Time for inference 8: 15.55 sec total, 131.72 tokens/sec
Decode latency: 14.89 sec
Prefill latency: 0.66 sec
Bandwidth achieved: 4785.21 GB/s
FLOPS achieved: 23.93 TF/s

Prefill latency: 0.6557685416191816 sec
Decode latency: 15.062931511551142 sec
Time for inference 9: 15.72 sec total, 130.28 tokens/sec
Decode latency: 15.06 sec
Prefill latency: 0.66 sec
Bandwidth achieved: 4733.13 GB/s
FLOPS achieved: 23.67 TF/s

Prefill latency: 0.6577451191842556 sec
Decode latency: 14.831195678561926 sec
Time for inference 10: 15.49 sec total, 132.22 tokens/sec
Decode latency: 14.83 sec
Prefill latency: 0.66 sec
Bandwidth achieved: 4803.34 GB/s
FLOPS achieved: 24.02 TF/s

==========
Batch Size: 8
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 14.8884 sec
Average prefill latency: 0.6574 sec
Average tokens/sec: 131.74
Memory used: 42.99 GB
