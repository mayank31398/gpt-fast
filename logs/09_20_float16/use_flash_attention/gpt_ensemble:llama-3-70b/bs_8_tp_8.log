W0920 11:23:31.660000 23408078620480 torch/distributed/run.py:779] 
W0920 11:23:31.660000 23408078620480 torch/distributed/run.py:779] *****************************************
W0920 11:23:31.660000 23408078620480 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0920 11:23:31.660000 23408078620480 torch/distributed/run.py:779] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=1280, bias=False)
        (wo): Linear(in_features=1024, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 1.23 seconds
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Prefill latency: 12.161987751722336 sec
Decode latency: 15.780422335490584 sec
Compilation time: 27.94 seconds
Compilation time: 27.98 secondsCompilation time: 27.97 seconds

Compilation time: 28.08 seconds
Compilation time: 27.91 seconds
Compilation time: 28.00 seconds
Compilation time: 27.93 seconds
Compilation time: 27.94 seconds
Prefill latency: 0.38292057253420353 sec
Decode latency: 15.404362248256803 sec
Prefill latency: 0.38232444785535336 sec
Decode latency: 15.523825615644455 sec
Prefill latency: 0.3837885484099388 sec
Decode latency: 15.73427321203053 sec
Prefill latency: 0.38408547453582287 sec
Decode latency: 15.226795356720686 sec
Prefill latency: 0.3848197627812624 sec
Decode latency: 15.187366012483835 sec
Time for inference 1: 15.57 sec total, 131.51 tokens/sec
Decode latency: 15.19 sec
Prefill latency: 0.38 sec
Bandwidth achieved: 2527.16 GB/s
FLOPS achieved: 12.64 TF/s

Prefill latency: 0.3843445498496294 sec
Decode latency: 15.468373050913215 sec
Time for inference 2: 15.85 sec total, 129.18 tokens/sec
Decode latency: 15.47 sec
Prefill latency: 0.38 sec
Bandwidth achieved: 2482.45 GB/s
FLOPS achieved: 12.41 TF/s

Prefill latency: 0.3843925502151251 sec
Decode latency: 15.082760421559215 sec
Time for inference 3: 15.47 sec total, 132.40 tokens/sec
Decode latency: 15.08 sec
Prefill latency: 0.38 sec
Bandwidth achieved: 2544.33 GB/s
FLOPS achieved: 12.72 TF/s

Prefill latency: 0.3849778510630131 sec
Decode latency: 15.107153156772256 sec
Time for inference 4: 15.49 sec total, 132.19 tokens/sec
Decode latency: 15.11 sec
Prefill latency: 0.38 sec
Bandwidth achieved: 2540.23 GB/s
FLOPS achieved: 12.70 TF/s

Prefill latency: 0.3843873105943203 sec
Decode latency: 14.942124590277672 sec
Time for inference 5: 15.33 sec total, 133.62 tokens/sec
Decode latency: 14.94 sec
Prefill latency: 0.38 sec
Bandwidth achieved: 2567.67 GB/s
FLOPS achieved: 12.84 TF/s

Prefill latency: 0.38422963209450245 sec
Decode latency: 15.62912118807435 sec
Time for inference 6: 16.01 sec total, 127.89 tokens/sec
Decode latency: 15.63 sec
Prefill latency: 0.38 sec
Bandwidth achieved: 2457.54 GB/s
FLOPS achieved: 12.29 TF/s

Prefill latency: 0.3842402771115303 sec
Decode latency: 15.175306314602494 sec
Time for inference 7: 15.56 sec total, 131.62 tokens/sec
Decode latency: 15.18 sec
Prefill latency: 0.38 sec
Bandwidth achieved: 2529.22 GB/s
FLOPS achieved: 12.65 TF/s

Prefill latency: 0.3843169119209051 sec
Decode latency: 15.330282520502806 sec
Time for inference 8: 15.72 sec total, 130.32 tokens/sec
Decode latency: 15.33 sec
Prefill latency: 0.38 sec
Bandwidth achieved: 2504.25 GB/s
FLOPS achieved: 12.52 TF/s

Prefill latency: 0.3846890479326248 sec
Decode latency: 15.381088627502322 sec
Time for inference 9: 15.77 sec total, 129.89 tokens/sec
Decode latency: 15.38 sec
Prefill latency: 0.38 sec
Bandwidth achieved: 2496.13 GB/s
FLOPS achieved: 12.48 TF/s

Prefill latency: 0.38460734859108925 sec
Decode latency: 15.229796564206481 sec
Time for inference 10: 15.62 sec total, 131.15 tokens/sec
Decode latency: 15.23 sec
Prefill latency: 0.38 sec
Bandwidth achieved: 2520.32 GB/s
FLOPS achieved: 12.60 TF/s

==========
Batch Size: 8
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 15.2533 sec
Average prefill latency: 0.3845 sec
Average tokens/sec: 130.98
Memory used: 24.96 GB
