W0920 11:37:39.928000 22977102833472 torch/distributed/run.py:779] 
W0920 11:37:39.928000 22977102833472 torch/distributed/run.py:779] *****************************************
W0920 11:37:39.928000 22977102833472 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0920 11:37:39.928000 22977102833472 torch/distributed/run.py:779] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=2560, bias=False)
        (wo): Linear(in_features=2048, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=14336, bias=False)
        (w2): Linear(in_features=7168, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 0.78 seconds
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Prefill latency: 11.777040712535381 sec
Decode latency: 15.089984647929668 sec
Compilation time: 26.62 seconds
Compilation time: 26.66 seconds
Compilation time: 26.79 seconds
Compilation time: 26.87 seconds
Prefill latency: 5.239448664709926 sec
Decode latency: 15.3396948967129 sec
Prefill latency: 5.235725447535515 sec
Decode latency: 15.00838639959693 sec
Prefill latency: 5.239023350179195 sec
Decode latency: 15.047875909134746 sec
Prefill latency: 5.248847119510174 sec
Decode latency: 15.420086406171322 sec
Prefill latency: 5.248914571478963 sec
Decode latency: 14.885856878012419 sec
Time for inference 1: 20.14 sec total, 813.67 tokens/sec
Decode latency: 14.89 sec
Prefill latency: 5.25 sec
Bandwidth achieved: 29560.36 GB/s
FLOPS achieved: 147.80 TF/s

Prefill latency: 5.242847315967083 sec
Decode latency: 15.343921322375536 sec
Time for inference 2: 20.59 sec total, 795.81 tokens/sec
Decode latency: 15.34 sec
Prefill latency: 5.24 sec
Bandwidth achieved: 28911.55 GB/s
FLOPS achieved: 144.56 TF/s

Prefill latency: 5.25634478032589 sec
Decode latency: 15.279671560972929 sec
Time for inference 3: 20.54 sec total, 797.78 tokens/sec
Decode latency: 15.28 sec
Prefill latency: 5.26 sec
Bandwidth achieved: 28982.81 GB/s
FLOPS achieved: 144.91 TF/s

Prefill latency: 5.253429375588894 sec
Decode latency: 15.084515359252691 sec
Time for inference 4: 20.34 sec total, 805.55 tokens/sec
Decode latency: 15.08 sec
Prefill latency: 5.25 sec
Bandwidth achieved: 29265.27 GB/s
FLOPS achieved: 146.33 TF/s

Prefill latency: 5.230891093611717 sec
Decode latency: 14.884303333237767 sec
Time for inference 5: 20.12 sec total, 814.47 tokens/sec
Decode latency: 14.88 sec
Prefill latency: 5.23 sec
Bandwidth achieved: 29589.43 GB/s
FLOPS achieved: 147.95 TF/s

Prefill latency: 5.243699911981821 sec
Decode latency: 15.003759946674109 sec
Time for inference 6: 20.25 sec total, 809.15 tokens/sec
Decode latency: 15.00 sec
Prefill latency: 5.24 sec
Bandwidth achieved: 29395.99 GB/s
FLOPS achieved: 146.98 TF/s

Prefill latency: 5.247136535122991 sec
Decode latency: 14.93937542103231 sec
Time for inference 7: 20.19 sec total, 811.59 tokens/sec
Decode latency: 14.94 sec
Prefill latency: 5.25 sec
Bandwidth achieved: 29484.62 GB/s
FLOPS achieved: 147.42 TF/s

Prefill latency: 5.236730618402362 sec
Decode latency: 15.030294520780444 sec
Time for inference 8: 20.27 sec total, 808.37 tokens/sec
Decode latency: 15.03 sec
Prefill latency: 5.24 sec
Bandwidth achieved: 29367.65 GB/s
FLOPS achieved: 146.84 TF/s

Prefill latency: 5.254751177504659 sec
Decode latency: 15.355348251760006 sec
Time for inference 9: 20.61 sec total, 794.91 tokens/sec
Decode latency: 15.36 sec
Prefill latency: 5.25 sec
Bandwidth achieved: 28878.81 GB/s
FLOPS achieved: 144.39 TF/s

Prefill latency: 5.236458487808704 sec
Decode latency: 15.187264090403914 sec
Time for inference 10: 20.42 sec total, 802.16 tokens/sec
Decode latency: 15.19 sec
Prefill latency: 5.24 sec
Bandwidth achieved: 29142.18 GB/s
FLOPS achieved: 145.71 TF/s

==========
Batch Size: 64
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 15.0994 sec
Average prefill latency: 5.2451 sec
Average tokens/sec: 805.35
Memory used: 74.46 GB
