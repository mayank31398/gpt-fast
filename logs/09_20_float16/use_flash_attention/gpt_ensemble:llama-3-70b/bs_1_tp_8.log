W0920 10:57:59.702000 22792769910592 torch/distributed/run.py:779] 
W0920 10:57:59.702000 22792769910592 torch/distributed/run.py:779] *****************************************
W0920 10:57:59.702000 22792769910592 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0920 10:57:59.702000 22792769910592 torch/distributed/run.py:779] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=1280, bias=False)
        (wo): Linear(in_features=1024, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 1.24 seconds
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Prefill latency: 11.701838999986649 sec
Decode latency: 15.319107687100768 sec
Compilation time: 27.01 seconds
Compilation time: 27.06 secondsCompilation time: 27.12 seconds

Compilation time: 27.04 seconds
Compilation time: 27.02 seconds
Compilation time: 27.13 seconds
Compilation time: 27.06 seconds
Compilation time: 27.07 seconds
Prefill latency: 0.08192619867622852 sec
Decode latency: 15.425014117732644 sec
Prefill latency: 0.07932151295244694 sec
Decode latency: 15.115230781957507 sec
Prefill latency: 0.08938145823776722 sec
Decode latency: 15.261687360703945 sec
Prefill latency: 0.0816351380199194 sec
Decode latency: 14.596212532371283 sec
Prefill latency: 0.08505010791122913 sec
Decode latency: 15.12377317622304 sec
Time for inference 1: 15.21 sec total, 16.83 tokens/sec
Decode latency: 15.12 sec
Prefill latency: 0.09 sec
Bandwidth achieved: 323.44 GB/s
FLOPS achieved: 1.62 TF/s

Prefill latency: 0.0810813419520855 sec
Decode latency: 14.94037146680057 sec
Time for inference 2: 15.02 sec total, 17.04 tokens/sec
Decode latency: 14.94 sec
Prefill latency: 0.08 sec
Bandwidth achieved: 327.48 GB/s
FLOPS achieved: 1.64 TF/s

Prefill latency: 0.0803530216217041 sec
Decode latency: 15.07890254817903 sec
Time for inference 3: 15.16 sec total, 16.89 tokens/sec
Decode latency: 15.08 sec
Prefill latency: 0.08 sec
Bandwidth achieved: 324.50 GB/s
FLOPS achieved: 1.62 TF/s

Prefill latency: 0.08002854324877262 sec
Decode latency: 15.299843342974782 sec
Time for inference 4: 15.38 sec total, 16.64 tokens/sec
Decode latency: 15.30 sec
Prefill latency: 0.08 sec
Bandwidth achieved: 319.85 GB/s
FLOPS achieved: 1.60 TF/s

Prefill latency: 0.08154986053705215 sec
Decode latency: 15.10830644890666 sec
Time for inference 5: 15.19 sec total, 16.85 tokens/sec
Decode latency: 15.11 sec
Prefill latency: 0.08 sec
Bandwidth achieved: 323.85 GB/s
FLOPS achieved: 1.62 TF/s

Prefill latency: 0.08096742630004883 sec
Decode latency: 14.573410807177424 sec
Time for inference 6: 14.66 sec total, 17.47 tokens/sec
Decode latency: 14.57 sec
Prefill latency: 0.08 sec
Bandwidth achieved: 335.68 GB/s
FLOPS achieved: 1.68 TF/s

Prefill latency: 0.07961842231452465 sec
Decode latency: 15.01717715151608 sec
Time for inference 7: 15.10 sec total, 16.96 tokens/sec
Decode latency: 15.02 sec
Prefill latency: 0.08 sec
Bandwidth achieved: 325.84 GB/s
FLOPS achieved: 1.63 TF/s

Prefill latency: 0.08079177513718605 sec
Decode latency: 15.089249819517136 sec
Time for inference 8: 15.17 sec total, 16.87 tokens/sec
Decode latency: 15.09 sec
Prefill latency: 0.08 sec
Bandwidth achieved: 324.27 GB/s
FLOPS achieved: 1.62 TF/s

Prefill latency: 0.08116653375327587 sec
Decode latency: 15.409684624522924 sec
Time for inference 9: 15.49 sec total, 16.52 tokens/sec
Decode latency: 15.41 sec
Prefill latency: 0.08 sec
Bandwidth achieved: 317.55 GB/s
FLOPS achieved: 1.59 TF/s

Prefill latency: 0.09401723556220531 sec
Decode latency: 14.943592986091971 sec
Time for inference 10: 15.04 sec total, 17.02 tokens/sec
Decode latency: 14.94 sec
Prefill latency: 0.09 sec
Bandwidth achieved: 327.13 GB/s
FLOPS achieved: 1.64 TF/s

==========
Batch Size: 1
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 15.0584 sec
Average prefill latency: 0.0825 sec
Average tokens/sec: 16.91
Memory used: 21.85 GB
