W0920 09:01:27.287000 23000308381504 torch/distributed/run.py:779] 
W0920 09:01:27.287000 23000308381504 torch/distributed/run.py:779] *****************************************
W0920 09:01:27.287000 23000308381504 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0920 09:01:27.287000 23000308381504 torch/distributed/run.py:779] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTDense(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x DenseTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=1536, bias=False)
        (wo): Linear(in_features=1024, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.00 seconds
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Prefill latency: 6.598108733072877 sec
Decode latency: 7.453610586002469 sec
Compilation time: 14.05 secondsCompilation time: 14.02 seconds

Compilation time: 14.10 seconds
Compilation time: 14.05 seconds
Prefill latency: 0.21108711138367653 sec
Decode latency: 7.3377149403095245 sec
Prefill latency: 0.21121320500969887 sec
Decode latency: 7.636580295860767 sec
Prefill latency: 0.21100961975753307 sec
Decode latency: 7.524321487173438 sec
Prefill latency: 0.21091846004128456 sec
Decode latency: 7.300606740638614 sec
Prefill latency: 0.21126676723361015 sec
Decode latency: 7.756134681403637 sec
Time for inference 1: 7.97 sec total, 514.02 tokens/sec
Decode latency: 7.76 sec
Prefill latency: 0.21 sec
Bandwidth achieved: 2334.11 GB/s
FLOPS achieved: 11.67 TF/s

Prefill latency: 0.21105829253792763 sec
Decode latency: 7.785204911604524 sec
Time for inference 2: 8.00 sec total, 512.17 tokens/sec
Decode latency: 7.79 sec
Prefill latency: 0.21 sec
Bandwidth achieved: 2325.68 GB/s
FLOPS achieved: 11.63 TF/s

Prefill latency: 0.2112299520522356 sec
Decode latency: 7.7848367635160685 sec
Time for inference 3: 8.00 sec total, 512.18 tokens/sec
Decode latency: 7.78 sec
Prefill latency: 0.21 sec
Bandwidth achieved: 2325.73 GB/s
FLOPS achieved: 11.63 TF/s

Prefill latency: 0.2111989427357912 sec
Decode latency: 7.838782548904419 sec
Time for inference 4: 8.05 sec total, 508.74 tokens/sec
Decode latency: 7.84 sec
Prefill latency: 0.21 sec
Bandwidth achieved: 2310.14 GB/s
FLOPS achieved: 11.55 TF/s

Prefill latency: 0.21173952147364616 sec
Decode latency: 7.561899734660983 sec
Time for inference 5: 7.77 sec total, 526.83 tokens/sec
Decode latency: 7.56 sec
Prefill latency: 0.21 sec
Bandwidth achieved: 2392.25 GB/s
FLOPS achieved: 11.96 TF/s

Prefill latency: 0.21140018105506897 sec
Decode latency: 7.689250191673636 sec
Time for inference 6: 7.90 sec total, 518.36 tokens/sec
Decode latency: 7.69 sec
Prefill latency: 0.21 sec
Bandwidth achieved: 2353.83 GB/s
FLOPS achieved: 11.77 TF/s

Prefill latency: 0.21140226535499096 sec
Decode latency: 7.950732132419944 sec
Time for inference 7: 8.16 sec total, 501.76 tokens/sec
Decode latency: 7.95 sec
Prefill latency: 0.21 sec
Bandwidth achieved: 2278.42 GB/s
FLOPS achieved: 11.39 TF/s

Prefill latency: 0.21116992831230164 sec
Decode latency: 7.719750223681331 sec
Time for inference 8: 7.93 sec total, 516.39 tokens/sec
Decode latency: 7.72 sec
Prefill latency: 0.21 sec
Bandwidth achieved: 2344.84 GB/s
FLOPS achieved: 11.72 TF/s

Prefill latency: 0.2110716998577118 sec
Decode latency: 7.7503109239041805 sec
Time for inference 9: 7.96 sec total, 514.41 tokens/sec
Decode latency: 7.75 sec
Prefill latency: 0.21 sec
Bandwidth achieved: 2335.88 GB/s
FLOPS achieved: 11.68 TF/s

Prefill latency: 0.2112989891320467 sec
Decode latency: 7.715472921729088 sec
Time for inference 10: 7.93 sec total, 516.66 tokens/sec
Decode latency: 7.72 sec
Prefill latency: 0.21 sec
Bandwidth achieved: 2346.07 GB/s
FLOPS achieved: 11.73 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 7.7552 sec
Average prefill latency: 0.2113 sec
Average tokens/sec: 514.15
Memory used: 11.99 GB
