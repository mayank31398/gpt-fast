W0920 09:48:40.464000 23136637568832 torch/distributed/run.py:779] 
W0920 09:48:40.464000 23136637568832 torch/distributed/run.py:779] *****************************************
W0920 09:48:40.464000 23136637568832 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0920 09:48:40.464000 23136637568832 torch/distributed/run.py:779] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTParallel(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x ParallelTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=768, bias=False)
        (wo): Linear(in_features=512, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=3584, bias=False)
        (w2): Linear(in_features=1792, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.01 seconds
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Prefill latency: 11.698114171624184 sec
Decode latency: 7.089989451691508 sec
Compilation time: 18.75 seconds
Compilation time: 18.65 seconds
Compilation time: 18.71 seconds
Compilation time: 18.74 seconds
Compilation time: 18.61 seconds
Compilation time: 18.72 seconds
Compilation time: 18.79 seconds
Compilation time: 18.72 seconds
Prefill latency: 0.040335243567824364 sec
Decode latency: 7.222272355109453 sec
Prefill latency: 0.039406731724739075 sec
Decode latency: 7.571108995005488 sec
Prefill latency: 0.0402887761592865 sec
Decode latency: 7.097493559122086 sec
Prefill latency: 0.038760533556342125 sec
Decode latency: 6.895462907850742 sec
Prefill latency: 0.03940422274172306 sec
Decode latency: 7.1132583152502775 sec
Time for inference 1: 7.15 sec total, 35.79 tokens/sec
Decode latency: 7.11 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 100.06 GB/s
FLOPS achieved: 0.50 TF/s

Prefill latency: 0.03844687342643738 sec
Decode latency: 7.078158568590879 sec
Time for inference 2: 7.12 sec total, 35.97 tokens/sec
Decode latency: 7.08 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 100.56 GB/s
FLOPS achieved: 0.50 TF/s

Prefill latency: 0.03813178837299347 sec
Decode latency: 6.885254820808768 sec
Time for inference 3: 6.92 sec total, 36.97 tokens/sec
Decode latency: 6.89 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 103.37 GB/s
FLOPS achieved: 0.52 TF/s

Prefill latency: 0.03907572850584984 sec
Decode latency: 7.137106243520975 sec
Time for inference 4: 7.18 sec total, 35.67 tokens/sec
Decode latency: 7.14 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 99.73 GB/s
FLOPS achieved: 0.50 TF/s

Prefill latency: 0.0383443720638752 sec
Decode latency: 6.959104968234897 sec
Time for inference 5: 7.00 sec total, 36.58 tokens/sec
Decode latency: 6.96 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 102.28 GB/s
FLOPS achieved: 0.51 TF/s

Prefill latency: 0.03938942588865757 sec
Decode latency: 7.219008516520262 sec
Time for inference 6: 7.26 sec total, 35.26 tokens/sec
Decode latency: 7.22 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 98.60 GB/s
FLOPS achieved: 0.49 TF/s

Prefill latency: 0.039444880560040474 sec
Decode latency: 7.186011055484414 sec
Time for inference 7: 7.23 sec total, 35.43 tokens/sec
Decode latency: 7.19 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 99.05 GB/s
FLOPS achieved: 0.50 TF/s

Prefill latency: 0.039199236780405045 sec
Decode latency: 6.881461076438427 sec
Time for inference 8: 6.92 sec total, 36.98 tokens/sec
Decode latency: 6.88 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 103.41 GB/s
FLOPS achieved: 0.52 TF/s

Prefill latency: 0.03826468251645565 sec
Decode latency: 7.011947525665164 sec
Time for inference 9: 7.05 sec total, 36.31 tokens/sec
Decode latency: 7.01 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 101.51 GB/s
FLOPS achieved: 0.51 TF/s

Prefill latency: 0.03911957889795303 sec
Decode latency: 7.185077555477619 sec
Time for inference 10: 7.23 sec total, 35.43 tokens/sec
Decode latency: 7.19 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 99.06 GB/s
FLOPS achieved: 0.50 TF/s

==========
Batch Size: 1
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 7.0656 sec
Average prefill latency: 0.0389 sec
Average tokens/sec: 36.04
Memory used: 4.30 GB
