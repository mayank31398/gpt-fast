W0920 09:28:04.428000 22595666093888 torch/distributed/run.py:779] 
W0920 09:28:04.428000 22595666093888 torch/distributed/run.py:779] *****************************************
W0920 09:28:04.428000 22595666093888 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0920 09:28:04.428000 22595666093888 torch/distributed/run.py:779] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTDense(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x DenseTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=2560, bias=False)
        (wo): Linear(in_features=2048, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=14336, bias=False)
        (w2): Linear(in_features=7168, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 0.99 seconds
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Prefill latency: 6.410668367519975 sec
Decode latency: 16.14034734107554 sec
Compilation time: 22.59 seconds
Compilation time: 22.50 seconds
Compilation time: 22.57 seconds
Compilation time: 22.55 seconds
Prefill latency: 0.690728735178709 sec
Decode latency: 16.24941085278988 sec
Prefill latency: 0.6937897522002459 sec
Decode latency: 16.058287201449275 sec
Prefill latency: 0.6922533344477415 sec
Decode latency: 15.99620713479817 sec
Prefill latency: 0.6937939636409283 sec
Decode latency: 15.968176059424877 sec
Prefill latency: 0.6935288980603218 sec
Decode latency: 15.944761637598276 sec
Time for inference 1: 16.64 sec total, 123.08 tokens/sec
Decode latency: 15.94 sec
Prefill latency: 0.69 sec
Bandwidth achieved: 4471.53 GB/s
FLOPS achieved: 22.36 TF/s

Prefill latency: 0.6935915239155293 sec
Decode latency: 16.08655690960586 sec
Time for inference 2: 16.78 sec total, 122.04 tokens/sec
Decode latency: 16.09 sec
Prefill latency: 0.69 sec
Bandwidth achieved: 4433.72 GB/s
FLOPS achieved: 22.17 TF/s

Prefill latency: 0.6931406538933516 sec
Decode latency: 15.936471467837691 sec
Time for inference 3: 16.63 sec total, 123.15 tokens/sec
Decode latency: 15.94 sec
Prefill latency: 0.69 sec
Bandwidth achieved: 4473.86 GB/s
FLOPS achieved: 22.37 TF/s

Prefill latency: 0.6928666513413191 sec
Decode latency: 16.08364488556981 sec
Time for inference 4: 16.78 sec total, 122.07 tokens/sec
Decode latency: 16.08 sec
Prefill latency: 0.69 sec
Bandwidth achieved: 4434.67 GB/s
FLOPS achieved: 22.17 TF/s

Prefill latency: 0.6936211679130793 sec
Decode latency: 16.00151990354061 sec
Time for inference 5: 16.70 sec total, 122.66 tokens/sec
Decode latency: 16.00 sec
Prefill latency: 0.69 sec
Bandwidth achieved: 4456.30 GB/s
FLOPS achieved: 22.28 TF/s

Prefill latency: 0.6937733944505453 sec
Decode latency: 16.282720297574997 sec
Time for inference 6: 16.98 sec total, 120.63 tokens/sec
Decode latency: 16.28 sec
Prefill latency: 0.69 sec
Bandwidth achieved: 4382.44 GB/s
FLOPS achieved: 21.91 TF/s

Prefill latency: 0.693718459457159 sec
Decode latency: 16.240703869611025 sec
Time for inference 7: 16.94 sec total, 120.93 tokens/sec
Decode latency: 16.24 sec
Prefill latency: 0.69 sec
Bandwidth achieved: 4393.34 GB/s
FLOPS achieved: 21.97 TF/s

Prefill latency: 0.6932624150067568 sec
Decode latency: 15.935741499066353 sec
Time for inference 8: 16.63 sec total, 123.15 tokens/sec
Decode latency: 15.94 sec
Prefill latency: 0.69 sec
Bandwidth achieved: 4474.04 GB/s
FLOPS achieved: 22.37 TF/s

Prefill latency: 0.6923127397894859 sec
Decode latency: 16.055526960641146 sec
Time for inference 9: 16.75 sec total, 122.28 tokens/sec
Decode latency: 16.06 sec
Prefill latency: 0.69 sec
Bandwidth achieved: 4442.28 GB/s
FLOPS achieved: 22.21 TF/s

Prefill latency: 0.6943345740437508 sec
Decode latency: 16.172974463552237 sec
Time for inference 10: 16.87 sec total, 121.41 tokens/sec
Decode latency: 16.17 sec
Prefill latency: 0.69 sec
Bandwidth achieved: 4410.80 GB/s
FLOPS achieved: 22.05 TF/s

==========
Batch Size: 8
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 16.0741 sec
Average prefill latency: 0.6934 sec
Average tokens/sec: 122.14
Memory used: 42.91 GB
