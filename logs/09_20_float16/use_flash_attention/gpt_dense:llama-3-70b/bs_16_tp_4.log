W0920 09:37:52.102000 23028772759360 torch/distributed/run.py:779] 
W0920 09:37:52.102000 23028772759360 torch/distributed/run.py:779] *****************************************
W0920 09:37:52.102000 23028772759360 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0920 09:37:52.102000 23028772759360 torch/distributed/run.py:779] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
GPTDense(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x DenseTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=2560, bias=False)
        (wo): Linear(in_features=2048, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=14336, bias=False)
        (w2): Linear(in_features=7168, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Time to load model: 1.14 seconds
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Prefill latency: 7.245728835463524 sec
Decode latency: 16.497290460392833 sec
Compilation time: 23.97 seconds
Compilation time: 24.03 seconds
Compilation time: 24.02 seconds
Compilation time: 23.74 seconds
Prefill latency: 1.380012208595872 sec
Decode latency: 16.258205097168684 sec
Prefill latency: 1.3830916304141283 sec
Decode latency: 16.37584342993796 sec
Prefill latency: 1.3794974889606237 sec
Decode latency: 16.16180538572371 sec
Prefill latency: 1.383170273154974 sec
Decode latency: 16.397622492164373 sec
Prefill latency: 1.3854464758187532 sec
Decode latency: 16.43166738562286 sec
Time for inference 1: 17.82 sec total, 229.88 tokens/sec
Decode latency: 16.43 sec
Prefill latency: 1.39 sec
Bandwidth achieved: 8351.40 GB/s
FLOPS achieved: 41.76 TF/s

Prefill latency: 1.384204300120473 sec
Decode latency: 16.101064186543226 sec
Time for inference 2: 17.49 sec total, 234.24 tokens/sec
Decode latency: 16.10 sec
Prefill latency: 1.38 sec
Bandwidth achieved: 8509.88 GB/s
FLOPS achieved: 42.55 TF/s

Prefill latency: 1.385059354826808 sec
Decode latency: 16.252593075856566 sec
Time for inference 3: 17.64 sec total, 232.22 tokens/sec
Decode latency: 16.25 sec
Prefill latency: 1.39 sec
Bandwidth achieved: 8436.37 GB/s
FLOPS achieved: 42.18 TF/s

Prefill latency: 1.3875812031328678 sec
Decode latency: 16.27279374562204 sec
Time for inference 4: 17.66 sec total, 231.92 tokens/sec
Decode latency: 16.27 sec
Prefill latency: 1.39 sec
Bandwidth achieved: 8425.50 GB/s
FLOPS achieved: 42.13 TF/s

Prefill latency: 1.38820780813694 sec
Decode latency: 16.36388135328889 sec
Time for inference 5: 17.75 sec total, 230.72 tokens/sec
Decode latency: 16.36 sec
Prefill latency: 1.39 sec
Bandwidth achieved: 8381.97 GB/s
FLOPS achieved: 41.91 TF/s

Prefill latency: 1.3821323476731777 sec
Decode latency: 16.383117828518152 sec
Time for inference 6: 17.77 sec total, 230.55 tokens/sec
Decode latency: 16.38 sec
Prefill latency: 1.38 sec
Bandwidth achieved: 8375.76 GB/s
FLOPS achieved: 41.88 TF/s

Prefill latency: 1.3839209005236626 sec
Decode latency: 16.159680323675275 sec
Time for inference 7: 17.54 sec total, 233.46 tokens/sec
Decode latency: 16.16 sec
Prefill latency: 1.38 sec
Bandwidth achieved: 8481.55 GB/s
FLOPS achieved: 42.41 TF/s

Prefill latency: 1.3864767048507929 sec
Decode latency: 16.346280876547098 sec
Time for inference 8: 17.73 sec total, 230.97 tokens/sec
Decode latency: 16.35 sec
Prefill latency: 1.39 sec
Bandwidth achieved: 8391.11 GB/s
FLOPS achieved: 41.96 TF/s

Prefill latency: 1.3877404686063528 sec
Decode latency: 16.16265033185482 sec
Time for inference 9: 17.55 sec total, 233.37 tokens/sec
Decode latency: 16.16 sec
Prefill latency: 1.39 sec
Bandwidth achieved: 8478.30 GB/s
FLOPS achieved: 42.39 TF/s

Prefill latency: 1.3835399840027094 sec
Decode latency: 16.078638527542353 sec
Time for inference 10: 17.46 sec total, 234.55 tokens/sec
Decode latency: 16.08 sec
Prefill latency: 1.38 sec
Bandwidth achieved: 8521.11 GB/s
FLOPS achieved: 42.61 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 16.2552 sec
Average prefill latency: 1.3854 sec
Average tokens/sec: 232.19
Memory used: 47.32 GB
