W0920 08:00:32.780000 22391703627584 torch/distributed/run.py:779] 
W0920 08:00:32.780000 22391703627584 torch/distributed/run.py:779] *****************************************
W0920 08:00:32.780000 22391703627584 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0920 08:00:32.780000 22391703627584 torch/distributed/run.py:779] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTLadder(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x LadderTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=768, bias=False)
        (wo): Linear(in_features=512, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=3584, bias=False)
        (w2): Linear(in_features=1792, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 0.07 seconds
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Prefill latency: 9.859606819227338 sec
Decode latency: 8.059201275929809 sec
Compilation time: 17.91 seconds
Compilation time: 17.87 seconds
Compilation time: 17.94 seconds
Compilation time: 17.92 seconds
Compilation time: 17.91 seconds
Compilation time: 17.95 seconds
Compilation time: 17.92 seconds
Compilation time: 17.87 seconds
Prefill latency: 0.04639889486134052 sec
Decode latency: 8.412149287760258 sec
Prefill latency: 0.06402666121721268 sec
Decode latency: 8.289166864007711 sec
Prefill latency: 0.04309970699250698 sec
Decode latency: 8.14217541180551 sec
Prefill latency: 0.06252620369195938 sec
Decode latency: 7.867778934538364 sec
Prefill latency: 0.044964512810111046 sec
Decode latency: 7.769576685503125 sec
Time for inference 1: 7.82 sec total, 32.75 tokens/sec
Decode latency: 7.77 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 91.58 GB/s
FLOPS achieved: 0.46 TF/s

Prefill latency: 0.04308631084859371 sec
Decode latency: 7.745767407119274 sec
Time for inference 2: 7.79 sec total, 32.86 tokens/sec
Decode latency: 7.75 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 91.88 GB/s
FLOPS achieved: 0.46 TF/s

Prefill latency: 0.04502706229686737 sec
Decode latency: 7.521162552759051 sec
Time for inference 3: 7.57 sec total, 33.83 tokens/sec
Decode latency: 7.52 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 94.59 GB/s
FLOPS achieved: 0.47 TF/s

Prefill latency: 0.04417995549738407 sec
Decode latency: 7.6678828075528145 sec
Time for inference 4: 7.71 sec total, 33.19 tokens/sec
Decode latency: 7.67 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 92.80 GB/s
FLOPS achieved: 0.46 TF/s

Prefill latency: 0.04498095437884331 sec
Decode latency: 7.620665127411485 sec
Time for inference 5: 7.67 sec total, 33.39 tokens/sec
Decode latency: 7.62 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 93.36 GB/s
FLOPS achieved: 0.47 TF/s

Prefill latency: 0.04482908360660076 sec
Decode latency: 7.605724288150668 sec
Time for inference 6: 7.65 sec total, 33.46 tokens/sec
Decode latency: 7.61 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 93.54 GB/s
FLOPS achieved: 0.47 TF/s

Prefill latency: 0.045132799074053764 sec
Decode latency: 7.800168674439192 sec
Time for inference 7: 7.85 sec total, 32.63 tokens/sec
Decode latency: 7.80 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 91.22 GB/s
FLOPS achieved: 0.46 TF/s

Prefill latency: 0.043085113167762756 sec
Decode latency: 7.906316740438342 sec
Time for inference 8: 7.95 sec total, 32.20 tokens/sec
Decode latency: 7.91 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 90.03 GB/s
FLOPS achieved: 0.45 TF/s

Prefill latency: 0.04312868043780327 sec
Decode latency: 7.7628339398652315 sec
Time for inference 9: 7.81 sec total, 32.79 tokens/sec
Decode latency: 7.76 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 91.68 GB/s
FLOPS achieved: 0.46 TF/s

Prefill latency: 0.04345525987446308 sec
Decode latency: 7.9510516207665205 sec
Time for inference 10: 8.00 sec total, 32.02 tokens/sec
Decode latency: 7.95 sec
Prefill latency: 0.04 sec
Bandwidth achieved: 89.52 GB/s
FLOPS achieved: 0.45 TF/s

==========
Batch Size: 1
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 7.7351 sec
Average prefill latency: 0.0442 sec
Average tokens/sec: 32.91
Memory used: 4.34 GB
