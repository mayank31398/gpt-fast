W0920 09:17:27.501000 22727583807296 torch/distributed/run.py:779] 
W0920 09:17:27.501000 22727583807296 torch/distributed/run.py:779] *****************************************
W0920 09:17:27.501000 22727583807296 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0920 09:17:27.501000 22727583807296 torch/distributed/run.py:779] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=1536, bias=False)
        (wo): Linear(in_features=1024, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.02 seconds
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Prefill latency: 1.2583365440368652 sec
Decode latency: 5.101087506860495 sec
Compilation time: 6.36 seconds
Compilation time: 6.41 seconds
Prefill latency: 0.02803313173353672 sec
Compilation time: 6.54 seconds
Compilation time: 6.96 seconds
Decode latency: 4.816335016861558 sec
Prefill latency: 0.027736734598875046 sec
Decode latency: 4.813138768076897 sec
Prefill latency: 0.027830446138978004 sec
Decode latency: 4.793725894764066 sec
Prefill latency: 0.027870113030076027 sec
Decode latency: 5.007533906027675 sec
Prefill latency: 0.0280131995677948 sec
Decode latency: 4.807951612398028 sec
Time for inference 1: 4.84 sec total, 52.93 tokens/sec
Decode latency: 4.81 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 240.34 GB/s
FLOPS achieved: 1.20 TF/s

Prefill latency: 0.027995772659778595 sec
Decode latency: 4.805777471512556 sec
Time for inference 2: 4.83 sec total, 52.95 tokens/sec
Decode latency: 4.81 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 240.44 GB/s
FLOPS achieved: 1.20 TF/s

Prefill latency: 0.02793065272271633 sec
Decode latency: 4.81656314060092 sec
Time for inference 3: 4.85 sec total, 52.83 tokens/sec
Decode latency: 4.82 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 239.92 GB/s
FLOPS achieved: 1.20 TF/s

Prefill latency: 0.027833495289087296 sec
Decode latency: 4.80494330637157 sec
Time for inference 4: 4.83 sec total, 52.96 tokens/sec
Decode latency: 4.80 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 240.49 GB/s
FLOPS achieved: 1.20 TF/s

Prefill latency: 0.027762169018387794 sec
Decode latency: 4.764818653464317 sec
Time for inference 5: 4.79 sec total, 53.41 tokens/sec
Decode latency: 4.76 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 242.51 GB/s
FLOPS achieved: 1.21 TF/s

Prefill latency: 0.02760898694396019 sec
Decode latency: 4.7624127920717 sec
Time for inference 6: 4.79 sec total, 53.43 tokens/sec
Decode latency: 4.76 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 242.64 GB/s
FLOPS achieved: 1.21 TF/s

Prefill latency: 0.027995454147458076 sec
Decode latency: 4.813526194542646 sec
Time for inference 7: 4.84 sec total, 52.87 tokens/sec
Decode latency: 4.81 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 240.06 GB/s
FLOPS achieved: 1.20 TF/s

Prefill latency: 0.027611128985881805 sec
Decode latency: 4.7812842559069395 sec
Time for inference 8: 4.81 sec total, 53.23 tokens/sec
Decode latency: 4.78 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 241.69 GB/s
FLOPS achieved: 1.21 TF/s

Prefill latency: 0.02750205248594284 sec
Decode latency: 4.764254538342357 sec
Time for inference 9: 4.79 sec total, 53.42 tokens/sec
Decode latency: 4.76 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 242.56 GB/s
FLOPS achieved: 1.21 TF/s

Prefill latency: 0.02757149562239647 sec
Decode latency: 4.7510887533426285 sec
Time for inference 10: 4.78 sec total, 53.56 tokens/sec
Decode latency: 4.75 sec
Prefill latency: 0.03 sec
Bandwidth achieved: 243.22 GB/s
FLOPS achieved: 1.22 TF/s

==========
Batch Size: 1
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 4.7873 sec
Average prefill latency: 0.0278 sec
Average tokens/sec: 53.16
Memory used: 6.11 GB
