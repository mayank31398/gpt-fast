W0920 09:35:14.839000 22491124090688 torch/distributed/run.py:779] 
W0920 09:35:14.839000 22491124090688 torch/distributed/run.py:779] *****************************************
W0920 09:35:14.839000 22491124090688 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0920 09:35:14.839000 22491124090688 torch/distributed/run.py:779] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=768, bias=False)
        (wo): Linear(in_features=512, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=3584, bias=False)
        (w2): Linear(in_features=1792, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 1.11 seconds
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Prefill latency: 1.3911986332386732 sec
Compilation time: 6.36 seconds
Compilation time: 6.25 seconds
Compilation time: 6.62 seconds
Decode latency: 5.293992539867759 sec
Compilation time: 6.69 seconds
Compilation time: 6.74 seconds
Prefill latency: 0.10187168419361115 sec
Compilation time: 6.90 seconds
Compilation time: 6.95 seconds
Compilation time: 7.03 seconds
Decode latency: 5.23477510176599 sec
Prefill latency: 0.10325437597930431 sec
Decode latency: 5.210591604933143 sec
Prefill latency: 0.10328355617821217 sec
Decode latency: 5.287902692332864 sec
Prefill latency: 0.10250204429030418 sec
Decode latency: 5.2351608369499445 sec
Prefill latency: 0.10188976861536503 sec
Decode latency: 5.2142943646758795 sec
Time for inference 1: 5.32 sec total, 770.35 tokens/sec
Decode latency: 5.21 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 2153.93 GB/s
FLOPS achieved: 10.77 TF/s

Prefill latency: 0.10180431231856346 sec
Decode latency: 5.2495080549269915 sec
Time for inference 2: 5.35 sec total, 765.30 tokens/sec
Decode latency: 5.25 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 2139.80 GB/s
FLOPS achieved: 10.70 TF/s

Prefill latency: 0.10204058326780796 sec
Decode latency: 5.248473608866334 sec
Time for inference 3: 5.35 sec total, 765.42 tokens/sec
Decode latency: 5.25 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 2140.15 GB/s
FLOPS achieved: 10.70 TF/s

Prefill latency: 0.10256097465753555 sec
Decode latency: 5.25581743940711 sec
Time for inference 4: 5.36 sec total, 764.30 tokens/sec
Decode latency: 5.26 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 2137.00 GB/s
FLOPS achieved: 10.69 TF/s

Prefill latency: 0.10305867530405521 sec
Decode latency: 5.249504836276174 sec
Time for inference 5: 5.35 sec total, 765.11 tokens/sec
Decode latency: 5.25 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 2139.26 GB/s
FLOPS achieved: 10.70 TF/s

Prefill latency: 0.1033154521137476 sec
Decode latency: 5.204568715766072 sec
Time for inference 6: 5.31 sec total, 771.56 tokens/sec
Decode latency: 5.20 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 2157.32 GB/s
FLOPS achieved: 10.79 TF/s

Prefill latency: 0.10295928455889225 sec
Decode latency: 5.209747392684221 sec
Time for inference 7: 5.31 sec total, 770.86 tokens/sec
Decode latency: 5.21 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 2155.34 GB/s
FLOPS achieved: 10.78 TF/s

Prefill latency: 0.10303674638271332 sec
Decode latency: 5.203695494681597 sec
Time for inference 8: 5.31 sec total, 771.72 tokens/sec
Decode latency: 5.20 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 2157.77 GB/s
FLOPS achieved: 10.79 TF/s

Prefill latency: 0.10206956416368484 sec
Decode latency: 5.268754478543997 sec
Time for inference 9: 5.37 sec total, 762.48 tokens/sec
Decode latency: 5.27 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 2131.93 GB/s
FLOPS achieved: 10.66 TF/s

Prefill latency: 0.10207734443247318 sec
Decode latency: 5.247138358652592 sec
Time for inference 10: 5.35 sec total, 765.59 tokens/sec
Decode latency: 5.25 sec
Prefill latency: 0.10 sec
Bandwidth achieved: 2140.62 GB/s
FLOPS achieved: 10.70 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 5.2352 sec
Average prefill latency: 0.1025 sec
Average tokens/sec: 767.27
Memory used: 9.37 GB
