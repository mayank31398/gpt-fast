W0920 09:40:38.558000 22412841396032 torch/distributed/run.py:779] 
W0920 09:40:38.558000 22412841396032 torch/distributed/run.py:779] *****************************************
W0920 09:40:38.558000 22412841396032 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0920 09:40:38.558000 22412841396032 torch/distributed/run.py:779] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=1536, bias=False)
        (wo): Linear(in_features=1024, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 0.99 seconds
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Prefill latency: 1.9945042710751295 sec
Compilation time: 7.29 seconds
Compilation time: 7.49 seconds
Compilation time: 7.51 seconds
Decode latency: 5.585079925134778 sec
Compilation time: 7.58 seconds
Prefill latency: 0.6387241613119841 sec
Decode latency: 5.410296883434057 sec
Prefill latency: 0.6405447423458099 sec
Decode latency: 5.3741137981414795 sec
Prefill latency: 0.6415485441684723 sec
Decode latency: 5.374587701633573 sec
Prefill latency: 0.6407076399773359 sec
Decode latency: 5.5063123647123575 sec
Prefill latency: 0.6415782440453768 sec
Decode latency: 5.402463775128126 sec
Time for inference 1: 6.05 sec total, 2710.31 tokens/sec
Decode latency: 5.40 sec
Prefill latency: 0.64 sec
Bandwidth achieved: 12307.13 GB/s
FLOPS achieved: 61.54 TF/s

Prefill latency: 0.6409768890589476 sec
Decode latency: 5.400959184393287 sec
Time for inference 2: 6.04 sec total, 2711.25 tokens/sec
Decode latency: 5.40 sec
Prefill latency: 0.64 sec
Bandwidth achieved: 12311.44 GB/s
FLOPS achieved: 61.56 TF/s

Prefill latency: 0.6407472770661116 sec
Decode latency: 5.402350287884474 sec
Time for inference 3: 6.04 sec total, 2710.80 tokens/sec
Decode latency: 5.40 sec
Prefill latency: 0.64 sec
Bandwidth achieved: 12309.36 GB/s
FLOPS achieved: 61.55 TF/s

Prefill latency: 0.6406198367476463 sec
Decode latency: 5.398773409426212 sec
Time for inference 4: 6.04 sec total, 2712.44 tokens/sec
Decode latency: 5.40 sec
Prefill latency: 0.64 sec
Bandwidth achieved: 12316.83 GB/s
FLOPS achieved: 61.58 TF/s

Prefill latency: 0.6402406711131334 sec
Decode latency: 5.369484432041645 sec
Time for inference 5: 6.01 sec total, 2725.84 tokens/sec
Decode latency: 5.37 sec
Prefill latency: 0.64 sec
Bandwidth achieved: 12377.68 GB/s
FLOPS achieved: 61.89 TF/s

Prefill latency: 0.6399041060358286 sec
Decode latency: 5.41140497662127 sec
Time for inference 6: 6.05 sec total, 2707.10 tokens/sec
Decode latency: 5.41 sec
Prefill latency: 0.64 sec
Bandwidth achieved: 12292.56 GB/s
FLOPS achieved: 61.46 TF/s

Prefill latency: 0.6418398767709732 sec
Decode latency: 5.39814162068069 sec
Time for inference 7: 6.04 sec total, 2712.16 tokens/sec
Decode latency: 5.40 sec
Prefill latency: 0.64 sec
Bandwidth achieved: 12315.54 GB/s
FLOPS achieved: 61.58 TF/s

Prefill latency: 0.6396442875266075 sec
Decode latency: 5.425999300554395 sec
Time for inference 8: 6.07 sec total, 2700.71 tokens/sec
Decode latency: 5.43 sec
Prefill latency: 0.64 sec
Bandwidth achieved: 12263.56 GB/s
FLOPS achieved: 61.32 TF/s

Prefill latency: 0.6405999306589365 sec
Decode latency: 5.380778258666396 sec
Time for inference 9: 6.02 sec total, 2720.53 tokens/sec
Decode latency: 5.38 sec
Prefill latency: 0.64 sec
Bandwidth achieved: 12353.55 GB/s
FLOPS achieved: 61.77 TF/s

Prefill latency: 0.6379062831401825 sec
Decode latency: 5.488222420215607 sec
Time for inference 10: 6.13 sec total, 2673.80 tokens/sec
Decode latency: 5.49 sec
Prefill latency: 0.64 sec
Bandwidth achieved: 12141.37 GB/s
FLOPS achieved: 60.71 TF/s

==========
Batch Size: 64
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 5.4079 sec
Average prefill latency: 0.6404 sec
Average tokens/sec: 2708.49
Memory used: 30.44 GB
