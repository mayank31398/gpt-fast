W0920 09:23:43.506000 22979228698432 torch/distributed/run.py:779] 
W0920 09:23:43.506000 22979228698432 torch/distributed/run.py:779] *****************************************
W0920 09:23:43.506000 22979228698432 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0920 09:23:43.506000 22979228698432 torch/distributed/run.py:779] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=1536, bias=False)
        (wo): Linear(in_features=1024, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 0.99 seconds
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Prefill latency: 1.3501964546740055 sec
Compilation time: 6.94 seconds
Compilation time: 7.09 seconds
Decode latency: 5.7686194851994514 sec
Compilation time: 7.12 seconds
Prefill latency: 0.046915994957089424 sec
Compilation time: 7.32 seconds
Decode latency: 5.253004847094417 sec
Prefill latency: 0.046495383605360985 sec
Decode latency: 5.322339916601777 sec
Prefill latency: 0.046408675611019135 sec
Decode latency: 5.30618816241622 sec
Prefill latency: 0.046575937420129776 sec
Decode latency: 5.274054193869233 sec
Prefill latency: 0.04672519490122795 sec
Decode latency: 5.282169818878174 sec
Time for inference 1: 5.33 sec total, 192.12 tokens/sec
Decode latency: 5.28 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 872.41 GB/s
FLOPS achieved: 4.36 TF/s

Prefill latency: 0.0466422438621521 sec
Decode latency: 5.234143411740661 sec
Time for inference 2: 5.28 sec total, 193.88 tokens/sec
Decode latency: 5.23 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 880.37 GB/s
FLOPS achieved: 4.40 TF/s

Prefill latency: 0.04643981531262398 sec
Decode latency: 5.300266185775399 sec
Time for inference 3: 5.35 sec total, 191.49 tokens/sec
Decode latency: 5.30 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 869.53 GB/s
FLOPS achieved: 4.35 TF/s

Prefill latency: 0.04659357853233814 sec
Decode latency: 5.288386968895793 sec
Time for inference 4: 5.34 sec total, 191.91 tokens/sec
Decode latency: 5.29 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 871.43 GB/s
FLOPS achieved: 4.36 TF/s

Prefill latency: 0.046751463785767555 sec
Decode latency: 5.2939460556954145 sec
Time for inference 5: 5.34 sec total, 191.70 tokens/sec
Decode latency: 5.29 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 870.51 GB/s
FLOPS achieved: 4.35 TF/s

Prefill latency: 0.04652940481901169 sec
Decode latency: 5.285292316228151 sec
Time for inference 6: 5.33 sec total, 192.02 tokens/sec
Decode latency: 5.29 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 871.92 GB/s
FLOPS achieved: 4.36 TF/s

Prefill latency: 0.046639349311590195 sec
Decode latency: 5.291231378912926 sec
Time for inference 7: 5.34 sec total, 191.81 tokens/sec
Decode latency: 5.29 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 870.96 GB/s
FLOPS achieved: 4.35 TF/s

Prefill latency: 0.04663070850074291 sec
Decode latency: 5.282664209604263 sec
Time for inference 8: 5.33 sec total, 192.11 tokens/sec
Decode latency: 5.28 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 872.36 GB/s
FLOPS achieved: 4.36 TF/s

Prefill latency: 0.046583693474531174 sec
Decode latency: 5.253618199378252 sec
Time for inference 9: 5.30 sec total, 193.17 tokens/sec
Decode latency: 5.25 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 877.15 GB/s
FLOPS achieved: 4.39 TF/s

Prefill latency: 0.046452851966023445 sec
Decode latency: 5.2698036059737206 sec
Time for inference 10: 5.32 sec total, 192.58 tokens/sec
Decode latency: 5.27 sec
Prefill latency: 0.05 sec
Bandwidth achieved: 874.47 GB/s
FLOPS achieved: 4.37 TF/s

==========
Batch Size: 4
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 5.2782 sec
Average prefill latency: 0.0466 sec
Average tokens/sec: 192.28
Memory used: 7.47 GB
