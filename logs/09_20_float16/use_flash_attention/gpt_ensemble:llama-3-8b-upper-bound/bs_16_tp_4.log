W0920 09:33:39.926000 22644264998720 torch/distributed/run.py:779] 
W0920 09:33:39.926000 22644264998720 torch/distributed/run.py:779] *****************************************
W0920 09:33:39.926000 22644264998720 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0920 09:33:39.926000 22644264998720 torch/distributed/run.py:779] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTEnsemble(
  (tok_embeddings): Embedding(128256, 4096)
  (layers): ModuleList(
    (0-31): 32 x EnsembleTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=4096, out_features=1536, bias=False)
        (wo): Linear(in_features=1024, out_features=4096, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=4096, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=4096, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=4096, out_features=128256, bias=False)
)
Time to load model: 0.96 seconds
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Prefill latency: 1.3836751356720924 sec
Decode latency: 4.95887803286314 sec
Compilation time: 6.34 seconds
Prefill latency: 0.1704757772386074 sec
Compilation time: 6.77 seconds
Compilation time: 6.85 seconds
Compilation time: 6.93 seconds
Decode latency: 4.8988240119069815 sec
Prefill latency: 0.17039530538022518 sec
Decode latency: 4.899274580180645 sec
Prefill latency: 0.1693081371486187 sec
Decode latency: 4.913900164887309 sec
Prefill latency: 0.17058183252811432 sec
Decode latency: 4.920819101855159 sec
Prefill latency: 0.1694954726845026 sec
Decode latency: 4.903481960296631 sec
Time for inference 1: 5.07 sec total, 807.28 tokens/sec
Decode latency: 4.90 sec
Prefill latency: 0.17 sec
Bandwidth achieved: 3665.74 GB/s
FLOPS achieved: 18.33 TF/s

Prefill latency: 0.17129109427332878 sec
Decode latency: 4.913489380851388 sec
Time for inference 2: 5.09 sec total, 805.40 tokens/sec
Decode latency: 4.91 sec
Prefill latency: 0.17 sec
Bandwidth achieved: 3657.22 GB/s
FLOPS achieved: 18.29 TF/s

Prefill latency: 0.17091983370482922 sec
Decode latency: 4.900809654965997 sec
Time for inference 3: 5.07 sec total, 807.48 tokens/sec
Decode latency: 4.90 sec
Prefill latency: 0.17 sec
Bandwidth achieved: 3666.64 GB/s
FLOPS achieved: 18.33 TF/s

Prefill latency: 0.17018406465649605 sec
Decode latency: 4.9089506305754185 sec
Time for inference 4: 5.08 sec total, 806.30 tokens/sec
Decode latency: 4.91 sec
Prefill latency: 0.17 sec
Bandwidth achieved: 3661.30 GB/s
FLOPS achieved: 18.31 TF/s

Prefill latency: 0.17072341218590736 sec
Decode latency: 4.911341696977615 sec
Time for inference 5: 5.08 sec total, 805.83 tokens/sec
Decode latency: 4.91 sec
Prefill latency: 0.17 sec
Bandwidth achieved: 3659.18 GB/s
FLOPS achieved: 18.30 TF/s

Prefill latency: 0.17020270600914955 sec
Decode latency: 4.884594295173883 sec
Time for inference 6: 5.06 sec total, 810.19 tokens/sec
Decode latency: 4.88 sec
Prefill latency: 0.17 sec
Bandwidth achieved: 3678.96 GB/s
FLOPS achieved: 18.39 TF/s

Prefill latency: 0.17062555626034737 sec
Decode latency: 4.9060539696365595 sec
Time for inference 7: 5.08 sec total, 806.69 tokens/sec
Decode latency: 4.91 sec
Prefill latency: 0.17 sec
Bandwidth achieved: 3663.09 GB/s
FLOPS achieved: 18.32 TF/s

Prefill latency: 0.1695833057165146 sec
Decode latency: 4.907372733578086 sec
Time for inference 8: 5.08 sec total, 806.64 tokens/sec
Decode latency: 4.91 sec
Prefill latency: 0.17 sec
Bandwidth achieved: 3662.84 GB/s
FLOPS achieved: 18.31 TF/s

Prefill latency: 0.17049377970397472 sec
Decode latency: 4.905994215980172 sec
Time for inference 9: 5.08 sec total, 806.71 tokens/sec
Decode latency: 4.91 sec
Prefill latency: 0.17 sec
Bandwidth achieved: 3663.15 GB/s
FLOPS achieved: 18.32 TF/s

Prefill latency: 0.17064198851585388 sec
Decode latency: 4.887214541435242 sec
Time for inference 10: 5.06 sec total, 809.69 tokens/sec
Decode latency: 4.89 sec
Prefill latency: 0.17 sec
Bandwidth achieved: 3676.68 GB/s
FLOPS achieved: 18.38 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 4.9029 sec
Average prefill latency: 0.1704 sec
Average tokens/sec: 807.22
Memory used: 11.85 GB
