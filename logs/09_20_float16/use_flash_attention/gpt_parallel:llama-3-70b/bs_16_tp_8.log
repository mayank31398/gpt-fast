W0920 10:39:35.728000 23090490767168 torch/distributed/run.py:779] 
W0920 10:39:35.728000 23090490767168 torch/distributed/run.py:779] *****************************************
W0920 10:39:35.728000 23090490767168 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0920 10:39:35.728000 23090490767168 torch/distributed/run.py:779] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTParallel(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x ParallelTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=1280, bias=False)
        (wo): Linear(in_features=1024, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 1.16 seconds
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Prefill latency: 12.579165460541844 sec
Decode latency: 15.184965504333377 sec
Compilation time: 27.78 seconds
Compilation time: 27.74 seconds
Compilation time: 27.81 seconds
Compilation time: 27.76 seconds
Compilation time: 27.74 seconds
Compilation time: 27.81 seconds
Compilation time: 27.77 seconds
Compilation time: 27.76 seconds
Prefill latency: 0.7453891988843679 sec
Decode latency: 15.300066972151399 sec
Prefill latency: 0.7419115733355284 sec
Decode latency: 14.923802392557263 sec
Prefill latency: 0.7442951109260321 sec
Decode latency: 14.615025669336319 sec
Prefill latency: 0.7422643918544054 sec
Decode latency: 15.363493157550693 sec
Prefill latency: 0.7443754393607378 sec
Decode latency: 14.7009666133672 sec
Time for inference 1: 15.45 sec total, 265.18 tokens/sec
Decode latency: 14.70 sec
Prefill latency: 0.74 sec
Bandwidth achieved: 5095.86 GB/s
FLOPS achieved: 25.48 TF/s

Prefill latency: 0.7427324187010527 sec
Decode latency: 15.083654159680009 sec
Time for inference 2: 15.83 sec total, 258.79 tokens/sec
Decode latency: 15.08 sec
Prefill latency: 0.74 sec
Bandwidth achieved: 4973.18 GB/s
FLOPS achieved: 24.87 TF/s

Prefill latency: 0.7421542927622795 sec
Decode latency: 15.139086175709963 sec
Time for inference 3: 15.88 sec total, 257.90 tokens/sec
Decode latency: 15.14 sec
Prefill latency: 0.74 sec
Bandwidth achieved: 4955.99 GB/s
FLOPS achieved: 24.78 TF/s

Prefill latency: 0.7442348394542933 sec
Decode latency: 14.69188380241394 sec
Time for inference 4: 15.44 sec total, 265.34 tokens/sec
Decode latency: 14.69 sec
Prefill latency: 0.74 sec
Bandwidth achieved: 5098.88 GB/s
FLOPS achieved: 25.49 TF/s

Prefill latency: 0.7454212736338377 sec
Decode latency: 14.187604365870357 sec
Time for inference 5: 14.93 sec total, 274.27 tokens/sec
Decode latency: 14.19 sec
Prefill latency: 0.75 sec
Bandwidth achieved: 5270.65 GB/s
FLOPS achieved: 26.35 TF/s

Prefill latency: 0.7445703558623791 sec
Decode latency: 14.570778120309114 sec
Time for inference 6: 15.32 sec total, 267.43 tokens/sec
Decode latency: 14.57 sec
Prefill latency: 0.74 sec
Bandwidth achieved: 5139.09 GB/s
FLOPS achieved: 25.70 TF/s

Prefill latency: 0.7450180612504482 sec
Decode latency: 14.31017815694213 sec
Time for inference 7: 15.06 sec total, 272.05 tokens/sec
Decode latency: 14.31 sec
Prefill latency: 0.75 sec
Bandwidth achieved: 5227.89 GB/s
FLOPS achieved: 26.14 TF/s

Prefill latency: 0.7437544260174036 sec
Decode latency: 14.62669008038938 sec
Time for inference 8: 15.37 sec total, 266.47 tokens/sec
Decode latency: 14.63 sec
Prefill latency: 0.74 sec
Bandwidth achieved: 5120.65 GB/s
FLOPS achieved: 25.60 TF/s

Prefill latency: 0.7447332553565502 sec
Decode latency: 14.740259950980544 sec
Time for inference 9: 15.49 sec total, 264.50 tokens/sec
Decode latency: 14.74 sec
Prefill latency: 0.74 sec
Bandwidth achieved: 5082.79 GB/s
FLOPS achieved: 25.41 TF/s

Prefill latency: 0.7443192172795534 sec
Decode latency: 14.528778424486518 sec
Time for inference 10: 15.27 sec total, 268.17 tokens/sec
Decode latency: 14.53 sec
Prefill latency: 0.74 sec
Bandwidth achieved: 5153.31 GB/s
FLOPS achieved: 25.77 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 14.6580 sec
Average prefill latency: 0.7441 sec
Average tokens/sec: 266.01
Memory used: 28.28 GB
