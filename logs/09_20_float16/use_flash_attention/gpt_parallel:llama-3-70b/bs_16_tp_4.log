W0920 10:35:16.931000 22867118171968 torch/distributed/run.py:779] 
W0920 10:35:16.931000 22867118171968 torch/distributed/run.py:779] *****************************************
W0920 10:35:16.931000 22867118171968 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0920 10:35:16.931000 22867118171968 torch/distributed/run.py:779] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTParallel(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x ParallelTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=2560, bias=False)
        (wo): Linear(in_features=2048, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=14336, bias=False)
        (w2): Linear(in_features=7168, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 0.95 seconds
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Prefill latency: 7.902360208332539 sec
Decode latency: 14.29287321306765 sec
Compilation time: 22.12 seconds
Compilation time: 22.16 seconds
Compilation time: 22.20 seconds
Compilation time: 22.16 seconds
Prefill latency: 1.3084559924900532 sec
Decode latency: 14.844155717641115 sec
Prefill latency: 1.3064615298062563 sec
Decode latency: 14.69440895318985 sec
Prefill latency: 1.3091399148106575 sec
Decode latency: 14.30050352588296 sec
Prefill latency: 1.3058661352843046 sec
Decode latency: 14.990365667268634 sec
Prefill latency: 1.30643929541111 sec
Decode latency: 14.684952130541205 sec
Time for inference 1: 15.99 sec total, 256.12 tokens/sec
Decode latency: 14.68 sec
Prefill latency: 1.31 sec
Bandwidth achieved: 9304.82 GB/s
FLOPS achieved: 46.52 TF/s

Prefill latency: 1.3075458221137524 sec
Decode latency: 14.779335286468267 sec
Time for inference 2: 16.09 sec total, 254.60 tokens/sec
Decode latency: 14.78 sec
Prefill latency: 1.31 sec
Bandwidth achieved: 9249.60 GB/s
FLOPS achieved: 46.25 TF/s

Prefill latency: 1.3072787411510944 sec
Decode latency: 14.822996778413653 sec
Time for inference 3: 16.13 sec total, 253.92 tokens/sec
Decode latency: 14.82 sec
Prefill latency: 1.31 sec
Bandwidth achieved: 9224.70 GB/s
FLOPS achieved: 46.12 TF/s

Prefill latency: 1.3082363549619913 sec
Decode latency: 14.82561332732439 sec
Time for inference 4: 16.13 sec total, 253.86 tokens/sec
Decode latency: 14.83 sec
Prefill latency: 1.31 sec
Bandwidth achieved: 9222.68 GB/s
FLOPS achieved: 46.11 TF/s

Prefill latency: 1.3030615132302046 sec
Decode latency: 14.868244372308254 sec
Time for inference 5: 16.17 sec total, 253.27 tokens/sec
Decode latency: 14.87 sec
Prefill latency: 1.30 sec
Bandwidth achieved: 9201.30 GB/s
FLOPS achieved: 46.01 TF/s

Prefill latency: 1.305816888809204 sec
Decode latency: 14.767522843554616 sec
Time for inference 6: 16.07 sec total, 254.82 tokens/sec
Decode latency: 14.77 sec
Prefill latency: 1.31 sec
Bandwidth achieved: 9257.37 GB/s
FLOPS achieved: 46.29 TF/s

Prefill latency: 1.313370980322361 sec
Decode latency: 14.836466377601027 sec
Time for inference 7: 16.15 sec total, 253.61 tokens/sec
Decode latency: 14.84 sec
Prefill latency: 1.31 sec
Bandwidth achieved: 9213.51 GB/s
FLOPS achieved: 46.07 TF/s

Prefill latency: 1.3055999353528023 sec
Decode latency: 14.464917192235589 sec
Time for inference 8: 15.77 sec total, 259.71 tokens/sec
Decode latency: 14.46 sec
Prefill latency: 1.31 sec
Bandwidth achieved: 9435.14 GB/s
FLOPS achieved: 47.18 TF/s

Prefill latency: 1.3088551200926304 sec
Decode latency: 14.45977789349854 sec
Time for inference 9: 15.77 sec total, 259.74 tokens/sec
Decode latency: 14.46 sec
Prefill latency: 1.31 sec
Bandwidth achieved: 9436.25 GB/s
FLOPS achieved: 47.18 TF/s

Prefill latency: 1.302312197163701 sec
Decode latency: 14.562640065327287 sec
Time for inference 10: 15.87 sec total, 258.16 tokens/sec
Decode latency: 14.56 sec
Prefill latency: 1.30 sec
Bandwidth achieved: 9379.00 GB/s
FLOPS achieved: 46.89 TF/s

==========
Batch Size: 16
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 14.7072 sec
Average prefill latency: 1.3069 sec
Average tokens/sec: 255.78
Memory used: 47.01 GB
