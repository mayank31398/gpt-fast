W0920 08:21:50.630000 22793836750656 torch/distributed/run.py:779] 
W0920 08:21:50.630000 22793836750656 torch/distributed/run.py:779] *****************************************
W0920 08:21:50.630000 22793836750656 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0920 08:21:50.630000 22793836750656 torch/distributed/run.py:779] *****************************************
flash_kv_decode is set to True
Using device=cuda
Loading model ...
GPTLadder(
  (tok_embeddings): Embedding(128256, 8192)
  (layers): ModuleList(
    (0-79): 80 x LadderTransformerBlock(
      semi_compiled = False
      (attention): Attention(
        (wqkv): Linear(in_features=8192, out_features=1280, bias=False)
        (wo): Linear(in_features=1024, out_features=8192, bias=False)
      )
      (feed_forward): FeedForward(
        (w1): Linear(in_features=8192, out_features=7168, bias=False)
        (w2): Linear(in_features=3584, out_features=8192, bias=False)
      )
      (ffn_norm): RMSNorm()
      (attention_norm): RMSNorm()
    )
  )
  (norm): RMSNorm()
  (output): Linear(in_features=8192, out_features=128256, bias=False)
)
Time to load model: 0.40 seconds
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/home/charlie/anaconda3/envs/gpt-fast/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Prefill latency: 10.200926342979074 sec
Decode latency: 17.97400277107954 sec
Compilation time: 28.20 seconds
Compilation time: 28.26 seconds
Compilation time: 28.22 seconds
Compilation time: 28.29 seconds
Compilation time: 28.27 seconds
Compilation time: 28.26 seconds
Compilation time: 28.18 seconds
Compilation time: 28.14 seconds
Prefill latency: 0.20377887599170208 sec
Decode latency: 17.64682513847947 sec
Prefill latency: 0.2039194516837597 sec
Decode latency: 16.958843879401684 sec
Prefill latency: 0.1972521636635065 sec
Decode latency: 16.85450861416757 sec
Prefill latency: 0.19667614623904228 sec
Decode latency: 17.194044722244143 sec
Prefill latency: 0.19736774265766144 sec
Decode latency: 17.29137785732746 sec
Time for inference 1: 17.49 sec total, 58.55 tokens/sec
Decode latency: 17.29 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 1125.11 GB/s
FLOPS achieved: 5.63 TF/s

Prefill latency: 0.19696083664894104 sec
Decode latency: 16.5929315648973 sec
Time for inference 2: 16.79 sec total, 60.99 tokens/sec
Decode latency: 16.59 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 1171.93 GB/s
FLOPS achieved: 5.86 TF/s

Prefill latency: 0.20593025907874107 sec
Decode latency: 17.057149939239025 sec
Time for inference 3: 17.26 sec total, 59.31 tokens/sec
Decode latency: 17.06 sec
Prefill latency: 0.21 sec
Bandwidth achieved: 1139.81 GB/s
FLOPS achieved: 5.70 TF/s

Prefill latency: 0.20030886307358742 sec
Decode latency: 17.470679096877575 sec
Time for inference 4: 17.67 sec total, 57.94 tokens/sec
Decode latency: 17.47 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 1113.50 GB/s
FLOPS achieved: 5.57 TF/s

Prefill latency: 0.19666065461933613 sec
Decode latency: 16.954526172950864 sec
Time for inference 5: 17.15 sec total, 59.70 tokens/sec
Decode latency: 16.95 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 1147.25 GB/s
FLOPS achieved: 5.74 TF/s

Prefill latency: 0.19703818671405315 sec
Decode latency: 17.086431482806802 sec
Time for inference 6: 17.28 sec total, 59.24 tokens/sec
Decode latency: 17.09 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 1138.47 GB/s
FLOPS achieved: 5.69 TF/s

Prefill latency: 0.1988212987780571 sec
Decode latency: 17.039171490818262 sec
Time for inference 7: 17.24 sec total, 59.40 tokens/sec
Decode latency: 17.04 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 1141.48 GB/s
FLOPS achieved: 5.71 TF/s

Prefill latency: 0.19709996320307255 sec
Decode latency: 17.173615023493767 sec
Time for inference 8: 17.37 sec total, 58.95 tokens/sec
Decode latency: 17.17 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 1132.76 GB/s
FLOPS achieved: 5.66 TF/s

Prefill latency: 0.1964226271957159 sec
Decode latency: 17.120292933657765 sec
Time for inference 9: 17.32 sec total, 59.13 tokens/sec
Decode latency: 17.12 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 1136.29 GB/s
FLOPS achieved: 5.68 TF/s

Prefill latency: 0.19625483080744743 sec
Decode latency: 17.788532664999366 sec
Time for inference 10: 17.99 sec total, 56.93 tokens/sec
Decode latency: 17.79 sec
Prefill latency: 0.20 sec
Bandwidth achieved: 1094.08 GB/s
FLOPS achieved: 5.47 TF/s

==========
Batch Size: 4
Prompt Length: 1024
Generated tokens: 256
Average decode latency: 17.1575 sec
Average prefill latency: 0.1983 sec
Average tokens/sec: 59.01
Memory used: 23.64 GB
